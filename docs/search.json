[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "Figure 1: Telling stories with data\n\n\nThis book will help you tell stories with data. It establishes a foundation on which you can build and share knowledge about an aspect of the world of interest to you based on data that you observe. Telling stories in small groups around a fire played a critical role in the development of humans and society (Wiessner 2014). Today our stories, based on data, can influence millions.\nIn this book we will explore, prod, push, manipulate, knead, and ultimately, try to understand the implications of data. A variety of features drive the choices in this book.\nThe motto of the university from which I took my PhD is naturam primum cognoscere rerum or roughly ‘learn the first nature of things’. But the original quote continues temporis aeterni quoniam, or roughly ‘for eternal time’. We will do both of these things. I focus on tools, approaches, and workflows that enable you to establish lasting and reproducible knowledge.\nWhen I talk of data in this book, it will typically be related to humans. Humans will be at the center of most of our stories, and we will tell social, cultural, and economic stories. In particular, throughout this book I will draw attention to inequity both in social phenomena and in data. Most data analysis reflects the world as it is. Many of the least well-off face a double burden in this regard: not only are they disadvantaged, but the extent is more difficult to measure. Respecting those whose data are in our dataset is a primary concern, and so is thinking of those who are systematically not in our dataset.\nWhile data are often specific to various contexts and disciplines, the approaches used to understand them tend to be similar. Data are also increasingly global with resources and opportunities available from a variety of sources. Hence, I draw on examples from many disciplines and geographies.\nTo become knowledge, our findings must be communicated to, understood, and trusted by other people. Scientific and economic progress can only be made by building on the work of others. And this is only possible if we can understand what they did. Similarly, if we are to create knowledge about the world, then we must enable others to understand precisely what we did, what we found, and how we went about our tasks. As such, in this book I will be particularly prescriptive about communication and reproducibility.\nImproving the quality of quantitative work is an enormous challenge, yet it is the challenge of our time. Data are all around us, but there is little enduring knowledge being created. This book hopes to contribute, in some small way, to changing that.\n\n\nThe typical person reading this book has some familiarity with first-year statistics, for instance they have run a regression. But it is not targeted at a particular level, instead providing aspects relevant to almost any quantitative course. I have taught from this book at high school, undergraduate, graduate, and professional, levels. Everyone has unique needs, but hopefully some aspect of this book speaks to you.\nEnthusiasm and interest have taken folks far. If you have those, then do not worry about too much else. Some of the most successful students have been those with no quantitative or coding background.\nThis book covers a lot of ground, but does not go into depth about any particular aspect. As such this book especially complements books such as: Data Science: A First Introduction (Timbers, Campbell, and Lee 2022), R for Data Science (Wickham and Grolemund 2017), An Introduction to Statistical Learning (James et al. 2017), Statistical Rethinking (McElreath 2020), Causal Inference: The Mixtape (Cunningham 2021), The Effect: An Introduction to Research Design and Causality (Huntington-Klein 2021), and Building Software Together (Wilson 2021). If you are interested in those books, then this might be a good one to start with.\n\n\n\nThis book is structured around six parts: I) Foundations, II) Communication, III) Acquisition, IV) Preparation, V) Modelling, and VI) Enrichment.\nPart I – Foundations – begins with Chapter 1, which provides an overview of what I am trying to achieve with this book and why you should read it. Chapter 2 provides some worked examples. The intention of these is that you can experience the full workflow recommended in this book without worrying too much about the specifics of what is happening. That workflow is: plan, simulate, acquire, model, and communicate. It is normal to not follow everything in this chapter, but you should go through it, typing out and executing the code yourself. If you only have time to read one chapter of this book, then I recommend that one. Chapter 3 goes through some essential tasks in R, which is the statistical programming language used in this book. It is more of a reference chapter, and you may find yourself returning to it from time to time. And Chapter 4 introduces some key tools for reproducibility used in the workflow that I advocate. These are things like using the command line, Quarto, R Projects, Git and GitHub, and using R in practice.\nPart II – Communication – considers three types of communication: written, static, and interactive. Chapter 5 details the features that quantitative writing should have and how to go about writing a crisp, technical, paper. Static communication in Chapter 6 introduces features like graphs, tables, and maps. Interactive communication in Chapter 7 covers aspects such as websites, web applications, and maps that can be manipulated.\nPart III – Acquisition – focuses on three aspects: farming data, gathering data, and hunting data. Farming data in Chapter 8 begins with essential concepts from sampling that govern our approach to data. It then focuses on datasets that are explicitly provided for us to use as data, for instance censuses and other government statistics. These are typically clean, pre-packaged datasets, and sometimes complete. Gathering data in Chapter 9 covers things like using Application Programming Interface (APIs), scraping data, getting data from PDFs, and Optical Character Recognition (OCR). The idea is that data are available, but not necessarily designed to be datasets, and that we must go and get them. Finally, hunting data in Chapter 10 covers aspects where more is expected of us. For instance, we may need to conduct an experiment, run an A/B test, or do some surveys.\nPart IV – Preparation – covers how to respectfully transform raw data into something that can be explored and shared. Chapter 11 begins by detailing some principles to follow when approaching the task of cleaning and preparing data, and then goes through specific steps to take and checks to implement. Chapter 12 focuses on methods of storing and retrieving those datasets, including the use of R packages, and then continues onto considerations and steps to take when wanting to disseminate datasets as broadly as possible, while at the same time respecting those whose data they are based on.\nPart V – Modelling – begins with exploratory data analysis in Chapter 13. This is the critical process of coming to understand the nature of a dataset, but not something that typically finds itself into the final product. In Chapter 14 the use of statistical models to explore data is introduced. Chapter 15 is the first of three applications of modelling. It focuses on attempts to make causal claims from observational data and covers approaches such as difference-in-differences, regression discontinuity, and instrumental variables. Chapter 16 is the second of the modelling applications chapters and focuses on multilevel regression with post-stratification where we use a statistical model to adjust a sample for known biases. Chapter 17 is the third and final modelling application and is focused on text-as-data.\nPart VI – Enrichment – introduces various next steps that would improve aspects of the workflow and approaches introduced in previous chapters. Chapter 18 goes through moving away from your own computer and toward using the cloud and then discusses deploying models through the use of packages, web applications, and APIs. Chapter 19 discusses various alternatives to the storage of data including feather and SQL; and also covers some ways to improve the performance of your code. Finally, Chapter 20 offers some concluding remarks, details some open problems, and suggests some next steps.\n\n\n\nYou have to do the work. You should actively go through material and code yourself. As King (2000) says ‘[a]mateurs sit and wait for inspiration, the rest of us just get up and go to work’. Do not passively read this book. My role is best described by Hamming (1996, 2–3):\n\nI am, as it were, only a coach. I cannot run the mile for you; at best I can discuss styles and criticize yours. You know you must run the mile if the athletics course is to be of benefit to you—hence you must think carefully about what you hear and read in this book if it is to be effective in changing you—which must obviously be the purpose…\n\nThis book is structured around a dense 12-week course. It provides enough material for advanced readers to be challenged, while establishing a core that all readers should master. Typically courses cover the material through to Chapter 14, and then pick another couple of chapters that are of particular interest.\nFrom as early as Chapter 2 you will have a workflow—plan, simulate, acquire, model, and communicate—allowing you to tell a convincing story with data. In each subsequent chapter you will add aspects and depth to this workflow that will allow you to speak with increasing sophistication and credibility. As this workflow expands it addresses the skills that are typically sought in industry. For instance, features such as: communication, ethics, reproducibility, research question development, data collection, data cleaning, data protection and dissemination, exploratory data analysis, statistical modelling, and scaling.\nOne of the defining aspects of this book is that ethics and inequity concerns are integrated throughout, rather than being clustered in one, easily ignorable, chapter. These aspects are critical, yet it can be difficult to immediately see their value, hence their tight integration.\nThis book is also designed to enable you to build a portfolio of work that you could show to a potential employer. If you want an industry job, then this is arguably the most important thing that you should be doing. Robinson and Nolis (2020, 55) describe how a portfolio is a collection of projects that show what you can do and is something that can help be successful in a job search.\nIn the novel The Last Samurai (DeWitt 2000, 326), a character says:\n\n[A] scholar should be able to look at any word in a passage and instantly think of another passage where it occurred; … [so a] text was like a pack of icebergs each word a snowy peak with a huge frozen mass of cross-references beneath the surface.\n\nIn an analogous way, this book not only provides you with text and instruction that is self-contained, but also helps develop critical masses of knowledge on which expertise is built. No chapter positions itself as the last word, instead they are written in relation to other work.\nEach chapter has the following features:\n\nA list of required materials that you should go through before you read that chapter. To be clear, you should first read that material and then return to this book. Each chapter also contains recommended materials for those who are particularly interested in the topic and want a starting place for further exploration.\nA summary of the key concepts and skills that are developed in that chapter. Technical chapters additionally contain a list of the main packages and functions that are used in the chapter. The combination of these features acts as a checklist for your learning, and you should return to them after completing the chapter.\nA series of short exercises that you should complete after going through the required materials, but before going through the chapter, to test your knowledge. After completing the chapter, you should go back through the exercises to make sure that you understand each aspect.\nOne or two tutorial questions are included at the end of each chapter to further encourage you to actively engage with the material. You could consider forming small groups to discuss your answers to these questions.\n\nSome chapters additionally feature:\n\nA section called ‘Oh, you think we have good data on that!’ which focuses on a particular setting, such as cause of death, in which it is often assumed that there is unimpeachable and unambiguous data but the reality tends to be quite far from that.\nA section called ‘Shoulders of giants’, which focuses on some of those who created the intellectual foundation on which we build.\n\nFinally, a set of papers is included in Appendix A. If you write these, you will be conducting original research on a topic that is of interest to you. Although open-ended research may be new to you, the extent to which you are able to: develop your own questions, use quantitative methods to explore them, and communicates your findings, is the measure of the success of this book.\n\n\n\nThe software that I use in this book is R (R Core Team 2021). This language was chosen because it is open source, widely used, general enough to cover the entire workflow, yet specific enough to have plenty of well-developed features. I do not assume that you have used R before, and so another reason for selecting R for this book is the community of R users. The community is especially welcoming of new-comers and there is a lot of complementary beginner-friendly material available.\n\nIf you do not have a programming language, then R is a great one to start with. The ability to code is useful well beyond this book. If you have a preferred programming language already, then it wouldn’t hurt to pick up R as well. That said, if you have a good reason to prefer another open-source programming language (for instance you use Python daily at work) then you may wish to stick with that. However, all examples in this book are in R.\nPlease download R and R Studio onto your own computer. You can download R for free here: http://cran.utstat.utoronto.ca/, and you can download R Studio Desktop for free here: https://rstudio.com/products/rstudio/download/#download. Please also create an account on R Studio Cloud: https://rstudio.cloud/. This will allow you to run R in the cloud.\nPackages are in typewriter text, for instance, tidyverse, while functions are also in typewriter text, but include brackets, for instance dplyr::filter().\n\n\n\nMany people generously gave code, data, examples, guidance, opportunities, thoughts, and time, that helped develop this book.\nThank you to David Grubbs and the team at CRC Press for taking a chance on me and providing invaluable support.\nThank you to Michael Chong and Sharla Gelfand for greatly helping to shape some of the approaches I advocate. However, both do much more than that and contribute in an enormous way to the spirit of generosity that characterizes the R community.\nThank you to Kelly Lyons for her support, guidance, mentorship, and friendship. Every day she demonstrates what an academic should be, and more broadly, what we should all aspire to be as a person.\nThank you to Greg Wilson for providing a structure to think about teaching, for being the catalyst for this book, and for helpful comments on drafts. Every day he provides an example of how to contribute to the intellectual community.\nThank you to an anonymous reviewer and Isabella Ghement, who thoroughly went through an early draft of this book and provided detailed feedback that improved this book.\nThank you to Hareem Naveed for helpful feedback and encouragement. Her industry experience was an invaluable resource as I grappled with questions of coverage and focus.\nThank you to Leslie Root, who came up with the idea around ‘Oh, you think we have good data on that!’.\nThank you to Ella Kaye, who suggested, and rightly insisted on, moving to Quarto.\nThank you to Lauren Kennedy through whose generous sharing of code, data, and countless conversations my thoughts about MRP have developed.\nThank you to my PhD supervisory panel John Tang, Martine Mariotti, Tim Hatton, and Zach Ward who gave me the freedom to explore the intellectual space that was of interest to me, the support to follow through on those interests, and the guidance to ensure that it all resulted in something tangible.\nThank you to Elle Côtè for enabling this book to be written.\nThis book has greatly benefited from the notes and teaching materials of others that are freely available online, especially: Chris Bail, Scott Cunningham, Andrew Heiss, Lisa Lendway, Grant McDermott, Nathan Matias, David Mimno, and Ed Rubin. Thank you to these folks. The changed norm of academics making their materials freely available online is a great one and one that I hope the free online version of this book helps contribute to.\nThank you to Samantha-Jo Caetano, who helped develop some of the assessment items. And also, to Lisa Romkey and Alan Chong who allowed me to adapt some aspects of their rubric.\nThank you to those who contributed substantially to the development of this book, including: A Mahfouz, Faria Khandaker, Keli Chiu, Paul Hodgetts, and Thomas William Rosenthal. I discussed most aspects of this book with them, and while they made specific contributions, they also changed and sharpened the way that I thought about almost everything covered here. Paul additionally made the art for this book.\nThank you to those who identified specific improvements, including: Aaron Miller, Amy Farrow, Arsh Lakhanpal, Cesar Villarreal Guzman, Flavia López, Hong Shi, Laura Cline, Lorena Almaraz De La Garza, Mounica Thanam, Reem Alasadi, Wijdan Tariq, Yang Wu, and Yewon Han.\nAs at Christmas 2021 this book was a disparate collection of notes. Thank you to Mum and Dad, who dropped everything and came over from the other side of the world for two months to give me the opportunity to re-write it all and put together a cohesive book.\nFinally, thank you to Monica Alexander. Without you I would not have written a book; I would not have even thought it possible. Many of the best ideas in this book are yours, and those that are not, you made better. Thank you for your inestimable help with writing this book, providing the base on which it builds (remember in the library showing me many times how to get certain rows in R!), giving me the time that I needed to write, encouragement when it turned out that writing a book just meant endlessly re-writing that which was perfect the day before, reading everything in this book many times, making coffee or cocktails as appropriate, and more.\nYou can contact me at: rohan.alexander@utoronto.ca.\n\n\n\nRohan Alexander\nToronto, Canada\nApril 2022\n\n\n\n\n\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. Yale Press.\n\n\nDeWitt, Helen. 2000. The Last Samurai. Talk Mirimax Books.\n\n\nHamming, Richard W. 1996. The Art of Doing Science and Engineering. Stripe Press.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. Chapman & Hall.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2017. An Introduction to Statistical Learning with Applications in r.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft. Scribner.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC Press.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRobinson, Emily, and Jacqueline Nolis. 2020. Build a Career in Data Science. https://livebook.manning.com/book/build-a-career-in-data-science?origin=product-look-inside.\n\n\nTimbers, Tiffany-Anne, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. https://r4ds.had.co.nz/.\n\n\nWiessner, Polly W. 2014. “Embers of Society: Firelight Talk Among the Ju/’Hoansi Bushmen.” Proceedings of the National Academy of Sciences 111 (39): 14027–35.\n\n\nWilson, Greg. 2021. Building Software Together. CRC Books."
  },
  {
    "objectID": "00-author.html",
    "href": "00-author.html",
    "title": "About the author",
    "section": "",
    "text": "He is interested in using statistical models to try to understand the world. And particularly how we get the data that go into those models; whose data are systematically missing; how we clean, prepare, and tidy data before they are modelled; the effects of all this on the implications of our models; and how we can reproducibly share the totality of this process. He tries to develop students that are skilled not only in using statistical methods across various disciplines, but also appreciate their limitations, and think deeply about the broader contexts of their work.\nHe enjoys teaching and aims to help students from a wide range of backgrounds learn how to use data to tell convincing stories. He teaches in both the Faculty of Information and the Department of Statistical Sciences at both undergraduate and graduate levels. He is a RStudio Certified Tidyverse Trainer.\nHe is married to Monica Alexander and they have two children. He probably spends too much money on books, and certainly too much time at libraries. If you have any book recommendations of your own, then he’d love to hear them."
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "1  Telling stories with data",
    "section": "",
    "text": "Required material"
  },
  {
    "objectID": "01-introduction.html#on-telling-stories",
    "href": "01-introduction.html#on-telling-stories",
    "title": "1  Telling stories with data",
    "section": "1.1 On telling stories",
    "text": "1.1 On telling stories\nOne of the first things that many parents regularly do when their children are born is read stories to them. In doing so they carry on a tradition that has occurred for millennia. Myths, fables, and fairy tales can be seen and heard all around us. Not only are they entertaining but they enable us to learn something about the world. While The Very Hungry Caterpillar (Carle 1969) may seem quite far from the world of dealing with data, there are similarities. Both are trying to tell a story and teaching us something about the world.\nWhen using data, we try to tell a convincing story. It may be as exciting as predicting elections, as banal as increasing internet advertising click rates, as serious as finding the cause of a disease, or as fun as forecasting basketball games. In any case the key elements are the same. The English author, E. M. Forster, described the aspects common to all novels as: story, people, plot, fantasy, prophecy, pattern, and rhythm (Forster 1927). Similarly, when we tell stories with data, there are common concerns, regardless of the setting:\n\nWhat is the dataset? Who generated the dataset and why?\nWhat is the process that underpins the dataset? Given that process, what is missing from the dataset or has been poorly measured? Could other datasets have been generated, and if so, how different could they have been to the one that we have?\nWhat is the dataset trying to say, and how can we let it say this? What else could it say? How do we decide between these?\nWhat are we hoping others will see from this dataset, and how can we convince them of this? How much work must we do to convince them?\nWho is affected by the processes and outcomes related to this dataset? To what extent are they represented in the dataset, and have they been part of conducting the analysis?\n\nIn the past, certain elements of telling stories with data were easier. For instance, experimental design has a long and robust tradition within agricultural and medical sciences, physics, and chemistry. Student’s t-distribution was identified in the early 1900s by a chemist, William Sealy Gosset, who worked at Guinness, a beer manufacturer (Boland 1984). It would have been straightforward for him to randomly sample the beer and change one aspect at a time.\nMany of the fundamentals of the statistical methods that we use today were developed in such settings, where it was typically possible to establish control groups and randomize; and in these settings there were fewer ethical concerns. A story told with the resulting data is likely to be fairly convincing.\nUnfortunately, little of this applies these days, given the diversity of settings to which statistical methods are applied. On the other hand, we have many advantages. For instance, we have well-developed statistical techniques, easier access to large datasets, and open-source statistical languages such as R (R Core Team 2021). But the difficulty of conducting traditional experiments means that we must also turn to other aspects to tell a convincing story."
  },
  {
    "objectID": "01-introduction.html#workflow-components",
    "href": "01-introduction.html#workflow-components",
    "title": "1  Telling stories with data",
    "section": "1.2 Workflow components",
    "text": "1.2 Workflow components\nThere are five core components to the workflow needed to tell stories with data:\n\nPlan and sketch an endpoint.\nSimulate some reasonable data and consider that.\nAcquire and prepare the real data.\nExplore and understand the real dataset.\nShare what was done and what was found.\n\nWe begin by planning and sketching an endpoint because this ensures that we think carefully about where we want to go. It forces us to deeply consider our situation, acts to keep us focused and efficient, and helps reduce scope creep. In Alice’s Adventures in Wonderland (Carroll 1865), Alice asks the Cheshire Cat which way she should go. The Cheshire Cat replies by asking where Alice would like to go. And when Alice replies that she does not mind, so long as she gets somewhere, the Cheshire Cat says then the direction does not matter because you will always get somewhere if you ‘walk long enough’. The issue, in our case, is that we typically cannot afford to walk aimlessly for long. While it may be that the endpoint needs to change, it is important that this is a deliberate, reasoned, decision. And that is only possible given an initial target. There is no need to spend too much time on this to get a lot of value from it. Often five minutes with paper and pen is enough.\nThe next step is to simulate data because that forces us into the details. It helps with cleaning and preparing the dataset because it focuses us on the classes in the dataset and the distribution of the values that we expect. For instance, if we were interested in the effect of age-groups on political preferences, then we may expect that our age-group column would be a factor, with four possible values: ‘18-29’, ‘30-44’, ‘45-59’, ‘60+’. The process of simulation provides us with clear features that our real dataset should satisfy. We could use these features to define tests that would guide our data cleaning and preparation. For instance, we could check our real dataset for age-groups that are not one of those four values. When those tests pass, we could be confident that our age-group column only contains values that we expect.\nSimulating data is also important when we turn to statistical modelling. When we are at that stage, we are concerned with whether the model reflects what is in the dataset. The issue is that if we go straight to modelling the real dataset, then we do not know whether we have a problem with our model. We initially simulate data so that we precisely know the underlying data generation process. We then apply the model to that simulated dataset. When we get out what we put in, then we know that our model is performing appropriately, and can turn to the real dataset. Without that initial application to simulated data, it would be more difficult to have confidence in our model.\nSimulation is often cheap—almost free given modern computing resources and statistical programming languages—and fast. It provides ‘an intimate feeling for the situation’ (Hamming 1996, 239). The way to proceed is to start with a simulation that just contains the essentials, get that working, and to then complicate it.\nAcquiring and preparing the data that we are interested in is an often-overlooked stage of the workflow. This is surprising because it can be one of the most difficult stages and requires many decisions to be made. This is increasingly the subject of research. For instance, it has been found that decisions made during this stage greatly affect statistical results (Huntington-Klein et al. 2021).\nAt this stage of the workflow, it is common to feel a little overwhelmed. Typically, the data we can acquire leave us a little scared. There may be too little of it, in which case we worry about how we are going to be able to make our statistical machinery work. Alternatively, we may have the opposite problem and be worried about how we can even begin to deal with such a large amount of data.\n\nPerhaps all the dragons in our lives are princesses who are only waiting to see us act, just once, with beauty and courage. Perhaps everything that frightens us is, in its deepest essence, something helpless that wants our love.\nRilke (1929)\n\nDeveloping comfort in this stage of the workflow unlocks the rest of it. The dataset that is needed to tell a convincing story is in there, but we need to iteratively remove everything that is not the data that we need, and to then shape that which is.\nAfter we have a dataset, we then want to explore and understand  certain relationships in that dataset. The use of statistical models to understand the implications of our data is not free of bias, nor are they ‘truth’; they do what we tell them to do. Within a workflow to tell stories with data, statistical models are tools and approaches that we use to explore our dataset, in the same way that we may use graphs and tables. They are not something that will provide us with a definitive result but will enable us to understand the dataset more clearly in a particular way.\nBy the time we get to this step in the workflow, to a large extent, the model will reflect the decisions that were made in earlier stages, especially acquisition and cleaning, as much as it reflects any type of underlying process. Sophisticated modelers know that their statistical models are like the bit of the iceberg above the surface: they build on, and are only possible due to, the majority that is underneath, in this case, the data. But when an expert at the whole workflow uses modelling, they recognize that the results that are obtained are additionally due to choices about whose data matters, decisions about how to measure and record the data, and other aspects that reflect the world as it is, well before that data is available to their specific workflow.\nFinally, we must share what we did and what we found, at as high a fidelity as is possible. Talking about knowledge that only you have, does not make you knowledgeable, and that includes knowledge that only ‘past you’ has. When communicating, we need to be clear about what decisions we made, why we made them, our findings, and the weaknesses of our approach. We are aiming to uncover something important (otherwise, why bother) so we write down everything, in the first instance, although this written communication may be supplemented with other forms of communication later. There are so many decisions that we must make in this workflow that we want to be sure that we are open about the entire thing—start to finish. This means much more than just the statistical modelling and creation of the graphs and tables, but everything. Without this, stories based on data do not have any credibility.\nThe world is not a rational meritocracy where everything is carefully and judiciously evaluated. Instead, we use shortcuts, hacks, and heuristics, based on our experience. Unclear communication will render even the best work moot, because it will not be thoroughly engaged with. While there is a minimum when it comes to communication, there is no upper limit to how impressive it can be. When it is the culmination of a thought-out workflow, at best, it obtains a certain sprezzatura, or studied carelessness. Achieving such mastery is the work of years."
  },
  {
    "objectID": "01-introduction.html#telling-stories-with-data",
    "href": "01-introduction.html#telling-stories-with-data",
    "title": "1  Telling stories with data",
    "section": "1.3 Telling stories with data",
    "text": "1.3 Telling stories with data\nA compelling story based on data can likely be told in around ten-to-twenty pages. Much less than this, and it is likely too light on some of the details. And while it is easy to write much more, often some reflection enables succinctness or for multiple stories to be separated. The best stories are typically based on research and independent learning.\nIt is possible to tell convincing stories even when it is not possible to conduct traditional experiments. These approaches do not rely on ‘big data’—which is not a panacea (Meng 2018)—but instead on better using the data that are available. A blend of theory and application, combined with practical skills, a sophisticated workflow, and an appreciation for what one does not know, is often enough to create lasting knowledge.\nThe best stories based on data tend to be multi-disciplinary. They take from whatever field they need to, but almost always draw on statistics, data visualization, computer science, experimental design, economics, engineering, and information science (to name a few). As such, an end-to-end workflow requires a blend of skills from these areas. The best way to learn these skills is to use real-world data to conduct research projects where you:\n\nobtain and clean relevant datasets;\ndevelop research questions;\nuse statistical techniques to explore those questions; and\ncommunicate in a meaningful way.\n\nThe key elements of telling convincing stories with data are:\n\nCommunication.\nReproducibility.\nEthics.\nQuestions.\nMeasurement.\nData collection.\nData cleaning.\nExploratory data analysis.\nModelling.\nScaling.\n\nThese elements are the foundation on which the workflow are built (Figure 1.1).\n\n\n\nFigure 1.1: The workflow builds on various elements\n\n\nThis is a lot to master, but communication is the most important. Simple analysis, communicated well, is more valuable than complicated analysis communicated poorly. This is because the latter cannot be understood or trusted by others. A lack of clear communication sometimes reflects a failure by the researcher to understand what is going on, or even what they are doing. And so, while the level of the analysis should match the dataset, instruments, task, and skillset, when a trade-off is required between clarity and complication, it can be sensible to err on the side of clarity.\nClear communication means writing in plain language with the help of tables, graphs, and technical terms, in a way that brings the audience along with you. It means setting out what was done and why, as well as what was found. The minimum hurdle is doing this in a way that enables another person to independently do what you did and find what you found. One challenge is that as you immerse yourself in the data, it can be difficult to remember what it was like when you first came to it. But that is where most of your audience will be coming from. Learning to provide an appropriate level of nuance and detail is especially difficult but is made easier by trying to write for the audience’s benefit.\nReproducibility is required to create lasting knowledge about the world. It means that everything that was done—all of it, end-to-end—can be independently redone. Ideally, autonomous end-to-end reproducibility is possible; anyone can get the code, data, and environment, to verify everything that was done. Unfettered access to code is almost always possible. While that is the default for data also, it is not always reasonable. For instance, studies in psychology may have small, personally identifying, samples. One way forward is to openly share simulated data with similar properties, along with defining a process by which the real data could be accessed, given appropriate bona fides.\nActive consideration of ethics is needed because the dataset likely concerns humans. This means considering things like: who is in the dataset, who is missing, and why? To what extent will our story perpetuate the past? And is this something that ought to happen? Even if the dataset does not concern humans, the story is likely being put together by humans, and we affect almost everything else. This means we have a moral responsibility to use data ethically, with concern for environmental impact, and inequity.\nThere are many definitions of ethics, but when it comes to telling stories with data, at a minimum it means considering the full context of the dataset (D’Ignazio and Klein 2020). In jurisprudence, a textual approach to law means literally considering the words of the law as they are printed, while a purposive approach means laws are interpreted within a broader context. An ethical approach to telling stories with data means adopting the latter approach, and considering the social, cultural, historical, and political forces that shape our world, and hence our data (Crawford 2021).\nCuriosity provides internal motivation to explore a dataset, and associated process, to a proper extent. Questions tend to beget questions, and these usually improve and refine as the process of coming to understand a dataset carries on. In contrast to the stock Popperian approach to hypothesis testing often taught, questions are typically developed through a continuous and evolving process (Franklin 2005). It can be difficult to find an initial question. Selecting an area of interest can help, as can sketching a broad claim with the intent of evolving it into a specific question, and finally, bringing together two different areas.\nDeveloping a comfort and ease in the messiness of real-world data means getting to ask new questions each time the data update. And knowing a dataset in detail tends to surface unexpected groupings or values that you can then work with subject-area experts to understand. Becoming a bit of a ‘mongrel’ by developing a base of knowledge across a variety of areas is especially valuable, as is becoming comfortable with the possibility of initially asking dumb questions.\nMeasurement and data collection are about deciding how our world will become data. They are challenging. The world is so vibrant that it is difficult to reduce it to something that is possible to consistently measure and collect. Take, for instance, someone’s height. We can, probably, all agree that we should take our shoes off before we measure height. But our height changes over the course of the day. And measuring someone’s height with a tape measure will give different results to using a laser. If we are comparing heights between people or over time, it therefore becomes important to measure at the same time each day, using the same method. But that quickly becomes unfeasible.\nMost of the questions we are interested in will use data that are more complicated than height. How do we measure how sad someone is? How do we measure pain? Who decides what we will measure and how we will measure it? There is a certain arrogance required to think that we can reduce the world to a value and then compare these. Ultimately, we must, but it is difficult to consistently define what is to be measured. This process is not value-free. The only way to reasonably come to terms with this brutal reduction is to deeply understand, and respect what we are measuring and collecting. What is the central essence, and what can be stripped away?\nPablo Picasso, the twentieth century Spanish painter, has a series of drawings where he depicts the outline of an animal using only one line (Figure 1.2). Despite their simplicity, we recognize which animal is being depicted—the drawing is sufficient to tell the animal is a dog, not a cat. Could this be used to determine whether the dog is sick? Probably not. We would likely want a more detailed drawing. The decision as to which features should be measured and collected, and which to ignore, turns on context and purpose.\n\n\n\nFigure 1.2: This drawing is clearly a dog, even though it is just one line\n\n\nData cleaning and preparation is a critical part of using data. We need to massage the data available to us into a dataset that we can use. This requires making a lot of decisions. The data cleaning and preparation stage is critical, and worthy of as much attention and care as any other.\nFollowing Kennedy et al. (2020) consider a survey that collected information about gender using four options: ‘man’, ‘woman’, ‘prefer not to say’, ‘other’, where ‘other’ dissolved into an open textbox. When we come to that dataset, we are likely to find that most responses are either ‘man’ or ‘woman’. We need to decide what to do about ‘prefer not to say’. If we drop it from our dataset, then we are actively ignoring these respondents. If we do not drop it, then it makes our analysis more complicated. Similarly, we need to decide how to deal with the open text responses. Again, we could drop these responses, but this ignores the experiences of some of our respondents. Another option is to merge this with ‘prefer not to say’, but that shows a disregard for our respondents, because they specifically did not choose that option.\nThere is no easy, nor always-correct, choice in many data cleaning and preparation situations. It depends on context and purpose. Data cleaning and preparation involves making many choices like this, and so it is vital to record every step so that others can understand what was done and why. Data never speak for themselves; they are the dummies of the ventriloquists that cleaned and prepared them.\nThe process of coming to understand the look and feel of a dataset is termed exploratory data analysis (EDA). This is an open-ended process. We need to understand the shape of our dataset before we can formally model it. The process of EDA is an iterative one that involves producing summary statistics, graphs, tables, and sometimes even some modelling. It is a process that never formally finishes and requires a variety of skills.\nIt is difficult to delineate where EDA ends and formal statistical modelling begins, especially when considering how beliefs and understanding develop (Hullman and Gelman 2021). But at its core, ‘EDA starts from the data’, and involves immersing ourselves in it (Cook, Reid, and Tanaka 2021). EDA is not something that is typically explicitly part of our final story. But it has a central role in how we come to understand the story we are telling. And so, it is critical that all the steps taken during EDA are recorded and shared.\nStatistical modelling has a long and robust history. Our knowledge of statistics has been built over hundreds of years. Statistics is not a series of dry theorems and proofs but is instead a way of exploring the world. It is analogous to ‘a knowledge of foreign languages or of algebra: it may prove of use at any time under any circumstances’ (Bowley 1901, 4). A statistical model is not a recipe to be blindly followed in an if-this-then-that way but is instead a way of understanding data (James et al. 2017). Modelling is usually required to infer statistical patterns from data. More formally, ‘statistical inference, or ’learning’ as it is called in computer science, is the process of using data to infer the distribution that generated the data’ (Wasserman 2005, 87).\nStatistical significance is not the same as scientific significance, and we are realizing the cost of what has been the dominant paradigm. It is rarely appropriate to put our data through some arbitrary pass/fail statistical test. Instead, the proper use for statistical modelling is as a kind of echolocation. We listen to what comes back to us from the model, to help learn about the shape of the world, while recognizing that it is only one representation of the world.\nThe use of statistical programming languages, such as R, enables us to rapidly scale our work. This refers to both inputs and outputs. It is basically just as easy to consider 10 observations as 1,000, or even 1,000,000. This enables us to more quickly see the extent to which our stories apply. It is also the case that our outputs can be consumed as easily by one person as by 10, or 100. Using an Application Programming Interface (API) it is even possible for our stories to be considered many thousands of times each second."
  },
  {
    "objectID": "01-introduction.html#how-do-our-worlds-become-data",
    "href": "01-introduction.html#how-do-our-worlds-become-data",
    "title": "1  Telling stories with data",
    "section": "1.4 How do our worlds become data?",
    "text": "1.4 How do our worlds become data?\n\nThere is the famous story by Eddington about some people who went fishing in the sea with a net. Upon examining the size of the fish they had caught, they decided there was a minimum size to the fish in the sea! Their conclusion arose from the tool used and not from reality.\nHamming (1996, 177)\n\nTo a certain extent we are wasting our time. We have a perfect model of the world—it is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could perfectly forecast a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex, world from which they were generated.\nThere are different approximations of ‘plausibly measurable’. Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data.\nMuch of statistics is focused on considering, thoroughly, the data that we have. And that was appropriate for when our data were predominately agricultural, astronomical, or from the physical sciences. This is not to say that systemic bias cannot exist or have an impact in non-human contexts, but with the rise of data science, mostly because of the value of its application to datasets generated by humans, we must also actively consider what is not in our dataset. Who is systematically missing from our dataset? Whose data does not fit nicely into the approach that we are using and are hence is being inappropriately simplified? If the process of the world becoming data necessarily involves abstraction and simplification, then we need to be clear about the points at which we can reasonably simplify, and those which would be inappropriate, recognizing that this will be application specific.\nThe process of our world becoming data necessarily involves measurement. Paradoxically, often those that do the measurement and are deeply immersed in the details have less trust in the data than those that are removed from it. Even seemingly clear tasks, such as measuring distance, defining boundaries, and counting populations, are surprisingly difficult in practice. Turning our world into data requires many decisions and imposes much error. Among many other considerations, we need to decide what will be measured, how accurately we will do this, and who will be doing the measurement.\n\nOh, you think we have good data on that! An important example of how something seemingly simple quickly becomes difficult is maternal mortality. That refers to the number of women who die while pregnant, or soon after a termination, from a cause related to the pregnancy or its management (WHO 2019). It is difficult but critical to turn the tragedy of such a death into cause-specific data because that helps mitigate future deaths. Some countries have well-developed civil registration and vital statistics (CRVS). These collect data about every death. But many countries do not have a CRVS and so not every death is recorded. Even if a death is recorded, defining a cause of death may be difficult, especially when there is a lack of qualified medical personal or equipment. Maternal mortality is especially difficult because there are typically many causes. Some CRVS have a checkbox on the form to specify whether the death should be counted as maternal mortality. But even some developed countries have only recently adopted this. For instance, it was only introduced in the US in 2003, and even in 2015 Alabama, California, and West Virginia had not adopted the standard question (MacDorman and Declercq 2018).\n\nWe typically use various instruments to turn the world into data. In astronomy, the development of better telescopes, and eventually satellites and probes, enabled new understanding of other worlds. Similarly, we have new instruments for turning our own world into data being developed each day. Where once a census was a generational-defining event, now we have regular surveys, transactions data available by the second, and almost all interactions on the internet become data of some kind. The development of such instruments has enabled exciting new stories.\nOur world imperfectly becomes data. If we are to nonetheless use data to learn about the world, then we need to actively seek to understand the ways they are imperfect and the implications of those imperfections."
  },
  {
    "objectID": "01-introduction.html#what-is-data-science-and-how-should-we-use-it-to-learn-about-the-world",
    "href": "01-introduction.html#what-is-data-science-and-how-should-we-use-it-to-learn-about-the-world",
    "title": "1  Telling stories with data",
    "section": "1.5 What is data science and how should we use it to learn about the world",
    "text": "1.5 What is data science and how should we use it to learn about the world\nThere is no agreed definition of data science, but a lot of people have tried. For instance, Wickham and Grolemund (2017) say it is ‘…an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge.’ Similarly, Leek and Peng (2020) say it is ‘…the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience.’ Baumer, Kaplan, and Horton (2021) say it is ‘…the science of extracting meaningful information from data’. Timbers, Campbell, and Lee (2022) say they define data science as ‘the process of generating insight from data through reproducible and auditable processes’. Foster (1968) points very clearly to data science when he says: ‘(s)tatistics are concerned with the processing and analysis of masses of data and with the development of mathematical methods of extracting information from data. Combine all this activity with computer methods and you have something more than the sum of its parts.’\nCraiu (2019) argues that the lack of certainty as to what data science is might not matter because ‘…who can really say what makes someone a poet or a scientist?’ He goes on to broadly say that a data scientist is ‘…someone with a data driven research agenda, who adheres to or aspires to using a principled implementation of statistical methods and uses efficient computation skills.’\nIn any case, alongside specific, technical, definitions, there is value in having a simple definition, even if we lose a bit of specificity. Probability is often informally defined as ‘counting things’ (McElreath 2020, 10). In a similar informal sense, data science can be defined as something like: humans measuring stuff, typically related to other humans, and using sophisticated averaging to explain and predict.\nThat may sound a touch cute, but Francis Edgeworth, the nineteenth century statistician and economist, considered statistics to be the science ‘of those Means which are presented by social phenomena’, so it is in good company (Edgeworth 1885). In any case, one feature of this definition is that it does not treat data as terra nullius, or nobody’s land. Statisticians tend to see data as the result of some process that we can never know, but that we try to use data to come to understand. Many statisticians care deeply about data and measurement, but there are many cases in statistics where data kind of just appear; they belong to no one. But that is never actually the case.\nData is generated, and then must be gathered, cleaned, and prepared, and these decisions matter. Every dataset is sui generis, or a class by itself, and so when you come to know one dataset well, you just know one dataset, not all datasets.\nMuch of data science focuses on the ‘science’, but it is important to also focus on ‘data’. And that is another feature of that cutesy definition of data science. A lot of data scientists are generalists, who are interested in a broad range of problems. Often, the thing that unites these is the need to gather, clean, and prepare messy data. And often it is the specifics of those data that requires the most time, that updates most often, and that are worthy of our most full attention.\nJordan (2019) describes being in a medical office and being given some probability, based on prenatal screening, that his child, then a fetus, had Down syndrome. By way of background, one can test to know for sure, but that test comes with the risk of the fetus not surviving, so this initial screening probability matters. Jordan (2019) found those probabilities were being determined based on a study done a decade earlier in the UK. The issue was that in the ensuing 10 years, imaging technology had improved so the test was not expecting such high-resolution images and there had been a subsequent (false) increase in Down syndrome diagnoses when the images improved. The data was the problem.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Michael Jordan is Pehong Chen Distinguished Professor at the University of California, Berkeley. After taking a PhD in Cognitive Science from University of California, San Diego, in 1985, he was appointed as an assistant professor at MIT, being promoted to full professor in 1997, and in 1998 he moved to Berkeley. One area of his research is statistical machine learning. One particularly important paper is Blei, Ng, and Jordan (2003), which enables text to be grouped together to define topics, and we cover this in Chapter 17.\n\n\nIt is not just the ‘science’ bit that is hard, it is the ‘data’ bit as well. For instance, researchers went back and examined one of the most popular text datasets in computer science, and they found that around 30 per cent of the data were inappropriately duplicated (Bandy and Vincent 2021). There is an entire field—linguistics—that specializes in these types of datasets, and inappropriate use of data is one of the dangers of any one field being hegemonic. The strength of data science is that it brings together folks with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past. This means that we must go out of our way to show respect for those who do not come from our own tradition, but who are nonetheless as similarly interested in a dataset as we are. Data science is multi-disciplinary and increasingly critical; hence it must reflect our world. There is a pressing need a diversity of backgrounds, of approaches, and of disciplines in data science.\nOur world is messy, and so are our data. To successfully tell stories with data you need to become comfortable with the fact that the process will be difficult. Hannah Fry, the British mathematician, describes spending six months rewriting code before it solved her problem (Thornhill 2021). You need to learn to stick with it. You also need to countenance failure, and you do this by developing resilience and having intrinsic motivation. The world of data is about considering possibilities and probabilities, and learning to make trade-offs between them. There is almost never anything that we know for certain, and there is no perfect analysis.\nUltimately, we are all just telling stories with data, but these stories are increasingly among the most important in the world."
  },
  {
    "objectID": "01-introduction.html#exercises-and-tutorial",
    "href": "01-introduction.html#exercises-and-tutorial",
    "title": "1  Telling stories with data",
    "section": "1.6 Exercises and tutorial",
    "text": "1.6 Exercises and tutorial\n\n1.6.1 Exercises\n\nAccording to Register (2020) data decisions affect (pick one)?\n\nReal people.\nNo one.\nThose in the training set.\nThose in the test set.\n\nWhat is data science (in your own words)?\nAccording to Keyes (2019) what is perhaps a more accurate definition of data science (pick one)?\n\nThe inhumane reduction of humanity down to what can be counted.\nThe quantitative analysis of large amounts of data for the purpose of decision-making.\nData science is an inter-disciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from many structural and unstructured data.\n\nImagine that you have a job in which including ‘race’ and/or sexuality as explanatory variables improves the performance of your model. What types of issues would you consider when deciding whether to include these variable in your analysis (in your own words)?\nRe-order the following steps of the workflow to be correct:\n\nSimulate.\nAcquire.\nShare.\nPlan.\nExplore.\n\nAccording to Crawford (2021), which of the following forces shape our world, and hence our data (select all that apply)?\n\nPolitical.\nHistorical.\nCultural.\nSocial.\n\nWhat is required to tell convincing stories (select all that apply)?\n\nSophisticated workflow.\nPractical skills.\nBig data.\nHumility about one’s own knowledge.\nTheory and application.\n\nWhy is ethics a key element of telling convincing stories (in your own words)?\nConsider a survey that asked about gender with the following responses: ‘man: 879’, ‘woman: 912’, ‘non-binary: 10’ ‘prefer not to say: 3’, and ‘other: 1’, which allowed respondents to enter their own text. What is the appropriate way to consider ‘prefer not to say’?\n\nDrop them.\nMerge into ‘other’.\nInclude them.\nIt depends.\n\n\n\n\n1.6.2 Tutorial\nThe purpose of this tutorial is to clarify in your mind the difficulty of measurement, even of seemingly simple things, and hence the likelihood of measurement issues in more complicated areas.\n\nPlease obtain some seeds for a fast-growing plant such as radishes, mustard greens, or arugula. Plant the seeds and measure how much soil you used. Water them and measure the water you used. Each day take a note of any changes. More generally, measure and record as much as you can. Note your thoughts about the difficulty of measurement. Eventually your seeds will sprout, and you should measure how big they are. We will return to use the data that you put together.\nWhile you are waiting for the seeds to sprout, and for one week only, please measure the length of your hair daily. Write a one-to-two-page paper about what you found and what you learned about the difficulty of measurement.\n\n\n\n\n\n\nBandy, Jack, and Nicholas Vincent. 2021. “Addressing \"Documentation Debt\" in Machine Learning Research: A Retrospective Datasheet for BookCorpus.” https://arxiv.org/abs/2105.05241.\n\n\nBaumer, Benjamin, Daniel Kaplan, and Nicholas Horton. 2021. Modern Data Science with r. 2nd ed. CRC Press.\n\n\nBlei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3 (Jan): 993–1022.\n\n\nBoland, Philip J. 1984. “A Biographical Glimpse of William Sealy Gosset.” The American Statistician 38 (3): 179–83.\n\n\nBowley, Arthur Lyon. 1901. Elements of Statistics. P. S. King.\n\n\nCarle, Eric. 1969. The Very Hungry Caterpillar. World Publishing Company.\n\n\nCarroll, Lewis. 1865. Alice’s Adventures in Wonderland. Macmillan.\n\n\nCook, Dianne, Nancy Reid, and Emi Tanaka. 2021. “The Foundation Is Available for Thinking about Data Visualization Inferentially.” Harvard Data Science Review, July. https://doi.org/10.1162/99608f92.8453435d.\n\n\nCraiu, Radu V. 2019. “The Hiring Gambit: In Search of the Twofer Data Scientist.” Harvard Data Science Review 1 (1).\n\n\nCrawford, Kate. 2021. Atlas of AI. Yale University Press.\n\n\nD’Ignazio, Catherine, and Lauren F Klein. 2020. Data Feminism. Mit Press.\n\n\nEdgeworth, Francis Ysidro. 1885. “Methods of Statistics.” Journal of the Statistical Society of London, 181–217.\n\n\nForster, E M. 1927. Aspects of the Novel. Edward Arnold.\n\n\nFoster, Gordon. 1968. “Computers, Statistics and Planning: Systems or Chaos?” Geary Lecture. https://www.esri.ie/system/files/media/file-uploads/2016-03/GLS2.pdf.\n\n\nFranklin, Laura R. 2005. “Exploratory Experiments.” Philosophy of Science 72 (5): 888–99.\n\n\nHamming, Richard W. 1996. The Art of Doing Science and Engineering. Stripe Press.\n\n\nHullman, Jessica, and Andrew Gelman. 2021. “Designing for Interactive Exploratory Data Analysis Requires Theories of Graphical Inference.” Harvard Data Science Review, July. https://doi.org/10.1162/99608f92.3ab8a587.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey R Bloem, Pralhad Burli, Naibin Chen, et al. 2021. “The Influence of Hidden Researcher Decisions in Applied Microeconomics.” Economic Inquiry.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2017. An Introduction to Statistical Learning with Applications in r.\n\n\nJordan, Michael I. 2019. “Artificial Intelligence—the Revolution Hasn’t Happened Yet.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.f06c6e61.\n\n\nKennedy, Lauren, Katharine Khanna, Daniel Simpson, and Andrew Gelman. 2020. “Using Sex and Gender in Survey Adjustment.” https://arxiv.org/abs/2009.14401.\n\n\nKeyes, Os. 2019. “Counting the Countless.” Real Life. https://reallifemag.com/counting-the-countless/.\n\n\nLeek, Jeff, and Roger D. Peng. 2020. “Advanced Data Science 2020.” http://jtleek.com/ads2020/index.html.\n\n\nMacDorman, Marian F, and Eugene Declercq. 2018. “The Failure of United States Maternal Mortality Reporting and Its Impact on Women’s Lives.” Birth (Berkeley, Calif.) 45 (2): 105.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC Press.\n\n\nMeng, Xiao-Li. 2018. “Statistical Paradises and Paradoxes in Big Data (i): Law of Large Populations, Big Data Paradox, and the 2016 US Presidential Election.” The Annals of Applied Statistics 12 (2): 685–726.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRegister, Yim. 2020. “Data Science Ethics in 6 Minutes.” https://youtu.be/mA4gypAiRYU.\n\n\nRilke, Rainer Maria. 1929. Letters to a Young Poet.\n\n\nThornhill, John. 2021. “Lunch with the FT: Mathematician Hannah Fry.” Financial Times.\n\n\nTimbers, Tiffany-Anne, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWHO. 2019. “Trends in Maternal Mortality 2000 to 2017: Estimates by WHO, UNICEF, UNFPA, World Bank Group and the United Nations Population Division.” https://www.who.int/reproductivehealth/publications/maternal-mortality-2000-2017/en/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. https://r4ds.had.co.nz/."
  },
  {
    "objectID": "02-drinking_from_a_fire_hose.html",
    "href": "02-drinking_from_a_fire_hose.html",
    "title": "2  Drinking from a fire hose",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "02-drinking_from_a_fire_hose.html#hello-world",
    "href": "02-drinking_from_a_fire_hose.html#hello-world",
    "title": "2  Drinking from a fire hose",
    "section": "2.1 Hello, World!",
    "text": "2.1 Hello, World!\nThe way to start, is to start. In this chapter we go through three complete examples of the workflow advocated in this book. This means we will: plan, simulate, acquire, explore, and share. If you are new to R, then some of the code may be a bit unfamiliar to you. If you are new to statistics, then some of the concepts may be unfamiliar. Do not worry. It will all soon become familiar.\nThe only way to learn how to tell stories, is to start telling stories yourself. This means that you should try to get these examples working. Do the sketches yourself, type everything out yourself (using R Studio Cloud if you are new to R and do not have it installed on your own computer), and execute it all. It is important, and normal, to realize that it will be challenging at the start.\n\nWhenever you’re learning a new tool, for a long time, you’re going to suck… But the good news is that is typical; that’s something that happens to everyone, and it’s only temporary.\nHadley Wickham as quoted by Barrett (2021).\n\nYou will be guided thoroughly here. Hopefully by experiencing the power of telling stories with data, you will feel empowered to stick with it.\nTo get started, go to R Studio Cloud and create an account. As we are not doing anything too involved the free version will be fine for now. Once you have an account and log in, then it should look something like Figure 2.1.\n\n\n\nFigure 2.1: Opening R Studio Cloud for the first time\n\n\nYou will be in ‘Your Projects’. From here you should start a new project (‘New Project’ -> ‘New RStudio Project’) (Figure 2.2). You can give the project a name by clicking on ‘Untitled Project’ and replacing it .\n\n\n\nFigure 2.2: Opening a new R Studio project\n\n\nWe will now go through three worked examples: Australian elections, Toronto homelessness, and neonatal mortality. These examples build increasing complexity, but from the first one, we will be telling a story with data."
  },
  {
    "objectID": "02-drinking_from_a_fire_hose.html#australian-elections",
    "href": "02-drinking_from_a_fire_hose.html#australian-elections",
    "title": "2  Drinking from a fire hose",
    "section": "2.2 Australian elections",
    "text": "2.2 Australian elections\nAustralia is a parliamentary democracy with 151 seats in the House of Representatives, which is the lower house and that from which government is formed. There are two major parties – ‘Liberal’ and ‘Labor’ – two minor parties – ‘National’ and ‘Green’ – and many smaller parties and independents. In this example we will create a graph of the number of seats that each party won in the 2019 Federal Election.\n\n2.2.1 Plan\nFor this example, we need to plan two aspects. The first is what the dataset that we need will look like, and the second is what the final graph will look like.\nThe basic requirement for the dataset is that it has the name of the seat (sometimes called a ‘division’ in Australia) and the party of the person elected. So, a quick sketch of the dataset that we would need could look something like Figure 2.3.\n\n\n\nFigure 2.3: Quick sketch of a dataset that could be useful for analyzing Australian elections\n\n\nWe also need to plan the graph that we are interested in. Given we want to display the number of seats that each party won, a quick sketch of what we might aim for is Figure 2.4.\n\n\n\nFigure 2.4: Quick sketch of a possible graph of the number of seats won by each party\n\n\n\n\n2.2.2 Simulate\nWe now simulate some data, to bring some specificity to our sketches.\nTo get started, within R Studio Cloud, make a new Quarto document (‘File’ -> ‘New File’ -> ‘Quarto Document…’). Give it a title, such as ‘Exploring the 2019 Australian Election’, and add your name as author. Leave the other options as their default, and then click ‘Create’. For this example, we will put everything into this one Quarto document. You should save it as ‘australian_elections.qmd’ (‘File’ -> ‘Save As…’).\nRemove almost all the default content, and then beneath the heading material create a new R code chunk (‘Code’ -> ‘Insert Chunk’) and add preamble documentation that explains:\n\nthe purpose of the document;\nthe author and contact details;\nwhen the file was written or last updated; and\npre-requisites that the file relies on.\n\n\n#### Preamble ####\n# Purpose: Read in data from the 2019 Australian Election and make a\n# graph of the number of seats each party won.\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 12 April 2022\n# Prerequisites: Need to know where to get Australian elections data.\n\nIn R, lines that start with ‘#’ are comments. This means that they are not run as code by R, but are instead designed to be read by humans. Each line of this preamble should start with a ‘#’. Also make it clear that this is the preamble section by surrounding that with ‘####’. The result should look like Figure 2.5.\n\n\n\nFigure 2.5: Screenshot of australian_elections.qmd after initial set-up and with a premable\n\n\nAfter this we need to set-up the workspace. This involves installing and loading any packages that will be needed. A package only needs to be installed once for each computer, but needs to be loaded each time it is to be used. In this case we are going to use tidyverse (Wickham 2017), janitor (Firke 2020), and tidyr (Wickham 2021). They will need to be installed because this is the first time they are being used, and then each will need to be loaded.\nAn example of installing the packages follows (excessive comments have been added to be clear about what is going on; in general, this level of commenting is unnecessary). Run this code by clicking the small green arrow associated with the R code chunk.\n\n#### Workspace set-up ####\ninstall.packages(\"tidyverse\") # Only need to do this once per computer\ninstall.packages(\"janitor\") # Only need to do this once per computer\ninstall.packages(\"tidyr\") # Only need to do this once per computer\n\nNow that the packages are installed, they need to be loaded. As that installation step only needs to be done once per computer, that code can be commented out so that it is not accidentally run.\n\n#### Workspace set-up ####\n# install.packages(\"tidyverse\") # Only need to do this once per computer\n# install.packages(\"janitor\") # Only need to do this once per computer\n# install.packages(\"tidyr\") # Only need to do this once per computer\n\nlibrary(tidyverse) # A collection of data-related packages\nlibrary(janitor) # Helps clean datasets\nlibrary(tidyr) # Helps make tidy datasets\n\nWe can render the entire document by clicking ‘Render’. When you do this, you will be asked to install some packages, which you should agree to. This will result in a html document.\nFor an introduction to the packages that were just installed, each package contains a help file that provides information about them and their functions. It can be accessed by appending a question mark before the package name and then running that code in the console. For instance ?tidyverse.\nTo simulate our data, we need to create a dataset with two columns: ‘Division’ and ‘Party’, and some values for each. In the case of ‘Division’ reasonable values would be a name of one of the 151 Australian divisions. In the case of ‘Party’ reasonable values would be one of the following five: ‘Liberal’, ‘Labor’, ‘National’, ‘Green’, ‘Other’. Again, this code can be run by clicking the small green arrow associated with the R code chunk.\n\nsimulated_data <-\n  tibble(\n    # Use 1 through to 151 to represent each riding\n    'Riding' = 1:151,\n    # Randomly choose one of five options, with replacement, 151 times\n    'Party' = sample(\n      x = c(\n        'Liberal', \n        'Labor', \n        'National', \n        'Green', \n        'Other'\n      ),\n      size = 151,\n      replace = TRUE\n    ))\n\nsimulated_data\n\n# A tibble: 151 × 2\n   Riding Party   \n    <int> <chr>   \n 1      1 National\n 2      2 Labor   \n 3      3 Labor   \n 4      4 Green   \n 5      5 National\n 6      6 Labor   \n 7      7 National\n 8      8 National\n 9      9 National\n10     10 Liberal \n# … with 141 more rows\n\n\nAt a certain point, your code will not run and you will want to ask others for help. We will discuss how to do this in Chapter 4, but for now, if you need help, then you should create a GitHub Gist. The first step is to create an account on GitHub (Figure 2.6). Thinking about an appropriate username is important because this will become part of your professional profile. So it would make sense to have a username that is professional, independent of any course, and ideally related to your real name.\n\n\n\nFigure 2.6: GitHub sign-up screen\n\n\nThen look for a ‘+’ in the top right, and then select ‘New gist’ (Figure 2.7).\n\n\n\nFigure 2.7: New GitHub Gist\n\n\nThen add all the code to that gist, not just the final bit that is giving an error. And give it a meaningful filename that includes ‘.R’ at the end, for instance, ‘australian_elections.R’. In Figure 2.8, we have incorrect capitalization library(Tidyverse).\n\n\n\nFigure 2.8: Create a public GitHub Gist to share code\n\n\nClick ‘Create public gist’. We can then share the URL to this Gist, explain what the problem is, and what we are trying to achieve. It will be much easier to help, because all the code is available.\n\n\n2.2.3 Acquire\nNow we want to get the actual data. The data we need is from the Australian Electoral Commission (AEC), which is the non-partisan agency that organizes Australian federal elections. We can pass a page from their website to read_csv() from the readr package (Wickham, Hester, and Bryan 2021). We do not need to explicitly load the readr package because it is part of the tidyverse. The <- or ‘assignment operator’ is allocating the output of read_csv() to an object called ‘raw_elections_data’.\n\n#### Read in the data ####\nraw_elections_data <- \n  read_csv(\n    file =\n      \"https://results.aec.gov.au/24310/Website/Downloads/HouseMembersElectedDownload-24310.csv\",\n    show_col_types = FALSE,\n    skip = 1\n    ) \n\n# We have read the data from the AEC website. We may like\n# to save it in case something happens or they move it. \nwrite_csv(\n  x = raw_elections_data, \n  file = \"australian_voting.csv\"\n  )\n\n\n\n\n\n\n\nWe can take a quick look at the dataset using head() which will show the first six rows, and tail() which will show the last six rows.\n\nhead(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm   Surname   PartyNm  PartyAb\n       <dbl> <chr>      <chr>         <dbl> <chr>     <chr>     <chr>    <chr>  \n1        179 Adelaide   SA            33019 Steve     GEORGANAS Austral… ALP    \n2        197 Aston      VIC           33330 Alan      TUDGE     Liberal  LP     \n3        198 Ballarat   VIC           32326 Catherine KING      Austral… ALP    \n4        103 Banks      NSW           33334 David     COLEMAN   Liberal  LP     \n5        180 Barker     SA            33042 Tony      PASIN     Liberal  LP     \n6        104 Barton     NSW           32681 Linda     BURNEY    Austral… ALP    \n\ntail(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm    Surname  PartyNm  PartyAb\n       <dbl> <chr>      <chr>         <dbl> <chr>      <chr>    <chr>    <chr>  \n1        152 Wentworth  NSW           33218 Dave       SHARMA   Liberal  LP     \n2        153 Werriwa    NSW           32756 Anne Maree STANLEY  Austral… ALP    \n3        150 Whitlam    NSW           32632 Stephen    JONES    Austral… ALP    \n4        178 Wide Bay   QLD           33200 Llew       O'BRIEN  Liberal… LNP    \n5        234 Wills      VIC           32350 Peter      KHALIL   Austral… ALP    \n6        316 Wright     QLD           33376 Scott      BUCHHOLZ Liberal… LNP    \n\n\nWe need to clean the data so that we can use it. We are trying to make it similar to the dataset that we thought we wanted in the planning stage. While it is fine to move away from the plan, this needs to be a deliberate, reasoned, decision. After reading in the dataset that we saved, the first thing that we will do is adjust the names to make them easier to type. Removing the spaces helps to type column names. We will do this using clean_names() from janitor (Firke 2020) which changes the names into ‘snake_case’.\n\n\n\n\n#### Basic cleaning ####\nraw_elections_data <- \n  read_csv(file = \"australian_voting.csv\",\n           show_col_types = FALSE\n           )\n\n\n# Make the names easier to type\ncleaned_elections_data <- \n  clean_names(raw_elections_data)\n\n# Have a look at the first six rows\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 8\n  division_id division_nm state_ab candidate_id given_nm  surname   party_nm    \n        <dbl> <chr>       <chr>           <dbl> <chr>     <chr>     <chr>       \n1         179 Adelaide    SA              33019 Steve     GEORGANAS Australian …\n2         197 Aston       VIC             33330 Alan      TUDGE     Liberal     \n3         198 Ballarat    VIC             32326 Catherine KING      Australian …\n4         103 Banks       NSW             33334 David     COLEMAN   Liberal     \n5         180 Barker      SA              33042 Tony      PASIN     Liberal     \n6         104 Barton      NSW             32681 Linda     BURNEY    Australian …\n# … with 1 more variable: party_ab <chr>\n\n\nThe names are faster to type because R Studio will auto-complete them. To do this, we begin typing the name of a column and then use ‘tab’ to auto-complete it.\nThere are many columns in the dataset, and we are primarily interested in two: ‘division_nm’, and ‘party_nm’. We can choose certain columns of interest using select() from dplyr (Wickham et al. 2020) which we loaded as part of the tidyverse. The ‘pipe operator’, |>, pushes the output of one line to be the first input of the function on the next line.\n\ncleaned_elections_data <- \n  cleaned_elections_data |> \n  # Select only certain columns\n  select(division_nm,\n         party_nm\n         )\n\n# Have a look at the first six rows\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division_nm party_nm              \n  <chr>       <chr>                 \n1 Adelaide    Australian Labor Party\n2 Aston       Liberal               \n3 Ballarat    Australian Labor Party\n4 Banks       Liberal               \n5 Barker      Liberal               \n6 Barton      Australian Labor Party\n\n\nSome of the names of the columns are still not obvious because they are abbreviated. We can look at the names of the columns with names(). And we can change the names using rename() from dplyr (Wickham et al. 2020).\n\nnames(cleaned_elections_data)\n\n[1] \"division_nm\" \"party_nm\"   \n\n\n\ncleaned_elections_data <- \n  cleaned_elections_data |> \n  rename(\n    division = division_nm,\n    elected_party = party_nm\n    )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party         \n  <chr>    <chr>                 \n1 Adelaide Australian Labor Party\n2 Aston    Liberal               \n3 Ballarat Australian Labor Party\n4 Banks    Liberal               \n5 Barker   Liberal               \n6 Barton   Australian Labor Party\n\n\nWe will now look at this dataset, and the ‘elected_party’ column in particular.\n\nhead(cleaned_elections_data$elected_party)\n\n[1] \"Australian Labor Party\" \"Liberal\"                \"Australian Labor Party\"\n[4] \"Liberal\"                \"Liberal\"                \"Australian Labor Party\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we want to simplify the party names to match what we simulated, using recode() from dplyr (Wickham et al. 2020).\n\ncleaned_elections_data <-\n  cleaned_elections_data |>\n  mutate(\n    elected_party =\n      recode(\n        elected_party,\n        \"Australian Labor Party\" = \"Labor\",\n        \"Liberal National Party\" = \"Liberal\",\n        \"The Nationals\" = \"Nationals\",\n        \"The Greens\" = \"Greens\",\n        \"Independent\" = \"Other\",\n        \"Katter's Australian Party (KAP)\" = \"Other\",\n        \"Centre Alliance\" = \"Other\"\n      )\n  )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  <chr>    <chr>        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\nOur data now matches our plan (Figure 2.3) pretty well. For every electoral division we have the party of the person that won it.\nHaving now nicely cleaned the dataset, we should save it, so that we can start with that cleaned dataset in the next stage. We should make sure to save it under a new file name so we are not replacing the raw data, and so that it is easy to identify the cleaned dataset later.\n\nwrite_csv(\n  x = cleaned_elections_data,\n  file = \"cleaned_elections_data.csv\"\n  )\n\n\n\n\n\n\n2.2.4 Explore\nAt this point we would like to explore the dataset that we created. One way to better understand a dataset is to make a graph. In particular, here we would like to build the graph that we planned in Figure 2.4.\nFirst, we read in the dataset that we just created.\n\n\n\n\n#### Read in the data ####\ncleaned_elections_data <- \n  read_csv(\n    file = \"cleaned_elections_data.csv\",\n    show_col_types = FALSE\n    )\n\nWe can get a quick count of how many seats each party won using count() from dplyr (Wickham et al. 2020).\n\ncleaned_elections_data |> \n  count(elected_party)\n\n# A tibble: 5 × 2\n  elected_party     n\n  <chr>         <int>\n1 Greens            1\n2 Labor            68\n3 Liberal          67\n4 Nationals        10\n5 Other             5\n\n\nTo build the graph that we are interested in, we will rely on the ggplot2 package (Wickham 2016). The key aspect of this package is that we build graphs by adding layers using ‘+’, which we call the ‘add operator’. In particular we will create a bar chart using geom_bar() from ggplot2 (Wickham 2016).\n\ncleaned_elections_data |> \n  ggplot(aes(x = elected_party)) + # aes abbreviates 'aesthetics' and enables\n  #  us to specify the x axis variable\n  geom_bar()\n\n\n\n\nThis accomplishes what we set out to do. But we can make it look a bit nicer by modifying the default options (Figure 2.9).\n\ncleaned_elections_data |> \n  ggplot(aes(x = elected_party)) +\n  geom_bar() +\n  theme_minimal() + # Make the theme neater\n  coord_flip() + # Swap the x and y axis to make parties easier to read\n  labs(x = \"Party\",\n       y = \"Number of seats\") # Make the labels more meaningful\n\n\n\n\nFigure 2.9: Number of seats won, by political party, at the 2019 Australian Federal Election\n\n\n\n\n\n\n2.2.5 Communicate\nTo this point we have downloaded some data, cleaned it, and made a graph. We would typically need to communicate what we have done at some length. In this case, we can write a few paragraphs about what we did, why we did it, and what we found to conclude our workflow. An example follows.\n\nAustralia is a parliamentary democracy with 151 seats in the House of Commons, which is the house that forms government. There are two major parties—‘Liberal’ and ‘Labor’—two minor parties—‘Nationals’ and ‘Greens’—and many smaller parties. The 2019 Federal Election occurred on 18 May, and more than 14 million votes were cast. We were interested in the number of seats that were won by each party.\nWe downloaded the results, on a seat-specific basis, from the Australian Electoral Commission website. We cleaned and tidied the dataset using the statistical programming language R (R Core Team 2021) including the packages tidyverse (Wickham et al. 2019) and janitor (Firke 2020). We then created a graph of the number of seats that each political party won (Figure 2.9).\nWe found that the Labor Party won 68 seats, followed by the Liberal Party with 67 seats. The minor parties won the following number of seats: Nationals, 10 seats and the Green Party, 1 seats. Finally, candidates from five other parties were elected.\nThe distribution of seats is skewed toward the two major parties which could reflect relatively stable preferences on the part of Australian voters, or possibly inertia due to the benefits of already being a major party such a national network and funding, or some other reason. A better understanding the reasons for this distribution are of interest in future work. While the dataset consists of everyone who voted, it worth noting that in Australia some are systematically excluded from voting; and it is much difficult for some to vote than others."
  },
  {
    "objectID": "02-drinking_from_a_fire_hose.html#toronto-homelessness",
    "href": "02-drinking_from_a_fire_hose.html#toronto-homelessness",
    "title": "2  Drinking from a fire hose",
    "section": "2.3 Toronto homelessness",
    "text": "2.3 Toronto homelessness\nToronto has a large homeless population (City of Toronto 2021). Freezing winters mean it is important there are enough places in shelters. In this example we will make a table of shelter usage in the second half of 2021 that compares average use in each month. Our expectation is that there is greater usage in the colder months, for instance, December, compared with warmer months, for instance, July.\n\n2.3.1 Plan\nThe dataset that we are interested in would need to have date, the shelter, and the number of beds that were occupied that night. A quick sketch of a dataset that would work is Figure 2.10.\n\n\n\nFigure 2.10: Quick sketch of a dataset that could be useful for understanding shelter usage in Toronto\n\n\nWe are interested in creating a table that has the monthly average number of beds occupied each night. The table would probably look something like Figure 2.11\n\n\n\nFigure 2.11: Quick sketch of a table of the average number of beds occupied each month\n\n\n\n\n2.3.2 Simulate\nThe next step is to simulate some data that could resemble our dataset. Simulation provides us with an opportunity to deeply think about our data generating process.\nWithin R Studio Cloud make a new Quarto Document, save it, and make a new R code chunk and add preamble documentation. Then install and/or load the libraries that are needed. We will again use tidyverse (Wickham 2017), janitor (Firke 2020), and tidyr (Wickham 2021). As those were installed earlier, they do not need to be installed again. In this example we will also use lubridate (Grolemund and Wickham 2011), which is part of the tidyverse and so it does not need to be installed independently. We will also use opendatatoronto (Gelfand 2020), and knitr (Xie 2021) and these will need to be installed.\n\n#### Preamble ####\n# Purpose: Get data about 2021 houseless shelter usage and make a table\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 January 2022\n# Prerequisites: - \n\n#### Workspace set-up ####\ninstall.packages(\"opendatatoronto\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"knitr\")\n\nlibrary(knitr)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(opendatatoronto)\nlibrary(tidyverse)\nlibrary(tidyr)\n\n\n\n\nTo add a bit more detail to the earlier example, libraries contain code that other people have written. There are a few common ones that you will see regularly, especially the tidyverse. To use a package, we must first install it and then we need to load it. A package only needs to be installed once per computer but must be loaded every time. So, the packages that we installed earlier do not need to be reinstalled here.\nGiven that folks freely gave up their time to make R and the packages that we use, it is important to cite them. To get the information that is needed, we can use citation(). When run without any arguments, that provides the citation information for R itself, and when run with an argument that is the name of a package, it provides the citation information for that package.\n\ncitation() # Get the citation information for R\n\n\nTo cite R in publications use:\n\n  R Core Team (2021). R: A language and environment for statistical\n  computing. R Foundation for Statistical Computing, Vienna, Austria.\n  URL https://www.R-project.org/.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\ncitation('tidyverse') # Get the citation information for a particular package\n\n\n  Wickham et al., (2019). Welcome to the tidyverse. Journal of Open\n  Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\n\n\nTurning to the simulation, we need three columns: ‘date’, ‘shelter’, and ‘occupancy’. This example will build on the earlier one by adding a seed using set.seed(). A seed enables us to always generate the same random data whenever we run the same code. Any integer can be used as the seed. In this case the seed will be 853. If you use that as your seed, then you should get the same random numbers as in this example. If you use a different seed, then you should expect different random numbers. Finally, we use rep() to repeat something a certain number of times. For instance, we repeat ‘Shelter 1’, 184 times which accounts fora about half a year.\n\n#### Simulate ####\nset.seed(853)   \n\nsimulated_occupancy_data <- \n  tibble(\n    date = rep(x = as.Date(\"2021-07-01\") + c(0:183), times = 3), \n    # Based on Dirk Eddelbuettel: https://stackoverflow.com/a/21502386\n    shelter = c(rep(x = \"Shelter 1\", times = 184), \n                rep(x = \"Shelter 2\", times = 184),\n                rep(x = \"Shelter 3\", times = 184)),\n    number_occupied = \n      rpois(n = 184*3,\n            lambda = 30) # Draw 552 times from the Poisson distribution\n    )\n\nhead(simulated_occupancy_data)\n\n# A tibble: 6 × 3\n  date       shelter   number_occupied\n  <date>     <chr>               <int>\n1 2021-07-01 Shelter 1              28\n2 2021-07-02 Shelter 1              29\n3 2021-07-03 Shelter 1              35\n4 2021-07-04 Shelter 1              25\n5 2021-07-05 Shelter 1              21\n6 2021-07-06 Shelter 1              30\n\n\nIn this simulation we first create a list of all the dates in 2021. We repeat that list three times. We assume data for three shelters for every day of the year. To simulate the number of beds that are occupied each night, we draw from a Poisson distribution, assuming a mean number of 30 beds occupied per shelter.\n\n\n2.3.3 Acquire\nWe use data made available about Toronto homeless shelters by the City of Toronto. The premise of the data is that each night at 4am a count is made of the occupied beds. To access the data, we use opendatatoronto (Gelfand 2020) and then save our own copy.\n\n#### Acquire ####\n# Based on code from: \n# https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n# Thank you to Heath Priston for assistance\ntoronto_shelters <- \n  # Each package is associated with a unique id which can be found in \n  # 'For Developers':\n  # https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n  list_package_resources(\"21c83b32-d5a8-4106-a54f-010dbe49f6f2\") |> \n  # Within that package, we are interested in the 2021 dataset\n  filter(name == \"daily-shelter-overnight-service-occupancy-capacity-2021\") |> \n  # Having reduce the dataset down to one row we can get the resource\n  get_resource()\n\nwrite_csv(\n  x = toronto_shelters, \n  file = \"toronto_shelters.csv\"\n  )\n\nhead(toronto_shelters)\n\n\n\n\n\n\n# A tibble: 6 × 32\n    `_id` OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME        SHELTER_ID\n    <dbl> <date>                   <dbl> <chr>                         <dbl>\n1 7272806 2021-01-01                  24 COSTI Immigrant Services         40\n2 7272807 2021-01-01                  24 COSTI Immigrant Services         40\n3 7272808 2021-01-01                  24 COSTI Immigrant Services         40\n4 7272809 2021-01-01                  24 COSTI Immigrant Services         40\n5 7272810 2021-01-01                  24 COSTI Immigrant Services         40\n6 7272811 2021-01-01                  24 COSTI Immigrant Services         40\n# … with 27 more variables: SHELTER_GROUP <chr>, LOCATION_ID <dbl>,\n#   LOCATION_NAME <chr>, LOCATION_ADDRESS <chr>, LOCATION_POSTAL_CODE <chr>,\n#   LOCATION_CITY <chr>, LOCATION_PROVINCE <chr>, PROGRAM_ID <dbl>,\n#   PROGRAM_NAME <chr>, SECTOR <chr>, PROGRAM_MODEL <chr>,\n#   OVERNIGHT_SERVICE_TYPE <chr>, PROGRAM_AREA <chr>, SERVICE_USER_COUNT <dbl>,\n#   CAPACITY_TYPE <chr>, CAPACITY_ACTUAL_BED <dbl>, CAPACITY_FUNDING_BED <dbl>,\n#   OCCUPIED_BEDS <dbl>, UNOCCUPIED_BEDS <dbl>, UNAVAILABLE_BEDS <dbl>, …\n\n\nNot much needs to be done to this to make it similar to the dataset that we were interested in (Figure 2.10). We need to change the names to make them easier to type using clean_names(), reduce the columns to only those that are relevant using select(), and only keep the second half of the year using filter().\n\ntoronto_shelters_clean <- \n  clean_names(toronto_shelters) |> \n  select(occupancy_date, id, occupied_beds) |> \n  filter(occupancy_date >= as_date(\"2021-07-01\"))\n\nhead(toronto_shelters_clean)\n\n# A tibble: 6 × 3\n  occupancy_date      id occupied_beds\n  <date>           <dbl>         <dbl>\n1 2021-12-27     7323151            50\n2 2021-12-27     7323152            18\n3 2021-12-27     7323153            28\n4 2021-12-27     7323154            50\n5 2021-12-27     7323155            NA\n6 2021-12-27     7323156            NA\n\n\nAll that remains is to save the cleaned dataset.\n\nwrite_csv(\n  x = toronto_shelters_clean, \n  file = \"cleaned_toronto_shelters.csv\"\n  )\n\n\n\n\n\n\n2.3.4 Explore\nFirst, we load the dataset that we just created.\n\n\n\n\n#### Explore ####\ntoronto_shelters_clean <- \n  read_csv(\n    \"cleaned_toronto_shelters.csv\",\n    show_col_types = FALSE\n    )\n\nThe dataset contains records on a daily basis for each shelter. We are interested in understanding average usage for each month. To do this, we need to and add a month column, which we do using month() from lubridate (Grolemund and Wickham 2011). By default, month() provides the number of the month, and so we include two arguments ‘label’ and ‘abbr’ to get the full name of the month. We remove rows that do not have any data for the number of beds using drop_na() from tidyr. And we then create a summary statistic on the basis of monthly groups, using summarize() from dplyr (Wickham et al. 2020). We use kable() from knitr (Xie 2021) to create a table.\n\n# Based on code from Florence Vallée-Dubois and Lisa Lendway\ntoronto_shelters_clean |>\n  mutate(occupancy_month = month(occupancy_date, \n                                 label = TRUE, \n                                 abbr = FALSE)) |>\n  drop_na(occupied_beds) |> # We only want rows that have data\n  group_by(occupancy_month) |> # We want to know the occupancy by month\n  summarize(number_occupied = mean(occupied_beds)) |>\n  kable()\n\n\n\n\noccupancy_month\nnumber_occupied\n\n\n\n\nJuly\n29.67137\n\n\nAugust\n30.83975\n\n\nSeptember\n31.65405\n\n\nOctober\n32.32991\n\n\nNovember\n33.26980\n\n\nDecember\n33.57806\n\n\n\n\n\nAs with before, this looks fine, and achieves what we set out to do. But we can make some tweaks to the defaults to make it look even better (Table 2.1). We can add a caption, make the column names easier to read, only show an appropriate level of decimal places, and improve the formatting.\n\ntoronto_shelters_clean |>\n  mutate(occupancy_month = month(occupancy_date, \n                                 label = TRUE, \n                                 abbr = FALSE)) |>\n  drop_na(occupied_beds) |> # We only want rows that have data\n  group_by(occupancy_month) |> # We want to know the occupancy by month\n  summarize(number_occupied = mean(occupied_beds)) |> \n  kable(col.names = c(\"Month\", \"Average daily number of occupied beds\"),\n        digits = 1,\n        booktabs = TRUE,\n        linesep = \"\"\n        )\n\n\n\nTable 2.1: Homeless shelter usage in Toronto in 2021\n\n\nMonth\nAverage daily number of occupied beds\n\n\n\n\nJuly\n29.7\n\n\nAugust\n30.8\n\n\nSeptember\n31.7\n\n\nOctober\n32.3\n\n\nNovember\n33.3\n\n\nDecember\n33.6\n\n\n\n\n\n\n\n\n2.3.5 Communicate\nWe need to write a few brief paragraphs about what we did, why we did it, and what we found to sum up our work. An example follows.\n\nToronto has a large homeless population. Freezing winters mean it is critical there are enough places in shelters. We are interested to understand how usage of shelters changes in colder months, compared with warmer months.\nWe use data provided by the City of Toronto about Toronto homeless shelter bed occupancy. Specifically, at 4am each night a count is made of the occupied beds. We are interested in averaging this over the month. We cleaned, tidied, and analyzed the dataset using the statistical programming language R (R Core Team 2021) as well as the packages tidyverse (Wickham 2017), janitor (Firke 2020), tidyr (Wickham 2021), opendatatoronto (Gelfand 2020), lubridate (Grolemund and Wickham 2011), and knitr (Xie 2021). We then made a table of the average number of occupied beds each night for each month (Table 2.1).\nWe found that the daily average number of occupied beds was higher in December 2021 than July 2021, with 34 occupied beds in December, compared with 30 in July (Table 2.1). More generally, there was a steady increase in the daily average number of occupied beds between July and December, with a slight overall increase each month.\nThe dataset is on the basis of shelters, and so our results may be skewed by changes that are specific to especially large or especially small shelters. It may be that particular shelters are especially attractive in colder months. Additionally, we were concerned with counts of the number of occupied beds, but if the supply of beds changes over the season, then an additional statistic of interest would be proportion occupied.\n\nAlthough this example is only a few paragraphs, it could be reduced to form an abstract, or increased to form a full report. The first paragraph is a general overview, the second focuses on the data, the third on the results, and the fourth is a discussion. Each of these could be expanded to form sections of a short report."
  },
  {
    "objectID": "02-drinking_from_a_fire_hose.html#neonatal-mortality",
    "href": "02-drinking_from_a_fire_hose.html#neonatal-mortality",
    "title": "2  Drinking from a fire hose",
    "section": "2.4 Neonatal mortality",
    "text": "2.4 Neonatal mortality\nNeonatal mortality refers to a death that occurs within the first month of life, and in particular, the neonatal mortality rate (NMR) is the number of neonatal deaths per 1,000 live births (UN IGME 2021). Reducing it is part of the third Sustainable Development Goal (Hug et al. 2019). In this example we will create a graph of the estimated NMR for the past fifty years for: Argentina, Australia, Canada, and Kenya.\n\n2.4.1 Plan\nFor this example, we need to think about what our dataset should look like, and what the graph should look like.\nThe dataset needs to have columns that specify the country, and the year. It also needs to have a column with the NMR estimate for that year for that country. Roughly, it should look like Figure 2.12.\n\n\n\nFigure 2.12: Quick sketch of a potentially useful NMR dataset\n\n\nWe are interested to make a graph with year on the x-axis and estimated NMR on the y-axis. Each country should have its own series similar to Figure 2.13\n\n\n\nFigure 2.13: Quick sketch of a graph of NMR by country over time\n\n\n\n\n2.4.2 Simulate\nWe would like to simulate some data that aligns with our plan. In this case we will need three columns: country, year, and NMR.\nWithin R Studio Cloud, make a new Quarto Document and save it. Add preamble documentation and set-up the workspace. We will use tidyverse (Wickham 2017), janitor (Firke 2020), and lubridate (Grolemund and Wickham 2011).\n\n#### Preamble ####\n# Purpose: Obtain and prepare data about neonatal mortality for four\n# countries for the past fifty years and create a graph.\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 January 2022\n# Prerequisites: - \n\n#### Workspace set-up ####\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(tidyverse)\n\n\n\n\nThe code contained in libraries can change from time to time as the authors update it and release new versions. We can see which version of a package we are using with packageVersion(). For instance, we are using version 1.3.1 of the tidyverse and version 2.1.0 of janitor.\n\npackageVersion('tidyverse')\n\n[1] '1.3.1'\n\npackageVersion('janitor')\n\n[1] '2.1.0'\n\n\nTo update the version of the package, we use update.packages().\n\nupdate.packages()\n\nThis does not need to be run, say, every day, but from time-to-time it is worth updating packages. While many packages take care to ensure backward compatibility, at a certain point this does not become reasonable, and so it is important to be aware the updating packages can result in old code needing to be updated.\nReturning to the simulation, we repeat the name of each country 50 times with rep(), and enable the passing of 50 years. Finally, we draw from the uniform distribution with runif() to simulate an estimated NMR value for that year for that country.\n\n#### Simulate data ####\nset.seed(853)\n\nsimulated_nmr_data <- \n  tibble(\n    country = \n      c(\n        rep('Argentina', 50),\n        rep('Australia', 50),\n        rep('Canada', 50),\n        rep('Kenya', 50)\n        ),\n    year = \n      rep(c(1971:2020), 4),\n    nmr = \n      runif(n = 200,\n            min = 0, \n            max = 100)\n  )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  <chr>     <int> <dbl>\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4 \n\n\nWhile this simulation works, it would be time-consuming and error-prone if we decided that instead of fifty years, we were interested in simulating, say, sixty years. One way to make this easier is to replace all instances of 50 with a variable. An example follows.\n\n#### Simulate data ####\nset.seed(853)\n\nnumber_of_years <- 50\n\nsimulated_nmr_data <- \n  tibble(\n    country = \n      c(\n        rep('Argentina', number_of_years),\n        rep('Australia', number_of_years),\n        rep('Canada', number_of_years),\n        rep('Kenya', number_of_years)\n        ),\n    year = \n      rep(c(1:number_of_years + 1970), 4),\n    nmr = \n      runif(n = number_of_years * 4,\n            min = 0, \n            max = 100)\n  )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  <chr>     <dbl> <dbl>\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4 \n\n\nThe result will be the same, but now if we want to change from fifty to sixty years is to make the change in one place.\nWe can have confidence in this simulated dataset because it is relatively straight-forward, and we wrote the code for it. But when we turn to the real dataset, it is more difficult to be sure that it is what it claims to be. Even if we trust the data, it is important that we can share that confidence with others. One way forward is to establish some checks that prove our data are as they should be. For instance, we expect that:\n\n‘country’ is exclusively one of these four: ‘Argentina’, ‘Australia’, ‘Canada’, or ‘Kenya’.\nConversely, ‘country’ contains all those four countries.\n‘year’ is no smaller than 1971 and no larger than 2020, and is an integer, not a letter or a number with decimal places.\n‘nmr’ is a value somewhere between 0 and 1,000, and is a number.\n\nWe can write a series of tests based on these features, that we expect that dataset to pass.\n\n# Tests for simulated data\nsimulated_nmr_data$country |> \n  unique() == c(\"Argentina\", \n                \"Australia\", \n                \"Canada\", \n                \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\nsimulated_nmr_data$country |> unique() |> length() == 4\n\n[1] TRUE\n\nsimulated_nmr_data$year |> min() == 1971\n\n[1] TRUE\n\nsimulated_nmr_data$year |> max() == 2020\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |> min() >= 0\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |> max() <= 1000\n\n[1] TRUE\n\nsimulated_nmr_data$nmr |> class() == \"numeric\"\n\n[1] TRUE\n\n\nHaving passed these tests, we can have confidence in the simulated dataset. More importantly, we can apply these tests to the real dataset. This enables us to have greater confidence in that dataset and to share that confidence with others.\n\n\n2.4.3 Acquire\nThe UN Inter-agency Group for Child Mortality Estimation (IGME) provides estimates of the NMR – https://childmortality.org/ – that we can download and save.\n\n#### Acquire data ####\nraw_igme_data <- \n  read_csv(\n    file =\n      \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n    show_col_types = FALSE) \n\nwrite_csv(\n  x = raw_igme_data, \n  file = \"igme.csv\"\n  )\n\n\n\n\nWe can take a quick look to get a better sense of it. We might be interested in what the dataset seems to look like (using head() and tail()), and what the names of the columns are (using names()).\n\nhead(raw_igme_data)\n\n# A tibble: 6 × 29\n  `Geographic area` Indicator Sex   `Wealth Quinti…` `Series Name` `Series Year`\n  <chr>             <chr>     <chr> <chr>            <chr>         <chr>        \n1 Afghanistan       Neonatal… Total Total            Multiple Ind… 2003         \n2 Afghanistan       Neonatal… Total Total            Multiple Ind… 2003         \n3 Afghanistan       Neonatal… Total Total            Multiple Ind… 2003         \n4 Afghanistan       Neonatal… Total Total            Multiple Ind… 2003         \n5 Afghanistan       Neonatal… Total Total            Multiple Ind… 2003         \n6 Afghanistan       Neonatal… Total Total            Afghanistan … 2010         \n# … with 23 more variables: `Regional group` <chr>, TIME_PERIOD <chr>,\n#   OBS_VALUE <dbl>, COUNTRY_NOTES <chr>, CONNECTION <lgl>,\n#   DEATH_CATEGORY <lgl>, CATEGORY <chr>, `Observation Status` <chr>,\n#   `Unit of measure` <chr>, `Series Category` <chr>, `Series Type` <chr>,\n#   STD_ERR <dbl>, REF_DATE <dbl>, `Age Group of Women` <chr>,\n#   `Time Since First Birth` <chr>, DEFINITION <chr>, INTERVAL <dbl>,\n#   `Series Method` <chr>, LOWER_BOUND <dbl>, UPPER_BOUND <dbl>, …\n\nnames(raw_igme_data)\n\n [1] \"Geographic area\"        \"Indicator\"              \"Sex\"                   \n [4] \"Wealth Quintile\"        \"Series Name\"            \"Series Year\"           \n [7] \"Regional group\"         \"TIME_PERIOD\"            \"OBS_VALUE\"             \n[10] \"COUNTRY_NOTES\"          \"CONNECTION\"             \"DEATH_CATEGORY\"        \n[13] \"CATEGORY\"               \"Observation Status\"     \"Unit of measure\"       \n[16] \"Series Category\"        \"Series Type\"            \"STD_ERR\"               \n[19] \"REF_DATE\"               \"Age Group of Women\"     \"Time Since First Birth\"\n[22] \"DEFINITION\"             \"INTERVAL\"               \"Series Method\"         \n[25] \"LOWER_BOUND\"            \"UPPER_BOUND\"            \"STATUS\"                \n[28] \"YEAR_TO_ACHIEVE\"        \"Model Used\"            \n\n\nWe would like to clean up the names and only keep the rows and columns that we are interested in. Based on our plan, we are interested in rows where ‘Sex’ is ‘Total’, ‘Series Name’ is ‘UN IGME estimate’, ‘Geographic area’ is one of ‘Argentina’, ‘Australia’, ‘Canada’, and ‘Kenya’, and the ‘Indicator’ is ‘Neonatal mortality rate’. After this we are interested in just a few columns: ‘geographic_area’, ‘time_period’, and ‘obs_value’.\n\ncleaned_igme_data <- \n  clean_names(raw_igme_data) |> \n  filter(sex == 'Total',\n         series_name == 'UN IGME estimate',\n         geographic_area %in% \n           c('Argentina', 'Australia', 'Canada', 'Kenya'),\n         indicator == 'Neonatal mortality rate') |> \n  select(geographic_area,\n         time_period,\n         obs_value)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 × 3\n  geographic_area time_period obs_value\n  <chr>           <chr>           <dbl>\n1 Argentina       1970-06          24.9\n2 Argentina       1971-06          24.7\n3 Argentina       1972-06          24.6\n4 Argentina       1973-06          24.6\n5 Argentina       1974-06          24.5\n6 Argentina       1975-06          24.1\n\n\nFinally, we need to fix two final aspects: the class of ‘time_period’ is character when we need it to be a year, and the name of ‘obs_value’ should be ‘nmr’ to be more informative.\n\ncleaned_igme_data <- \n  cleaned_igme_data |> \n  mutate(time_period = str_remove(time_period, \"-06\"),\n         time_period = as.integer(time_period)) |> \n  filter(time_period >= 1971) |> \n  rename(nmr = obs_value,\n         year = time_period,\n         country = geographic_area)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  <chr>     <int> <dbl>\n1 Argentina  1971  24.7\n2 Argentina  1972  24.6\n3 Argentina  1973  24.6\n4 Argentina  1974  24.5\n5 Argentina  1975  24.1\n6 Argentina  1976  23.3\n\n\nFinally, we can check that our dataset passes the tests that we developed based on the simulated dataset.\n\n# Test the cleaned dataset\ncleaned_igme_data$country |> \n  unique() == c(\"Argentina\", \n                \"Australia\", \n                \"Canada\", \n                \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\ncleaned_igme_data$country |> unique() |> length() == 4\n\n[1] TRUE\n\ncleaned_igme_data$year |> min() == 1971\n\n[1] TRUE\n\ncleaned_igme_data$year |> max() == 2020\n\n[1] TRUE\n\ncleaned_igme_data$nmr |> min() >= 0\n\n[1] TRUE\n\ncleaned_igme_data$nmr |> max() <= 1000\n\n[1] TRUE\n\ncleaned_igme_data$nmr |> class() == \"numeric\"\n\n[1] TRUE\n\n\nAll that remains is to save the nicely cleaned dataset.\n\nwrite_csv(\n  x = cleaned_igme_data, \n  file = \"cleaned_igme_data.csv\"\n  )\n\n\n\n\n\n\n2.4.4 Explore\nWe would like to make a graph of estimated NMR using the cleaned dataset. First, we read in the dataset.\n\n\n\n\n#### Explore ####\ncleaned_igme_data <- \n  read_csv(\n    file = \"cleaned_igme_data.csv\",\n    show_col_types = FALSE\n    )\n\nWe can now make the graph that we are interested in (Figure 2.14). We are interested in showing how NMR has changed over time and the differences between countries.\n\ncleaned_igme_data |> \n  ggplot(aes(x = year, y = nmr, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Neonatal Mortality Rate (NMR)\",\n       color = \"Country\") +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 2.14: Neonatal Mortality Rate (NMR), for Argentina, Australia, Canada, and Kenya (1971-2020)\n\n\n\n\n\n\n2.4.5 Communicate\nTo this point we downloaded some data, cleaned it, wrote some tests, and made a graph. We would typically need to communicate what we have done at some length. In this case, we will write a few paragraphs about what we did, why we did it, and what we found.\n\nNeonatal mortality refers to a death that occurs within the first month of life. In particular, the neonatal mortality rate (NMR) is the number of neonatal deaths per 1,000 live births (Alexander and Alkema 2018). We obtain estimates for NMR for four countries—Argentina, Australia, Canada, China, and Kenya—over the past fifty years.\nThe UN Inter-agency Group for Child Mortality Estimation (IGME) provides estimates of the NMR at the website: https://childmortality.org/. We downloaded their estimates then cleaned and tidied the dataset using the statistical programming language R (R Core Team 2021).\nWe found considerable change in the estimated NMR over time and between the four countries of interest (Figure 2.14). We found that the 1970s tended to be associated with reductions in the estimated NMR. Australia and Canada were estimated to have a low NMR at that point and remained there through 2020, with slight improvements. The estimates for Argentina and Kenya continued to have substantial reductions through 2020.\nOur results suggest considerable improvements in estimated NMR over time. But it is worth emphasizing that estimates of the NMR are based on a statistical model and underlying data. The paradox of data availability is that often high-quality data are less easily available for countries with worse outcomes. For instance, Alexander and Alkema (2018) say ‘[t]here is large variability in the availability of data on neonatal mortality’. Our conclusions are subject to the model that underpins the estimates, and the quality of the underlying data and we did not independently verify either of these."
  },
  {
    "objectID": "02-drinking_from_a_fire_hose.html#exercises-and-tutorial",
    "href": "02-drinking_from_a_fire_hose.html#exercises-and-tutorial",
    "title": "2  Drinking from a fire hose",
    "section": "2.5 Exercises and tutorial",
    "text": "2.5 Exercises and tutorial\n\n2.5.1 Exercises\n\nFollowing Barrett (2021), please write a stack of four or five atomic habits, related to learning data science, that you could implement this week.\nWhat is not one of the four challenges for mitigating bias mentioned in Hao (2019) (pick one)?\n\nUnknown unknowns.\nImperfect processes.\nThe definitions of fairness.\nLack of social context.\nDisinterest given profit considerations.\n\nWhen was the dataset that underpins Chambliss (1989) collected (pick one)?\n\nAugust 1983 to August 1984\nJanuary 1983 to August 1984\nJanuary 1983 to January 1984\nAugust 1983 to January 1984\n\nWhen Chambliss (1989) talks of stratification, what is he talking about?\nHow does Chambliss (1989) define ‘excellence’ (pick one)?\n\nProlonged performance at world-class level.\nAll Olympic medal winners.\nConsistent superiority of performance.\nAll national-level athletes.\n\nThink about the following quote from Chambliss (1989, 81) and list three small skills or activities that could help you achieve excellence in data science.\n\n\nExcellence is mundane. Superlative performance is really a confluence of dozens of small skills or activities, each one learned or stumbled upon, which have been carefully drilled into habit and then are fitted together in a synthesized whole. There is nothing extraordinary or super-human in any one of those actions; only the fact that they are done consistently and correctly, and all together, produce excellence.\n\n\nWhich of the following are arguments for read_csv() from readr (Wickham, Hester, and Bryan 2021) (select all that apply)? (Hint: You can access the help for the function with ?readr::read_csv().)\n\n‘all_cols’\n‘file’\n‘show_col_types’\n‘number’\n\nWe used rpois() and runif() to draw from the Poisson and Uniform distributions, respectively. Which of the following can be used to draw from the Normal and Binomial distributions (select all that apply)?\n\nrnormal() and rbinom()\nrnorm() and rbinomial()\nrnormal() and rbinomial()\nrnorm() and rbinom()\n\nWhat is the result of sample(x = letters, size = 2) when the seed is set to ‘853’? What about when the seed is set to ‘1234’ (pick one)?\n\n‘“i” “q”’ and ‘“p” “v”’\n‘“e” “l”’ and ‘“e” “r”’\n‘“i” “q”’ and ‘“e” “r”’\n‘“e” “l”’ and ‘“p” “v”’\n\nWhich function provides the recommended citation to cite R (pick one)?\n\ncite('R').\ncite().\ncitation('R').\ncitation().\n\nHow do we get the citation information for opendatatoronto (pick one)?\n\ncite()\ncitation()\ncite(‘opendatatoronto’)\ncitation(‘opendatatoronto’)\n\nWhich argument needs to be changed to change the headings in kable() from knitr (Xie 2021) (pick one)?\n\n‘booktabs’\n‘col.names’\n‘digits’\n‘linesep’\n‘caption’\n\nWhich function is used to update packages (pick one)?\n\nupdate.packages()\nupgrade.packages()\nrevise.packages()\nrenovate.packages()\n\nWhat are some features that we might typically expect of a column that claimed to be a year (select all that apply)?\n\nThe class is ‘character’.\nThere are no negative numbers.\nThere are letters in the column.\nEach entry has four digits.\n\nPlease consider the following code, add a small mistake to it, and then create a GitHub Gist that contains all of the code, and submit the URL.\n\n\nlibrary(tidyverse)\n\nmidwest |> \n  ggplot(aes(x = poptotal, y = popdensity, color = state)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n2.5.2 Tutorial\nThe purpose of this tutorial is to provide an opportunity to do a small self-contained project. We will redo the Australian Elections worked example, but for Canada.\nCanada is a parliamentary democracy with 338 seats in the House of Commons, which is the lower house and that from which government is formed. There are two major parties – ‘Liberal’ and ‘Conservative’ – three minor parties – ‘Bloc Québécois’, ‘New Democratic’, and ‘Green’ – and many smaller parties and independents. In this example we will create a graph of the number of seats that each party won in the 2019 Federal Election.\nBegin by planning what the dataset that we need will look like, and what the final graph will look like. The basic requirement for the dataset is that it has the name of the seat (sometimes called a ‘riding’ in Canada) and the party of the person elected.\nPlease do a quick sketch of the dataset that we would need. And then do a quick sketch of a graph that we might be interested in.\nThen put together a Quarto Document that simulates some data. Add preamble documentation, then load the packages that are needed: tidyverse, janitor, and tidyr. Add numbers for the riding, then use sample() to randomly choose one of six options, with replacement, 338 times.\nNext we need to get the actual data, from Elections Canada, and the file that we need to download is: “https://www.elections.ca/res/rep/off/ovr2019app/51/data_donnees/table_tableau11.csv”.\nClean the names, and then select the two columns that are of interest: ‘electoral_district_name_nom_de_circonscription’, and ‘elected_candidate_candidat_elu’. Finally, rename the columns to remove the French and simplify the names.\nThe column that we need is about the elected canadidates. That has the surname of the elected candidate, followed by a comma, followed by their first name, followed by a space, followed by the name of the party in both English and French, separated by a slash. Break-up this column into its pieces using separate() from tidyr (Wickham 2021).\n\ncleaned_elections_data <- \n  cleaned_elections_data |> \n  # Separate the column into two based on the slash\n  separate(col = elected_candidate,\n           into = c('other', 'party'),\n           sep = '/') |> \n  # Remove the 'other' column\n  select(-other)\n\nThen recode the party names from French to English to match what we simulated.\nAt this point we can make a nice graph of the number of ridings won by each party in the 2019 Canadian Federal Election.\n\n\n\n\n\nAlexander, Monica, and Leontine Alkema. 2018. “Global Estimation of Neonatal Mortality Using a Bayesian Hierarchical Splines Regression Model.” Demographic Research 38: 335–72.\n\n\nBarrett, Malcolm. 2021. Data Science as an Atomic Habit. https://malco.io/2021/01/04/data-science-as-an-atomic-habit/.\n\n\nChambliss, Daniel F. 1989. “The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers.” Sociological Theory 7 (1): 70–86.\n\n\nCity of Toronto. 2021. 2021 Street Needs Assessment. https://www.toronto.ca/city-government/data-research-maps/research-reports/housing-and-homelessness-research-and-reports/.\n\n\nFirke, Sam. 2020. Janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGelfand, Sharla. 2020. Opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. http://www.jstatsoft.org/v40/i03/.\n\n\nHao, Karen. 2019. “This is how AI bias really happens—and why it’s so hard to fix.” MIT Technology Review.\n\n\nHug, Lucia, Monica Alexander, Danzhen You, Leontine Alkema, and UN Inter-agency Group for Child. 2019. “National, Regional, and Global Levels and Trends in Neonatal Mortality Between 1990 and 2017, with Scenario-Based Projections to 2030: A Systematic Analysis.” The Lancet Global Health 7 (6): e710–20.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nUN IGME. 2021. “Levels and Trends in Child Mortality, 2021.” https://childmortality.org/wp-content/uploads/2021/12/UNICEF-2021-Child-Mortality-Report.pdf.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2017. Tidyverse: Easily Install and Load the ’Tidyverse’. https://CRAN.R-project.org/package=tidyverse.\n\n\n———. 2021. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2020. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2021. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nXie, Yihui. 2021. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/."
  },
  {
    "objectID": "03-r_essentials.html",
    "href": "03-r_essentials.html",
    "title": "3  R essentials",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "03-r_essentials.html#background",
    "href": "03-r_essentials.html#background",
    "title": "3  R essentials",
    "section": "3.1 Background",
    "text": "3.1 Background\nIn this chapter we focus on foundational skills needed to use the statistical programming language R (R Core Team 2021) to tell stories with data. Some of it may not make sense at first, but these are skills and approaches that we will often use. You should initially just go through this chapter quickly, noting aspects that you do not understand. And then come back to this chapter from time to time as you continue through the rest of the book. That way you will see how the various bits fit into context.\nR is an open-source language for statistical programming. You can download R for free from the Comprehensive R Archive Network (CRAN). R Studio is an Integrated Development Environment (IDE) for R which makes the language easier to use and can be downloaded for free from R Studio.\nThe past ten years or so, have been characterized by the increased use of the tidyverse. This is ‘…an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures’ (Wickham 2020b). There are three distinctions to be clear about: the original R language, typically referred to as ‘base’; the ‘tidyverse’ which is a coherent collection of packages that build on top of base, and other packages.\nEssentially everything that we can do in the tidyverse, we can also do in base. But, as the tidyverse was built especially for data science it is often easier to use the tidyverse, especially when learning. Additionally, often everything that we can do in the tidyverse, we can also do with other packages. But, as the tidyverse is a coherent collection of packages, it is often easier to use the tidyverse, again, especially when learning. Eventually there are cases where it makes sense to trade-off the convenience and coherence of the tidyverse for some features of base or other packages. Indeed, we will see that at various points later in this book. For instance, the tidyverse can be slow, and so if one needs to import thousands of CSVs then it can make sense to switch away from read_csv(). The appropriate use of base and non-tidyverse packages, or even other languages, rather than dogmatic insistence on a particular solution, is a sign of intellectual maturity.\nCentral to our use of the statistical programming language R is data, and most of the data that we use will have humans at the heart of it. Sometimes, dealing with human-centered data in this way can have a numbing effect, results in over-generalization, and potentially problematic work. Another sign of intellectual maturity is when it has the opposite effect.\n\nIn practice, I find that far from distancing you from questions of meaning, quantitative data forces you to confront them. The numbers draw you in. Working with data like this is an unending exercise in humility, a constant compulsion to think through what you can and cannot see, and a standing invitation to understand what the measures really capture—what they mean, and for whom.\nHealy (2020)"
  },
  {
    "objectID": "03-r_essentials.html#broader-impacts",
    "href": "03-r_essentials.html#broader-impacts",
    "title": "3  R essentials",
    "section": "3.2 Broader impacts",
    "text": "3.2 Broader impacts\n\n“We shouldn’t have to think about the societal impact of our work because it’s hard and other people can do it for us” is a really bad argument. I stopped doing CV [computer vision] research because I saw the impact my work was having. I loved the work but the military applications and privacy concerns eventually became impossible to ignore. But basically all facial recognition work would not get published if we took Broader Impacts sections seriously. There is almost no upside and enormous downside risk. To be fair though i should have a lot of humility here. For most of grad school I bought in to the myth that science is apolitical and research is objectively moral and good no matter what the subject is.\nJoe Redmon, 20 February 2020\n\nAlthough the term ‘data science’ is ubiquitous in academia, industry, and even more generally, it is difficult to define. One deliberately antagonistic definition of data science is ‘[t]he inhumane reduction of humanity down to what can be counted’ (Keyes 2019). While purposefully controversial, this definition highlights one reason for the increased demand for data science and quantitative methods over the past decade—individuals and their behavior are now at the heart of it. Many of the techniques have been around for many decades, but what makes them popular now is this human focus.\nUnfortunately, even though much of the work may be focused on individuals, issues of privacy and consent, and ethical concerns more broadly, rarely seem front of mind. While there are some exceptions, in general, even at the same time as claiming that AI, machine learning, and data science are going to revolutionize society, consideration of these types of issues appears to have been largely treated as something that would be nice to have, rather than something that we may like to think of before we embrace the revolution.\nFor the most part, these types of issues are not new. In the sciences, there has been considerable recent ethical consideration around CRISPR technology and gene editing (Brokowski and Adli 2019), but in an earlier time similar conversations were had, for instance, about Wernher von Braun being allowed to building rockets for the US (Neufeld 2002). In medicine, of course, these concerns have been front-of-mind for some time (Association and Medicine 1848). Data science seems determined to have its own Tuskegee-moment rather than think about, and proactively deal appropriately with, these issues, based on the experiences of other fields.\nThat said, there is some evidence that data scientists are beginning to be more concerned about the ethics surrounding the practice. For instance, NeurIPS, a prestigious machine learning conference, has required a statement on ethics to accompany all submissions since 2020.\n\nIn order to provide a balanced perspective, authors are required to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. Authors should take care to discuss both positive and negative outcomes.\nNeurIPS 2020 Conference Call For Papers\n\nThe purpose of ethical consideration and concern for the broader impact of data science is not to prescriptively rule things in or out, but to provide an opportunity to raise some issues that should be paramount. The variety of data science applications, the relative youth of the field, and the speed of change, mean that such considerations are sometimes knowingly set aside, and this is acceptable to the rest of the field. This contrasts with fields such as science, medicine, engineering, and accounting. Possibly those fields are more self-aware (Figure 3.1).\n\n\n\nFigure 3.1: Probability, from Randall Munroe’s XKCD."
  },
  {
    "objectID": "03-r_essentials.html#r-r-studio-and-r-studio-cloud",
    "href": "03-r_essentials.html#r-r-studio-and-r-studio-cloud",
    "title": "3  R essentials",
    "section": "3.3 R, R Studio, and R Studio Cloud",
    "text": "3.3 R, R Studio, and R Studio Cloud\nR and R Studio are complementary, but they are not the same thing. Dr Liza Bolton, Assistant Professor, Teaching Stream, University of Toronto explains their relationship by analogy where R is like the engine and R Studio is like the car. Although some of us use a car engine directly, most of us use a car to interact with the engine.\n\n3.3.1 R\nR is an open-source and free programming language that is focused on general statistics. Free in this context does not refer to a price of zero, but instead to the freedom that the creators give users to largely do what they want with it, although it also does have a price of zero. This is in contrast with an open-source programming language that is designed for general purpose, such as Python, or an open-source programming language that is focused on probability, such as Stan. It was created by Ross Ihaka and Robert Gentleman at the University of Auckland in the 1990s, and traces its provenance to S, which was developed at Bell Labs in the 1970s. It is maintained by the R Core Team and changes to this ‘base’ of code occur methodically and with concern given to a variety of different priorities.\nMany people build on this stable base, to extend the capabilities of R to better and more quickly suit their needs. They do this by creating packages. Typically, although not always, a package is a collection of R code, mostly functions, and this allows us to more easily do things that we want to do. These packages are managed by repositories such as CRAN and Bioconductor.\nIf you want to use a package then you first need to install it on your computer, and then you need to load it when you want to use it. Dr Di Cook, Professor of Business Analytics at Monash University, describes this as analogous to a lightbulb. If you want light in your house, first you need to fit a lightbulb, and then you need to turn the switch on. Installing a package, say, install.packages(\"tidyverse\"), is akin to fitting a lightbulb into a socket—you only need to do this once for each lightbulb. But then each time you want light you need to turn on the switch to the lightbulb, which in the R packages case, means calling the library, say, library(tidyverse).\n\nShoulders of giants Dr Di Cook is Professor of Business Analytics at Monash University. After taking a PhD in statistics from Rutgers University in 1993 where she focused on statistical graphics, she was appointed as an assistant professor at Iowa State University, being promoted to full professor in 2005, and in 2015 she moved to Monash. One area of her research is data visualisation, especially interactive and dynamic graphics. One particularly important paper is Buja, Cook, and Swayne (1996) which proposes a taxonomy of interactive data visualization and associated software XGobi.\n\nTo install a package on your computer (again, we will need to do this only once per computer) we use install.packages().\n\ninstall.packages(\"tidyverse\")\n\nAnd then when we want to use the package, we use library().\n\nlibrary(tidyverse)\n\nHaving downloaded it, we can open R and use it directly. It is primarily designed to be interacted with through the command line. While this is functional, it can be useful to have a richer environment than the command line provides. In particular, it can be useful to install an Integrated Development Environment (IDE), which is an application that brings together various bits and pieces that will be used often. One common IDE for R is R Studio, although others such as Visual Studio are also used.\n\n\n3.3.2 R Studio\nR Studio is distinct to R, and they are different entities. R Studio builds on top of R to make it easier to use R. This is in the same way that one could use the internet from the command line, but most folks use a browser such as Chrome, Firefox, or Safari.\nR Studio is free in the sense that we do not pay for it. It is also free in the sense of being able to take the code, modify it, and distribute that code. But it is important to recognize that R Studio is a company and so it is possible that the current situation could change. It can be downloaded from R Studio.\nWhen we open R Studio it will look like Figure 3.2.\n\n\n\nFigure 3.2: Opening R Studio for the first time\n\n\nThe left pane is a console in which you can type and execute R code line by line. Try it with 2+2 by clicking next to the prompt ‘>’, typing ‘2+2’, and then pressing ‘return/enter’.\n\n2 + 2\n\n[1] 4\n\n\nThe pane on the top right has information about your environment. For instance, when we create variables a list of their names and some properties will appear there. Try to type the following code, replacing my name with your name, next to the prompt, and again press enter:\n\nmy_name <- \"Rohan\"\n\nYou should notice a new value in the environment pane with the variable name and its value.\nThe pane in the bottom right is a file manager. At the moment it should just have two files: an R History file and a R Project file. We will get to what these are later, but for now we will create and save a file.\nRun the following code, without worrying too much about the details for now. And you should see a new ‘.rds’ file in your list of files.\n\nsaveRDS(object = my_name, file = \"my_first_file.rds\")\n\n\n\n3.3.3 R Studio Cloud\nWhile you can and should download R Studio to your own computer, initially we will use R Studio Cloud. This is an online version that is provided by R Studio. We will use this so that you can focus on getting comfortable with R and R Studio in an environment that is consistent. This way you do not have to worry about what computer you have or installation permissions, amongst other things.\nThe free version of R Studio Cloud is free as is ‘no financial cost’. The trade-off is that it is not very powerful, and it is sometimes slow, but for the purposes of getting started it is enough."
  },
  {
    "objectID": "03-r_essentials.html#getting-started",
    "href": "03-r_essentials.html#getting-started",
    "title": "3  R essentials",
    "section": "3.4 Getting started",
    "text": "3.4 Getting started\nWe will now start going through some code. It is important to actively write this all out yourself.\nWhile working line-by-line in the console is fine, it is easier to write out a whole script that can then be run. We will do this by making an R Script (‘File’ -> ‘New File’ -> ‘R Script’). The console pane will fall to the bottom left and an R Script will open in the top left. We will write some code that will get all of the Australian federal politicians and then construct a small table about the genders of the prime ministers. Some of this code will not make sense at this stage, but just type it all out to get in the habit and then run it. To run the whole script, we can click ‘Run’ or we can highlight certain lines and then click ‘Run’ to just run those lines.\n\n# Install the packages that we need\ninstall.packages(\"tidyverse\")\ninstall.packages(\"AustralianPoliticians\")\n\n\n# Load the packages that we need to use this time\nlibrary(tidyverse)\nlibrary(AustralianPoliticians)\n\n# Make a table of the counts of genders of the prime ministers\nAustralianPoliticians::get_auspol('all') |> \n  as_tibble() |> \n  filter(wasPrimeMinister == 1) |> \n  count(gender)\n\n# A tibble: 2 × 2\n  gender     n\n  <chr>  <int>\n1 female     1\n2 male      29\n\n\nWe can see that, as at the end of 2021, one female has been prime minister (Julia Gillard), while the other 29 prime ministers were male\nOne critical operator when programming is the ‘pipe’: |>. We read this as ‘and then’. This takes the output of a line of code and uses it as the first input to the next line of code. It makes code easier to read.\nThe idea of the pipe is that we take a dataset, and then do something to it. We used this in the earlier example. Another example follows where we will look at the first six lines of a dataset by piping it to head(). Notice that head() does not explicitly take any arguments in this example. It knows which data to display because the pipe does it implicitly.\n\nAustralianPoliticians::get_auspol('all') |> \n  head()\n\n# A tibble: 6 × 20\n  uniqueID   surname allOtherNames          firstName commonName displayName    \n  <chr>      <chr>   <chr>                  <chr>     <chr>      <chr>          \n1 Abbott1859 Abbott  Richard Hartley Smith  Richard   <NA>       Abbott, Richard\n2 Abbott1869 Abbott  Percy Phipps           Percy     <NA>       Abbott, Percy  \n3 Abbott1877 Abbott  Macartney              Macartney Mac        Abbott, Mac    \n4 Abbott1886 Abbott  Charles Lydiard Aubrey Charles   Aubrey     Abbott, Aubrey \n5 Abbott1891 Abbott  Joseph Palmer          Joseph    <NA>       Abbott, Joseph \n6 Abbott1957 Abbott  Anthony John           Anthony   Tony       Abbott, Tony   \n# … with 14 more variables: earlierOrLaterNames <chr>, title <chr>,\n#   gender <chr>, birthDate <date>, birthYear <dbl>, birthPlace <chr>,\n#   deathDate <date>, member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,\n#   wikidataID <chr>, wikipedia <chr>, adb <chr>, comments <chr>\n\n\nWe can save this R Script as ‘my_first_r_script.R’ (‘File’ -> ‘Save As’). At this point, our workspace should look something like Figure 3.3.\n\n\n\nFigure 3.3: After running an R Script\n\n\nOne thing to be aware of is that each R Studio Cloud workspace is essentially a new computer. Because of this, we need to install any package that we want to use for each workspace. For instance, before we can use the tidyverse, we need to install it with install.packages(\"tidyverse\"). This contrasts with using one’s own computer.\nA few final notes on R Studio Cloud:\n\nIn the Australian politician’s example, we got our data from the website GitHub using an R package, but we can get data into a workspace from a local computer in a variety of ways. One way is to use the ‘upload’ button in the ‘Files’ panel.\nR Studio Cloud allows some degree of collaboration. For instance, you can give someone else access to a workspace that you create. This could be useful for collaborating on an assignment, although it is not quite full featured yet and you cannot both be in the workspace at the same time, in contrast to, say, Google Docs.\nThere are a variety of weaknesses of R Studio Cloud, in particular the RAM limits. Additionally, like any web application, things break from time to time or go down."
  },
  {
    "objectID": "03-r_essentials.html#the-dplyr-verbs",
    "href": "03-r_essentials.html#the-dplyr-verbs",
    "title": "3  R essentials",
    "section": "3.5 The dplyr verbs",
    "text": "3.5 The dplyr verbs\nOne of the key packages that we will use is the tidyverse (Wickham et al. 2019b). The tidyverse is actually a package of packages, which means when we install the tidyverse, we actually install a whole bunch of different packages. The key package in the tidyverse in terms of manipulating data is dplyr (Wickham et al. 2020).\nThere are five dplyr functions that are regularly used, and we will now go through each of these. These are commonly referred to as the dplyr verbs.\n\nselect()\nfilter()\narrange()\nmutate()\nsummarise() or equally summarize()\n\nWe will also cover group_by(), and count() here as they are closely related.\nAs we have already installed the tidyverse, we just need to load it.\n\nlibrary(tidyverse)\n\nAnd we will begin by again using some data about Australian politicians from the AustralianPoliticians package (Alexander and Hodgetts 2021).\n\nlibrary(AustralianPoliticians)\n\naustralian_politicians <- \n  get_auspol('all')\n\nhead(australian_politicians)\n\n# A tibble: 6 × 20\n  uniqueID   surname allOtherNames          firstName commonName displayName    \n  <chr>      <chr>   <chr>                  <chr>     <chr>      <chr>          \n1 Abbott1859 Abbott  Richard Hartley Smith  Richard   <NA>       Abbott, Richard\n2 Abbott1869 Abbott  Percy Phipps           Percy     <NA>       Abbott, Percy  \n3 Abbott1877 Abbott  Macartney              Macartney Mac        Abbott, Mac    \n4 Abbott1886 Abbott  Charles Lydiard Aubrey Charles   Aubrey     Abbott, Aubrey \n5 Abbott1891 Abbott  Joseph Palmer          Joseph    <NA>       Abbott, Joseph \n6 Abbott1957 Abbott  Anthony John           Anthony   Tony       Abbott, Tony   \n# … with 14 more variables: earlierOrLaterNames <chr>, title <chr>,\n#   gender <chr>, birthDate <date>, birthYear <dbl>, birthPlace <chr>,\n#   deathDate <date>, member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,\n#   wikidataID <chr>, wikipedia <chr>, adb <chr>, comments <chr>\n\n\n\n3.5.1 select()\nWe use select() to pick particular columns of a dataset. For instance, we might like to select the ‘firstName’ column.\n\naustralian_politicians |> \n  select(firstName) |> \n  head()\n\n# A tibble: 6 × 1\n  firstName\n  <chr>    \n1 Richard  \n2 Percy    \n3 Macartney\n4 Charles  \n5 Joseph   \n6 Anthony  \n\n\nIn R, there are many ways to do things. Sometimes these are different ways to do the same thing, and other times they are different ways to do almost the same thing. For instance, another way to pick a particular column of a dataset is to use the ‘extract’ operator ‘$’. This is from base, as opposed to select() which is from the tidyverse.\n\naustralian_politicians$firstName |> \n  head()\n\n[1] \"Richard\"   \"Percy\"     \"Macartney\" \"Charles\"   \"Joseph\"    \"Anthony\"  \n\n\nThe two appear similar—both pick the ‘firstName’ column—but they differ in the class of what they return. For the sake of completeness, if we combine select() with pull() then we get the same class of output as if we had used the extract operator.\n\naustralian_politicians |> \n  select(firstName) |> \n  pull() |> \n  head()\n\n[1] \"Richard\"   \"Percy\"     \"Macartney\" \"Charles\"   \"Joseph\"    \"Anthony\"  \n\n\nWe can also use select() to remove columns, by negating the column name.\n\naustralian_politicians |> \n  select(-firstName) |> \n  head()\n\n# A tibble: 6 × 19\n  uniqueID   surname allOtherNames commonName displayName earlierOrLaterN… title\n  <chr>      <chr>   <chr>         <chr>      <chr>       <chr>            <chr>\n1 Abbott1859 Abbott  Richard Hart… <NA>       Abbott, Ri… <NA>             <NA> \n2 Abbott1869 Abbott  Percy Phipps  <NA>       Abbott, Pe… <NA>             <NA> \n3 Abbott1877 Abbott  Macartney     Mac        Abbott, Mac <NA>             <NA> \n4 Abbott1886 Abbott  Charles Lydi… Aubrey     Abbott, Au… <NA>             <NA> \n5 Abbott1891 Abbott  Joseph Palmer <NA>       Abbott, Jo… <NA>             <NA> \n6 Abbott1957 Abbott  Anthony John  Tony       Abbott, To… <NA>             <NA> \n# … with 12 more variables: gender <chr>, birthDate <date>, birthYear <dbl>,\n#   birthPlace <chr>, deathDate <date>, member <dbl>, senator <dbl>,\n#   wasPrimeMinister <dbl>, wikidataID <chr>, wikipedia <chr>, adb <chr>,\n#   comments <chr>\n\n\nFinally, we can select() based on conditions. For instance, we can select() all of the columns that start with, say, ‘birth’.\n\naustralian_politicians |> \n  select(starts_with(\"birth\")) |> \n  head()\n\n# A tibble: 6 × 3\n  birthDate  birthYear birthPlace  \n  <date>         <dbl> <chr>       \n1 NA              1859 Bendigo     \n2 1869-05-14        NA Hobart      \n3 1877-07-03        NA Murrurundi  \n4 1886-01-04        NA St Leonards \n5 1891-10-18        NA North Sydney\n6 1957-11-04        NA London      \n\n\nThere are a variety of similar ‘selection helpers’ including starts_with(), ends_with(), and contains(). More information about these is available in the help page for select() which can be accessed by running ?select().\nAt this point, we will use select() to reduce the width of our dataset.\n\naustralian_politicians <-\n  australian_politicians |>\n  select(uniqueID,\n         surname,\n         firstName,\n         gender,\n         birthDate,\n         birthYear,\n         deathDate,\n         member,\n         senator,\n         wasPrimeMinister)\n\naustralian_politicians |> head()\n\n# A tibble: 6 × 10\n  uniqueID   surname firstName gender birthDate  birthYear deathDate  member\n  <chr>      <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>\n1 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0\n2 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n3 Abbott1877 Abbott  Macartney male   1877-07-03        NA 1960-12-30      0\n4 Abbott1886 Abbott  Charles   male   1886-01-04        NA 1975-04-30      1\n5 Abbott1891 Abbott  Joseph    male   1891-10-18        NA 1965-05-07      1\n6 Abbott1957 Abbott  Anthony   male   1957-11-04        NA NA              1\n# … with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>\n\n\n\n\n3.5.2 filter()\nWe use filter() to pick particular rows of a dataset. For instance, we might be only interested in politicians that became prime minister.\n\naustralian_politicians |> \n  filter(wasPrimeMinister == 1)\n\n# A tibble: 30 × 10\n   uniqueID    surname firstName gender birthDate  birthYear deathDate  member\n   <chr>       <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>\n 1 Abbott1957  Abbott  Anthony   male   1957-11-04        NA NA              1\n 2 Barton1849  Barton  Edmund    male   1849-01-18        NA 1920-01-07      1\n 3 Bruce1883   Bruce   Stanley   male   1883-04-15        NA 1967-08-25      1\n 4 Chifley1885 Chifley Joseph    male   1885-09-22        NA 1951-06-13      1\n 5 Cook1860    Cook    Joseph    male   1860-12-07        NA 1947-07-30      1\n 6 Curtin1885  Curtin  John      male   1885-01-08        NA 1945-07-05      1\n 7 Deakin1856  Deakin  Alfred    male   1856-08-03        NA 1919-10-07      1\n 8 Fadden1894  Fadden  Arthur    male   1894-04-13        NA 1973-04-21      1\n 9 Fisher1862  Fisher  Andrew    male   1862-08-29        NA 1928-10-22      1\n10 Forde1890   Forde   Francis   male   1890-07-18        NA 1983-01-28      1\n# … with 20 more rows, and 2 more variables: senator <dbl>,\n#   wasPrimeMinister <dbl>\n\n\nWe could also give filter() two conditions. For instance, we could look at politicians that become prime minister and were named Joseph, using the ‘and’ operator ‘&’.\n\naustralian_politicians |> \n  filter(wasPrimeMinister == 1 & firstName == \"Joseph\")\n\n# A tibble: 3 × 10\n  uniqueID    surname firstName gender birthDate  birthYear deathDate  member\n  <chr>       <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>\n1 Chifley1885 Chifley Joseph    male   1885-09-22        NA 1951-06-13      1\n2 Cook1860    Cook    Joseph    male   1860-12-07        NA 1947-07-30      1\n3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA 1939-04-07      1\n# … with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>\n\n\nWe get the same result if we use a comma instead of an ampersand.\n\naustralian_politicians |> \n  filter(wasPrimeMinister == 1, firstName == \"Joseph\")\n\n# A tibble: 3 × 10\n  uniqueID    surname firstName gender birthDate  birthYear deathDate  member\n  <chr>       <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>\n1 Chifley1885 Chifley Joseph    male   1885-09-22        NA 1951-06-13      1\n2 Cook1860    Cook    Joseph    male   1860-12-07        NA 1947-07-30      1\n3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA 1939-04-07      1\n# … with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>\n\n\nSimilarly, we could look at politicians who were named, say, Myles or Ruth using the ‘or’ operator ‘|’\n\naustralian_politicians |> \n  filter(firstName == \"Myles\" | firstName == \"Ruth\")\n\n# A tibble: 3 × 10\n  uniqueID     surname  firstName gender birthDate  birthYear deathDate  member\n  <chr>        <chr>    <chr>     <chr>  <date>         <dbl> <date>      <dbl>\n1 Coleman1931  Coleman  Ruth      female 1931-09-27        NA 2008-03-27      0\n2 Ferricks1875 Ferricks Myles     male   1875-11-12        NA 1932-08-20      0\n3 Webber1965   Webber   Ruth      female 1965-03-24        NA NA              0\n# … with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>\n\n\nWe could also pipe the result. For instance we could pipe from filter() to select().\n\naustralian_politicians |> \n  filter(firstName == \"Ruth\" | firstName == \"Myles\") |> \n  select(firstName, surname)\n\n# A tibble: 3 × 2\n  firstName surname \n  <chr>     <chr>   \n1 Ruth      Coleman \n2 Myles     Ferricks\n3 Ruth      Webber  \n\n\nIf we happen to know the particular row number that is of interest then we could filter() to only that particular row. For instance, say the row 853 was of interest.\n\naustralian_politicians |> \n  filter(row_number() == 853)\n\n# A tibble: 1 × 10\n  uniqueID     surname  firstName gender birthDate  birthYear deathDate member\n  <chr>        <chr>    <chr>     <chr>  <date>         <dbl> <date>     <dbl>\n1 Jakobsen1947 Jakobsen Carolyn   female 1947-09-11        NA NA             1\n# … with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>\n\n\nThere is also a dedicated function to do this, which is slice().\n\naustralian_politicians |> \n  slice(853)\n\n# A tibble: 1 × 10\n  uniqueID     surname  firstName gender birthDate  birthYear deathDate member\n  <chr>        <chr>    <chr>     <chr>  <date>         <dbl> <date>     <dbl>\n1 Jakobsen1947 Jakobsen Carolyn   female 1947-09-11        NA NA             1\n# … with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>\n\n\nWhile this may seem somewhat esoteric, it is especially useful if we would like to remove a particular row using negation, or duplicate specific rows. For instance, we could remove the first row.\n\naustralian_politicians |> \n  slice(-1)\n\n# A tibble: 1,782 × 10\n   uniqueID    surname firstName gender birthDate  birthYear deathDate  member\n   <chr>       <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>\n 1 Abbott1869  Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n 2 Abbott1877  Abbott  Macartney male   1877-07-03        NA 1960-12-30      0\n 3 Abbott1886  Abbott  Charles   male   1886-01-04        NA 1975-04-30      1\n 4 Abbott1891  Abbott  Joseph    male   1891-10-18        NA 1965-05-07      1\n 5 Abbott1957  Abbott  Anthony   male   1957-11-04        NA NA              1\n 6 Abel1939    Abel    John      male   1939-06-25        NA NA              1\n 7 Abetz1958   Abetz   Eric      male   1958-01-25        NA NA              0\n 8 Adams1943   Adams   Judith    female 1943-04-11        NA 2012-03-31      0\n 9 Adams1951   Adams   Dick      male   1951-04-29        NA NA              1\n10 Adamson1857 Adamson John      male   1857-02-18        NA 1922-05-02      0\n# … with 1,772 more rows, and 2 more variables: senator <dbl>,\n#   wasPrimeMinister <dbl>\n\n\nWe could also only, say, only keep the first three rows.\n\naustralian_politicians |> \n  slice(1:3)\n\n# A tibble: 3 × 10\n  uniqueID   surname firstName gender birthDate  birthYear deathDate  member\n  <chr>      <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>\n1 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0\n2 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n3 Abbott1877 Abbott  Macartney male   1877-07-03        NA 1960-12-30      0\n# … with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>\n\n\nFinally, we could duplicate the first two rows.\n\naustralian_politicians |> \n  slice(1:2, 1:n())\n\n# A tibble: 1,785 × 10\n   uniqueID   surname firstName gender birthDate  birthYear deathDate  member\n   <chr>      <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>\n 1 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0\n 2 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n 3 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0\n 4 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n 5 Abbott1877 Abbott  Macartney male   1877-07-03        NA 1960-12-30      0\n 6 Abbott1886 Abbott  Charles   male   1886-01-04        NA 1975-04-30      1\n 7 Abbott1891 Abbott  Joseph    male   1891-10-18        NA 1965-05-07      1\n 8 Abbott1957 Abbott  Anthony   male   1957-11-04        NA NA              1\n 9 Abel1939   Abel    John      male   1939-06-25        NA NA              1\n10 Abetz1958  Abetz   Eric      male   1958-01-25        NA NA              0\n# … with 1,775 more rows, and 2 more variables: senator <dbl>,\n#   wasPrimeMinister <dbl>\n\n\n\n\n3.5.3 arrange()\nWe use arrange() to change the order of the dataset based on the values of particular columns. For instance, we could arrange the politicians by their birthday.\n\naustralian_politicians |> \n  arrange(birthDate)\n\n# A tibble: 1,783 × 10\n   uniqueID     surname  firstName gender birthDate  birthYear deathDate  member\n   <chr>        <chr>    <chr>     <chr>  <date>         <dbl> <date>      <dbl>\n 1 Braddon1829  Braddon  Edward    male   1829-06-11        NA 1904-02-02      1\n 2 Ferguson1830 Ferguson John      male   1830-03-15        NA 1906-03-30      0\n 3 Zeal1830     Zeal     William   male   1830-12-05        NA 1912-03-11      0\n 4 Fraser1832   Fraser   Simon     male   1832-08-21        NA 1919-07-30      0\n 5 Groom1833    Groom    William   male   1833-03-09        NA 1901-08-08      1\n 6 Sargood1834  Sargood  Frederick male   1834-05-30        NA 1903-01-02      0\n 7 Fysh1835     Fysh     Philip    male   1835-03-01        NA 1919-12-20      1\n 8 Playford1837 Playford Thomas    male   1837-11-26        NA 1915-04-19      0\n 9 Solomon1839  Solomon  Elias     male   1839-09-02        NA 1909-05-23      1\n10 McLean1840   McLean   Allan     male   1840-02-03        NA 1911-07-13      1\n# … with 1,773 more rows, and 2 more variables: senator <dbl>,\n#   wasPrimeMinister <dbl>\n\n\nWe could modify arrange() with desc() to change from ascending to descending order.\n\naustralian_politicians |> \n  arrange(desc(birthDate))\n\n# A tibble: 1,783 × 10\n   uniqueID      surname firstName gender birthDate  birthYear deathDate member\n   <chr>         <chr>   <chr>     <chr>  <date>         <dbl> <date>     <dbl>\n 1 SteeleJohn19… Steele… Jordon    male   1994-10-14        NA NA             0\n 2 Chandler1990  Chandl… Claire    female 1990-06-01        NA NA             0\n 3 Roy1990       Roy     Wyatt     male   1990-05-22        NA NA             1\n 4 Thompson1988  Thomps… Phillip   male   1988-05-07        NA NA             1\n 5 Paterson1987  Paters… James     male   1987-11-21        NA NA             0\n 6 Burns1987     Burns   Joshua    male   1987-02-06        NA NA             1\n 7 Smith1986     Smith   Marielle  female 1986-12-30        NA NA             0\n 8 KakoschkeMoo… Kakosc… Skye      female 1985-12-19        NA NA             0\n 9 Simmonds1985  Simmon… Julian    male   1985-08-29        NA NA             1\n10 Gorman1984    Gorman  Patrick   male   1984-12-12        NA NA             1\n# … with 1,773 more rows, and 2 more variables: senator <dbl>,\n#   wasPrimeMinister <dbl>\n\n\nAnd we could arrange based on more than one column. For instance, if two politicians have the same first name, then we could arrange based on their birthday.\n\naustralian_politicians |> \n  arrange(firstName, birthDate)\n\n# A tibble: 1,783 × 10\n   uniqueID      surname firstName gender birthDate  birthYear deathDate  member\n   <chr>         <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>\n 1 Blain1894     Blain   Adair     male   1894-11-21        NA 1983-04-28      1\n 2 Dein1889      Dein    Adam      male   1889-03-04        NA 1969-05-09      1\n 3 Armstrong1909 Armstr… Adam      male   1909-07-01        NA 1982-02-22      1\n 4 Bandt1972     Bandt   Adam      male   1972-03-11        NA NA              1\n 5 Ridgeway1962  Ridgew… Aden      male   1962-09-18        NA NA              0\n 6 Bennett1933   Bennett Adrian    male   1933-01-21        NA 2006-05-09      1\n 7 Gibson1935    Gibson  Adrian    male   1935-11-03        NA 2015-04-30      1\n 8 Wynne1850     Wynne   Agar      male   1850-01-15        NA 1934-05-12      1\n 9 Robertson1882 Robert… Agnes     female 1882-07-31        NA 1968-01-29      0\n10 Pittard1902   Pittard Alan      male   1902-11-15        NA 1992-12-25      1\n# … with 1,773 more rows, and 2 more variables: senator <dbl>,\n#   wasPrimeMinister <dbl>\n\n\nWe could achieve the same result by piping between two instances of arrange().\n\naustralian_politicians |> \n  arrange(birthDate) |> \n  arrange(firstName)\n\n# A tibble: 1,783 × 10\n   uniqueID      surname firstName gender birthDate  birthYear deathDate  member\n   <chr>         <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>\n 1 Blain1894     Blain   Adair     male   1894-11-21        NA 1983-04-28      1\n 2 Dein1889      Dein    Adam      male   1889-03-04        NA 1969-05-09      1\n 3 Armstrong1909 Armstr… Adam      male   1909-07-01        NA 1982-02-22      1\n 4 Bandt1972     Bandt   Adam      male   1972-03-11        NA NA              1\n 5 Ridgeway1962  Ridgew… Aden      male   1962-09-18        NA NA              0\n 6 Bennett1933   Bennett Adrian    male   1933-01-21        NA 2006-05-09      1\n 7 Gibson1935    Gibson  Adrian    male   1935-11-03        NA 2015-04-30      1\n 8 Wynne1850     Wynne   Agar      male   1850-01-15        NA 1934-05-12      1\n 9 Robertson1882 Robert… Agnes     female 1882-07-31        NA 1968-01-29      0\n10 Pittard1902   Pittard Alan      male   1902-11-15        NA 1992-12-25      1\n# … with 1,773 more rows, and 2 more variables: senator <dbl>,\n#   wasPrimeMinister <dbl>\n\n\nWhen we use arrange() it is important to be clear about precedence. For instance, changing to birthday and then first name would give a different arrangement.\n\naustralian_politicians |> \n  arrange(birthYear, firstName)\n\n# A tibble: 1,783 × 10\n   uniqueID    surname firstName gender birthDate birthYear deathDate  member\n   <chr>       <chr>   <chr>     <chr>  <date>        <dbl> <date>      <dbl>\n 1 Edwards1842 Edwards Richard   male   NA             1842 1915-10-29      1\n 2 Sawers1844  Sawers  William   male   NA             1844 1916-05-19      1\n 3 Barker1846  Barker  Stephen   male   NA             1846 1924-06-21      0\n 4 Corser1852  Corser  Edward    male   NA             1852 1928-07-31      1\n 5 Lee1856     Lee     Henry     male   NA             1856 1927-08-12      1\n 6 Grant1857   Grant   John      male   NA             1857 1928-05-19      0\n 7 Palmer1859  Palmer  Albert    male   NA             1859 1919-08-14      1\n 8 Riley1859   Riley   Edward    male   NA             1859 1943-07-21      1\n 9 Abbott1859  Abbott  Richard   male   NA             1859 1940-02-28      0\n10 Kennedy1860 Kennedy Thomas    male   NA             1860 1929-02-16      1\n# … with 1,773 more rows, and 2 more variables: senator <dbl>,\n#   wasPrimeMinister <dbl>\n\n\nA nice way to arrange by a variety of columns is to use across(). It enables us to use the ‘selection helpers’ such as starts_with() that were mentioned in association with select().\n\naustralian_politicians |> \n  arrange(across(c(firstName, birthYear))) |> \n  head()\n\n# A tibble: 6 × 10\n  uniqueID      surname  firstName gender birthDate  birthYear deathDate  member\n  <chr>         <chr>    <chr>     <chr>  <date>         <dbl> <date>      <dbl>\n1 Blain1894     Blain    Adair     male   1894-11-21        NA 1983-04-28      1\n2 Armstrong1909 Armstro… Adam      male   1909-07-01        NA 1982-02-22      1\n3 Bandt1972     Bandt    Adam      male   1972-03-11        NA NA              1\n4 Dein1889      Dein     Adam      male   1889-03-04        NA 1969-05-09      1\n5 Ridgeway1962  Ridgeway Aden      male   1962-09-18        NA NA              0\n6 Bennett1933   Bennett  Adrian    male   1933-01-21        NA 2006-05-09      1\n# … with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>\n\naustralian_politicians |> \n  arrange(across(starts_with('birth'))) |> \n  head()\n\n# A tibble: 6 × 10\n  uniqueID     surname  firstName gender birthDate  birthYear deathDate  member\n  <chr>        <chr>    <chr>     <chr>  <date>         <dbl> <date>      <dbl>\n1 Braddon1829  Braddon  Edward    male   1829-06-11        NA 1904-02-02      1\n2 Ferguson1830 Ferguson John      male   1830-03-15        NA 1906-03-30      0\n3 Zeal1830     Zeal     William   male   1830-12-05        NA 1912-03-11      0\n4 Fraser1832   Fraser   Simon     male   1832-08-21        NA 1919-07-30      0\n5 Groom1833    Groom    William   male   1833-03-09        NA 1901-08-08      1\n6 Sargood1834  Sargood  Frederick male   1834-05-30        NA 1903-01-02      0\n# … with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>\n\n\n\n\n3.5.4 mutate()\nWe use mutate() when we want to make a new column. For instance, perhaps we want to make a new column that is 1 if a person was both a member and a senator and 0 otherwise. That is to say that our new column would denote politicians that served in both the upper and the lower house.\n\naustralian_politicians <- \n  australian_politicians |> \n  mutate(was_both = if_else(member == 1 & senator == 1, 1, 0))\n\naustralian_politicians |> \n  select(member, senator, was_both)\n\n# A tibble: 1,783 × 3\n   member senator was_both\n    <dbl>   <dbl>    <dbl>\n 1      0       1        0\n 2      1       1        1\n 3      0       1        0\n 4      1       0        0\n 5      1       0        0\n 6      1       0        0\n 7      1       0        0\n 8      0       1        0\n 9      0       1        0\n10      1       0        0\n# … with 1,773 more rows\n\n\nWe could use mutate() with math, such as addition and subtraction. For instance, we could calculate the age that the politicians are (or would have been) in 2022.\n\naustralian_politicians <- \n  australian_politicians |> \n  mutate(age = 2022 - lubridate::year(birthDate))\n\naustralian_politicians |> \n  select(uniqueID, age)\n\n# A tibble: 1,783 × 2\n   uniqueID     age\n   <chr>      <dbl>\n 1 Abbott1859    NA\n 2 Abbott1869   153\n 3 Abbott1877   145\n 4 Abbott1886   136\n 5 Abbott1891   131\n 6 Abbott1957    65\n 7 Abel1939      83\n 8 Abetz1958     64\n 9 Adams1943     79\n10 Adams1951     71\n# … with 1,773 more rows\n\n\nThere are a variety of functions that are especially useful when constructing new columns. These include log() which will compute the natural logarithm, lead() which will bring values up by one row, lag() which will push values down by one row, and cumsum() which creates a cumulative sum of the column.\n\naustralian_politicians |> \n  select(uniqueID, age) |> \n  mutate(log_age = log(age)) |> \n  head()\n\n# A tibble: 6 × 3\n  uniqueID     age log_age\n  <chr>      <dbl>   <dbl>\n1 Abbott1859    NA   NA   \n2 Abbott1869   153    5.03\n3 Abbott1877   145    4.98\n4 Abbott1886   136    4.91\n5 Abbott1891   131    4.88\n6 Abbott1957    65    4.17\n\naustralian_politicians |> \n  select(uniqueID, age) |> \n  mutate(lead_age = lead(age)) |> \n  head()\n\n# A tibble: 6 × 3\n  uniqueID     age lead_age\n  <chr>      <dbl>    <dbl>\n1 Abbott1859    NA      153\n2 Abbott1869   153      145\n3 Abbott1877   145      136\n4 Abbott1886   136      131\n5 Abbott1891   131       65\n6 Abbott1957    65       83\n\naustralian_politicians |> \n  select(uniqueID, age) |> \n  mutate(lag_age = lag(age)) |> \n  head()\n\n# A tibble: 6 × 3\n  uniqueID     age lag_age\n  <chr>      <dbl>   <dbl>\n1 Abbott1859    NA      NA\n2 Abbott1869   153      NA\n3 Abbott1877   145     153\n4 Abbott1886   136     145\n5 Abbott1891   131     136\n6 Abbott1957    65     131\n\naustralian_politicians |> \n  select(uniqueID, age) |> \n  filter(!is.na(age)) |> \n  mutate(cumulative_age = cumsum(age)) |> \n  head()\n\n# A tibble: 6 × 3\n  uniqueID     age cumulative_age\n  <chr>      <dbl>          <dbl>\n1 Abbott1869   153            153\n2 Abbott1877   145            298\n3 Abbott1886   136            434\n4 Abbott1891   131            565\n5 Abbott1957    65            630\n6 Abel1939      83            713\n\n\nAs we have in earlier examples, we can also use mutate() in combination with across(). This includes the potential use of the selection helpers. For instance, we could count the number of characters in both the first and last names at the same time.\n\naustralian_politicians |> \n  mutate(across(c(firstName, surname), str_count)) |> \n  select(uniqueID, firstName, surname)\n\n# A tibble: 1,783 × 3\n   uniqueID   firstName surname\n   <chr>          <int>   <int>\n 1 Abbott1859         7       6\n 2 Abbott1869         5       6\n 3 Abbott1877         9       6\n 4 Abbott1886         7       6\n 5 Abbott1891         6       6\n 6 Abbott1957         7       6\n 7 Abel1939           4       4\n 8 Abetz1958          4       5\n 9 Adams1943          6       5\n10 Adams1951          4       5\n# … with 1,773 more rows\n\n\nFinally, we use case_when() when we need to make a new column on the basis of more than two conditional statements. For instance, we may have some years and want to group them into decades.\n\naustralian_politicians |> \n  mutate(year_of_birth = lubridate::year(birthDate),\n         decade_of_birth = \n           case_when(\n             year_of_birth <= 1929 ~ \"pre-1930\",\n             year_of_birth <= 1939 ~ \"1930s\",\n             year_of_birth <= 1949 ~ \"1940s\",\n             year_of_birth <= 1959 ~ \"1950s\",\n             year_of_birth <= 1969 ~ \"1960s\",\n             year_of_birth <= 1979 ~ \"1970s\",\n             year_of_birth <= 1989 ~ \"1980s\",\n             TRUE ~ \"Unknown or error\"\n             )\n  ) |> \n  select(uniqueID, year_of_birth, decade_of_birth)\n\n# A tibble: 1,783 × 3\n   uniqueID   year_of_birth decade_of_birth \n   <chr>              <dbl> <chr>           \n 1 Abbott1859            NA Unknown or error\n 2 Abbott1869          1869 pre-1930        \n 3 Abbott1877          1877 pre-1930        \n 4 Abbott1886          1886 pre-1930        \n 5 Abbott1891          1891 pre-1930        \n 6 Abbott1957          1957 1950s           \n 7 Abel1939            1939 1930s           \n 8 Abetz1958           1958 1950s           \n 9 Adams1943           1943 1940s           \n10 Adams1951           1951 1950s           \n# … with 1,773 more rows\n\n\nWe could accomplish this with a series of if_else() statements, but case_when() is more clear. The cases are evaluated in order and as soon as there is a match case_when() does not continue to the remainder of the cases. So it can be useful to have a catch-all at the end that will signal if there is a potential issue that we might like to know about.\n\n\n3.5.5 summarise()\nWe use summarise() when we would like to make new, condensed, summary variables. For instance, perhaps we would like to know the minimum, average, and maximum of some column.\n\naustralian_politicians |> \n  summarise(youngest = min(age, na.rm = TRUE),\n            oldest = max(age, na.rm = TRUE),\n            average = mean(age, na.rm = TRUE))\n\n# A tibble: 1 × 3\n  youngest oldest average\n     <dbl>  <dbl>   <dbl>\n1       28    193    101.\n\n\nAs an aside, summarise() and summarize() are equivalent and we can use either.\n\naustralian_politicians |> \n  summarize(youngest = min(age, na.rm = TRUE),\n            oldest = max(age, na.rm = TRUE),\n            average = mean(age, na.rm = TRUE))\n\n# A tibble: 1 × 3\n  youngest oldest average\n     <dbl>  <dbl>   <dbl>\n1       28    193    101.\n\n\nBy default, summarise() will provide one row of output for a whole dataset. For instance, in the earlier example we found the youngest, oldest, and average across all politicians. However, we can create more groups in our dataset using group_by(). And we can then apply another function within the context of those groups. We could use many functions on the basis of groups, but the summarise() function is particularly powerful in conjunction with group_by(). For instance, we could group by gender, and then get age-based summary statistics.\n\naustralian_politicians |> \n  group_by(gender) |> \n  summarise(youngest = min(age, na.rm = TRUE),\n            oldest = max(age, na.rm = TRUE),\n            average = mean(age, na.rm = TRUE))\n\n# A tibble: 2 × 4\n  gender youngest oldest average\n  <chr>     <dbl>  <dbl>   <dbl>\n1 female       32    140    66.0\n2 male         28    193   106. \n\n\nSimilarly, we could look at youngest, oldest, and mean age at death by gender.\n\naustralian_politicians |>\n  mutate(days_lived = deathDate - birthDate) |> \n  filter(!is.na(days_lived)) |> \n  group_by(gender) |> \n  summarise(\n    min_days = min(days_lived),\n    mean_days = mean(days_lived) |> round(),\n    max_days = max(days_lived)\n    )\n\n# A tibble: 2 × 4\n  gender min_days   mean_days  max_days  \n  <chr>  <drtn>     <drtn>     <drtn>    \n1 female 14856 days 28857 days 35560 days\n2 male   12380 days 27376 days 36416 days\n\n\nAnd so we learn that female members of parliament on average lived slightly longer than male members of parliament.\nWe can use group_by() on the basis of more than one group. For instance, we could look at the average number of days lived by gender and by house.\n\naustralian_politicians |>\n  mutate(days_lived = deathDate - birthDate) |> \n  filter(!is.na(days_lived)) |> \n  group_by(gender, member) |> \n  summarise(\n    min_days = min(days_lived),\n    mean_days = mean(days_lived) |> round(),\n    max_days = max(days_lived)\n    )\n\n# A tibble: 4 × 5\n# Groups:   gender [2]\n  gender member min_days   mean_days  max_days  \n  <chr>   <dbl> <drtn>     <drtn>     <drtn>    \n1 female      0 21746 days 29517 days 35560 days\n2 female      1 14856 days 27538 days 33442 days\n3 male        0 13619 days 27133 days 36416 days\n4 male        1 12380 days 27496 days 36328 days\n\n\nWe can use count() to create counts by groups. For instance, the number of politicians by gender.\n\naustralian_politicians |> \n  group_by(gender) |> \n  count()\n\n# A tibble: 2 × 2\n# Groups:   gender [2]\n  gender     n\n  <chr>  <int>\n1 female   240\n2 male    1543\n\n\nIn addition to the count(), we could make a proportion.\n\naustralian_politicians |> \n  group_by(gender) |> \n  count() |> \n  ungroup() |> \n  mutate(proportion = n/(sum(n)))\n\n# A tibble: 2 × 3\n  gender     n proportion\n  <chr>  <int>      <dbl>\n1 female   240      0.135\n2 male    1543      0.865\n\n\nUsing count() is essentially the same as using group_by() and then summarise(), and we get the same result in that way.\n\naustralian_politicians |> \n  group_by(gender) |> \n  summarise(n = n())\n\n# A tibble: 2 × 2\n  gender     n\n  <chr>  <int>\n1 female   240\n2 male    1543\n\n\nAnd there is a similarly helpful function for mutate(), which is add_count(). The difference is that the number will be added in a column.\n\naustralian_politicians |> \n  group_by(gender) |> \n  add_count() |> \n  select(uniqueID, gender, n)\n\n# A tibble: 1,783 × 3\n# Groups:   gender [2]\n   uniqueID   gender     n\n   <chr>      <chr>  <int>\n 1 Abbott1859 male    1543\n 2 Abbott1869 male    1543\n 3 Abbott1877 male    1543\n 4 Abbott1886 male    1543\n 5 Abbott1891 male    1543\n 6 Abbott1957 male    1543\n 7 Abel1939   male    1543\n 8 Abetz1958  male    1543\n 9 Adams1943  female   240\n10 Adams1951  male    1543\n# … with 1,773 more rows"
  },
  {
    "objectID": "03-r_essentials.html#base",
    "href": "03-r_essentials.html#base",
    "title": "3  R essentials",
    "section": "3.6 Base",
    "text": "3.6 Base\nWhile the tidyverse was established relatively recently to help with data science, R existed long before this. There is a host of functionality that is built into R especially around the core needs of programming and statisticians.\nIn particular, we will cover:\n\nclass()\ndata simulation\nfunction(), for(), and apply()\n\nThere is no need to install any additional packages, as this functionality comes with R.\n\n3.6.1 class()\nIn everyday usage ‘a, b, c, …’ are letters and ‘1, 2, 3,…’ are numbers. And we use letters and numbers differently, for instance we do not add letters. Similarly, R needs to have some way of distinguishing different classes of content. And to define the properties that each class has, ‘how it behaves, and how it relates to other types of objects’ (Wickham 2019a).\nClasses have a hierarchy. For instance, we are ‘human’, which is itself ‘animal’. All ‘humans’ are ‘animals’, but not all ‘animals’ are ‘humans’. Similarly, all integers are numbers, but not all numbers are integers. We can find out the class of an object in R with class().\n\na_number <- 8\nclass(a_number)\n\n[1] \"numeric\"\n\na_letter <- \"a\"\nclass(a_letter)\n\n[1] \"character\"\n\n\nThe classes that we cover here are ‘numeric’, ‘character’, ‘factor’, ‘date’, and ‘data.frame’.\nThe first thing to know is that, in the same way that a frog can become a prince, we can sometimes change the class of an object in R. For instance, we could start with a ‘numeric’, change it to a ‘character’ with as.character(), and then a ‘factor’ with as.factor(). But if we tried to make it into a date with as.Date() we would get an error because no all numbers have the properties that are needed to be a date.\n\na_number <- 8\na_number\n\n[1] 8\n\nclass(a_number)\n\n[1] \"numeric\"\n\na_number <- as.character(a_number)\na_number\n\n[1] \"8\"\n\nclass(a_number)\n\n[1] \"character\"\n\na_number <- as.factor(a_number)\na_number\n\n[1] 8\nLevels: 8\n\nclass(a_number)\n\n[1] \"factor\"\n\n\nCompared with ‘numeric’ and ‘character’ classes, the ‘factor’ class might be less familiar. A ‘factor’ is used for categorical data that can only take certain values (Wickham 2019a). For instance, typical usage of a factor variable would be a binary, such as ‘day’ or ‘night’. It is also often used for age-groups, such as ‘18-29’, ‘30-44’, ‘45-60’, ‘60+’ (as opposed to age, which would often be a ‘numeric’); and sometimes for level of education: ‘less than high school’, ‘high school’, ‘college’, ‘undergraduate degree’, ‘postgraduate degree’. We can find the allowed levels for a ‘factor’ using levels().\n\nage_groups <- factor(\n  c('18-29', '30-44', '45-60', '60+')\n)\nage_groups\n\n[1] 18-29 30-44 45-60 60+  \nLevels: 18-29 30-44 45-60 60+\n\nclass(age_groups)\n\n[1] \"factor\"\n\nlevels(age_groups)\n\n[1] \"18-29\" \"30-44\" \"45-60\" \"60+\"  \n\n\nDates are an especially tricky class and quickly become complicated. Nonetheless, at a foundational level, we can use as.Date() to convert a character that looks like a date into an actual date. This enables us to, say, perform addition and subtraction, when we would not be able to do that with a character.\n\nlooks_like_a_date_but_is_not <- \"2022-01-01\"\nlooks_like_a_date_but_is_not\n\n[1] \"2022-01-01\"\n\nclass(looks_like_a_date_but_is_not)\n\n[1] \"character\"\n\nis_a_date <- as.Date(looks_like_a_date_but_is_not)\nis_a_date\n\n[1] \"2022-01-01\"\n\nclass(is_a_date)\n\n[1] \"Date\"\n\nis_a_date + 3\n\n[1] \"2022-01-04\"\n\n\nThe final class that we discuss here is ‘data.frame’. This looks like a spreadsheet and is commonly used to store the data that we will analyze. Formally, ‘a data frame is a list of equal-length vectors’ (Wickham 2019a). It will have column and row names which we can see using colnames() and rownames(), although often the names of the rows are just numbers.\nTo illustrate this, we use the ‘ResumeNames’ dataset from AER (Kleiber and Zeileis 2008). This package can be installed in the same way as any other package from CRAN. This dataset comprises cross-sectional data about resume content, especially the name used on the resume, and associated information about whether the candidate received a call-back for 4,870 fictitious resumes. The dataset was created by Bertrand and Mullainathan (2004) who sent fictitious resumes in response to job advertisements in Boston and Chicago that differed in whether the resume was assigned a ‘very African American sounding name or a very White sounding name’. They found considerable discrimination whereby ‘White names receive 50 percent more callbacks for interviews’. Hangartner, Kopp, and Siegenthaler (2021) generalize this using an online Swiss platform and find that immigrants and minority ethnic groups are contacted less by recruiters, as are women when the profession is men-dominated, and vice versa.\n\ninstall.packages(\"AER\")\n\n\nlibrary(AER)\ndata(\"ResumeNames\", package = \"AER\")\n\n\nResumeNames |> \n  head()\n\n     name gender ethnicity quality call    city jobs experience honors\n1 Allison female      cauc     low   no chicago    2          6     no\n2 Kristen female      cauc    high   no chicago    3          6     no\n3 Lakisha female      afam     low   no chicago    1          6     no\n4 Latonya female      afam    high   no chicago    4          6     no\n5  Carrie female      cauc    high   no chicago    3         22     no\n6     Jay   male      cauc     low   no chicago    2          6    yes\n  volunteer military holes school email computer special college minimum equal\n1        no       no   yes     no    no      yes      no     yes       5   yes\n2       yes      yes    no    yes   yes      yes      no      no       5   yes\n3        no       no    no    yes    no      yes      no     yes       5   yes\n4       yes       no   yes     no   yes      yes     yes      no       5   yes\n5        no       no    no    yes   yes      yes      no      no    some   yes\n6        no       no    no     no    no       no     yes     yes    none   yes\n      wanted requirements reqexp reqcomm reqeduc reqcomp reqorg\n1 supervisor          yes    yes      no      no     yes     no\n2 supervisor          yes    yes      no      no     yes     no\n3 supervisor          yes    yes      no      no     yes     no\n4 supervisor          yes    yes      no      no     yes     no\n5  secretary          yes    yes      no      no     yes    yes\n6      other           no     no      no      no      no     no\n                          industry\n1                    manufacturing\n2                    manufacturing\n3                    manufacturing\n4                    manufacturing\n5 health/education/social services\n6                            trade\n\nclass(ResumeNames)\n\n[1] \"data.frame\"\n\ncolnames(ResumeNames)\n\n [1] \"name\"         \"gender\"       \"ethnicity\"    \"quality\"      \"call\"        \n [6] \"city\"         \"jobs\"         \"experience\"   \"honors\"       \"volunteer\"   \n[11] \"military\"     \"holes\"        \"school\"       \"email\"        \"computer\"    \n[16] \"special\"      \"college\"      \"minimum\"      \"equal\"        \"wanted\"      \n[21] \"requirements\" \"reqexp\"       \"reqcomm\"      \"reqeduc\"      \"reqcomp\"     \n[26] \"reqorg\"       \"industry\"    \n\n\nWe can examine the class of the vectors, i.e. the columns, that make-up a data frame by specifying the column name.\n\nclass(ResumeNames$name)\n\n[1] \"factor\"\n\nclass(ResumeNames$jobs)\n\n[1] \"integer\"\n\n\nSometimes it is helpful to be able to change the classes of many columns at once. We can do this by using mutate() and across().\n\nclass(ResumeNames$name)\n\n[1] \"factor\"\n\nclass(ResumeNames$gender)\n\n[1] \"factor\"\n\nclass(ResumeNames$ethnicity)\n\n[1] \"factor\"\n\nResumeNames |>\n  mutate(across(c(name, gender, ethnicity), as.character)) |>\n  head()\n\n     name gender ethnicity quality call    city jobs experience honors\n1 Allison female      cauc     low   no chicago    2          6     no\n2 Kristen female      cauc    high   no chicago    3          6     no\n3 Lakisha female      afam     low   no chicago    1          6     no\n4 Latonya female      afam    high   no chicago    4          6     no\n5  Carrie female      cauc    high   no chicago    3         22     no\n6     Jay   male      cauc     low   no chicago    2          6    yes\n  volunteer military holes school email computer special college minimum equal\n1        no       no   yes     no    no      yes      no     yes       5   yes\n2       yes      yes    no    yes   yes      yes      no      no       5   yes\n3        no       no    no    yes    no      yes      no     yes       5   yes\n4       yes       no   yes     no   yes      yes     yes      no       5   yes\n5        no       no    no    yes   yes      yes      no      no    some   yes\n6        no       no    no     no    no       no     yes     yes    none   yes\n      wanted requirements reqexp reqcomm reqeduc reqcomp reqorg\n1 supervisor          yes    yes      no      no     yes     no\n2 supervisor          yes    yes      no      no     yes     no\n3 supervisor          yes    yes      no      no     yes     no\n4 supervisor          yes    yes      no      no     yes     no\n5  secretary          yes    yes      no      no     yes    yes\n6      other           no     no      no      no      no     no\n                          industry\n1                    manufacturing\n2                    manufacturing\n3                    manufacturing\n4                    manufacturing\n5 health/education/social services\n6                            trade\n\nclass(ResumeNames$name)\n\n[1] \"factor\"\n\nclass(ResumeNames$gender)\n\n[1] \"factor\"\n\nclass(ResumeNames$ethnicity)\n\n[1] \"factor\"\n\n\nThere are many ways for code to not run but having an issue with the class is always among the first things to check. Common issues are variables that we think should be ‘character’ or ‘numeric’, actually being ‘factor’. And variables that we think should be ‘numeric’ actually being ‘character’.\n\n\n3.6.2 Simulating data\nSimulating data is a key skill for telling believable stories with data. In order to simulate data, we need to be able to randomly draw from statistical distributions and other collections. R has a variety of functions to make this easier, including: the normal distribution, rnorm(); the uniform distribution, runif(); the Poisson distribution, rpois; the binomial distribution, rbinom; and many others. To randomly sample from a collection of items, we can use sample().\nWhen dealing with randomness, the need for reproducibility makes it important, paradoxically, that the randomness is repeatable. That is to say, another person needs to be able to draw the random numbers that we draw. We do this by setting a seed for our random draws using set.seed().\nWe could get observations from the standard normal distribution and put the those into a data frame.\n\nset.seed(853)\n\nnumber_of_observations <- 5\n\nsimulated_data <- \n  data.frame(\n    person = c(1:number_of_observations),\n    std_normal_observations = rnorm(n = number_of_observations,\n                                    mean = 0,\n                                    sd = 1)\n    )\n\nsimulated_data\n\n  person std_normal_observations\n1      1             -0.35980342\n2      2             -0.04064753\n3      3             -1.78216227\n4      4             -1.12242282\n5      5             -1.00278400\n\n\nWe could then add draws from the uniform, Poisson, and binomial distributions, using cbind() to bring the columns of the original dataset and the new one together.\n\nsimulated_data <-\n  data.frame(\n    uniform_observations = \n      runif(n = number_of_observations, min = 0, max = 10),\n    poisson_observations = \n      rpois(n = number_of_observations, lambda = 100),\n    binomial_observations = \n      rbinom(n = number_of_observations, size = 2, prob = 0.5)\n  ) |>\n  cbind(simulated_data)\n\nsimulated_data\n\n  uniform_observations poisson_observations binomial_observations person\n1            9.6219155                   81                     2      1\n2            7.2269016                   91                     1      2\n3            0.8252921                   84                     1      3\n4            1.0379810                  100                     1      4\n5            3.0942004                   97                     1      5\n  std_normal_observations\n1             -0.35980342\n2             -0.04064753\n3             -1.78216227\n4             -1.12242282\n5             -1.00278400\n\n\nFinally, we will add a favorite color to each observation with sample().\n\nsimulated_data <- \n  data.frame(\n    favorite_color = sample(x = c(\"blue\", \" white \"), \n                             size = number_of_observations,\n                             replace = TRUE)\n    ) |>\n  cbind(simulated_data)\n\nsimulated_data\n\n  favorite_color uniform_observations poisson_observations\n1           blue            9.6219155                   81\n2           blue            7.2269016                   91\n3           blue            0.8252921                   84\n4         white             1.0379810                  100\n5           blue            3.0942004                   97\n  binomial_observations person std_normal_observations\n1                     2      1             -0.35980342\n2                     1      2             -0.04064753\n3                     1      3             -1.78216227\n4                     1      4             -1.12242282\n5                     1      5             -1.00278400\n\n\nWe set the option ‘replace’ to ‘TRUE’ because we are only choosing between two items, but each time we choose we want the possibility that either are chosen. Depending on the simulation we may need to think about whether ‘replace’ should be ‘TRUE’ or ‘FALSE’. Another useful optional argument in sample() is to adjust the probability with which each item is drawn. The default is that all options are equally likely, but we could specify particular probabilities if we wanted to with ‘prob’. As always with functions, we can find more in the help file, for instance ?sample.\n\n\n3.6.3 function(), for(), and apply()\nR ‘is a functional programming language’ (Wickham 2019a). This means that we foundationally write, use, and compose functions, which are collections of code that accomplish something specific.\nThere are a lot of functions in R that other people have written, and we can use. Almost any common statistical or data science task that we might need to accomplish likely already has a function that has been written by someone else and made available to us, either as part of the base R installation or a package. But we will need to write our own functions from time to time, especially for more-specific tasks. We define a function using function(), and then assign a name. We will likely need to include some inputs and outputs for the function. Inputs are specified between round brackets. The specific task that the function is to accomplish goes between braces.\n\nprint_names <- function(some_names) {\n  print(some_names)\n}\n\nprint_names(c(\"rohan\", \"monica\"))\n\n[1] \"rohan\"  \"monica\"\n\n\nWe can specify defaults for the inputs in case the person using the function does not supply them.\n\nprint_names <- function(some_names = c(\"edward\", \"hugo\")) {\n  print(some_names)\n}\n\nprint_names()\n\n[1] \"edward\" \"hugo\"  \n\n\nOne common scenario is that we want to apply a function multiple times. Like many programming languages, we can use a for() loop for this. The look of a for() loop in R is similar to function(), in that we define what we are iterating over in the round brackets, and the function to apply in braces.\n\nfor (i in 1:3) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\n\nx <- cbind(x1 = 66, x2 = c(4:1, 2:5))\ndimnames(x)[[1]] <- letters[1:8]\nclass(x)\n\n[1] \"matrix\" \"array\" \n\napply(x, 2, mean, trim = .2)\n\nx1 x2 \n66  3 \n\n\nBecause R is a programming language that is focused on statistics, we are often interested in arrays or matrices. We us apply() to apply a function to rows (‘MARGIN = 1’) or columns (‘MARGIN = 2’).\n\nsimulated_data\n\n  favorite_color uniform_observations poisson_observations\n1           blue            9.6219155                   81\n2           blue            7.2269016                   91\n3           blue            0.8252921                   84\n4         white             1.0379810                  100\n5           blue            3.0942004                   97\n  binomial_observations person std_normal_observations\n1                     2      1             -0.35980342\n2                     1      2             -0.04064753\n3                     1      3             -1.78216227\n4                     1      4             -1.12242282\n5                     1      5             -1.00278400\n\napply(X = simulated_data, MARGIN = 2, FUN = unique)\n\n$favorite_color\n[1] \"blue\"    \" white \"\n\n$uniform_observations\n[1] \"9.6219155\" \"7.2269016\" \"0.8252921\" \"1.0379810\" \"3.0942004\"\n\n$poisson_observations\n[1] \" 81\" \" 91\" \" 84\" \"100\" \" 97\"\n\n$binomial_observations\n[1] \"2\" \"1\"\n\n$person\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n$std_normal_observations\n[1] \"-0.35980342\" \"-0.04064753\" \"-1.78216227\" \"-1.12242282\" \"-1.00278400\""
  },
  {
    "objectID": "03-r_essentials.html#making-graphs-with-ggplot2",
    "href": "03-r_essentials.html#making-graphs-with-ggplot2",
    "title": "3  R essentials",
    "section": "3.7 Making graphs with ggplot2",
    "text": "3.7 Making graphs with ggplot2\nIf the key package in the tidyverse in terms of manipulating data is dplyr (Wickham et al. 2020), then the key package in the tidyverse in terms of creating graphs is ggplot2 (Wickham 2016). It is part of the tidyverse collection of packages and so does not need to be explicitly installed or loaded if the tidyverse has been loaded.\nMore formally, ggplot2 works by defining layers which build to form a graph, based around the ‘grammar of graphics’ (hence, the ‘gg’). Instead of the pipe operator (|>) ggplot uses the add operator +.\nThere are three key aspects that need to be specified to build a graph with ggplot2:\n\ndata;\naesthetics / mapping; and\ntype.\n\nTo get started we will obtain some GDP data for OECD countries (OECD 2022).\n\nlibrary(tidyverse)\n\noecd_gdp <- \n  read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.QGDP.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\n\nwrite_csv(oecd_gdp, 'inputs/data/oecd_gdp.csv')\n\n\n\n# A tibble: 6 × 8\n  LOCATION INDICATOR SUBJECT MEASURE  FREQUENCY TIME  Value `Flag Codes`\n  <chr>    <chr>     <chr>   <chr>    <chr>     <chr> <dbl> <chr>       \n1 OECD     QGDP      TOT     PC_CHGPP A         1962   5.70 <NA>        \n2 OECD     QGDP      TOT     PC_CHGPP A         1963   5.20 <NA>        \n3 OECD     QGDP      TOT     PC_CHGPP A         1964   6.38 <NA>        \n4 OECD     QGDP      TOT     PC_CHGPP A         1965   5.35 <NA>        \n5 OECD     QGDP      TOT     PC_CHGPP A         1966   5.75 <NA>        \n6 OECD     QGDP      TOT     PC_CHGPP A         1967   3.96 <NA>        \n\n\nWe are interested, firstly, in making a bar chart of GDP change in the third quarter of 2021 for ten countries: Australia, Canada, Chile, Indonesia, Germany, Great Britain, New Zealand, South Africa, Spain, and the US.\n\noecd_gdp_most_recent <- \n  oecd_gdp |> \n  filter(TIME == \"2021-Q3\",\n         SUBJECT == \"TOT\",\n         LOCATION %in% c(\"AUS\", \"CAN\", \"CHL\", \"DEU\", \"GBR\",\n                         \"IDN\", \"ESP\", \"NZL\", \"USA\", \"ZAF\"),\n         MEASURE == \"PC_CHGPY\") |> \n  mutate(european = if_else(LOCATION %in% c(\"DEU\", \"GBR\", \"ESP\"),\n                             \"European\",\n                             \"Not european\"),\n         hemisphere = if_else(LOCATION %in% c(\"CAN\", \"DEU\", \"GBR\", \"ESP\", \"USA\"),\n                             \"Northern Hemisphere\",\n                             \"Southern Hemisphere\"),\n         )\n\nWe start with ggplot and specify a mapping/aesthetic, which in this case means specifying the x-axis and the y-axis.\n\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value))\n\n\n\n\nNow we need to specify the type of graph that we are interested in. In this case we want a bar chart and we do this by adding geom_bar().\n\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value)) +\n  geom_bar(stat=\"identity\")\n\n\n\n\nWe can color the bars by whether the country is European by adding another aesthetic, ‘fill’.\n\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat=\"identity\")\n\n\n\n\nFinally, we could make it look nicer by: adding labels, labs(); changing the color, scale_fill_brewer(); and the background, theme_classic().\n\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat=\"identity\") + \n  labs(title = \"Quarterly change in GDP for ten OECD countries in 2021Q3\", \n       x = \"Countries\", \n       y = \"Change (%)\",\n       fill = \"Is European?\") +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\nFacets enable us to that we create subplots that focus on specific aspects of our data. They are invaluable because they allow us to add another variable to a graph without having to make a 3D graph. We use facet_wrap() to add a facet and specify the variable that we would like to facet by. In this case, we facet by hemisphere.\n\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat=\"identity\") + \n  labs(title = \"Quarterly change in GDP for ten OECD countries in 2021Q3\", \n       x = \"Countries\", \n       y = \"Change (%)\",\n       fill = \"Is European?\") +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(~hemisphere, \n              scales = \"free_x\")"
  },
  {
    "objectID": "03-r_essentials.html#exploring-the-tidyverse",
    "href": "03-r_essentials.html#exploring-the-tidyverse",
    "title": "3  R essentials",
    "section": "3.8 Exploring the tidyverse",
    "text": "3.8 Exploring the tidyverse\nWe have focused on two aspects of the tidyverse: dplyr, and ggplot2. However, the tidyverse comprises a variety of different packages and functions. We will now go through four common aspects:\n\nImporting data and tibble().\nJoining and pivoting datasets.\nString manipulation and stringr.\nFactor variables and forcats.\n\nHowever, the first task is to deal with the nomenclature, and in particular to be specific about what is ‘tidy’ about the ‘tidyverse’. The name refers to tidy data, and the benefit of that is that while there are a variety of ways for data to be messy, tidy data satisfy three rules. This means the structure of the datasets consistent regardless of the specifics, and makes it easier to apply functions that expect certain types of input. Tidy data refers to a dataset where (Wickham and Grolemund 2017; Wickham 2014, 4):\n\nEvery variable is in a column of its own.\nEvery observation is in its own row.\nEvery value is in its own cell.\n\nTable 3.1 is tidy. Table 3.2 is not tidy because age and hair share a column.\n\ntibble(\n  person = c(\"Rohan\", \"Monica\", \"Edward\", \"Hugo\"),\n  age = c(35, 35, 2, 0),\n  hair = c(\"Black\", \"Blonde\", \"Brown\", \"None\")\n  ) |>\n  knitr::kable(\n    col.names = c(\"Person\", \"Age\", \"Hair\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n    )\n\n\n\nTable 3.1: Example of tidy data\n\n\nPerson\nAge\nHair\n\n\n\n\nRohan\n35\nBlack\n\n\nMonica\n35\nBlonde\n\n\nEdward\n2\nBrown\n\n\nHugo\n0\nNone\n\n\n\n\n\n\n\ntibble(\n  person = c(\n    \"Rohan\",\n    \"Rohan\",\n    \"Monica\",\n    \"Monica\",\n    \"Edward\",\n    \"Edward\",\n    \"Hugo\",\n    \"Hugo\"\n  ),\n  variable = c(\"Age\", \"Hair\", \"Age\", \"Hair\", \"Age\", \"Hair\", \"Age\", \"Hair\"),\n  value = c(\"35\", \"Black\", \"35\", \"Blonde\", \"2\", \"Brown\", \"0\", \"None\")\n) |>\n  knitr::kable(\n    col.names = c(\"Person\", \"Variable\", \"Value\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\n\n\n\nTable 3.2: Example of data that are not tidy\n\n\nPerson\nVariable\nValue\n\n\n\n\nRohan\nAge\n35\n\n\nRohan\nHair\nBlack\n\n\nMonica\nAge\n35\n\n\nMonica\nHair\nBlonde\n\n\nEdward\nAge\n2\n\n\nEdward\nHair\nBrown\n\n\nHugo\nAge\n0\n\n\nHugo\nHair\nNone\n\n\n\n\n\n\n\n3.8.1 Importing data and tibble()\nThere are a variety of ways to get data into R so that we can use it. For CSV files, there is read_csv() from readr (Wickham, Hester, and Bryan 2021), and for dta files, there is read_dta() from haven (Wickham and Miller 2020).\nCSVs are a common format and have many advantages including the fact that they typically do not modify the data. Each column is separated by a comma, and each row is a record. We can provide read_csv() with a URL or a local file to read. There are a variety of different options that can be passed to read_csv() including the ability to specify whether the dataset has column names, the types of the columns, and how many lines to skip. If we do not specify the types of the columns then read_csv() will make a guess by looking at the dataset.\nWe use read_dta() to read .dta files, which are commonly produced by the statistical program Stata. This means that they are common in fields such as sociology, political science, and economics. This format separates the data from its labels and so we typically reunite these using to_factor() from labelled (Larmarange 2021). haven is part of the tidyverse, but is not automatically loaded by default, in contrast to a package such as ggplot2, and so we would need to run library(haven).\nTypically a dataset enters R as a ‘data.frame’. While this can be useful, another helpful class for a dataset is ‘tibble’. These can be created using tibble() from the tibble package which is part of the tidyverse. A tibble is a data frame, with some particular changes that make it easier to work with, including not converting strings to factors by default, showing the class of columns, and printing nicely.\nWe can make a tibble manually, if need be, for instance, when we simulate data. But we typically import data directly as a tibble, for instance, when we use read_csv().\n\npeople_as_dataframe <- \n  data.frame(names = c(\"rohan\", \"monica\"),\n             website = c(\"rohanalexander.com\", \"monicaalexander.com\"),\n             fav_color = c(\"blue\", \" white \")\n             )\nclass(people_as_dataframe)\n\n[1] \"data.frame\"\n\npeople_as_dataframe\n\n   names             website fav_color\n1  rohan  rohanalexander.com      blue\n2 monica monicaalexander.com    white \n\npeople_as_tibble <- \n  tibble(names = c(\"rohan\", \"monica\"),\n         website = c(\"rohanalexander.com\", \"monicaalexander.com\"),\n         fav_color = c(\"blue\", \" white \")\n         )\npeople_as_tibble\n\n# A tibble: 2 × 3\n  names  website             fav_color\n  <chr>  <chr>               <chr>    \n1 rohan  rohanalexander.com  \"blue\"   \n2 monica monicaalexander.com \" white \"\n\nclass(people_as_tibble)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n3.8.2 Dataset manipulation with joins and pivots\nThere are two dataset manipulations that are often needed: joins and pivots.\nWe often have a situation where we have two, or more, datasets and we are interested in combining them. We can join datasets together in a variety of ways. A common way is to use left_join() from dplyr (Wickham et al. 2020). This is most useful where there is one main dataset that we are using and there is another dataset with some useful variables that we want to add to that. The critical aspect is that we have column/s that we can use to link the two datasets. Here we will create two tibbles and then join them on the basis of names.\n\nmain_dataset <- \n  tibble(\n    names = c('rohan', 'monica', 'edward', 'hugo'),\n    status = c('adult', 'adult', 'child', 'infant')\n  )\nmain_dataset\n\n# A tibble: 4 × 2\n  names  status\n  <chr>  <chr> \n1 rohan  adult \n2 monica adult \n3 edward child \n4 hugo   infant\n\nsupplementary_dataset <- \n  tibble(\n    names = c('rohan', 'monica', 'edward', 'hugo'),\n    favorite_food = c('pasta', 'salmon', 'pizza', 'milk')\n  )\nsupplementary_dataset\n\n# A tibble: 4 × 2\n  names  favorite_food\n  <chr>  <chr>        \n1 rohan  pasta        \n2 monica salmon       \n3 edward pizza        \n4 hugo   milk         \n\nmain_dataset <- \n  main_dataset |> \n  left_join(supplementary_dataset, by = \"names\")\n\nmain_dataset\n\n# A tibble: 4 × 3\n  names  status favorite_food\n  <chr>  <chr>  <chr>        \n1 rohan  adult  pasta        \n2 monica adult  salmon       \n3 edward child  pizza        \n4 hugo   infant milk         \n\n\nThere are a variety of other options to join datasets, including inner_join(), right_join(), and full_join().\nAnother common dataset manipulation task is pivoting them. Datasets tend to be either long or wide. Generally, in the tidyverse, and certainly for ggplot2, we need long data. To go from one to the other we use pivot_longer() and pivot_wider() from tidyr (Wickham 2021).\nWe will create some wide data on whether ‘mark’ or ‘lauren’ won a running race in each of three years.\n\npivot_example_data <- \n  tibble(year = c(2019, 2020, 2021),\n         mark = c(\"first\", \"second\", \"first\"),\n         lauren = c(\"second\", \"first\", \"second\"))\n\npivot_example_data\n\n# A tibble: 3 × 3\n   year mark   lauren\n  <dbl> <chr>  <chr> \n1  2019 first  second\n2  2020 second first \n3  2021 first  second\n\n\nThis dataset is in wide format at the moment. To get it into long format, we need a column that specifies the person, and another that specifies the result. We use pivot_longer() to achieve this.\n\ndata_pivoted_longer <- \n  pivot_example_data |> \n  tidyr::pivot_longer(cols = c(\"mark\", \"lauren\"),\n              names_to = \"person\",\n               values_to = \"position\")\n\nhead(data_pivoted_longer)\n\n# A tibble: 6 × 3\n   year person position\n  <dbl> <chr>  <chr>   \n1  2019 mark   first   \n2  2019 lauren second  \n3  2020 mark   second  \n4  2020 lauren first   \n5  2021 mark   first   \n6  2021 lauren second  \n\n\nOccasionally, we need to go from long data to wide data. We use pivot_wider() to do this.\n\ndata_pivoted_wider <- \n  data_pivoted_longer |> \n  tidyr::pivot_wider(names_from = \"person\",\n                     values_from = \"position\")\n\nhead(data_pivoted_wider)\n\n# A tibble: 3 × 3\n   year mark   lauren\n  <dbl> <chr>  <chr> \n1  2019 first  second\n2  2020 second first \n3  2021 first  second\n\n\n\n\n3.8.3 String manipulation and stringr\nIn R we often create a string with double quotes, although using single quotes works too. For instance c(\"a\", \"b\") consists of two strings ‘a’ and ‘b’, that are contained in a character vector. There are a variety of ways to manipulate strings in R and we focus on stringr (Wickham 2019b). This is automatically loaded when we load the tidyverse.\nIf we want to look for whether a string contains certain content, then we can use str_detect(). And if we want to remove or change some particular content then we can use str_remove() or str_replace().\n\ndataset_of_strings <- \n  tibble(\n    names = c(\"rohan alexander\", \n              \"monica alexander\", \n              \"edward alexander\", \n              \"hugo alexander\")\n  )\n\ndataset_of_strings |> \n  mutate(is_rohan = str_detect(names, \"rohan\"),\n         make_howlett = str_replace(names, \"alexander\", \"howlett\"),\n         remove_rohan = str_remove(names, \"rohan\")\n         )\n\n# A tibble: 4 × 4\n  names            is_rohan make_howlett   remove_rohan      \n  <chr>            <lgl>    <chr>          <chr>             \n1 rohan alexander  TRUE     rohan howlett  \" alexander\"      \n2 monica alexander FALSE    monica howlett \"monica alexander\"\n3 edward alexander FALSE    edward howlett \"edward alexander\"\n4 hugo alexander   FALSE    hugo howlett   \"hugo alexander\"  \n\n\nThere are a variety of other functions that are often especially useful in data cleaning. For instance, we can use str_length() to find out how long a string is, and str_c() to bring strings together.\n\ndataset_of_strings |> \n  mutate(length_is = str_length(string = names),\n         name_and_length = str_c(names, length_is, sep = \" - \")\n         )\n\n# A tibble: 4 × 3\n  names            length_is name_and_length      \n  <chr>                <int> <chr>                \n1 rohan alexander         15 rohan alexander - 15 \n2 monica alexander        16 monica alexander - 16\n3 edward alexander        16 edward alexander - 16\n4 hugo alexander          14 hugo alexander - 14  \n\n\nFinally, separate() from tidyr, although not part of stringr, is indispensable for string manipulation. It turns one character column into many.\n\ndataset_of_strings |> \n  separate(col = names,\n           into = c(\"first\", \"last\"),\n           sep = \" \",\n           remove = FALSE)\n\n# A tibble: 4 × 3\n  names            first  last     \n  <chr>            <chr>  <chr>    \n1 rohan alexander  rohan  alexander\n2 monica alexander monica alexander\n3 edward alexander edward alexander\n4 hugo alexander   hugo   alexander\n\n\n\n\n3.8.4 Factor variables and forcats\nA factor is a collection of strings that are categories. Sometimes there will be an inherent ordering. For instance, the days of the week have an order – Monday, Tuesday, Wednesday, … – which is not alphabetical. But there is no requirement for that to be the case, for instance gender: female, male, and other; or pregnancy status: pregnant or not pregnant. Factors feature prominently in base R. They can be useful because they ensure that only appropriate strings are allowed. For instance, if ‘days_of_the_week’ was a factor variable then ‘January’ would not be allowed. But they can add a great deal of complication, and so they have a less prominent role in tidyverse. Nonetheless taking advantage of factors is useful in certain circumstances. For instance, when plotting the days of the week we probably want them in the usual ordering than in the alphabetical ordering that would result if we had them as a character variable. While factors are built into base R, one tidyverse package that is especially useful when using factors is forcats (Wickham 2020a).\nSometimes we have a character vector, and we will want it ordered in a particular way. The default is that a character vector is ordered alphabetically, but we may not want that. For instance, the days of the week would look strange on a graph if they were alphabetically ordered: Friday, Monday, Saturday, Sunday, Thursday, Tuesday, and Wednesday!\nThe way to change the ordering is to change the variable from a character to a factor. We can use fct_relevel() from forcats (Wickham 2020a) to specify an ordering.\n\nset.seed(853)\n\ndays_data <-\n  tibble(\n    days =\n      c(\n        \"Monday\",\n        \"Tuesday\",\n        \"Wednesday\",\n        \"Thursday\",\n        \"Friday\",\n        \"Saturday\",\n        \"Sunday\"\n      ),\n    some_value = c(sample.int(100, 7))\n  )\n\ndays_data <-\n  days_data |>\n  mutate(\n    days_as_factor = factor(days),\n    days_as_factor = fct_relevel(\n      days,\n      \"Monday\",\n      \"Tuesday\",\n      \"Wednesday\",\n      \"Thursday\",\n      \"Friday\",\n      \"Saturday\",\n      \"Sunday\"\n    )\n  )\n\nAnd we can compare the results by graphing first with the original character vector on the x-axis, and then another graph with the factor vector on the x-axis.\n\ndays_data |> \n  ggplot(aes(x = days, y = some_value)) +\n  geom_point()\n\ndays_data |> \n  ggplot(aes(x = days_as_factor, y = some_value)) +\n  geom_point()"
  },
  {
    "objectID": "03-r_essentials.html#exercises-and-tutorial",
    "href": "03-r_essentials.html#exercises-and-tutorial",
    "title": "3  R essentials",
    "section": "3.9 Exercises and tutorial",
    "text": "3.9 Exercises and tutorial\n\n3.9.1 Exercises\n\nWhat is R?\n\nA open-source statistical programming language\nA programming language created by Guido van Rossum\nA closed source statistical programming language\nAn integrated development environment (IDE)\n\nWhat are three advantages of R? What are three disadvantages?\nWhat is R Studio?\n\nAn integrated development environment (IDE).\nA closed source paid program.\nA programming language created by Guido van Rossum\nA statistical programming language.\n\nWhat is the class of the output of 2 + 2 (pick one)?\n\ncharacter\nfactor\nnumeric\ndate\n\nSay we had run: my_name <- 'Rohan'. What would be the result of running print(my_name) (pick one)?\n\n‘Edward’\n‘Monica’\n‘Hugo’\n‘Rohan’\n\nSay we had a dataset with two columns: ‘name’, and ‘age’. Which verb should we use to pick just ‘name’ (pick one)?\n\ntidyverse::select()\ntidyverse::mutate()\ntidyverse::filter()\ntidyverse::rename()\n\nSay we had loaded AustralianPoliticians and tidyverse and then run the following code: australian_politicians <- AustralianPoliticians::get_auspol('all'). How could we select all of the columns that end with ‘Name’ (pick one)?\n\naustralian_politicians |> select(contains(\"Name\"))\naustralian_politicians |> select(starts_with(\"Name\"))\naustralian_politicians |> select(matches(\"Name\"))\naustralian_politicians |> select(ends_with(\"Name\"))\n\nUnder what circumstances, in terms of the names of the columns, would the use of ‘contains()’ potentially give different answers to using ‘ends_with()’ in the above question?\nWhich of the following are not tidyverse verbs (pick one)?\n\nselect()\nfilter()\narrange()\nmutate()\nvisualize()\n\nWhich function would make a new column (pick one)?\n\nselect()\nfilter()\narrange()\nmutate()\nvisualize()\n\nWhich function would focus on particular rows (pick one)?\n\nselect()\nfilter()\narrange()\nmutate()\nsummarise()\n\nWhich combination of two functions could provide a mean of a dataset, by sex (pick two)?\n\nsummarise()\nfilter()\narrange()\nmutate()\ngroup_by()\n\nAssume a variable called ‘age’ is an integer. Which line of code would create a column that is its exponential (pick one)?\n\nmutate(exp_age = exponential(age))\nmutate(exp_age = exponent(age))\nmutate(exp_age = exp(age))\nmutate(exp_age = expon(age))\n\nAssume a column called ‘age’. Which line of code could create a column that contains the value from five rows above?\n\nmutate(five_before = lag(age))\nmutate(five_before = lead(age))\nmutate(five_before = lag(age, n = 5))\nmutate(five_before = lead(age, n = 5))\n\nWhat would be the output of class('edward') (pick one)?\n\n‘numeric’\n‘character’\n‘data.frame’\n‘vector’\n\nWhich function would enable us to draw once from three options ‘blue, white, red’, with 10 per cent probability on ‘blue’ and ‘white’, and the remainder on ‘red’?\n\nsample(c('blue', 'white', 'red'), prob = c(0.1, 0.1, 0.8))\nsample(c('blue', 'white', 'red'), size = 1)\nsample(c('blue', 'white', 'red'), size = 1, prob = c(0.8, 0.1, 0.1))\nsample(c('blue', 'white', 'red'), size = 1, prob = c(0.1, 0.1, 0.8))\n\nWhich code simulates 10,000 draws from a normal distribution with a mean of 27 and a standard deviation of 3 (pick one)?\n\nrnorm(10000, mean = 27, sd = 3)\nrnorm(27, mean = 10000, sd = 3)\nrnorm(3, mean = 10000, sd = 27)\nrnorm(27, mean = 3, sd = 1000)\n\nWhat are the three key aspects of the grammar of graphics (select all)?\n\ndata\naesthetics\ntype\ngeom_histogram()\n\n\n\n\n3.9.2 Tutorial\n\nI think we should be suspicious when we find ourselves attracted to data—very, very thin and weak data—that seem to justify beliefs that have held great currency in lots of societies throughout history, in a way that is conducive to the oppression of large segments of the population\nAmia Srinivasan, 22 September 2021\n\nReflect on the quote from Amia Srinivasan, Chichele Professor of Social and Political Theory, All Souls College, Oxford, and D’Ignazio and Klein (2020), especially Chapter 6, and spend at least two pages discussing them in relation to a dataset that you are familiar with.\n\n\n\n\n\nAlexander, Rohan, and Paul A. Hodgetts. 2021. AustralianPoliticians: Provides Datasets about Australian Politicians. https://CRAN.R-project.org/package=AustralianPoliticians.\n\n\nAssociation, American Medical, and New York Academy of Medicine. 1848. Code of Medical Ethics. Academy of Medicine.\n\n\nBertrand, Marianne, and Sendhil Mullainathan. 2004. “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.” American Economic Review 94 (4): 991–1013.\n\n\nBrokowski, Carolyn, and Mazhar Adli. 2019. “CRISPR Ethics: Moral Considerations for Applications of a Powerful Tool.” Journal of Molecular Biology 431 (1): 88–101.\n\n\nBuja, Andreas, Dianne Cook, and Deborah F Swayne. 1996. “Interactive High-Dimensional Data Visualization.” Journal of Computational and Graphical Statistics 5 (1): 78–99.\n\n\nD’Ignazio, Catherine, and Lauren F Klein. 2020. Data Feminism. Mit Press.\n\n\nHangartner, Dominik, Daniel Kopp, and Michael Siegenthaler. 2021. “Monitoring Hiring Discrimination Through Online Recruitment Platforms.” Nature 589 (7843): 572–76.\n\n\nHealy, Kieran. 2020. The Kitchen Counter Observatory. https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/.\n\n\nKeyes, Os. 2019. “Counting the Countless.” Real Life. https://reallifemag.com/counting-the-countless/.\n\n\nKleiber, Christian, and Achim Zeileis. 2008. Applied Econometrics with R. New York: Springer-Verlag. https://CRAN.R-project.org/package=AER.\n\n\nLarmarange, Joseph. 2021. Labelled: Manipulating Labelled Data. https://CRAN.R-project.org/package=labelled.\n\n\nNeufeld, Michael J. 2002. “Wernher von Braun, the SS, and Concentration Camp Labor: Questions of Moral, Political, and Criminal Responsibility.” German Studies Review 25 (1): 57–78.\n\n\nOECD. 2022. Quarterly GDP. https://data.oecd.org/gdp/quarterly-gdp.htm.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nThieme, Nick. 2018. “R Generation.” Significance 15 (4): 14–19. https://doi.org/10.1111/j.1740-9713.2018.01169.x.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (1): 1–23.\n\n\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019a. Advanced r. CRC Press.\n\n\n———. 2019b. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2020a. Forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2020b. Tidyverse. https://www.tidyverse.org/.\n\n\n———. 2021. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019a. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\n———, et al. 2019b. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2020. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. https://r4ds.had.co.nz/.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2021. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, and Evan Miller. 2020. Haven: Import and Export ’SPSS’, ’Stata’ and ’SAS’ Files. https://CRAN.R-project.org/package=haven."
  },
  {
    "objectID": "04-workflow.html",
    "href": "04-workflow.html",
    "title": "4  Reproducible workflows",
    "section": "",
    "text": "Required material\nKey concepts and skills"
  },
  {
    "objectID": "04-workflow.html#introduction",
    "href": "04-workflow.html#introduction",
    "title": "4  Reproducible workflows",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\n\nSuppose you have cancer and you have to choose between a black box AI surgeon that cannot explain how it works but has a 90% cure rate and a human surgeon with an 80% cure rate. Do you want the AI surgeon to be illegal?\nGeoffrey Hinton, 20 February 2020.\n\n\nThe number one thing to keep in mind about machine learning is that performance is evaluated on samples from one dataset, but the model is used in production on samples that may not necessarily follow the same characteristics… The finance industry has a saying for this: “past performance is no guarantee of future results”. Your model scoring X on your test dataset doesn’t mean it will perform at level X on the next N situations it encounters in the real world. The future may not be like the past.\nSo when asking the question, “would you rather use a model that was evaluated as 90% accurate, or a human that was evaluated as 80% accurate”, the answer depends on whether your data is typical per the evaluation process. Humans are adaptable, models are not. If significant uncertainty is involved, go with the human. They may have inferior pattern recognition capabilities (versus models trained on enormous amounts of data), but they understand what they do, they can reason about it, and they can improvise when faced with novelty\nIf every possible situation is known and you want to prioritize scalability and cost-reduction, go with the model. Models exist to encode and operationalize human cognition in well-understood situations. (“well understood” meaning either that it can be explicitly described by a programmer, or that you can amass a dataset that densely samples the distribution of possible situations – which must be static)\nFrançois Chollet, 20 February 2020.\n\nIf science is about systematically building and organizing knowledge in terms of testable explanations and predictions, then data science takes this and focuses on data. This means that building, organizing, and sharing knowledge is a critical aspect. Creating knowledge, once, in a way that only you can do it, does not meet this standard. Hence, the need for reproducible data science workflows.\nAlexander (2019) talks about how reproducible research means it can be exactly redone, given all the materials used. This underscores the importance of providing the code, data, and environment. The minimum expectation is that another person is independently able to use your code, data, and environment, to get your results, including figures and tables. Ironically there are different definitions of reproducibility between disciplines. Barba (2018) surveys a variety of disciplines and concludes that the predominant language usage implies the following definitions: Reproducible research is when ‘[a]uthors provide all the necessary data and the computer codes to run the analysis again, re-creating the results.’ A replication is a study ‘that arrives at the same scientific findings as another study, collecting new data (possibly with different methods) and completing new analyses.’\nRegardless of what it is specifically called, Gelman (2016) identifies how large an issue this is in various social sciences. The problem with work that is not reproducible, is that it does not contribute to our stock of knowledge about the world. Since Gelman (2016), a great deal of work has been done in many social sciences and the situation has improved a little, but much work remains. And the situation is similar in the life sciences (Heil et al. 2021) and computer science (Pineau et al. 2021).\nSome of the examples that Gelman (2016) talks about, which turned out to not reproduce, such as himmicanes and power pose, are not that important in the scheme of things. But at the same time, we saw, and continue to see, similar approaches being used in areas with big impacts. For instance, many governments have created ‘nudge’ units that implement public policy (Sunstein and Reisch 2017). Governments are increasingly using algorithms that they do not make open (Chouldechova et al. 2018). And Herndon, Ash, and Pollin (2014) document how a paper in economics that was used by governments to justify austerity policies following the Global Financial Crisis, turned out to not be reproducible.\nAt a minimum, and with few exceptions, we must release our code, datasets, and environment. Without these, it is difficult to know what a finding speaks to (Miyakawa 2020). More banally, we also do not know if there are mistakes or aspects that were inadvertently overlooked (Merali 2010; Hillel 2017; Silver 2020). Increasingly, we consider a paper to be an advertisement, and for the associated code, data, and environment to be the actual work. Steve Jobs, a co-founder of Apple, talked about how the best craftsmen ensure that even the aspects of their work that no one else will ever see are as well-finished and high-quality as the aspects that are public facing (Isaacson 2011). The same is true in data science, where often one of the distinguishing aspects of high-quality work is that the README and code comments are as polished as the abstract of the associated paper.\nWorkflows exist within a cultural and social context, which imposes an additional reason for the need for them to be reproducible. For instance, Wang and Kosinski (2018) use deep neural networks to train a model to distinguish between gay and heterosexual men. (Murphy (2017) provides a summary of the paper, the associated issues, and comments from its authors.) To do this, Wang and Kosinski (2018, 248) needed a dataset of photos of folks that were ‘adult, Caucasian, fully visible, and of a gender that matched the one reported on the user’s profile’. They verified this using Amazon Mechanical Turk, an online platform that pays workers a small amount of money to complete specific tasks. The instructions provided to the Mechanical Turk workers for this task specify that Obama, who had a white mother and a black father, should be classified as ‘Black’; and that Latino is an ethnicity, rather than a race (Mattson 2017). The classification task may seem objective, but, perhaps unthinkingly, echoes the views of Americans with a certain class and background.\nThis is just one specific concern about one part of the Wang and Kosinski (2018) workflow. Broader concerns are raised by others including Gelman, Mattson, and Simpson (2018). The main issue is that statistical models are specific to the data on which they were trained. And the only reason that we can identify likely issues in the model of Wang and Kosinski (2018) is because, despite not releasing the specific dataset that they used, they were nonetheless open about their procedure. For our work to be credible, it needs to be reproducible by others.\nSome of the steps that we can take to make our work more reproducible include:\n\nEnsure the entire workflow is documented. This may involve addressing questions such as:\n\nHow was the raw dataset obtained and is access likely to be persistent and available to others?\nWhat specific steps are being taken to transform the raw data in the data that were analyzed, and how can this be made available to others?\nWhat analysis has been done, and how clearly can this be shared?\nHow has the final paper or report been built and to what extent can others follow that process themselves?\n\nNot worrying about perfect reproducibility initially, but instead focusing on trying to improve with each successive project. For instance, each of the following requirements are increasingly more onerous and there is no need to be concerned about not being able to the last, until we can do the first:\n\nCan you run your entire workflow again?\nCan ‘another person’ run your entire workflow again?\nCan ‘future-you’ run your entire workflow again?\nCan ‘future-another-person’ run your entire workflow again?\n\nIncluding a detailed discussion about the limitations of the dataset and the approach in the final paper or report.\n\nThe workflow that we follow is illustrated in Figure 4.1. But it can be more concisely summarized as: ‘Think an awful lot, mostly read and write, sometimes code’.\n\n\n\nFigure 4.1: Workflow for telling stories with data\n\n\nThere are various tools that we can use at the different stages that will improve the reproducibility of this workflow. This includes Quarto, R Projects, and Git and GitHub."
  },
  {
    "objectID": "04-workflow.html#quarto",
    "href": "04-workflow.html#quarto",
    "title": "4  Reproducible workflows",
    "section": "4.2 Quarto",
    "text": "4.2 Quarto\n\n4.2.1 Getting started\nQuarto integrates code and natural language in a way that is called ‘literate programming’ (Knuth 1984). It uses a mark-up language similar to HyperText Markup Language (HTML) or LaTeX, in comparison to a ‘What You See Is What You Get’ (WYSIWYG) language, such as Microsoft Word. This means that all the aspects are consistent, for instance, all top-level heading will look the same. But it means that we use symbols to designate how we would like certain aspects to appear. And it is only when we build the mark-up that we get to see what it looks like. A visual editor option can also be used which hides the need for the user to do this mark-up themselves.\nQuarto replaced R Markdown, which was a variant of Markdown specifically designed to allow R code chunks to be included. One advantage of literate programming is that we get a ‘live’ document in which code executes and then forms part of the document. Another advantage of Quarto is that very similar code can compile into a variety of documents, including HTML pages and PDFs. Quarto also has default options set up for including title, author, and date sections. One disadvantage is that it can take a while for a document to compile because all the code needs to run. Tierney (2022) provides an especially useful and detailed Quarto usage guide.\nWe can create a new Quarto document within R Studio (‘File’ -> ‘New File’ -> ‘Quarto Document…’).\n\n\n4.2.2 Essential commands\nEssential markdown commands include those for emphasis, headers, lists, links, and images. A reminder of these is included in R Studio (‘Help’ -> ‘Markdown Quick Reference’). It is your choice as to whether you want to use the visual or source editor. But either way, it is good to understand these essentials because it will not always be possible to use a visual editor, for instance if you are quickly looking at a Quarto document in GitHub.\n\nEmphasis: *italic*, **bold**\nHeaders (these go on their own line with a blank line before and after): # First level header ## Second level header ### Third level header\nUnordered list, with sub-lists:\n\n* Item 1\n* Item 2\n    + Item 2a\n    + Item 2b\n\nOrdered list, with sub-lists:\n\n1. Item 1\n2. Item 2\n3. Item 3\n    + Item 3a\n    + Item 3b\n\nURLs can be added by linking text [the address of this book](https://www.tellingstorieswithdata.com) results in the address of this book.\nA paragraph is created by leaving a blank line.\n\nA paragraph about some idea, nicely spaced from the following paragraph.\n\nAnother paragraph about another idea, nicely spaced from the earlier paragraph.\nOnce we have added some aspects, then we may want to see the actual document. To build the document click ‘Render’.\n\n\n4.2.3 R chunks\nWe can include code for R and many other languages in code chunks within a Quarto document. Then when we render the document, the code will run and be included in the document.\nTo create an R chunk, we start with three backticks and then within curly braces we tell Quarto that this is an R chunk. Anything inside this chunk will be considered R code and run as such. For instance, we could load the tidyverse and AER and make a graph of the number of times a survey respondent visited the doctor in the past two weeks.\n```{r}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |>\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n```\nThe output of that code is Figure 4.2.\n\n\n\n\n\nFigure 4.2: Number of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\n\n\nThere are various evaluation options that are available in chunks. We include these, each on a new line, by opening the line with the chunk-specific comment delimiter ‘#|’ and then the option. Helpful options include:\n\necho: false: run the code and include the output, but do not print the code in the document.\ninclude: false: run the code but do not output anything and do not print the code in the document.\neval: false: do not run the code, and hence do not include the outputs, but do print the code in the document.\nwarning: false: do not display warnings.\nmessage: false: do not display messages.\n\nFor instance, we could include the output, but not the code, and suppress any warnings.\n```{r}\n#| echo: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits %>%\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\nIt is important to leave a blank line on either side of an R chunk, otherwise it may not run properly. It is also important that lower case is used, i.e. ‘true’ not ‘TRUE’.\nMost people did not visit a doctor in the past week.\n\n```{r}\n#| echo: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits %>%\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\n\nThere were some people that visited a doctor once, and then very few people that visited two or more times.\nIt is also important that the Quarto document itself loads any datasets that are needed. It is not enough that they are in the environment. This is because the Quarto document evaluates the code in the document when it is built, not necessarily the environment.\n\n\n4.2.4 Top matter\nTop matter consists of defining aspects such as the title, author, and date. It is contained within three dashes at the top of a Quarto document. For instance, the following would specify a title, date that automatically updated to the date the document was rendered, and an author.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: \"14 April 2022\"\nformat: html\n---\nAn abstract is a short summary of the paper, and we could add that to the top matter as well.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: \"14 April 2022\"\nabstract: \"This is my abstract.\"\nformat: html\n---\nBy default, Quarto will create an HTML document, but we can change the output format to produce a PDF. This uses LaTeX in the background and may require the installation of supporting packages. In particular it is common to need to first install tinytex (Xie 2019).\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: \"14 April 2022\"\nabstract: \"This is my abstract.\"\nformat: pdf\n---\nWe can include references by specifying a BibTeX file in the top matter and then calling it within the text, as needed.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: 1 January 2022\nformat: pdf\nabstract: \"This is my abstract.\"\nbibliography: bibliography.bib\n---\nWe would need to make a separate file called ‘bibliography.bib’ and save it next to the Quarto file. In the BibTeX file we need an entry for the item that is to be referenced. For instance, the citation for R can be obtained with citation() and this can be added to the ‘bibliography.bib’ file. Similarly, the citation for a package can be found by including the package name, for instance citation('tidyverse'). It can be helpful to use Google Scholar, or doi2bib, to get citations for books or articles.\n@Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\nWe need to create a unique key that we use to refer to this item in the text. This can be anything, provided it is unique, but meaningful ones can be easier to remember, for instance ‘citeR’.\n@Manual{citeR,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@Article{citetidyverse,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\nTo cite R in the Quarto document we include @citeR, which would put the brackets around the year, like this: R Core Team (2021) or [@citeR], which would put the brackets around the whole thing, like this: (R Core Team 2021).\nThe reference list at the end of the paper is automatically built based on calling the BibTeX file and including the references in the paper. At the end of the Quarto document, including a heading ‘# References’ and the actual citations will be included after that. When the Quarto file is rendered, Quarto sees these in the content, goes to BibTeX to get the reference details that it needs, builds the reference list, and then adds it to the end of the rendered document.\n\n\n4.2.5 Cross-references\nIt can be useful to cross-reference figures, tables, and equations. This makes it easier to refer to them in the text. To do this for a figure we refer to the name of the R chunk that creates or contains the figure. For instance, if we had the following code:\n```{r}\n#| label: fig-uniquename\n#| fig-cap: Number of illnesses in the past two weeks, based on the 1977--1978 Australian Health Survey\n#| echo: true\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |>\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n```\nThen (@fig-uniquename) would produce: (Figure 4.3) as the name of the R chunk is fig-uniquename. We need to add ‘fig’ to the start of the chunk name so that Quarto knows that this is a figure. We then include a ‘fig-cap:’ in the R chunk that specifies a caption.\n\n\n\n\n\nFigure 4.3: Number of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\n\n\nWe can take a similar approach to cross-reference tables. For instance, (@tbl-docvisittable) will produce: (Table 4.1). In this case we specify ‘tbl’ at the start of the label so that Quarto knows that it is a table. And we specify a caption for the table with ‘tbl-cap:’.\n```{r}\n#| label: tbl-docvisittable\n#| echo: true\n#| tbl-cap: \"Number of visits to the doctor in the past two weeks, based on the 1977--1978 Australian Health Survey\"\n\nDoctorVisits |> \n  count(visits) |> \n  knitr::kable()\n```\n\n\n\n\nTable 4.1: Number of visits to the doctor in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\nvisits\nn\n\n\n\n\n0\n4141\n\n\n1\n782\n\n\n2\n174\n\n\n3\n30\n\n\n4\n24\n\n\n5\n9\n\n\n6\n12\n\n\n7\n12\n\n\n8\n5\n\n\n9\n1\n\n\n\n\n\n\nFinally, we can also cross-reference equations. To that we need to add a tag {#eq-macroidentity} which we then reference.\n$$\nY = C + I + G + (X - M)\n$$ {#eq-macroidentity}\nFor instance, we then use @eq-macroidentity to produce Equation 4.1.\n\\[\nY = C + I + G + (X - M)\n\\qquad(4.1)\\]\nWhen using cross-references, it is important that the labels are relatively simple. In general, try to keep the names simple but unique, avoid punctuation and stick to letters and hyphens. Do not use underbars, because that can cause an error."
  },
  {
    "objectID": "04-workflow.html#r-projects-and-file-structure",
    "href": "04-workflow.html#r-projects-and-file-structure",
    "title": "4  Reproducible workflows",
    "section": "4.3 R projects and file structure",
    "text": "4.3 R projects and file structure\nProjects are widely used in software development and exist to keeps all the files (data, analysis, report, etc) associated with a particular project together and related to each other. An R project can be created in R Studio ‘File’ -> ‘New Project’, then select ‘Empty project’, name the project and decide where to save it. For instance, a project focused on maternal mortality, may be called ‘maternalmortality’, and might be saved within a folder of other projects. The use of R projects enables ‘reliable, polite behavior across different computers or users and over time.’ (Jennifer Bryan and Hester 2020). This is because it removes the context of that folder from its broader existence. So files exist in relation to the base of the R project, not the base of the computer.\nOnce a project has been created, a new file with the extension ‘.RProj’ will appear in that folder. As an example, of a folder with an R Project, an example Quarto document, and an appropriate file structure is available here. That can be downloaded: ‘Code’ -> ‘Download ZIP’.\nThe main advantage of using an R Project is that we are more easily able to reference other files in a self-contained way. That means when others want to reproduce our work, they know that all the file references and structure should not need to be changed. It means that files are referenced in relation to where the ‘.Rproj’ file is. For instance, instead of reading a csv from, say, \"~/Documents/projects/book/data/\" you can read it in from book/data/. It may be that someone else does not have a ‘projects’ folder, and so the former would not work for them, while the latter would.\nThe use of R projects is required to meet the minimal level of reproducibility. The use of functions such as setwd(), and computer-specific file paths, bind work to a specific computer in a way that is not appropriate. Trisovic et al. (2022) describe the use of absolute paths, rather than relative paths, as a common error that they had to correct in their large-scale study of R code, uploaded to the Harvard Dataverse, that underpins research papers.\nThere are a variety of ways to set-up a folder. A variant of Wilson et al. (2017) that is often useful is shown in the example. Here we have an ‘inputs’ folder that contains raw data (which should never be modified (Wilson et al. 2017)) and literature related to the project (which cannot be modified). An ‘outputs’ folder contains data that we create using R, as well as the paper that we are writing. And a ‘scripts’ folder is what modifies the raw data and saves it into ‘outputs’. We will do most of our work in ‘scripts’, and the Quarto file for the paper in ‘outputs’. Useful other aspects include a ‘README.md’ which will specify overview details about the project, and a LICENSE. Another helpful variant of this project skeleton is provided by Mineault and The Good Research Code Handbook Community (2021)."
  },
  {
    "objectID": "04-workflow.html#version-control",
    "href": "04-workflow.html#version-control",
    "title": "4  Reproducible workflows",
    "section": "4.4 Version control",
    "text": "4.4 Version control\nWe implement version control through a combination of Git and GitHub. There are a variety of reasons for this including:\n\nenhance the reproducibility of work by making it easier to share code and data;\nmake it easier to share work;\nimprove workflow by encouraging systematic approaches; and\nmake it easier to work in teams.\n\nGit is a version control system. The way one often starts doing version control is to have various versions of the one file: ‘first_go.R’, ‘first_go-fixed.R’, ‘first_go-fixed-with-mons-edits.R’. But this soon becomes cumbersome. One often soon turns to dates, for instance: ‘2022-01-01-analysis.R’, ‘2022-01-02-analysis.R’, ‘2022-01-03-analysis.R’, etc. While this keeps a record it can be difficult to search when we need to go back, because it can be difficult to remember the date some change was made. In any case, it quickly gets unwieldy for a project that is being regularly worked on.\nInstead of this, we use Git so that we can have one version of the file, say, ‘analysis.R’ and then use Git to keep a record of the changes to that file, and a snapshot of that file at a given point in time. We determine when Git takes that snapshot, and when we take that snapshot. We additionally include a message saying what changed between this snapshot and the last. In that way, there is only ever one version of the file, but the history can be more easily searched.\nOne complication is that Git was designed for teams of software developers. As such, while it works, it can be a little ungainly for non-developers. But in general it is the case that Git has been usefully adapted for data science, even when the only collaborator one may have is one’s future self (Jennifer Bryan 2018).\nGitHub, GitLab, and various other companies offer easier-to-use services that build on Git. While there are tradeoffs, we introduce GitHub here because it is the predominant platform (Eghbal 2020, 21). Git and GitHub are built into R Studio Cloud, which provides a nice option if you have issues with local installation. One of the initial challenging aspects of Git is the terminology. Folders are called ‘repos’. Creating a snapshot is called a ‘commit’. One gets used to it eventually, but feeling confused initially is normal. Jenny Bryan (2020) is especially useful for setting up and using Git and GitHub.\n\n4.4.1 Git\nWe first need to git check whether Git is installed. Open R Studio, go to the Terminal, type the following, and then enter/return.\n\ngit --version\n\nIf you get a version number, then you are done (Figure 4.4).\n\n\n\nFigure 4.4: Using Terminal to check whether Git is installed in R Studio\n\n\nGit is pre-installed in R Studio Cloud, it should be pre-installed on Mac, and it may be pre-installed on Windows. If you do not get a version number in response, then you need to install it. To do that you should follow the instructions specific to your operating system in Jenny Bryan (2020, chap. 5).\nGiven Git is installed we need to tell it our username and email. We need to do this because Git adds this information whenever we take a ‘snapshot’, or to use Git’s language, whenever we make a commit.\nAgain, within the Terminal, type the following, replacing the details with yours, and then enter/return after each line.\n\ngit config --global user.name 'Rohan Alexander'\ngit config --global user.email 'rohan.alexander@utoronto.ca'\ngit config --global --list\n\nWhen this set-up has been done properly, the values that you entered for ‘user.name’ and ‘user.email’ will be returned after the last line (Figure 4.5).\n\n\n\nFigure 4.5: Adding a username and email address to Git in R Studio\n\n\nThese details–username and email address–will be public. There are various ways to hide the email address if necessary, and GitHub provides instructions about this. Jenny Bryan (2020, chap. 7) provides more detailed instructions about this step, and a trouble-shooting guide.\n\n\n4.4.2 GitHub\nNow that Git is set-up, we need to set-up GitHub. We created an account in Chapter 2, which we use again here. After being signed in we first need to make a new folder, which is called a ‘repo’ in Git. Look for a ‘+’ in the top right, and then select ‘New Repository’ (Figure 4.6).\n\n\n\nFigure 4.6: Start process of creating a new repository\n\n\nAt this point we can add a sensible name for the repo. Leave it as ‘public’ for now, because it can always be deleted later. And check the box to ‘Initialize this repository with a README’. Change ‘Add .gitignore’ to R. After that, click ‘Create repository’ (Figure 4.7).\n\n\n\nFigure 4.7: Creating a new repository in GitHub\n\n\nThis will take us to a screen that is fairly empty, but the details that we need are in the green ‘Clone or Download’ button, which we can copy by clicking the clipboard (Figure 4.8).\n\n\n\nFigure 4.8: Copy the URL of the new repository\n\n\nNow returning to R Studio, in R Studio Cloud, we create a ‘New Project’ using ‘New Project from Git Repository’. It will ask for the URL that we just copied (Figure 4.9). If you are using a local machine, then this same step is accomplished through the menu: ‘File’ -> ‘New Project…’ -> ‘Version Control’ -> ‘Git’, then paste in the URL, give the folder a meaningful name, check ‘Open in new session’, then ‘Create Project’.\n\n\n\nFigure 4.9: Adding the project to R Studio Cloud\n\n\nAt this point, a new folder has been created locally that we can use. We will want to be able to push it back to GitHub, and for that we will need to use a Personal Access Token (PAT) to link our R Studio Workspace with our GitHub account. To create a PAT, while signed into GitHub in the browser, run usethis::create_github_token() in your R session. GitHub will open in the browser with various options filled out (Figure 4.10). It can be useful to give the PAT an informative name by replacing ‘Note’, for instance ‘PAT for R Studio’, then ‘Generate token’.\n\n\n\nFigure 4.10: Creating a PAT\n\n\nWe only have one shot to copy this token, and if we make a mistake then we will need to generate a new one. Do not include the PAT in any R script or Quarto document. Instead run gitcreds::gitcreds_set(), which will then prompt you to add your PAT in the console.\nTo use GitHub for a project that we are actively working on we follow a procedure:\n\nThe first thing to do is almost always to get any changes with ‘pull’. To do this, open the Git pane in R Studio, and click the blue down arrow. This gets any changes to the folder, as it is on GitHub, into our own version of the folder.\nWe can then make our changes to our copy of the folder. For instance, we could update the README, and then save it as normal.\nOnce this is done, we need to ‘add’, ‘commit’, and ‘push’. In the Git pane in R Studio, select the files to be added. This adds them to the staging area. Then click ‘Commit’ (Figure 4.11). A new window will open. Add a commit message which is informative about the change that was made, and then click ‘Commit’ in that new window (Figure 4.12). Finally, click ‘Push’ to send the changes to GitHub.\n\n\n\n\nFigure 4.11: Adding files to be committed\n\n\n\n\n\nFigure 4.12: Making a commit\n\n\nThere are a few common pain-points when it comes to Git and GitHub. We recommend committing and pushing regularly, especially when you are new to version control. This increases the number of snapshots that you could come back if needed. All commits should have an informative commit message. If you are new to version control, then the expectation of a good commit message is that it contains a short summary of the change, followed by a blank line, and then an explanation of the change including what the change is, and why it is being made. For instance, if your commit adds graphs to a paper, then a commit message could be:\nAdd graphs\n\nGraphs of unemployment and inflation added into Data section.\nThere is some evidence of a relationship between overall quality and commit behavior (Sprint and Conci 2019). In an ideal scenario the commit messages act as a kind of journal of the project.\nGit and GitHub were designed for software developers, rather than data scientists. GitHub limits the size of the files it will consider to 100MB, and even 50MB will prompt a warning. Data science projects regularly involve datasets that are larger than this. In Chapter 12 we discuss the use of data deposits, which can be especially useful when a project is completed, but when we are actively working on a project it can be useful to ignore the file, at least as far as Git and GitHub are concerned. We do this using a ‘.gitignore’ file, in which we list all of the files that we do not want to track using Git. The starter folder contains an example ‘.gitignore’ file. And it can be helpful to run usethis::git_vaccinate(), which will add a variety of files to a global ‘.gitignore’ file in case you forget to do it on a project-basis.\n\n\n\n\n\nWe used the Git pane in R Studio which removed the need to use the Terminal, but it did not remove the need to go to GitHub and set-up a new project. Having set-up Git and GitHub, we can further improve this aspect of our workflow with usethis (Wickham and Bryan 2020).\nFirst check that Git is set-up with usethis::git_sitrep(). This should print information about the username and email. We can use usethis::use_git_config() to update these details if needed.\n\nusethis::use_git_config(\n  user.name = \"Rohan Alexander\", \n  user.email = \"rohan.alexander@utoronto.ca\"\n  )\n\nRather than starting a new project in GitHub, and then adding it locally, we can now use usethis::use_git() to initiate it and commit the files. Having committed, we can use usethis::use_github() to push to GitHub, which will create the folder on GitHub as well."
  },
  {
    "objectID": "04-workflow.html#using-r-in-practice",
    "href": "04-workflow.html#using-r-in-practice",
    "title": "4  Reproducible workflows",
    "section": "4.5 Using R in practice",
    "text": "4.5 Using R in practice\n\n4.5.1 Dealing with errors\n\nWhen you are programming, eventually your code will break, when I say eventually, I mean like probably 10 or 20 times a day.\nGelfand (2021)\n\nEveryone who uses R, or any programming language for that matter, has trouble find them at some point. This is normal. Programming is hard. At some point code will not run or will throw an error. This happens to everyone. It is common to get frustrated, but to move forward we develop strategies to work through the issues:\n\nIf you are getting an error message, then sometimes it will be useful. Try to read it carefully to see if there is anything of use in it.\nTry to search, say on Google, for the error message. It can be useful to include ‘tidyverse’ or ‘in R’ in the search to help make the results more appropriate. Sometimes Stack Overflow results can be useful.\nLook at the help file for the function, by putting ‘?’ before the function, for instance, ?pivot_wider(). A common issue is to use a slightly incorrect argument name or format, such as accidentally including a string instead of an object name.\nLook at where the error is happening and remove code until the error is resolved, and then slowly add code back again.\nCheck the class of the object, with class(), for instance, class(data_set$data_column). Ensure that it is what it expected.\nRestart R (‘Session’ -> ‘Restart R and Clear Output’) and load everything again.\nRestart the computer.\nSearch for what you are trying to do, rather than the error, being sure to include ‘tidyverse’ or ‘in R’ in the search to help make the results more appropriate. For instance, ‘save PDF of graph in R made using ggplot’. Sometimes there are relevant blog posts or Stack Overflow answers that will help.\nMaking a small, self-contained, reproducible example ‘reprex’ to see if the issue can be isolated and to enable others to help.\n\nMore generally, while this is rarely possible to do, it is almost always helpful to take a break and come back the next day.\n\n\n4.5.2 Reproducible examples\n\nNo one can advise or help you—no one. There is only one thing you should do. Go into yourself.\nRilke (1929)\n\nAsking for help is a skill like any other. We get better at it with practice. It is important to try not to say ‘this doesn’t work’, ‘I tried everything’, ‘your code does not work’, or ‘here is the error message, what do I do?’. In general, it is not possible to help based on these comments, because there are too many possible issues. You need to make it easy for others to help you. This involves a few steps.\n\nProvide a small, self-contained, example of your data, and code, and detail what is going wrong.\nDocument what you have tried so far, including which Stack Overflow and R Studio Community pages have you looked at, and why are they not quite what you are after?\nBe clear about the outcome that you would like.\n\nBegin by creating a minimal REPRoducible EXample, a ‘reprex’. This is code that contains what is needed to reproduce the error, but only what is needed. This means that the code it likely a smaller, simpler, version that nonetheless reproduces the error.\nSometimes this process enables one to solve the problem. If it does not, then it gives someone else a fighting chance of being able to help. It is important to recognize that there is almost no chance that you have got a problem that someone has not addressed before. It is more likely that the main difficulty is in trying to communicate what you are trying to do and what is happening, in a way that allows others to recognize both. Developing tenacity is important.\nTo develop reproducible examples, reprex (Jennifer Bryan et al. 2019) is especially useful. To use it we:\n\nLoad the reprex package: library(reprex).\nHighlight, and copy, the code that is giving issues.\nRun reprex() in the Console.\n\nIf the code is self-contained, then it will preview in the Viewer. If it is not, then it will error, and the code needs to be re-written so that it is self-contained.\nIf you need data to reproduce the error, then you should use data that is built into R. There are a large number of datasets that are built into R and can be seen using library(help = \"datasets\"). But if possible, you should use a common option such as ‘mtcars’ or ‘faithful’. Combining a reprex with a GitHub Gist that was introduced in Chapter 2, increases the chances that someone is able to help you.\n\n\n4.5.3 Mentality\n\n(Y)ou are a real, valid, competent user and programmer no matter what IDE you develop in or what tools you use to make your work work for you\n(L)et’s break down the gates, there’s enough room for everyone\nSharla Gelfand, 10 March 2020.\n\nIf you write code, then you are a programmer regardless of how you do it, what you are using it for, or who you are. But there are a few traits that one tends to notice great programmers have in common.\n\nFocused: Often having an aim to ‘learn R’ or something similar tends to be problematic, because there is no real end point to that. It tends to be more efficient to have smaller, more specific goals, such as ‘make a histogram about the 2022 Australian Election with ggplot’. This is something that can be focused on and achieved in a few hours. The issue with goals that are more nebulous, such as ‘I want to learn R’, is that it becomes easy to get lost on tangents, much more difficult to get help. This can be demoralizing and lead to folks quitting too early.\nCurious: It is almost always useful to have a go. In general, the worst that happens is that you waste your time. You can rarely break something irreparably with code. If you want to know what happens if you pass a ‘vector’ instead of a ‘dataframe’ to ggplot() then try it.\nPragmatic: At the same time, it can be useful to stick within reasonable bounds, and make one small change each time. For instance, say you want to run some regressions, and are curious about the possibility of using the tidymodels package (Kuhn and Wickham 2020) instead of lm(). A pragmatic way to proceed is to use one aspect from the tidymodels package initially and then make another change next time.\nTenacious: Again, this is a balancing act. There are always unexpected problems and issues with every project. On the one hand, persevering despite these is a good tendency. But on the other hand, sometimes one does need to be prepared to give up on something if it does not seem like a break-through is possible. Here mentors can be useful as they tend to be a better judge of what is reasonable. It is also where appropriate planning is useful.\nPlanned: It is almost always useful to excessively plan what you are going to do. For instance, you may want to make a histogram of the 2019 Canadian Election. You should plan the steps that are needed and even to sketch out how each step might be implemented. For instance, the first step is to get the data. What packages might be useful? Where might the data be? What is the back-up plan if the data do not exist there?\nDone is better than perfect: We all have various perfectionist tendencies to a certain extent, but it can be useful to initially try to turn them off to a certain extent. In the first instance, try to write code that works, especially in the early days. You can always come back and improve aspects of it. But it is important to actually ship. Ugly code that gets the job done, is better than beautiful code that is never finished.\n\n\n\n4.5.4 Code comments and style\nCode must be commented (Lee 2018). Comments should focus on why certain code was written, (and to a lesser extent, why a common option is not selected).\nThere is no one way to write code, especially in R. However, there are some general guidelines that will make it easier for you even if you are just working on your own. It is important to recognize that most projects will evolve over time, and one purpose served by code comments are as ‘[m]essages left for your future self (or near-future others) [that] help retrace and justify your decisions’ (Bowers 2011).\nComments in R can be added by including the # symbol. We do not have to put a comment at the start of the line, it can be midway through. In general, we do not need to comment what every aspect of your code is doing but we should comment parts that are not obvious. For instance, if we read in some value then we may like to comment where it is coming from.\nYou should comment why you are doing something (Wickham 2021). What are you trying to achieve?\nYou must comment to explain weird things. Like if you are removing some specific row, say row 27, then why are you removing that row? It may seem obvious in the moment, but future-you in six months will not remember.\nYou should break your code into sections. For instance, setting up the workspace, reading in datasets, manipulating and cleaning the dataset, analyzing the datasets, and finally producing tables and figures. Each of these should be separated with comments explaining what is going on, and sometimes into separate files, depending on the length.\nAdditionally, at the top of each file it is important to basic information, such as the purpose of the file, and pre-requisites or dependencies, the date, the author and contact information, and finally and red-flags or todos.\nAt the very least every R script needs a preamble and a clear demarcation of sections.\n#### Preamble ####\n# Purpose: Brief sentence about what this script does\n# Author: Your name\n# Data: The date it was written\n# Contact: Add your email\n# License: Think about how your code may be used\n# Pre-requisites: \n# - Maybe you need some data or some other script to have been run?\n\n\n#### Workspace setup ####\n# do not keep the install.packages line - comment out if need be\n# Load libraries\nlibrary(tidyverse)\n\n# Read in the raw data. \nraw_data <- readr::read_csv(\"inputs/data/raw_data.csv\")\n\n\n#### Next section ####\n..."
  },
  {
    "objectID": "04-workflow.html#exercises-and-tutorial",
    "href": "04-workflow.html#exercises-and-tutorial",
    "title": "4  Reproducible workflows",
    "section": "4.6 Exercises and tutorial",
    "text": "4.6 Exercises and tutorial\n\n4.6.1 Exercises\n\nAccording to Alexander (2019) research is reproducible if (pick one)?\n\nIt is published in peer-reviewed journals.\nAll of the materials used in the study are provided.\nIt can be reproduced exactly without the authors providing materials.\nIt can be reproduced exactly, given all the materials used in the study.\n\nAccording to the timeline of Gelman (2016), a) when did Paul Meehl identify various issues; and b) when did null hypothesis significance testing (NHST) become controversial (pick one)?\n\n1970s-1980s; 1990s-2000.\n1960s-1970s; 1980s-1990.\n1970s-1980s; 1980s-1990.\n1960s-1970s; 1990s-2000.\n\nWhich of the following are components of the project layout recommended by Wilson et al. (2017) (select all that apply)?\n\nrequirements.txt\ndoc\ndata\nLICENSE\nCITATION\nREADME\nsrc\nresults\n\nBased on Alexander (2021) please write a paragraph about some of the barriers you overcame, or still face, with regard to sharing code that you wrote.\nAccording to Gelfand (2021), what is the key part of ‘If you need help getting unstuck, the first step is to create a reprex, or reproducible example. The goal of a reprex is to package your problematic code in such a way that other people can run it and feel your pain. Then, hopefully, they can provide a solution and put you out of your misery.’ (pick one)?\n\npackage your problematic code\nother people can run it and feel your pain\nthe first step is to create a reprex\nthey can provide a solution and put you out of your misery\n\nAccording to Gelfand (2021), what are the three key aspects of a reprex (select all that apply)?\n\ndata\nonly the libraries that are necessary and all the libraries that are necessary\nrelevant code and only relevant code\n\nAccording to Wickham (2021) for naming files, how would these files ‘00_get_data.R’, ‘get data.R’ be classified (pick one)?\n\nbad; bad.\ngood; bad.\nbad; good.\ngood; good.\n\nWhich of the following would result in bold text in Quarto (pick one)?\n\n**bold**\n##bold##\n*bold*\n#bold#\n\nWhich option would hide the warnings in a Quarto R chunk (pick one)?\n\necho: false\ninclude: false\neval: false\nwarning = false\nmessage = false\n\nWhich options would run the R code chunk and display the results, but not show the code in a Quarto R chunk (pick one)?\n\necho: false\ninclude: false\neval: false\nwarning = false\nmessage = false\n\nWhy are R Projects important (select all that apply)?\n\nThey help with reproducibility.\nThey make it easier to share code.\nThey make your workspace more organized.\nThey ensure reproducibility.\n\nPlease discuss a circumstance in which an R Project would be useful.\nConsider this sequence: ‘git pull, git status, ________, git status, git commit -m \"My message\", git push’. What is the missing step (pick one)?\n\ngit add -A.\ngit status.\ngit pull.\ngit push.\n\nAssuming the libraries and datasets have been loaded, what is the mistake in this code: DoctorVisits |> select(\"visits\") (pick one)?\n\n\"visits\"\nDoctorVisits\nselect\n|>\n\nWhat is a reprex and why is it important to be able to make one (select all that apply)?\n\nA reproducible example that enables your error to be reproduced.\nA reproducible example that helps others help you.\nA reproducible example during the construction of which you may solve your own problem.\nA reproducible example that demonstrates you have actually tried to help yourself.\n\nThe following code produces an error. Please use reprex (Jennifer Bryan et al. 2019) to build a reproducible example that you could use to get help with it, and submit the reprex using a GitHub Gist. You should simplify many aspects including reducing the number of libraries, changing the dataset, and simplying the filter() and mutate().\n\n\nlibrary(tidyverse)\n\noecd_gdp <- \n  read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.QGDP.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\n\nhead(oecd_gdp)\n\nlibrary(forcats)\nlibrary(dplyr)\n\noecd_gdp_most_recent <- \n  oecd_gdp |> \n  filter(TIME == \"2021-Q3\",\n         SUBJECT == \"TOT\",\n         LOCATION %in% c(\"AUS\", \"CAN\", \"CHL\", \"DEU\", \"GBR\",\n                         \"IDN\", \"ESP\", \"NZL\", \"USA\", \"ZAF\"),\n         MEASURE == \"PC_CHGPY\") |> \n  mutate(european = if_else(LOCATION %in% c(\"DEU\", \"GBR\", \"ESP\"),\n                             \"European\",\n                             \"Not european\"),\n         hemisphere = if_else(LOCATION %in% c(\"CAN\", \"DEU\", \"GBR\", \"ESP\", \"USA\"),\n                             \"Northern Hemisphere\",\n                             \"Southern Hemisphere\"),\n         )\n\nlibrary(ggplot)\nlibrary(patchwork)\n\noecd_gdp_most_recent |> \n  ggplot(mapping = aes(x = LOCATION, y = Value)) |> \n  geom_bar(stat=\"identity\")\n\n\n\n4.6.2 Tutorial\nPlease put together a small Quarto file that downloads a dataset using opendatatoronto, cleans it, and makes a graph. Then exchange it with someone else. Ask them to both read the code and to run it, and to then provide you with feedback about both aspects. Write a one-to-two pages of single-spaced content, about the comments that you received and changes that you could make going forward.\n\n\n4.6.3 Paper\nAt about this point, Paper One Appendix A.1 would be appropriate.\n\n\n\n\n\nAlexander, Monica. 2019. “Reproducibility in Demographic Research.” https://www.monicaalexander.com/posts/2019-10-20-reproducibility/.\n\n\n———. 2021. “Overcoming Barriers to Sharing Code.” YouTube, February. https://youtu.be/yvM2C6aZ94k.\n\n\nBarba, Lorena A. 2018. “Terminologies for Reproducible Research.” https://arxiv.org/abs/1802.03311.\n\n\nBowers, Jake. 2011. “Six Steps to a Better Relationship with Your Future Self.” The Political Methodologist 18 (2): 2–8.\n\n\nBryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk about Version Control?” The American Statistician 72 (1): 20–27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\nBryan, Jennifer, and Jim Hester. 2020. What They Forgot to Teach You about r. https://rstats.wtf/index.html.\n\n\nBryan, Jennifer, Jim Hester, David Robinson, and Hadley Wickham. 2019. Reprex: Prepare Reproducible Example Code via the Clipboard. https://CRAN.R-project.org/package=reprex.\n\n\nBryan, Jenny. 2020. Happy Git and GitHub for the useR. https://happygitwithr.com.\n\n\nChouldechova, Alexandra, Diana Benavides-Prado, Oleksandr Fialko, and Rhema Vaithianathan. 2018. “A Case Study of Algorithm-Assisted Decision Making in Child Maltreatment Hotline Screening Decisions.” In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, edited by Sorelle A. Friedler and Christo Wilson, 81:134–48. Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v81/chouldechova18a.html.\n\n\nEghbal, Nadia. 2020. Working in Public: The Making and Maintenance of Open Source Software. Stripe Press.\n\n\nGelfand, Sharla. 2021. “Make a ReprEx... Please.” YouTube, February. https://youtu.be/G5Nm-GpmrLw.\n\n\nGelman, Andrew. 2016. “What Has Happened down Here Is the Winds Have Changed.” https://statmodeling.stat.columbia.edu/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/.\n\n\nGelman, Andrew, Greggor Mattson, and Daniel Simpson. 2018. “Gaydar and the Fallacy of Decontextualized Measurement.” Sociological Science 5 (12): 270–80. https://doi.org/10.15195/v5.a12.\n\n\nHeil, Benjamin J, Michael M Hoffman, Florian Markowetz, Su-In Lee, Casey S Greene, and Stephanie C Hicks. 2021. “Reproducibility Standards for Machine Learning in the Life Sciences.” Nature Methods 18 (10): 1132–35.\n\n\nHerndon, Thomas, Michael Ash, and Robert Pollin. 2014. “Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff.” Cambridge Journal of Economics 38 (2): 257–79.\n\n\nHillel, Wayne. 2017. How Do We Trust Our Science Code? https://www.hillelwayne.com/how-do-we-trust-science-code/.\n\n\nIsaacson, Walter. 2011. Steve Jobs. Simon & Schuster.\n\n\nKnuth, D. E. 1984. “Literate Programming.” The Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nKuhn, Max, and Hadley Wickham. 2020. Tidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles. https://www.tidymodels.org.\n\n\nLee, Benjamin D. 2018. “Ten Simple Rules for Documenting Scientific Software.” Public Library of Science San Francisco, CA USA.\n\n\nMattson, Greggor. 2017. “Artificial Intelligence Discovers Gayface. Sigh.” https://greggormattson.com/2017/09/09/artificial-intelligence-discovers-gayface/amp/.\n\n\nMerali, Zeeya. 2010. “Computational Science:... Error.” Nature 467 (7317): 775–77.\n\n\nMineault, Patrick, and The Good Research Code Handbook Community. 2021. “The Good Research Code Handbook.” https://doi.org/10.5281/ZENODO.5796873.\n\n\nMiyakawa, Tsuyoshi. 2020. “No Raw Data, No Science: Another Possible Source of the Reproducibility Crisis.” Molecular Brain. Springer.\n\n\nMurphy, Heather. 2017. Why Stanford Researchers Tried to Create a ’Gaydar’ Machine.\n\n\nPineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo Larochelle. 2021. “Improving Reproducibility in Machine Learning Research: A Report from the NeurIPS 2019 Reproducibility Program.” Journal of Machine Learning Research 22.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRilke, Rainer Maria. 1929. Letters to a Young Poet.\n\n\nSilver, Nate. 2020. We Fixed an Issue with How Our Primary Forecast Was Calculating Candidates’ Demographic Strengths. https://fivethirtyeight.com/features/we-fixed-a-mistake-in-how-our-primary-forecast-was-calculating-candidates-demographic-strengths/.\n\n\nSprint, Gina, and Jason Conci. 2019. “Mining Github Classroom Commit Behavior in Elective and Introductory Computer Science Courses.” The Journal of Computing Sciences in Colleges 35 (1).\n\n\nSunstein, Cass R, and Lucia A Reisch. 2017. The Economics of Nudge. Routledge.\n\n\nTierney, Nicholas. 2022. Quarto for Scientists. https://qmd4sci.njtierney.com.\n\n\nTrisovic, Ana, Matthew K. Lau, Thomas Pasquier, and Mercè Crosas. 2022. “A Large-Scale Study on Research Code Quality and Execution.” Scientific Data 9 (1). https://doi.org/10.1038/s41597-022-01143-6.\n\n\nWang, Yilun, and Michal Kosinski. 2018. “Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation from Facial Images.” Journal of Personality and Social Psychology 114 (2): 246.\n\n\nWickham, Hadley. 2021. The Tidyverse Style Guide. https://style.tidyverse.org/index.html.\n\n\nWickham, Hadley, and Jennifer Bryan. 2020. Usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): 1–20. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\nXie, Yihui. 2019. “TinyTeX: A Lightweight, Cross-Platform, and Easy-to-Maintain LaTeX Distribution Based on TeX Live.” TUGboat, no. 1: 30–32. https://tug.org/TUGboat/Contents/contents40-1.html."
  },
  {
    "objectID": "05-on_writing.html",
    "href": "05-on_writing.html",
    "title": "5  On writing",
    "section": "",
    "text": "If you want to be a writer, you must do two things above all others: read a lot and write a lot. There’s no way around these two things that I’m aware of, no shortcut.\nS. King (2000, 145)\nRequired material\nKey concepts and skills"
  },
  {
    "objectID": "05-on_writing.html#introduction",
    "href": "05-on_writing.html#introduction",
    "title": "5  On writing",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\n\n[T]he duty of a scientist is not only to find new things, but to communicate them successfully in at least three forms: 1) Writing papers and books. 2) Prepared public talks. 3) Impromptu talks.\nHamming (1996, 65)\n\n\nPeople who need to write: founders, VCs, lawyers, software engineers, designers, painters, data scientists, musicians, filmmakers, creative directors, physical trainers, teachers, writers. Learn to write.\nSahil Lavingia, 3 February 2020.\n\n\nWriting well has done just as much for me as knowing how to code. I’d add that if you’re intimidated by writing, start a blog and write often about something you’re interested in. You’ll get better. At least that’s what I’ve done for the past 10 years. :)\nVicki Boykis, 3 February 2020.\n\nWe need to write in order to tell our stories. Writing allows us to communicate efficiently. It is also a way to work out what we believe and allows us to get feedback on our ideas. Effective papers are tightly written and well-organized, which makes the story flow and easy to follow. Proper sentence structure, spelling, vocabulary, and grammar are important because they remove distractions and enable each point to be clearly articulated. Effective papers demonstrate understanding of the topic by confidently using relevant terms and techniques and considering issues without being overly verbose. Graphs, tables, and references are used to enhance both the story and its credibility.\nThis chapter is about writing. By the end of it, you will have a better idea of how to write short, detailed, quantitative papers that communicate what you want them to, and do not waste the reader’s time. We write for the reader, not for ourselves. Specifically, we write to be useful to the reader, where ‘[u]seful writing tells people something true and important that they didn’t already know, and tells them as unequivocally as possible’ (Graham 2020). That said, the greatest benefit of writing nonetheless often accrues to the writer, even when we write for our audience. This is because the process of writing is a way to work out what we think and how we came to believe it.\n\nThe way to do a piece of writing is three or four times over, never once. For me, the hardest part comes first, getting something—anything—out in front of me. Sometimes in a nervous frenzy I just fling words as if I were flinging mud at a wall. Blurt out, heave out, babble out something—anything—as a first draft. With that, you have a achieved a sort of nucleus. Then, as you work it over and alter it, you begin to shape sentences that score higher with the ear and the eye. Edit it again—top to bottom. The chances are that about now you’ll be seeing something that you are sort of eager for others to see. And all that takes times. What I have left out is the interstitial time. You finish that first awful blurting, and then you put the thing aside. You get in your car and drive home. On the way, your mind is still knitting at the words. You think of a better way to say something, a good phrase to correct a certain problem. Without that drafted version—if it did not exist—you obviously would not be thinking of things that would improve it. In short, you may be actually writing only two or three hours a day, but your mind, in one way or another, is working on it twenty-four hours a day—yes, while you sleep—but only if some sort of draft of earlier version already exists. Until it exists, writing has not really begun.\nMcPhee (2017, 159)\n\nThe process of writing is a process of re-writing. And the critical task is to get to a first draft as quickly as possible. A complete first draft of a five-to-ten-page quantitative paper can be done in a day. Until that complete first draft exists, it is useful to try to not to delete or even revise anything that was written, regardless of how bad it may seem. Just write.\nOne of the most intimidating things in the world is a blank page, and we deal with this by immediately adding headings such as: ‘Introduction’, ‘Data’, ‘Model’, ‘Results’, and ‘Discussion’. And then add fields in the top matter for the various bits and pieces that are needed, such as ‘title’, ‘date’, ‘author’ and ‘abstract’. This creates a generic outline, and its role is akin to placing on the counter, the ingredients that we will use to prepare dinner (McPhee 2017).\nHaving established this generic outline, we need to develop an understanding of what we are exploring through developing a research question. In theory, we develop a research question, answer it, and then we do all the writing; but that rarely actually happens (Franklin 2005). Instead, we typically have some idea of the question, and our answer, and these become less vague as we write. This is because it is through the process of writing that we refine our thinking (S. King 2000, 131). Having put down some thoughts about the research question, we can start to add dot points in each of the sections, adding sub-sections, with informative sub-headings as needed. We then go back and expand those dot points into paragraphs.\nWhile writing the first draft it is important to ignore the feeling that one is not good enough, or that it is impossible. Just write. We need words on paper, even if they are bad, and the first draft is when we accomplish this. Remove all distractions and just write. Perfectionism is the enemy, and should be set aside. Sometimes this can be accomplished by getting up very early to write, or by creating a deadline, or with a glass or two of wine. One friend puts her baby to sleep to the sound of her typing, with the result being that she must keep typing otherwise the baby will wake up. Creating a sense of urgency can be useful and rather than adding proper citations as we go, which could slow us down, just add something like ‘[TODO: CITE R HERE]’. Do similar with graphs and tables. That is, include textual descriptions such as ‘[TODO: ADD GRAPH THAT SHOWS EACH COUNTRY OVER TIME HERE]’ instead of actual graphs and tables. Focus on adding content, even if it is poorly written, or not ideal. When this is all done, a first draft exists!\nThis first draft will be bad. But it is by writing a bad first draft that we can get to a good second draft, a great third draft, and eventually excellence (Lamott 1994, 20). That first draft will be too long, it will not make sense, it will contain claims that cannot be supported, and some claims that should not be. Having focused on adding content while writing the first draft, to turn that into a second draft, we use the ‘delete’ key extensively, as well as ‘cut’ and ‘paste’. Printing out the paper and using a red pen to move or remove is especially helpful. The process of going from a first draft to a second draft is best done in one sitting, to help with flow and consistency of the story. One aspect of this first re-write is enhancing the story that we want to tell. And another aspect is taking out everything that is not the story (S. King 2000, 57).\nAs we go through what was written in each of the sections, we try to bring some sense to it, with special consideration to how it supports our story. This revision process is the essence of writing (McPhee 2017, 160). We should also fix the references, and add the real graphs and tables. As part of this re-writing process, the paper’s central message tends to develop, and our answers to the research questions tend to become clearer. At this point, aspects such as the introduction can be returned to and, finally, the abstract. Typos and other issues affect the credibility of the work, and so it is important that these are fixed as part of the second draft.\nWe now have a paper that is sensible. The job is to now make it brilliant. Print it out again, and again go through it on paper. It is especially important to brutally remove everything that does not contribute to the story. At about this stage, we may be starting to get too close to the paper. We write for our reader, and so this is a great opportunity to give it to someone else for their comments. We ask them for feedback that enables us to better understand the weak parts of the story. After addressing these, it can be helpful to go through the paper once more, this time reading it aloud. A paper tends to never be ‘done’ and it is more that at a certain point we either run out of time or become sick of the sight of it."
  },
  {
    "objectID": "05-on_writing.html#developing-research-questions",
    "href": "05-on_writing.html#developing-research-questions",
    "title": "5  On writing",
    "section": "5.2 Developing research questions",
    "text": "5.2 Developing research questions\nBoth qualitative and quantitative approaches have their place, but here we focus on quantitative approaches. Qualitative research is important as well, and often the most interesting work has a little of both. When conducting quantitative analysis, we are subject to issues such as data quality, scales, measurement, and sources. We are often especially interested in trying to tease out causality. Regardless, we are trying to learn something about the world. Our research questions need to take this all into account.\nBroadly, there are two ways to go about research:\n\ndata-first; or\nquestion-first.\n\nWhen being data-first, the main issue is working out the questions that can be reasonably answered with the available data. When deciding what these are, it is useful to consider:\n\nTheory: Is there a reasonable expectation that there is something causal that could be determined? For instance, if the question involves charting the stock market, then it might be better to consider haruspex because at least that way we would have something to eat. Questions usually need to have some plausible theoretical underpinning to help avoid spurious relationships.\nImportance: There are plenty of trivial questions that can be answered, but it important to not waste our time or that of the reader. Having an important question can also help with motivation when we find ourselves in, say, the fourth straight week of cleaning data and de-bugging code. It can also make it easier to attract talented employees and funding. That said, there is a balance that is needed. But it is important that the question has a decent chance of being answered. And so attacking a generational-defining question might be best broken up into smaller chunks.\nAvailability: Is there a reasonable expectation of additional data being available in the future? This could allow us to answer related questions and turn this one paper into a research agenda.\nIteration: Is this something that could be run multiple times, or is it a once-off analysis? If it is the former, then it becomes possible to start answering specific research questions and then iterate. But if we can only get access to the data once then we need to think about broader questions.\n\nThere’s a saying, sometimes attributed to Xiao-Li Meng that all of statistics is a missing data problem. And so paradoxically, another way to ask data-first questions to think about which data we do not have. For instance, returning to the neonatal and maternal mortality examples discussed in, respectively, Chapter 1 and Chapter 2, the fundamental problem is that we do not have perfect and complete data about cause of death. If we did, then we could count the number of relevant deaths. Having established the missing data problem, we can take a data-driven approach by looking at the data we do have, and then ask research questions that speak to the extent that we can use that to approximate our hypothetical perfect and complete dataset.\nOne way that some researchers are data-first, is that they develop a particular expertise in the data of some geographical or historical circumstance. For instance, they may be especially knowledgeable about the present-day UK, or late nineteenth century Japan. They then look at the questions that other researchers are asking in other circumstances, and bring their data to that question. For instance, it is common to see a particular question initially asked for the US, and then a host of researchers answer that same question for the UK, Canada, Australia, and many other countries.\nA variant of data-driven research is model-driven research. Here a researcher becomes an expert on some particular statistical approach and then applies that approach whenever there are appropriate datasets.\nWhen trying to be question-first, there is the inverse different issue of being concerned about data availability. The ‘FINER framework’ is used in medicine to help guide the development of research questions. It recommends asking questions that are: Feasible, Interesting, Novel, Ethical, and Relevant (Hulley 2007). Farrugia et al. (2010) builds on FINER with PICOT, which recommends additional considerations: Population, Intervention, Comparison group, Outcome of interest, and Time. It can feel overwhelming trying to write out a question. One way to go about it is to ask a very specific question. Another is to decide whether we are interested in descriptive, predictive, inferential, or causal analysis.\nThese then lead to different types of questions, for instance, descriptive analysis: ‘What does \\(x\\) look like?’; predictive analysis: ‘What will happen to \\(x\\)?’; inferential: ‘How can we explain \\(x\\)?’; and causal: ‘What impact does \\(x\\) have on \\(y\\)?’. Each of these have a role to play.\n\nOften time will be constrained, possibly in interesting ways and these can guide the specifics of the research question. If we are interested in the effect of Trump’s tweets on the stock market, then that can be done just by looking at the minutes (milliseconds?) after he tweets. But what if we are interested in the effect of a cancer drug on long term outcomes? If the effect takes 20 years, then we must either wait a while, or we need to look at people who were treated in 2000, but then we have selection effects and different circumstances to if we give the drug today. Often the only reasonable thing to do is to build a statistical model, but then we need adequate sample sizes, etc.\nWhen answering questions usually, the creation of a counterfactual is crucial. Briefly, a counterfactual is an if-then statement in which the ‘if’ is false. Consider the example of Humpty Dumpty from Lewis Carroll’s Through the Looking-Glass (Carroll 1871).\n\n‘What tremendously easy riddles you ask!’ Humpty Dumpty growled out. ‘Of course I don’t think so! Why, if ever I did fall off—which there’s no chance of—but if I did—’ Here he pursed his lips and looked so solemn and grand that Alice could hardly help laughing. ‘If I did fall,’ he went on, ‘The King has promised me—with his very own mouth-to-to-’\n\nHumpty is satisfied with what would happen if he were to fall off, even though he is similarly satisfied that this would never happen. It is this comparison group that often determines the answer to a question. For instance, consider the effect of VO2 max on the outcome of bike race. If we compare over the general population then it is an important variable, but if we only compare over elite athletes, then it is less important, because of selection.\n\n\n\n\n\n\n\n\n\nTwo aspects to be especially aware of when deciding on a research question are selection bias, and measurement bias.\nSelection bias occurs when the results depend on who is in the sample. One of the pernicious aspects of selection bias is that we need to know about its existence in order to do anything about it. But many default diagnostics will not identify selection bias. In an A/B testing set-up, which we discuss in Chapter @ref(hunt-data), A/A testing can help us to identify selection bias. And more generally, comparing the properties of the sample, such as age-group, gender, and education, with characteristics of the population. But the fundamental problem with selection bias and observational data, is that, as Dr Jill Sheppard, Lecturer, Australian National University, says, people who respond to surveys are weird. And this weirdness likely generalizes to almost any method of collecting data.\nThere is a pernicious aspect of selection bias, which is that it pervades every aspect of our analysis. Even a sample that starts off as representative, may become selected over time. For instance, the survey panels used for polling, discussed in Chapter 8, need to be updated from time to time because the folks who do not get anything out of it stop responding.\nAnother bias to be aware of is measurement bias, which when the results are affected by how the data were collected.For instance, if we were to ask respondents their income, then we may get different answers in-person, compared with an online survey."
  },
  {
    "objectID": "05-on_writing.html#writing",
    "href": "05-on_writing.html#writing",
    "title": "5  On writing",
    "section": "5.3 Writing",
    "text": "5.3 Writing\n\nI had not indeed published anything before I commenced “The Professor”, but in many a crude effort, destroyed almost as soon as composed, I had got over any such taste as I might once have had for ornamented and redundant composition, and come to prefer what was plain and homely.\nThe Professor (Brontë 1857).\n\nWe discuss the following components: title, abstract, introduction, data, results, discussion, figures, tables, equations, and technical terms. Throughout all sections of a paper it is important that we are as brief and specific as possible.\n\n5.3.1 Title\nA title is the first opportunity that we have to engage our reader in our story. Ideally, we are able to tell our reader exactly what we found. Effective titles are critical because otherwise papers will be ignored by readers. While a title does not have to be ‘cute’, it does need to be effective. This means it needs to make the story clear.\nOne example of a title that is good enough is ‘On the 2016 Brexit referendum’. This title is useful because the reader at least knows what the paper will be about. But it is not particular informative or enticing. A slightly better variant could be ‘On the ’Vote Leave’ outcome in the 2016 Brexit referendum’. This variant adds specifically which is particularly informative. Finally, another variant would be ‘Vote Leave outperforms in rural areas in the 2016 Brexit referendum: Evidence from a Bayesian hierarchical model’. Here the reader knows the approach of the paper and also the main take-away.\nWe will consider a few examples of particularly effective titles. Hug et al. (2019) uses ‘National, regional, and global levels and trends in neonatal mortality between 1990 and 2017, with scenario-based projections to 2030: a systematic analysis’. Here it is clear what the paper is about and the methods that are used. R. Alexander and Alexander (2021) uses ‘The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament between 1901 and 2018’. While the method used in that paper is not clear from the title, the main finding it, along with a good deal of information about what the content will be. And finally, M. J. Alexander, Kiang, and Barbieri (2018) uses ‘Trends in Black and White Opioid Mortality in the United States, 1979–2015’.\nA title is often among the last aspects of a paper to be finalized. While getting through the first draft, we would typically just use a working title that is good enough to get the job done. We then refine it over the course of redrafting. The title needs to reflect the final story of the paper, and this is not usually something that we know at the start. We are interested in striking a balance between getting our reader interested enough to read the paper, and conveying enough of the content so as to be useful (Hayot 2014). We can think here of classic books, such as Macaulay’s History of England from the Accession of James the Second, or Churchill’s A History of the English-Speaking Peoples. Both are clear about what the content is, and, for their target audience, spark interest.\nOne specific approach is the form: ‘Exciting content: Specific content’, for instance, ‘Returning to their roots: Examining the performance of ’Vote Leave’ in the 2016 Brexit referendum’. Kennedy and Gelman (2020) provides a particular nice example of this approach with ‘Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample’, as does Craiu (2019) with ‘The Hiring Gambit: In Search of the Twofer Data Scientist’. A close variant of this is ‘A question? And an answer’. For instance, Cahill, Weinberger, and Alkema (2020) with ‘What increase in modern contraceptive use is needed in FP2020 countries to reach 75% demand satisfied by 2030? An assessment using the Accelerated Transition Method and Family Planning Estimation Model’. As one gains experience with this variant, it becomes possible to know when it is appropriate to drop the answer part yet remain effective, such as Briggs (2021) with ‘Why Does Aid Not Target the Poorest?’. Another specific approach is ‘Specific content then broad content’ or inversely. For instance, ‘Rurality, elites, and support for ’Vote Leave’ in the 2016 Brexit referendum’ or ‘Support for ’Vote Leave’ in the 2016 Brexit referendum, rurality and elites. This approach is used by Tolley and Paquet (2021) with ‘Gender, municipal party politics, and Montreal’s first woman mayor’.\n\n\n5.3.2 Abstract\nFor a five-to-ten-page paper, a good abstract is a three to five sentence paragraph. For a longer paper the abstract can be slightly longer. The abstract needs to specify the story of the paper, and the objective of an abstract is to convey what was done and why it matters. To do this an abstract typically touches on the context of the work, its objectives, approach, and findings.\nMore specifically, a good recipe for an abstract is: first sentence: specify the general area of the paper and encourage the reader; second sentence: specify the dataset and methods at a general level; third sentence: specify the headline result; and a fourth sentence about implications.\nWe see this pattern in a variety of abstracts. For instance, Tolley and Paquet (2021) draw in the reader with their first sentence by mentioning the election of the first woman mayor in 400 years. The second sentence is clear about what is done in the paper. The third paper tells the reader how it is done i.e. a survey. And the fourth sentence adds some detail. The fifth and final sentence makes the main take-away from the paper clear.\n\nIn 2017, Montreal elected Valérie Plante, the first woman mayor in the city’s 400-year history. Using this election as a case study, we show how gender did and did not influence the outcome. A survey of Montreal electors suggests that gender was not a salient factor in vote choice. Although gender did not matter much for voters, it did shape the organization of the campaign and party. We argue that Plante’s victory can be explained in part by a strategy that showcased a less leader-centric party and a degendered campaign that helped counteract stereotypes about women’s unsuitability for positions of political leadership.\n\nSimilarly, Beauregard and Sheppard (2021) make broader environment clear within the first two sentences, and the specific contribution of this paper to that environment. The third and fourth sentences makes the data source clear and also the main findings. The fifth and sixth sentences add specificity here that would be of interest to likely readers of this abstract i.e. academic political science experts. And then the final sentence makes it clear the position of the authors.\n\nPrevious research on support for gender quotas focuses on attitudes toward gender equality and government intervention as explanations. We argue the role of attitudes toward women in understanding support for policies aiming to increase the presence of women in politics is ambivalent—both hostile and benevolent forms of sexism contribute in understanding support, albeit in different ways. Using original data from a survey conducted on a probability-based sample of Australian respondents, our findings demonstrate that hostile sexists are more likely to oppose increasing of women’s presence in politics through the adoption of gender quotas. Benevolent sexists, on the other hand, are more likely to support these policies than respondents exhibiting low levels of benevolent sexism. We argue this is because benevolent sexism holds that women are pure and need protection; they do not have what it takes to succeed in politics without the assistance of quotas. Finally, we show that while women are more likely to support quotas, ambivalent sexism has the same relationship with support among both women and men. These findings suggest that aggregate levels of public support for gender quotas do not necessarily represent greater acceptance of gender equality generally.\n\nAnd finally, Briggs (2021) begins with a claim that seems unquestionably true. In the second sentence he then claims to have found that it is false. The third sentence specifies the extent of this claim, and the fourth sentence details how he comes to this position, before providing more detail. The final two sentences speak broad implications and importance.\n\nForeign-aid projects typically have local effects, so they need to be placed close to the poor if they are to reduce poverty. I show that, conditional on local population levels, World Bank (WB) project aid targets richer parts of countries. This relationship holds over time and across world regions. I test five donor-side explanations for pro-rich targeting using a pre-registered conjoint experiment on WB Task Team Leaders (TTLs). TTLs perceive aid-receiving governments as most interested in targeting aid politically and controlling implementation. They also believe that aid works better in poorer or more remote areas, but that implementation in these areas is uniquely difficult. These results speak to debates in distributive politics, international bargaining over aid, and principal-agent issues in international organizations. The results also suggest that tweaks to WB incentive structures to make ease of project implementation less important may encourage aid to flow to poorer parts of countries.\n\nThe journal Nature provides a guide for constructing an abstract. They recommend a structure that results in an abstract of six parts, that add up to around 200 words.\n\nA basic introductory sentence that is comprehensible to a wide audience.\nA more detailed sentence about background that is relevant to likely readers.\nA sentence that states the general problem.\nSentences that summarize and then explain the main results.\nA sentence about general context.\nAnd finally, a sentence about the broader perspective.\n\nIt is critical that the first sentence of an abstract is not vacuous. Assuming the reader continued past the title, this first sentence is the next opportunity that we have to implore them to keep reading our paper. And then the second sentence of the abstract, and so on. Work and re-work the abstract until it is so good that you would be fine if that is the only thing that was read.\n\n\n5.3.3 Introduction\nAn introduction needs to be self-contained and convey everything that a reader needs to know. It is important to recognize that we are not writing a mystery story. Instead, we want to give-away the most important points in the introduction. For a six-page paper, an introduction may be two or three paragraphs of main content. Hayot (2014, 90) describes the goal of an introduction is to engage the reader, locate them in some discipline and background, and then tell them what happens in the rest of the paper. It is completely reader-focused.\nThe introduction should set the scene and give the reader some background. For instance, we typically start a little broader. This provides some context to the paper. We then describe how the paper fits into that context, and give some high-level results, especially focused on the one key result that is the main part of the story. We provide more detail here than we provided in the abstract, but not the full extent. And the final bit of main content is to broadly discuss next steps. Finally, we finish the introduction with an additional short final paragraph that highlights the structure of the paper.\nAs an example (with made-up details):\n\nThe UK Conservative Party has always done well in rural electorates. And the 2016 Brexit vote was no different with a significant different in support between rural and urban areas. But even by the standard of rural support for conservative issues, support for ‘Vote Leave’ was unusually strong with ‘Vote Leave’ being most heavily supported in the East Midlands and the East of England, while the strongest support for ‘Remain’ was in Greater London.\nIn this paper we look at why the performance of ‘Vote Leave’ in the 2016 Brexit referendum was so correlated with rurality. We construct a model in which support for ‘Vote Leave’ at a voting area level, is explained by the number of farms in the area, the average internet connectivity, and the median age. We find that as the median age of an area increases, the likelihood that an area supported ‘Vote Leave’ decreases by 14 percentage points. Future work could look at the effect of having a Conservative MP which would allow a more nuanced understanding of these effects.\nThe remainder of this paper is structured as follows: Section 2 discusses the data, Section 3 discusses the model, Section 4 presents the results, and finally Section 5 discusses our findings and some weaknesses.\n\nThe introduction needs to be self-contained and tell your reader everything that they need to know. A reader should be able to only read the introduction and have an accurate picture of all the major aspects that they would if they were to read the whole paper. It would be rare to include graphs or tables in the introduction. An introduction always closes with the structure of the paper. For instance (and this is just a rough guide) an introduction for a 10-page paper, should probably be about 3 or 4 paragraphs, or 10 per cent, but it depends on specifics.\n\n\n5.3.4 Data\nRobert Caro, Lyndon B. Johnson’s biographer, describes the importance of conveying ‘a sense of place’ when writing biography (Caro 2019, 141). This he defines as ‘the physical setting in which a book’s action is occurring: to see it clearly enough, in sufficient detail, so that he feels as if he himself were present while the action is occurring.’ He provides the following example:\n\nWhen Rebekah walked out the front door of that little house, there was nothing—a roadrunner streaking behind some rocks with something long and wet dangling from his beak, perhaps, or a rabbit disappearing around a bush so fast that all she really saw was the flash of a white tail—but otherwise nothing. There was no movement except for the ripple of the leaves in the scattered trees, no sound except for the constant whisper of the wind… If Rebekah climbed, almost in desperation, the hill in the back of the house, what she saw from its crest was more hills, an endless vista of hills, hills on which there was visible not a single house… hills on which nothing moved, empty hills with, above them, empty sky; a hawk circling silently overhead was an event. Bus most of all, there was nothing human, no one to talk to.\nCaro (2019, 146)\n\nHow thoroughly we can imagine the circumstances of Rebekah Baines Johnson (Lyndon B. Johnson’s mother). We need to provide our reader with the same sense of place for our dataset.  When writing our papers, we need to achieve that same sense of place, for our data, as Caro is able to provide for the Hill county. We do this by being as explicit as possible about showing our dataset. We typically have a whole section about it and this is designed to show the reader, as closely as possible, the actual data that underpin our story.\nWhen writing the data section, we are beginning our answer to the critical question about our claims, which is, how is it possible to know this? (McPhee 2017, 78). The preeminent example of a data section is provided by Doll and Hill (1950), who are interested in the effect smoking between control and treatment groups. They begin by clearly describing their dataset. They then use tables to display relevant cross-tabs. And use graphs to contrast their groups.\nIn the data section we need to thoroughly discuss the variables in the dataset that we are using. If there are other datasets that could have been used, but were not, then these should be mentioned and our choices justified. If variables were constructed or combined, then this process and motivation should be explained.\nTo get a sense of the data, it is important that the reader is able to understand what the data that underpin the results look like. This means that we should graph the actual data that are used in our analysis, or as close to them as possible. And we should also include tables of summary statistics. If the dataset was created from some other source, then it can also help to include an example of that original source. For instance, if the dataset was created from survey responses then the survey form should be included, potentially in an appendix.\nThe data section will also have figures and tables. Here some judgment is required. While it is important that the reader has the opportunity to understand the details, it may be that some are better placed in an appendix. Figure and tables are a critical aspect of convincing people of a story. In a graph we can show the data and then let the reader decide for themselves. And using a table, we can more easily summarize our dataset. At the very least, every variable needs to be shown in a graph and summarized in a table. Figures and tables should be numbered and then cross-referenced in the text, for instance, “Figure 1 shows…”, “Table 1 describes…”. For every graph and table there should be extensive accompanying text that describes their main aspects, and adds additional detail.\nWe discuss the components of graphs and tables, including titles and labels, in Chapter @ref(static-communication). But here we will discuss captions, as they are between text and the graph or table. Captions need to be informative and self-contained. As Cleveland (1994, 57) says, the ‘interplay between graph, caption, and text is a delicate one’, however the reader should be able to read only the caption and understand what the graph or table shows. A caption that is two of three lines long would is not necessarily inappropriate. And all aspects of the graph or table should be explained. For instance, consider Figures Figure 5.1 and Figure 5.2 from Bowley (1901, 151), which are both exceptionally clear, and self-contained.\n\n\n\nFigure 5.1: Example of a well-captioned figure\n\n\n\n\n\nFigure 5.2: Example of a well-captioned table\n\n\nThe choice between a table and a graph comes down to how much information is to be conveyed. In general, if there is specific information that should be considered, such as a summary statistic, then a table is a good option, while if we are interested in the reader making comparisons and understanding trends then a graph is a good option (Gelman, Pasarica, and Dodhia 2002).\nFinally, if there is relevant literature then we would discuss it throughout the paper as appropriate. For instance, when there is literature relevant to the data then it should be discussed in this section, literature relevant to the model, results, or discussion should be mentioned as appropriate in those sections. It is rarely necessary to have a separate literature review section.\n\n\n5.3.5 Model\nWe will often build a statistical model that we will use to explore the data, and we often have a specific section about this. At a minimum it is important to clearly specify equation/s that describe the model being used, and explain their components with plain language and cross-references.\n\n\n\n\n\n\n\n\n\n\n\n\nThe model section typically begins with the model being written out, explained, and justified. Depending on the expected reader, some background may be needed. After specifying the model with appropriate mathematical notation and cross-referencing it, the components of the model are then typically defined and explained. It is especially important to define each aspect of the notation. This helps convince the reader that the model was well-chosen and enhances the credibility of the paper. The model’s variables should correspond to those that were discussed in the data section, making a clear link between the two sections.\nThere should be some discussion of how features enter the model and why. For instance, some examples could include, why use ages rather than age-groups, why does state/province have a levels effect, and why is gender a categorical variable. In general, we are trying to convey a sense that this is the model for the situation. We want the reader to understand how the aspects that were discussed in the data section assert themselves in the modelling decisions that were made.\nThe model section should close with some discussion of the assumptions that underpin the model, and a brief discussion of alternative models, or variants, and strengths and weaknesses made clear. It should be clear in the reader’s mind why it was this model that was chosen.\nAt some point in this section, it is usually appropriate to specify the software that was used to run the model, and to provide some evidence of thought about the circumstances in which the model may not be appropriate. The later point would typically be expanded on in the discussion. And there should be evidence of model validation and checking, model convergence, and/or diagnostic issues. Again, there is a balance needed here, and some of this content may be more appropriate placed in appendices.\nWhen technical terms are used, they should be briefly explained in plain language for readers who might not be familiar with it. For instance, M. Alexander (2019) integrates an explanation of the Gini coefficient that brings the reader along.\n\nTo look at the concentration of baby names, let’s calculate the Gini coefficient for each country, sex and year. The Gini coefficient measures dispersion or inequality among values of a frequency distribution. It can take any value between 0 and 1. In the case of income distributions, a Gini coefficient of 1 would mean one person has all the income. In this case, a Gini coefficient of 1 would mean that all babies have the same name. In contrast, a Gini coefficient of 0 would mean names are evenly distributed across all babies.\n\n\n\n5.3.6 Results\nTwo excellent examples of results sections provided by Kharecha and Hansen (2013) and Kiang et al. (2021). In the results section, we want to communicate the outcomes of the model in a clear way and without too much in the way of discussion of implications. The results section likely requires summary statistics, tables, and graphs. Each of those aspects should be cross-referenced and have text associated with them that details what is seen in them. This section should strictly relay results; that is, we are interested in what the results are, rather than what they mean.\nThis section would also typically include table/s of coefficient estimates based on the modelling that we used to further explore the data. Various features of the estimates should be discussed, and differences between the models explained. It may be that different subsets of the data are considered separately. Again, all graphs and tables need to have plain language text accompany them. A rough guide is that the amount of text should be at least equal to the amount of space taken up by the tables and graphs. For instance, if a full page is used to display a table of coefficient estimates, then that should be cross-referenced and accompanied by at least a full page of text about that table.\n\n\n5.3.7 Discussion\nA discussion section may be the final section of a paper and would typically have four or five sub-sections.\nThe discussion section would typically begin with a sub-section that comprises a one- or two-paragraph summary of what was done in the paper. This would be followed by two or three sub-sections that are devoted to the key things that we learn about the world from this paper. For instance, there are typically a few implications that come from the modelling results. These few sub-sections are the main opportunity to justify or detail the implications of the story being told in the paper. Typically, these sub-sections do not see newly introduced graphs or tables, but are instead focused on what we learn from those that were introduced in earlier sections. It may be that some of the results are discussed in relation to what others have found, and differences could be attempted to be reconciled here.\nFollowing these sub-sections of what we learn about the world, we would typically have a sub-section focused on some of the weaknesses of what was done. This could concern aspects such as the data that were used, the approach, and the model. And the final sub-section is typically a few paragraphs that specify what is left to learn, and how future work could proceed.\nIn general, we would expect this section to take at least twenty-five per cent of the total paper. For instance, in an eight-page paper, we would expect at least two pages of discussion.\n\n\n5.3.8 Brevity, typos, and grammar\nBrevity is important. Partly this is because we write for the reader, and the reader has other priorities. But it is also because as the writer it focuses us to consider what our most important points are, how we can best support them, and where our arguments are weakest. Jean Chrétien, the former Canadian Prime Minister, describes how ‘[t]o allow me to get to the heart of an issue quickly, I asked the officials to summarize their documents in two or three pages and attach the rest of the materials as background information. I soon discovered that this was a problem only for those who didn’t really know what they were talking about.’ (Chrétien 2007, 105).\nThis experience is not unique to Canada. For instance, Oliver Letwin, the former British Conservative Cabinet member, describes there as being ‘a huge amount of terrible guff, at huge, colossal, humongous length coming from some departments’ and how he asked ‘for them to be one quarter of the length’ (Hughes and Rutter 2016). He found that the departments were able to accommodate this request without losing anything important.\nThis experience is also not new. For instance, Churchill asked for brevity during the Second World War, saying ‘the discipline of setting out the real points concisely will prove an aid to clearer thinking’. And the letter from Szilard and Einstein to FDR that was the catalyst for the Manhattan Project was only two pages.\nThis experience is also not unique to academia. For instance, one of the foundations of Amazon, which is one of the world’s largest companies, is clear writing. Specifically, instead of PowerPoint presentations, Jeff Bezos asked for ‘[w]ell structured, narrative text… [which] forces better thought and better understanding of what’s more important than what, and how things are related.’\nZinsser (1976) goes further and describes ‘the secret of good writing’ being ‘to strip every sentence to its cleanest components.’ Every sentence should be simplified to its essence. And every word that does not contribute should be removed.\nTypos and other grammatical mistakes affect the credibility of claims. If the reader cannot trust us to use a spell-checker, then why should they trust us to use logistic regression? Microsoft Word and Google Docs are useful here for their spell-checkers: copy/paste from R Markdown, look for the red and green lines, and fix them in R Markdown.\nWe are not worried about the n-th degree of grammatical content. Instead, we are interested in grammar and sentence structure that occurs in conversational language use (S. King 2000, 118). The way to develop that comfort is by reading a lot and asking others to read your work also.\nUnnecessary words, typos, and grammatical issues should be removed from papers with a fanatical zeal.\n\n\n5.3.9 Rules\nA variety of authors have established rules for writing, including famously, Orwell (1946), which were reimagined by The Economist (2013). And Fiske and Kuriwaki (2021) have a list of rules for scientific papers. A further reimagining, focused on telling stories with data, could be:\n\nFocus on the reader and their needs. Everything else is comment.\nEstablish a logical structure and rely on that structure to tell the story.\nWrite a first draft as quickly as possible.\nRe-write that extensively and without favor.\nAim to be concise and direct. Remove as many words as possible.\nUsing words precisely. Stock-markets rise or fall, not improve or worsen.\nUse short sentence where possible.\nAvoid jargon.\nWrite as though your work will be on the front page of a newspaper. Because it could be."
  },
  {
    "objectID": "05-on_writing.html#exercises-and-tutorial",
    "href": "05-on_writing.html#exercises-and-tutorial",
    "title": "5  On writing",
    "section": "5.4 Exercises and tutorial",
    "text": "5.4 Exercises and tutorial\n\n5.4.1 Exercises\n\nAccording to the Introduction of Zinsser (1976), whose picture hangs in Zinsser’s office?\n\nCharlotte Bronte\nE. M. Forster\nE. B. White\nStephen King\n\nAccording to Chapter 2 of Zinsser (1976), what is the secret to good writing?\n\nCorrect sentence structure and grammar.\nThe use of long words, adverbs, and passive voice.\nThorough planning.\nStrip every sentence to its cleanest components.\n\nAccording to Chapter 2 of Zinsser (1976), what must a writer constantly ask?\n\nWhat am I trying to say?\nWho am I writing for?\nHow can this be re-written?\nWhy does this matter?\n\nWhich two repeated words, for instance in Chapter 3, characterize the advice of Zinsser (1976)?\n\nRe-write, re-write.\nRemove, remove.\nSimplify, simplify.\nLess, less.\n\nAccording to Chapter 5 of Zinsser (1976), a writer should never say anything in writing that they wouldn’t say in?\n\nPrivate\nPublic\nConversation\nSpeeches\n\nAccording to Chapter 6 of Zinsser (1976), what are the only tools that a writer has?\n\nPapers\nWords\nParagraphs\nSentences\n\nAccording to G. King (2006), what is the key task of subheadings (pick one)?\n\nEnable a reader who randomly falls asleep but keeps turning pages to know where they are.\nBe broad and sweeping so that a reader is impressed by the importance of the paper.\nUse acronyms to integrate the paper into the literature.\n\nAccording to G. King (2006), what is the maximum length of an abstract (pick one)?\n\nTwo hundred words.\nTwo hundred and fifty words.\nOne hundred words.\nOne hundred and fifty words.\n\nAccording to G. King (2006), in a paper, raw computer output should be (pick one)?\n\nCommented out.\nNot included.\nIncluded.\n\nAccording to G. King (2006), if our standard error was 0.05 then which of the following specificity for a coefficient would be silly (select all that apply)?\n\n2.7182818\n2.718282\n2.72\n2.7\n2.7183\n2.718\n3\n2.71828\n\nWhen should we try not to use the ‘delete’ key (pick one)?\n\nWhile writing the first draft.\nWhile writing the second draft.\nWhile writing the third draft.\nThe ‘delete’ key should always be used.\n\nHow long should a first draft take to write of a five-to-ten-page paper (pick one)?\n\nOne hour\nOne day\nOne week\nOne month\n\nWhat is a key aspect of the re-drafting process (select all that apply)?\n\nGoing through it with a red pen to remove unneeded words.\nPrinting the paper and reading a physical copy.\nCutting and pasting to enhance flow.\nReading it aloud.\nExchanging it with others.\n\nWhat are three features of a good research question (write a paragraph or two)?\nWhat are some of the challenges of being ‘data-first’ (write a paragraph or two)?\nWhat are some of the challenges of being ‘question-first’ (write a paragraph or two)?\nWhat is a counterfactual (pick one)?\n\nIf-then statements in which the if does not happen.\nIf-then statements in which the if happens.\nStatements that are either true or false.\nStatements that are neither true or false.\n\nWhich of the following is the best title (pick one)?\n\n“Problem Set 1”\n“Unemployment”\n“Examining England’s Unemployment (2010-2020)”\n“England’s Unemployment Increased between 2010 and 2020”\n\nWhich of the following is the best title (pick one)?\n\n“Problem Set 2”\n“Standard errors”\n“On standard errors with small samples”\n\nWhich word/s can be removed from the following sentence without substantially affecting its meaning (select all that apply)? ‘Like many parents, when our children were born, one of the first things that my wife and I did regularly was read stories to them.’\n\nfirst\nregularly\nstories\n\nPlease write a new title for either Barron et al. (2018) or Fourcade and Healy (2017).\nPlease write a new title for the first article from the list of articles from The New Yorker that you read.\nPlease write a new title for the other article from the list of articles from The New Yorker that you read.\nPlease write a new four-sentence abstract for Chambliss (1989)\nPlease write a new four-sentence abstract for Doll and Hill (1950) or Student (1908) or Kharecha and Hansen (2013).\nPlease write an abstract for the first article from the list of ‘miscellaneous’ articles that you read.\nPlease write an abstract for the other article from the list of ‘miscellaneous’ articles that you read.\nUsing only the 1000-most popular words in the English language – https://xkcd.com/simplewriter/ – re-write the following so that it retains its original meaning:\n\n\nWhen using data, we try to tell a convincing story. It may be as exciting as predicting elections, as banal as increasing internet advertising click rates, as serious as finding the cause of a disease, or as fun as forecasting basketball games. In any case the key elements are the same.\n\n\n\n5.4.2 Tutorial\nCaro (2019, xii) writes at least one thousand words almost every day. In this tutorial we will write every day for a week. Each day pick one of the papers specified in the required materials and complete the following tasks:\n\nTranscribe, by writing each word yourself, the entire introduction.\n(This idea comes from McPhee (2017, 186).) Re-write the introduction so that it is five lines (or 10 per cent, whichever is less) shorter.\nTranscribe, by writing each word yourself, the abstract.\nRe-write a new, four-sentence, abstract for the paper.\n(This idea comes from comes from Chelsea Parlett-Pelleriti.) Write a second version of your new abstract using only the one thousand most popular words in the English language: https://xkcd.com/simplewriter/.\nDetail three points about the way the paper is written that you like\nDetail one point about the way the paper is written that you do not like.\n\nPlease use R Markdown to produce a single PDF for the whole week. Make judicious use of headings and sub-headings to structure your submission. Submit the PDF.\n\n\n\n\n\nAlexander, Monica. 2019. “The Concentration and Uniqueness of Baby Names in Australia and the US,” January. https://www.monicaalexander.com/posts/2019-20-01-babynames/.\n\n\nAlexander, Monica J, Mathew V Kiang, and Magali Barbieri. 2018. “Trends in Black and White Opioid Mortality in the United States, 1979–2015.” Epidemiology (Cambridge, Mass.) 29 (5): 707.\n\n\nAlexander, Rohan, and Monica Alexander. 2021. “The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament Between 1901 and 2018.” https://arxiv.org/abs/2111.09299.\n\n\nBarron, Alexander TJ, Jenny Huang, Rebecca L Spang, and Simon DeDeo. 2018. “Individuals, Institutions, and Innovation in the Debates of the French Revolution.” Proceedings of the National Academy of Sciences 115 (18): 4607–12.\n\n\nBeauregard, Katrine, and Jill Sheppard. 2021. “Antiwomen but Proquota: Disaggregating Sexism and Support for Gender Quota Policies.” Political Psychology 42 (2): 219–37.\n\n\nBland, J Martin, and DouglasG Altman. 1986. “Statistical Methods for Assessing Agreement Between Two Methods of Clinical Measurement.” The Lancet 327 (8476): 307–10.\n\n\nBowley, Arthur Lyon. 1901. Elements of Statistics. P. S. King.\n\n\nBriggs, Ryan C. 2021. “Why Does Aid Not Target the Poorest?” International Studies Quarterly 65 (3): 739–52.\n\n\nBrontë, Charlotte. 1857. The Professor.\n\n\nCahill, Niamh, Michelle Weinberger, and Leontine Alkema. 2020. “What Increase in Modern Contraceptive Use Is Needed in Fp2020 Countries to Reach 75% Demand Satisfied by 2030? An Assessment Using the Accelerated Transition Method and Family Planning Estimation Model.” Gates Open Research 4.\n\n\nCaro, Robert. 2019. Working. 1st ed. Knopf.\n\n\nCarroll, Lewis. 1871. Through the Looking-Glass. Macmillan.\n\n\nChambliss, Daniel F. 1989. “The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers.” Sociological Theory 7 (1): 70–86.\n\n\nChrétien, Jean. 2007. My Years as Prime Minister. Knopf Canada.\n\n\nCleveland, William. 1994. The Elements of Graphing Data. 2nd ed. Hobart Press.\n\n\nCraiu, Radu V. 2019. “The Hiring Gambit: In Search of the Twofer Data Scientist.” Harvard Data Science Review 1 (1).\n\n\nDoll, Richard, and A Bradford Hill. 1950. “Smoking and Carcinoma of the Lung.” British Medical Journal 2 (4682): 739.\n\n\nFarrugia, Patricia, Bradley A Petrisor, Forough Farrokhyar, and Mohit Bhandari. 2010. “Research Questions, Hypotheses and Objectives.” Canadian Journal of Surgery 53 (4): 278.\n\n\nFiske, Susan T, and Shiro Kuriwaki. 2021. “Words to the Wise on Writing Scientific Papers.”\n\n\nFourcade, Marion, and Kieran Healy. 2017. “Seeing Like a Market.” Socio-Economic Review 15 (1): 9–29.\n\n\nFranklin, Laura R. 2005. “Exploratory Experiments.” Philosophy of Science 72 (5): 888–99.\n\n\nGelman, Andrew, Cristian Pasarica, and Rahul Dodhia. 2002. “Let’s Practice What We Preach: Turning Tables into Graphs.” The American Statistician 56 (2): 121–30.\n\n\nGraham, Paul. 2020. How to Write Usefully. http://paulgraham.com/useful.html.\n\n\nHamming, Richard W. 1996. The Art of Doing Science and Engineering. Stripe Press.\n\n\nHayot, Eric. 2014. The Elements of Academic Style. Columbia University Press.\n\n\nHeckman, James J. 1979. “Sample Selection Bias as a Specification Error.” Econometrica: Journal of the Econometric Society, 153–61.\n\n\nHug, Lucia, Monica Alexander, Danzhen You, Leontine Alkema, and UN Inter-agency Group for Child. 2019. “National, Regional, and Global Levels and Trends in Neonatal Mortality Between 1990 and 2017, with Scenario-Based Projections to 2030: A Systematic Analysis.” The Lancet Global Health 7 (6): e710–20.\n\n\nHughes, Nicola, and Jill Rutter. 2016. Oliver Letwin. https://www.instituteforgovernment.org.uk/ministers-reflect/person/oliver-letwin/.\n\n\nHulley, Stephen B. 2007. Designing Clinical Research. Lippincott Williams & Wilkins.\n\n\nJoyner, MICHAEL J. 1991. “Modeling: Optimal Marathon Performance on the Basis of Physiological Factors.” Journal of Applied Physiology 70 (2): 683–87.\n\n\nKennedy, Lauren, and Andrew Gelman. 2020. “Know Your Population and Know Your Model: Using Model-Based Regression and Poststratification to Generalize Findings Beyond the Observed Sample.” https://arxiv.org/abs/1906.11323.\n\n\nKharecha, Pushker A, and James E Hansen. 2013. “Prevented Mortality and Greenhouse Gas Emissions from Historical and Projected Nuclear Power.” Environmental Science & Technology 47 (9): 4889–95.\n\n\nKiang, Mathew V, Alexander C Tsai, Monica J Alexander, David H Rehkopf, and Sanjay Basu. 2021. “Racial/Ethnic Disparities in Opioid-Related Mortality in the USA, 1999–2019: The Extreme Case of Washington DC.” Journal of Urban Health 98 (5): 589–95.\n\n\nKing, Gary. 2006. “Publication, Publication.” PS: Political Science & Politics 39 (1): 119–25.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft. Scribner.\n\n\nKoenker, Roger, and Achim Zeileis. 2009. “On Reproducible Econometric Research.” Journal of Applied Econometrics 24 (5): 833–47.\n\n\nLamott, Anne. 1994. Bird by Bird: Some Instructions on Writing and Life. Anchor Books.\n\n\nLucas Jr, Robert E. 1978. “Asset Prices in an Exchange Economy.” Econometrica: Journal of the Econometric Society, 1429–45.\n\n\nMcPhee, John. 2017. Draft No. 4. Farrar, Straus; Giroux.\n\n\nOrwell, George. 1946. Politics and the English Language. https://www.orwellfoundation.com/the-orwell-foundation/orwell/essays-and-other-works/politics-and-the-english-language/.\n\n\nSamuel, Arthur L. 1959. “Some Studies in Machine Learning Using the Game of Checkers.” IBM Journal of Research and Development 3 (3): 210–29.\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika, 1–25.\n\n\nThe Economist. 2013. Johnson: Those Six Little Rules: George Orwell on Writing. https://www.economist.com/prospero/2013/07/29/johnson-those-six-little-rules.\n\n\nTolley, Erin, and Mireille Paquet. 2021. “Gender, Municipal Party Politics, and Montreal’s First Woman Mayor.” Canadian Journal of Urban Research 30 (1): 40–52.\n\n\nWardrop, Robert L. 1995. “Simpson’s Paradox and the Hot Hand in Basketball.” The American Statistician 49 (1): 24–28.\n\n\nZinsser, William. 1976. On Writing Well."
  },
  {
    "objectID": "06-static_communication.html",
    "href": "06-static_communication.html",
    "title": "6  Static communication",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "06-static_communication.html#introduction",
    "href": "06-static_communication.html#introduction",
    "title": "6  Static communication",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nWhen telling stories with data, we would like the data to do much of the work of convincing our reader. The paper is the medium, and the data are the message. To that end, we want to try to show our reader the data that allowed us to come to our understanding of the story. We use graphs, tables, and maps to help achieve this.\nThe critical task is to show the actual data that underpin our analysis, or as close to it as we can. For instance, if our dataset consists of 2,500 responses to a survey, then at some point in our paper we would expect a graph that contains 2,500 points. To do this we build graphs using ggplot2 (Wickham 2016). We will go through a variety of different options here including bar charts, scatterplots, line plots, and histograms.\nIn contrast to the role of graphs, which is to show the actual data, or as close to it as possible, the role of tables is typically to show an extract of the dataset or convey various summary statistics. We will build tables using knitr (Xie 2021) and kableExtra (Zhu 2020) initially, and then modelsummary (Arel-Bundock 2021a).\nFinally, we cover maps as a variant of graphs that are used to show a particular type of data. We will build static maps using ggmap (Kahle and Wickham 2013), having obtained the geocoded data that we need using tidygeocoder (Cambon and Belanger 2021)."
  },
  {
    "objectID": "06-static_communication.html#graphs",
    "href": "06-static_communication.html#graphs",
    "title": "6  Static communication",
    "section": "6.2 Graphs",
    "text": "6.2 Graphs\nGraphs are a critical aspect of compelling stories told with data.\n\nGraphs allow us to explore data to see overall patterns and to see detailed behavior; no other approach can compete in revealing the structure of data so thoroughly. Graphs allow us to view complex mathematical models fitted to data, and they allow us to assess the validity of such models.\nCleveland (1994, 5)\n\nIn a way, the graphing of data is an information coding process where we create a glyph, or purposeful mark, that we mean to convey information to our audience. The audience must decode our glyph. The success of our graph turns on how much information is lost in this process. It is the decoding that is the critical aspect (Cleveland 1994, 221), which means that we are creating graphs for the audience. If nothing else is possible, the most important feature is to convey as much of the actual data as possible.\nTo see why this is important we begin by using the dataset ‘datasaurus_dozen’ from datasauRus (Locke and D’Agostino McGowan 2018). After installing and loading the necessary packages, we can take a quick look at the dataset.\n\ninstall.packages('datasauRus')\n\n\nlibrary(tidyverse)\nlibrary(datasauRus)\n\nhead(datasaurus_dozen)\n\n# A tibble: 6 × 3\n  dataset     x     y\n  <chr>   <dbl> <dbl>\n1 dino     55.4  97.2\n2 dino     51.5  96.0\n3 dino     46.2  94.5\n4 dino     42.8  91.4\n5 dino     40.8  88.3\n6 dino     38.7  84.9\n\ndatasaurus_dozen |> \n  count(dataset)\n\n# A tibble: 13 × 2\n   dataset        n\n   <chr>      <int>\n 1 away         142\n 2 bullseye     142\n 3 circle       142\n 4 dino         142\n 5 dots         142\n 6 h_lines      142\n 7 high_lines   142\n 8 slant_down   142\n 9 slant_up     142\n10 star         142\n11 v_lines      142\n12 wide_lines   142\n13 x_shape      142\n\n\nWe can see that the dataset consists of values for ‘x’ and ‘y’, which should be plotted on the x-axis and y-axis, respectively. We can further see that there are thirteen different values in the variable ‘dataset’ including: “dino”, “star”, “away”, and “bullseye”. We will focus on those four and generate summary statistics for each (Table 6.1).\n\n# From Julia Silge: \n# https://juliasilge.com/blog/datasaurus-multiclass/\ndatasaurus_dozen |>\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |>\n  group_by(dataset) |>\n  summarise(across(c(x, y),\n                   list(mean = mean,\n                        sd = sd)),\n            x_y_cor = cor(x, y)) |>\n  knitr::kable(\n    col.names = c(\"Dataset\", \n                  \"x mean\", \n                  \"x sd\", \n                  \"y mean\", \n                  \"y sd\", \n                  \"correlation\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\n\n\n\nTable 6.1: Mean and standard deviation for four ‘datasaurus’ datasets\n\n\nDataset\nx mean\nx sd\ny mean\ny sd\ncorrelation\n\n\n\n\naway\n54.3\n16.8\n47.8\n26.9\n-0.1\n\n\nbullseye\n54.3\n16.8\n47.8\n26.9\n-0.1\n\n\ndino\n54.3\n16.8\n47.8\n26.9\n-0.1\n\n\nstar\n54.3\n16.8\n47.8\n26.9\n-0.1\n\n\n\n\n\n\nDespite the similarities of the summary statistics, it turns out the different ‘datasets’ are actually very different beasts when we graph the actual data (Figure 6.1).\n\ndatasaurus_dozen |> \n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |>\n  ggplot(aes(x=x, y=y, colour=dataset)) +\n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(dataset), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\")\n\n\n\n\nFigure 6.1: Graph of four ‘datasaurus’ datasets\n\n\n\n\nThis is a variant of the famous ‘Anscombe’s Quartet’. The key takeaway is that it is important to plot the actual data and not rely on summary statistics. The ‘anscombe’ dataset is built into R.\n\nhead(anscombe)\n\n  x1 x2 x3 x4   y1   y2    y3   y4\n1 10 10 10  8 8.04 9.14  7.46 6.58\n2  8  8  8  8 6.95 8.14  6.77 5.76\n3 13 13 13  8 7.58 8.74 12.74 7.71\n4  9  9  9  8 8.81 8.77  7.11 8.84\n5 11 11 11  8 8.33 9.26  7.81 8.47\n6 14 14 14  8 9.96 8.10  8.84 7.04\n\n\nIt consists of six observations for four different datasets, again with x and y values for each observation. We need to manipulate this dataset with pivot_longer() to get it into a ‘tidy format’.\n\n# From Nick Tierney: \n# https://www.njtierney.com/post/2020/06/01/tidy-anscombe/\n# Code from pivot_longer() vignette.\n\ntidy_anscombe <- \n  anscombe |>\n  pivot_longer(everything(),\n               names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\"\n               )\n\nWe can again first create some summary statistics (Table 6.2) and then graph the data (Figure 6.2). And we again see the importance of graphing the actual data, rather than relying on summary statistics.\n\ntidy_anscombe |>\n  group_by(set) |>\n  summarise(across(c(x, y),\n                   list(mean = mean, sd = sd)),\n            x_y_cor = cor(x, y)) |>\n  knitr::kable(\n    col.names = c(\"Dataset\", \n                  \"x mean\", \n                  \"x sd\", \n                  \"y mean\", \n                  \"y sd\", \n                  \"correlation\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\n\n\n\nTable 6.2: Mean and standard deviation for Anscombe\n\n\nDataset\nx mean\nx sd\ny mean\ny sd\ncorrelation\n\n\n\n\n1\n9\n3.3\n7.5\n2\n0.8\n\n\n2\n9\n3.3\n7.5\n2\n0.8\n\n\n3\n9\n3.3\n7.5\n2\n0.8\n\n\n4\n9\n3.3\n7.5\n2\n0.8\n\n\n\n\n\n\n\ntidy_anscombe |> \n  ggplot(aes(x = x, y = y, colour = set)) +\n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(set), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\")\n\n\n\n\nFigure 6.2: Recreation of Anscombe’s Quartet\n\n\n\n\nWe can add two specific evaluation options in an R Markdown chunk to have two graphs appear side-by-side (?fig-anscombegraphsidebyside). These are ‘out-width: “49%”’ and ‘fig-show: “hold”’. Another helpful option is ‘fig.align = “center”’ which ensures the graph is placed in the horizonal center of the page.\n\ntidy_anscombe |> \n  ggplot(aes(x = x, y = y, colour = set)) +\n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(set), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\")\n\ntidy_anscombe |> \n  ggplot(aes(x = x, y = y, colour = set)) +\n  geom_point() +\n  theme_classic() +\n  facet_wrap(vars(set), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\")\n\n\n\n\n\n\nFigure 6.3: Two variants of Anscombe’s Quartet\n\n\n\n\n\n\n\nFigure 6.4: Two variants of Anscombe’s Quartet\n\n\n\n\n\n\n\n6.2.1 Bar charts\nWe typically use a bar chart when we have a categorical variable that we want to focus on. We saw an example of this in Chapter 2 where we constructed a graph of the number of occupied beds. The geom that we primarily use is geom_bar(), but there are many variants to cater for specific situations.\nWe will use a dataset from the 1997-2001 British Election Panel Study that was put together by Fox and Andersen (2006).\n\n# Vincent Arel Bundock provides access to this dataset.\nbeps <- \n  read_csv(\n    file = \n    \"https://vincentarelbundock.github.io/Rdatasets/csv/carData/BEPS.csv\"\n    )\n\n\nhead(beps)\n\n# A tibble: 6 × 11\n   ...1 vote    age economic.cond.n… economic.cond.h… Blair Hague Kennedy Europe\n  <dbl> <chr> <dbl>            <dbl>            <dbl> <dbl> <dbl>   <dbl>  <dbl>\n1     1 Libe…    43                3                3     4     1       4      2\n2     2 Labo…    36                4                4     4     4       4      5\n3     3 Labo…    35                4                4     5     2       3      3\n4     4 Labo…    24                4                2     2     1       3      4\n5     5 Labo…    41                2                2     1     1       4      6\n6     6 Labo…    47                3                4     4     4       2      4\n# … with 2 more variables: political.knowledge <dbl>, gender <chr>\n\n\nThe dataset consists of which party the person supports, along with various demographic, economic, and political variables. In particular, we have the age of the respondents. We begin by creating age-groups from the ages, and making a bar chart of the age-groups using geom_bar() (Figure 6.5).\n\nbeps <- \n  beps |> \n  mutate(age_group = \n           case_when(age < 35 ~ \"<35\",\n                     age < 50 ~ \"35-49\",\n                     age < 65 ~ \"50-64\",\n                     age < 80 ~ \"65-79\",\n                     age < 100 ~ \"80-99\"\n                     ),\n         age_group = factor(age_group,\n                            levels = c(\"<35\",\n                                       \"35-49\",\n                                       \"50-64\",\n                                       \"65-79\",\n                                       \"80-99\"\n                                       )\n                            )\n         )\n\nbeps |>  \n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar()\n\n\n\n\nFigure 6.5: Distribution of ages in the 1997-2001 British Election Panel Study\n\n\n\n\nBy default, geom_bar() has created a count of the number of times each age-group appears in the dataset. It does this because the default ‘stat’ for geom_bar() is ‘count’. This saves us from having to create that statistic ourselves. But if we had already constructed a count (for instance, with beps |> count(age)), then we could also specify a column of values for the y-axis and then use stat = \"identity\".\nWe may also like to consider different groupings of the data, for instance, looking at age-groups by which party the respondent supports (Figure 6.6).\n\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar()\n\n\n\n\nFigure 6.6: Distribution of ages, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\nThe default is that these different groups are stacked, but they can be placed side-by-side with position = \"dodge\" (Figure 6.7).\n\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\nFigure 6.7: Distribution of age-groups, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\nAt this point, we may like to address the general look of the graph. There are various themes that are built into ggplot2. Some of these include theme_bw(), theme_classic(), theme_dark(), and theme_minimal(). A full list is available at the ggplot2 cheatsheet. We can use these themes by adding them as a layer (Figure 6.8). Here we can use patchwork (Pedersen 2020) to bring together multiple graphs. To do this we assign the graph to a name, and then use ‘+’ to signal which should be next to each other, ‘/’ to signal which would be on top, and brackets for precedence.\n\nlibrary(patchwork)\n\ntheme_bw <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\") +\n  theme_bw()\n\ntheme_classic <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\") +\n  theme_classic()\n\ntheme_dark <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\") +\n  theme_dark()\n\ntheme_minimal <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()\n\n(theme_bw + theme_classic) / (theme_dark + theme_minimal)\n\n\n\n\nFigure 6.8: Distribution of age-groups, and vote preference, in the 1997-2001 British Election Panel Study, illustrating different themes\n\n\n\n\nWe can install themes from other packages, including ggthemes (Arnold 2021), and hrbrthemes (Rudis 2020). And we can also build our own.\nThe default labels use dby ggplot2 are from the name of the relevant variable, and it is often useful to add more detail. We could add a title and caption at this point. A caption can be useful to add information about the source of the dataset. A title can be useful when the graph is going to be considered outside of the context of our paper. But in the case of a graph that will be included in a paper, the need to cross-reference all graphs that are in a paper means that included a title within labs() is unnecessary (Figure 6.9).\n\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\",\n       title = \"Distribution of age-groups, and vote preference, in\n       the 1997-2001 British Election Panel Study\",\n       caption = \"Source: 1997-2001 British Election Panel Study.\")\n\n\n\n\nFigure 6.9: Distribution of age-groups, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\nWe use facets to create ‘many little graphics that are variations of a single graphic’ (Wilkinson 2005, 219). They are especially useful when we want to specifically compare across some variable, but have already used color. For instance, we may be interested to explain vote, by age and gender (Figure 6.10).\n\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") +\n  facet_wrap(vars(gender))\n\n\n\n\nFigure 6.10: Distribution of age-group by gender, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\nWe could change facet_wrap() to wrap vertically instead of horizontally with dir = \"v\". Alternatively, we could specify a number of rows, say nrow = 2, or a number of columns, say ncol = 2. Additionally, by default, both facets will have the same scales. We could enable both facets to have different scales with scales = \"free\", or just the x-axis scales = \"free_x\", or just the y-axis scales = \"free_y\" (Figure 6.11).\n\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") +\n  facet_wrap(vars(gender),\n             dir = \"v\",\n             scales = \"free\")\n\n\n\n\nFigure 6.11: Distribution of age-group by gender, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\nFinally, we can change the labels of the facets using labeller() (Figure 6.12).\n\nnew_labels <- c(female = \"Female\", male = \"Male\")\n\nbeps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") +\n  facet_wrap(vars(gender),\n             dir = \"v\",\n             scales = \"free\",\n             labeller = labeller(gender = new_labels))\n\n\n\n\nFigure 6.12: Distribution of age-group by gender, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\nThere are a variety of different ways to change the colors, and many palettes are available including from RColorBrewer (Neuwirth 2014), which we specify with scale_fill_brewer(), and viridis (Garnier et al. 2021), which we specify with scale_fill_viridis() and is particularly focused on color-blind palettes (Figure 6.13).\n\nlibrary(viridis)\nlibrary(patchwork)\n\nRColorBrewerBrBG <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") + \n  scale_fill_brewer(palette = \"Blues\")\n\nRColorBrewerSet2 <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") +\n  scale_fill_brewer(palette = \"Set1\")\n\nviridis <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number of respondents\",\n       fill = \"Voted for\") + \n  scale_fill_viridis(discrete = TRUE)\n\nviridismagma <- \n  beps |> \n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group of respondent\",\n       y = \"Number\",\n       fill = \"Voted for\") +\n   scale_fill_viridis(discrete = TRUE, \n                      option = \"magma\")\n\n(RColorBrewerBrBG + RColorBrewerSet2) /\n  (viridis + viridismagma)\n\n\n\n\nFigure 6.13: Distribution of age-group and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\nDetails of the variety of palettes available in RColorBrewer and viridis are available in their help files. Many different palettes are available, and we can also build our own. That said, color is something to be considered with a great deal of care and it should only be added to increase the amount of information that is communicated (Cleveland 1994). Colors should not be added to graphs unnecessarily—that is to say, they must play some role. Typically, that role is to distinguish different groups, and that implies making the colors dissimilar. Colors may also be appropriate if there is some relationship between the color and the variable, for instance if making a graph of sales of, say, mangoes and raspberries, it could help the reader if the colors were yellow and red, respectively (Franconeri et al. 2021, 121).\n\n\n6.2.2 Scatterplots\nWe are often interested in the relationship between two variables. We can use scatterplots to show this information. Unless there is a good reason to move to a different option, a scatterplot is almost always the best choice (Weissgerber et al. 2015). Indeed, ‘among all forms of statistical graphics, the scatterplot may be considered the most versatile and generally useful invention in the entire history of statistical graphics.’ (Friendly and Wainer 2021, 121) To illustrate scatterplots, we use WDI (Arel-Bundock 2021b) to download some economic indicators from the World Bank, and in particular WDIsearch() to find the unique key that need to pass to WDI() to facilitate the download.\n\nOh, you think we have good data on that! Gross Domestic Product (GDP) ‘combines in a single figure, and with no double counting, all the output (or production) carried out by all the firms, non-profit institutions, government bodies and households in a given country during a given period, regardless of the type of goods and services produced, provided that the production takes place within the country’s economic territory’ (OECD (2014), p. 15). The modern concept was developed by Simon Kuznets and is widely used and reported. There is a certain comfort in having a definitive and concrete single number to describe something as complicated as the entire economic activity of a country. And it is crucial that we have such summary statistics. But as with any summary statistic, its strength is also its weakness. A single number necessarily loses information about constituent components, and these distributional differences are critical. It highlights short term economic progress over longer term improvements. And ‘the quantitative definiteness of the estimates makes it easy to forget their dependence upon imperfect data and the consequently wide margins of possible error to which both totals and components are liable’ (Kuznets 1941, xxvi). Reliance on any one summary measure of economic performance presents a misguided picture not only of a country’s economy, but also of its peoples.\n\n\ninstall.packages('WDI')\n\n\nlibrary(tidyverse)\nlibrary(WDI)\nWDIsearch(\"gdp growth\")\n\n     indicator              name                                    \n[1,] \"5.51.01.10.gdp\"       \"Per capita GDP growth\"                 \n[2,] \"6.0.GDP_growth\"       \"GDP growth (annual %)\"                 \n[3,] \"NV.AGR.TOTL.ZG\"       \"Real agricultural GDP growth rates (%)\"\n[4,] \"NY.GDP.MKTP.KD.ZG\"    \"GDP growth (annual %)\"                 \n[5,] \"NY.GDP.MKTP.KN.87.ZG\" \"GDP growth (annual %)\"                 \n\nWDIsearch(\"inflation\")\n\n     indicator              name                                               \n[1,] \"FP.CPI.TOTL.ZG\"       \"Inflation, consumer prices (annual %)\"            \n[2,] \"FP.FPI.TOTL.ZG\"       \"Inflation, food prices (annual %)\"                \n[3,] \"FP.WPI.TOTL.ZG\"       \"Inflation, wholesale prices (annual %)\"           \n[4,] \"NY.GDP.DEFL.87.ZG\"    \"Inflation, GDP deflator (annual %)\"               \n[5,] \"NY.GDP.DEFL.KD.ZG\"    \"Inflation, GDP deflator (annual %)\"               \n[6,] \"NY.GDP.DEFL.KD.ZG.AD\" \"Inflation, GDP deflator: linked series (annual %)\"\n\nWDIsearch(\"population, total\")\n\n          indicator                name \n      \"SP.POP.TOTL\" \"Population, total\" \n\nWDIsearch(\"Unemployment, total\")\n\n     indicator          \n[1,] \"SL.UEM.TOTL.NE.ZS\"\n[2,] \"SL.UEM.TOTL.ZS\"   \n     name                                                                 \n[1,] \"Unemployment, total (% of total labor force) (national estimate)\"   \n[2,] \"Unemployment, total (% of total labor force) (modeled ILO estimate)\"\n\n\n\nworld_bank_data <- \n  WDI(indicator = c(\"FP.CPI.TOTL.ZG\",\n                    \"NY.GDP.MKTP.KD.ZG\",\n                    \"SP.POP.TOTL\",\n                    \"SL.UEM.TOTL.NE.ZS\"\n                    ),\n      country = c(\"AU\", \"ET\", \"IN\", \"US\")\n      )\n\n\n\n\n\n\n\nWe may like to change the names to be more meaningful, and only keep the columns that we need.\n\nworld_bank_data <- \n  world_bank_data |> \n  rename(inflation = FP.CPI.TOTL.ZG,\n         gdp_growth = NY.GDP.MKTP.KD.ZG,\n         population = SP.POP.TOTL,\n         unemployment_rate = SL.UEM.TOTL.NE.ZS\n         ) |> \n  select(-iso2c)\n\nhead(world_bank_data)\n\n# A tibble: 6 × 6\n  country    year inflation gdp_growth population unemployment_rate\n  <chr>     <dbl>     <dbl>      <dbl>      <dbl>             <dbl>\n1 Australia  1960     3.73       NA      10276477                NA\n2 Australia  1961     2.29        2.48   10483000                NA\n3 Australia  1962    -0.319       1.29   10742000                NA\n4 Australia  1963     0.641       6.21   10950000                NA\n5 Australia  1964     2.87        6.98   11167000                NA\n6 Australia  1965     3.41        5.98   11388000                NA\n\n\nTo get started we can use geom_point() to make a scatterplot showing GDP growth and inflation, by country (Figure 6.14).\n\nworld_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point()\n\n\n\n\nFigure 6.14: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the US\n\n\n\n\nAs with bar charts, we change the theme, and update the labels (Figure 6.15), although again, we would normally not need both a caption and a title and would just use one.\n\nworld_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       title = \"Relationship between inflation and GDP growth\",\n       caption = \"Data source: World Bank.\")\n\n\n\n\nFigure 6.15: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the US\n\n\n\n\nHere we use ‘color’ instead of ‘fill’ because we are using dots rather than bars. This also then slightly affects how we change the palette (Figure 6.16).\n\nlibrary(patchwork)\n\nRColorBrewerBrBG <-\n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Blues\")\n\nRColorBrewerSet2 <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\")\n\nviridis <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_colour_viridis_d()\n\nviridismagma <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_colour_viridis_d(option = \"magma\")\n\nRColorBrewerBrBG / \n  RColorBrewerSet2 /\n  viridis /\n  viridismagma\n\n\n\n\nFigure 6.16: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the US\n\n\n\n\nThe points of a scatterplot sometimes overlap. We can address this situation in one of two ways:\n\nAdding a degree of transparency to our dots with ‘alpha’ (Figure 6.17). The value for ‘alpha’ can vary between 0, which is fully transparent, and 1, which is completely opaque.\nAdding a small about of noise, which slightly moves the points, using geom_jitter() (Figure 6.18). By default, the movement is uniform in both directions, but we can specify which direction movement occurs with ‘width’ or ‘height’. The decision between these two options turns on the degree to which exact accuracy matters, and the number of points.\n\n\nworld_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\n\n\n\n\nFigure 6.17: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the US\n\n\n\n\n\nworld_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\n\n\n\n\nFigure 6.18: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the US\n\n\n\n\nA common use case for a scatterplot is to illustrate a relationship between two variables. It can be useful to add a line of best fit using geom_smooth() (Figure 6.19). By default, geom_smooth() will use LOESS smoothing is used for datasets with less than 1,000 observations, but we can specify the relationship using ‘method’, change the color with ‘color’ and remove standard errors with ‘se’. Using geom_smooth() adds a layer to the graph, and so it inherits aesthetics from ggplot(). For instance, that is why we initially have one line for each country in Figure 6.19). We could overwrite that by specifying a particular color, which we do in the third graph of Figure 6.19).\n\ndefaults <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\n\nstraightline <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\n\nonestraightline <- \n  world_bank_data |>\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, color = \"black\", se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\",\n       y = \"Inflation\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\")\n\ndefaults / \n  straightline /\n  onestraightline\n\n\n\n\nFigure 6.19: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the US\n\n\n\n\n\n\n6.2.3 Line plots\nWe can use a line plot when we have variables that should be joined together, for instance, an economic time series. We will continue with the dataset from the World Bank and focus on US GDP growth using geom_line() (Figure 6.20).\n\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_line()\n\n\n\n\nFigure 6.20: US GDP growth (1961-2020)\n\n\n\n\nAs before, we can adjust the theme, say with theme_minimal() and labels with labs() (Figure 6.21).\n\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"GDP growth\",\n       caption = \"Data source: World Bank.\")\n\n\n\n\nFigure 6.21: US GDP growth (1961-2020)\n\n\n\n\nWe can use a slight variant of geom_line(), geom_step() to focus attention on the change from year to year (Figure 6.22).\n\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_step() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"GDP growth\",\n       caption = \"Data source: World Bank.\")\n\n\n\n\nFigure 6.22: US GDP growth (1961-2020)\n\n\n\n\nThe Phillips curve is the name given to plot of the relationship between unemployment and inflation over time. An inverse relationship is sometimes found in the data, for instance in the UK between 1861 and 1957 (Phillips 1958). We have a variety of ways to investigate this including:\n\nAdding a second line to our graph. For instance, we could add inflation (Figure 6.23). This may require use to use pivot_longer() to ensure that the data are in tidy format.\nUsing geom_path() to links values in the order they appear in the dataset. In Figure 6.24) we show a Phillips curve for the US between 1960 and 2020. Figure 6.24) does not appear to show any clear relationship between unemployment and inflation.\n\n\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  select(-population, -gdp_growth) |>\n  pivot_longer(cols = c(\"inflation\", \"unemployment_rate\"),\n               names_to = \"series\",\n               values_to = \"value\"\n               ) |>\n  ggplot(mapping = aes(x = year, y = value, color = series)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\",\n       y = \"Value\",\n       color = \"Economic indicator\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Inflation\", \"Unemployment\")) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 6.23: Unemployment and inflation for the US (1960-2020)\n\n\n\n\n\nworld_bank_data |>\n  filter(country == \"United States\") |>\n  ggplot(mapping = aes(x = unemployment_rate, y = inflation)) +\n  geom_path() +\n  theme_minimal() +\n  labs(x = \"Unemployment rate\",\n       y = \"Inflation\",\n       caption = \"Data source: World Bank.\")\n\n\n\n\nFigure 6.24: Phillips curve for the US (1960-2020)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.2.4 Histograms\nA histogram is useful to show the shape of a continuous variable and works by constructing counts of the number of observations in different subsets of the support, called ‘bins’. In Figure 6.25) we examine the distribution of GDP in Ethiopia.\n\nworld_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram()\n\n\n\n\nFigure 6.25: Distribution of GDP in Ethiopia (1960-2020)\n\n\n\n\nAnd again we can add a theme and labels (Figure 6.26).\n\nworld_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram() +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\n\n\n\nFigure 6.26: Distribution of GDP in Ethiopia (1960-2020)\n\n\n\n\nThe key component determining the shape of a histogram is the number of bins. This can be specified in one of two ways (Figure 6.27):\n\nspecifying the number of ‘bins’ to include, or\nspecifying how wide they should be with ‘binwidth’.\n\n\ntwobins <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(bins = 2) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\nfivebins <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(bins = 5) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\ntwentybins <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(bins = 20) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\nhalfbinwidth <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 0.5) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\ntwobinwidth <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 2) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\nfivebinwidth <- \n  world_bank_data |> \n  filter(country == \"Ethiopia\") |> \n  ggplot(mapping = aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 5) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       caption = \"Data source: World Bank.\")\n\n(twobins + fivebins + twentybins) / \n  (halfbinwidth + twobinwidth + fivebinwidth)\n\n\n\n\nFigure 6.27: Distribution of GDP in Ethiopia (1960-2020)\n\n\n\n\nThe histogram is smoothing the data, and the number of bins affects how much smoothing occurs. When there are only two bins then the data are very smooth, but we have lost a great deal of accuracy. More specifically, ‘the histogram estimator is a piecewise constant function where the height of the function is proportional to the number of observations in each bin’ (Wasserman 2005, 303). Too few bins result in a biased estimator, while too many bins results in an estimator with high variance. Our decision as to the number of bins, or their width, is concerned with trying to balance bias and variance. This will depend on a variety of concerns including the subject matter and the goal (Cleveland 1994, 135).\nFinally, while we can use ‘fill’ to distinguish between different types of observations, it can get quite messy. It is usually better to give away showing the distribution with columns and instead trace the outline of the distribution, using geom_freqpoly() (Figure 6.28) or to build it up using dots with geom_dotplot() (Figure 6.29) .\n\nworld_bank_data |> \n  ggplot(mapping = aes(x = gdp_growth, color = country)) +\n  geom_freqpoly() +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       color = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\") \n\n\n\n\nFigure 6.28: Distribution of GDP in four countries (1960-2020)\n\n\n\n\n\nworld_bank_data |> \n  ggplot(mapping = aes(x = gdp_growth, group = country, fill = country)) +\n  geom_dotplot(method = 'histodot', alpha = 0.4) +\n  theme_minimal() +\n  labs(x = \"GDP\",\n       y = \"Number of occurrences\",\n       fill = \"Country\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\") \n\n\n\n\nFigure 6.29: Distribution of GDP in four countries (1960-2020)\n\n\n\n\n\n\n6.2.5 Boxplots\nBoxplots are almost never an appropriate choice because they hide the distribution of data, rather than show it. Unless we need to compare the summary statistics of many variables at once, then they should almost never be used. This is because the same boxplot can apply to very different distributions. To see this, consider some simulated data from the beta distribution of two types. One type of data contains draws from two beta distributions: one that is right skewed and another that is left skewed. The other type of data contains draws from a beta distribution with no skew.\n\nset.seed(853)\n\nboth_left_and_right_skew <- \n  c(\n    rbeta(500, 5, 2),\n    rbeta(500, 2, 5)\n    )\n\nno_skew <- \n  rbeta(1000, 1, 1)\n\nbeta_distributions <- \n  tibble(\n    observation = c(both_left_and_right_skew, no_skew),\n    source = c(rep(\"Left and right skew\", 1000),\n               rep(\"No skew\", 1000)\n               )\n  )\n\nWe can first compare the boxplots of the two series (Figure 6.30).\n\nbeta_distributions |> \n  ggplot(aes(x = source, y = observation)) +\n  geom_boxplot() +\n  theme_classic()\n\n\n\n\nFigure 6.30: Data drawn from beta distributions with different parameters\n\n\n\n\nBut if we plot the actual data then we can see how different they are (Figure 6.31).\n\nbeta_distributions |> \n  ggplot(aes(x = observation, color = source)) +\n  geom_freqpoly(binwidth = 0.05) +\n  theme_classic()\n\n\n\n\nFigure 6.31: Data drawn from beta distributions with different parameters\n\n\n\n\nOne way forward, if a boxplot must be included, is to include the actual data as a layer on top of the boxplot. For instance, in Figure 6.32) we show the distribution of inflation across the four countries.\n\nworld_bank_data |> \n  ggplot(mapping = aes(x = country, y = inflation)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.3, width = 0.15, height = 0) +\n  theme_minimal() +\n  labs(x = \"Country\",\n       y = \"Inflation\",\n       caption = \"Data source: World Bank.\") +\n  scale_color_brewer(palette = \"Set1\") \n\n\n\n\nFigure 6.32: Distribution of unemployment data for four countries (1960-2020)"
  },
  {
    "objectID": "06-static_communication.html#tables",
    "href": "06-static_communication.html#tables",
    "title": "6  Static communication",
    "section": "6.3 Tables",
    "text": "6.3 Tables\nTables are critical for telling a compelling story. Tables can communicate less information than a graph, but they can do so at a high fidelity. We primarily use tables in three ways:\n\nTo show some of our actual dataset, for which we use kable() from knitr (Xie 2021), alongside kableExtra (Zhu 2020).\nTo communicate summary statistics, for which we use modelsummary (Arel-Bundock 2021a).\nTo display regression results, for which we also use modelsummary (Arel-Bundock 2021a).\n\n\n6.3.1 Showing part of a dataset\nWe illustrate showing part of a dataset using kable() from knitr and drawing on kableExtra for enhancement. We again use the World Bank dataset that we downloaded earlier.\n\nlibrary(knitr)\nhead(world_bank_data)\n\n# A tibble: 6 × 6\n  country    year inflation gdp_growth population unemployment_rate\n  <chr>     <dbl>     <dbl>      <dbl>      <dbl>             <dbl>\n1 Australia  1960     3.73       NA      10276477                NA\n2 Australia  1961     2.29        2.48   10483000                NA\n3 Australia  1962    -0.319       1.29   10742000                NA\n4 Australia  1963     0.641       6.21   10950000                NA\n5 Australia  1964     2.87        6.98   11167000                NA\n6 Australia  1965     3.41        5.98   11388000                NA\n\n\nTo begin, we can display the first ten rows with the default kable() settings.\n\nworld_bank_data |> \n  slice(1:10) |> \n  kable() \n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nyear\ninflation\ngdp_growth\npopulation\nunemployment_rate\n\n\n\n\nAustralia\n1960\n3.7288136\nNA\n10276477\nNA\n\n\nAustralia\n1961\n2.2875817\n2.483271\n10483000\nNA\n\n\nAustralia\n1962\n-0.3194888\n1.294468\n10742000\nNA\n\n\nAustralia\n1963\n0.6410256\n6.214949\n10950000\nNA\n\n\nAustralia\n1964\n2.8662420\n6.978540\n11167000\nNA\n\n\nAustralia\n1965\n3.4055728\n5.980893\n11388000\nNA\n\n\nAustralia\n1966\n3.2934132\n2.381966\n11651000\nNA\n\n\nAustralia\n1967\n3.4782609\n6.303650\n11799000\nNA\n\n\nAustralia\n1968\n2.5210084\n5.095103\n12009000\nNA\n\n\nAustralia\n1969\n3.2786885\n7.043526\n12263000\nNA\n\n\n\n\n\nIn order to be able to cross-reference it in text, we need to add a caption with ‘caption’. We can also make the column names more information with ‘col.names’ and specify the number of digits to be displayed (Table 6.3).\n\nworld_bank_data |> \n  slice(1:10) |> \n  kable(    \n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\", \"Unemployment rate\"),\n    digits = 1\n  )\n\n\n\nTable 6.3: First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\nUnemployment rate\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10276477\nNA\n\n\nAustralia\n1961\n2.3\n2.5\n10483000\nNA\n\n\nAustralia\n1962\n-0.3\n1.3\n10742000\nNA\n\n\nAustralia\n1963\n0.6\n6.2\n10950000\nNA\n\n\nAustralia\n1964\n2.9\n7.0\n11167000\nNA\n\n\nAustralia\n1965\n3.4\n6.0\n11388000\nNA\n\n\nAustralia\n1966\n3.3\n2.4\n11651000\nNA\n\n\nAustralia\n1967\n3.5\n6.3\n11799000\nNA\n\n\nAustralia\n1968\n2.5\n5.1\n12009000\nNA\n\n\nAustralia\n1969\n3.3\n7.0\n12263000\nNA\n\n\n\n\n\n\nWhen producing PDFs, the ‘booktabs’ option makes a host of small changes to the default display and results in tables that look better (Table 6.4). When using ‘booktabs’ we additionally should specify ‘linesep’ otherwise kable() adds a small space every five lines. (None of this will show up for html output.)\n\nworld_bank_data |> \n  slice(1:10) |> \n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\", \"Unemployment rate\"),\n    digits = 1,\n    booktabs = TRUE, \n    linesep = \"\"\n  )\n\n\n\nTable 6.4: First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\nUnemployment rate\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10276477\nNA\n\n\nAustralia\n1961\n2.3\n2.5\n10483000\nNA\n\n\nAustralia\n1962\n-0.3\n1.3\n10742000\nNA\n\n\nAustralia\n1963\n0.6\n6.2\n10950000\nNA\n\n\nAustralia\n1964\n2.9\n7.0\n11167000\nNA\n\n\nAustralia\n1965\n3.4\n6.0\n11388000\nNA\n\n\nAustralia\n1966\n3.3\n2.4\n11651000\nNA\n\n\nAustralia\n1967\n3.5\n6.3\n11799000\nNA\n\n\nAustralia\n1968\n2.5\n5.1\n12009000\nNA\n\n\nAustralia\n1969\n3.3\n7.0\n12263000\nNA\n\n\n\n\n\n\n\nworld_bank_data |> \n  slice(1:10) |> \n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\", \"Unemployment rate\"),\n    digits = 1,\n    booktabs = TRUE\n  )\n\n\n\nTable 6.5: First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\nUnemployment rate\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10276477\nNA\n\n\nAustralia\n1961\n2.3\n2.5\n10483000\nNA\n\n\nAustralia\n1962\n-0.3\n1.3\n10742000\nNA\n\n\nAustralia\n1963\n0.6\n6.2\n10950000\nNA\n\n\nAustralia\n1964\n2.9\n7.0\n11167000\nNA\n\n\nAustralia\n1965\n3.4\n6.0\n11388000\nNA\n\n\nAustralia\n1966\n3.3\n2.4\n11651000\nNA\n\n\nAustralia\n1967\n3.5\n6.3\n11799000\nNA\n\n\nAustralia\n1968\n2.5\n5.1\n12009000\nNA\n\n\nAustralia\n1969\n3.3\n7.0\n12263000\nNA\n\n\n\n\n\n\nWe can specify the alignment of the columns using a character vector of ‘l’ (left), ‘c’ (centre), and ‘r’ (right) (Table 6.6). Additionally, we can change the formatting. For instance, we could specify groupings for numbers that are at least one thousand using ‘format.args = list(big.mark = “,”)’.\n\nworld_bank_data |> \n  slice(1:10) |> \n  mutate(year = as.factor(year)) |>\n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \n                  \"Population\", \"Unemployment rate\"),\n    digits = 1,\n    booktabs = TRUE, \n    linesep = \"\",\n    align = c('l', 'l', 'c', 'c', 'r', 'r'),\n    format.args = list(big.mark = \",\")\n  )\n\n\n\nTable 6.6: First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US\n\n\n\n\n\n\n\n\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\nUnemployment rate\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10,276,477\nNA\n\n\nAustralia\n1961\n2.3\n2.5\n10,483,000\nNA\n\n\nAustralia\n1962\n-0.3\n1.3\n10,742,000\nNA\n\n\nAustralia\n1963\n0.6\n6.2\n10,950,000\nNA\n\n\nAustralia\n1964\n2.9\n7.0\n11,167,000\nNA\n\n\nAustralia\n1965\n3.4\n6.0\n11,388,000\nNA\n\n\nAustralia\n1966\n3.3\n2.4\n11,651,000\nNA\n\n\nAustralia\n1967\n3.5\n6.3\n11,799,000\nNA\n\n\nAustralia\n1968\n2.5\n5.1\n12,009,000\nNA\n\n\nAustralia\n1969\n3.3\n7.0\n12,263,000\nNA\n\n\n\n\n\n\nWe can use kableExtra (Zhu 2020) to add extra functionality to kable. For instance, we could add a row that groups some of the columns (Table 6.6).\n\nlibrary(kableExtra)\n\nworld_bank_data |> \n  slice(1:10) |> \n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \n                  \"Population\", \"Unemployment rate\"),\n    digits = 1,\n    booktabs = TRUE, \n    linesep = \"\",\n    align = c('l', 'l', 'c', 'c', 'r', 'r'),\n  ) |> \n  add_header_above(c(\" \" = 2, \"Economic indicators\" = 4))\n\n\nTable 6.7:  First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US \n \n\n\nEconomic indicators\n\n  \n    Country \n    Year \n    Inflation \n    GDP growth \n    Population \n    Unemployment rate \n  \n \n\n  \n    Australia \n    1960 \n    3.7 \n    NA \n    10276477 \n    NA \n  \n  \n    Australia \n    1961 \n    2.3 \n    2.5 \n    10483000 \n    NA \n  \n  \n    Australia \n    1962 \n    -0.3 \n    1.3 \n    10742000 \n    NA \n  \n  \n    Australia \n    1963 \n    0.6 \n    6.2 \n    10950000 \n    NA \n  \n  \n    Australia \n    1964 \n    2.9 \n    7.0 \n    11167000 \n    NA \n  \n  \n    Australia \n    1965 \n    3.4 \n    6.0 \n    11388000 \n    NA \n  \n  \n    Australia \n    1966 \n    3.3 \n    2.4 \n    11651000 \n    NA \n  \n  \n    Australia \n    1967 \n    3.5 \n    6.3 \n    11799000 \n    NA \n  \n  \n    Australia \n    1968 \n    2.5 \n    5.1 \n    12009000 \n    NA \n  \n  \n    Australia \n    1969 \n    3.3 \n    7.0 \n    12263000 \n    NA \n  \n\n\n\n\n\n\nAnother especially nice way to build tables is to use gt (Iannone, Cheng, and Schloerke 2020).\n\nlibrary(gt)\n\nworld_bank_data |> \n  slice(1:10) |> \n  gt() \n\n\n\n\n  \n  \n    \n      country\n      year\n      inflation\n      gdp_growth\n      population\n      unemployment_rate\n    \n  \n  \n    Australia\n1960\n3.7288136\nNA\n10276477\nNA\n    Australia\n1961\n2.2875817\n2.483271\n10483000\nNA\n    Australia\n1962\n-0.3194888\n1.294468\n10742000\nNA\n    Australia\n1963\n0.6410256\n6.214949\n10950000\nNA\n    Australia\n1964\n2.8662420\n6.978540\n11167000\nNA\n    Australia\n1965\n3.4055728\n5.980893\n11388000\nNA\n    Australia\n1966\n3.2934132\n2.381966\n11651000\nNA\n    Australia\n1967\n3.4782609\n6.303650\n11799000\nNA\n    Australia\n1968\n2.5210084\n5.095103\n12009000\nNA\n    Australia\n1969\n3.2786885\n7.043526\n12263000\nNA\n  \n  \n  \n\n\n\n\nAgain, we can add a caption and more informative column labels (Table 6.8).\n\nworld_bank_data |>\n  slice(1:10) |>\n  gt() |>\n  cols_label(\n      country = \"Country\",\n      year = \"Year\",\n      inflation = \"Inflation\",\n      gdp_growth = \"GDP growth\",\n      population = \"Population\",\n      unemployment_rate = \"Unemployment rate\"\n    )\n\n\n\n\nTable 6.8:  First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US \n  \n  \n    \n      Country\n      Year\n      Inflation\n      GDP growth\n      Population\n      Unemployment rate\n    \n  \n  \n    Australia\n1960\n3.7288136\nNA\n10276477\nNA\n    Australia\n1961\n2.2875817\n2.483271\n10483000\nNA\n    Australia\n1962\n-0.3194888\n1.294468\n10742000\nNA\n    Australia\n1963\n0.6410256\n6.214949\n10950000\nNA\n    Australia\n1964\n2.8662420\n6.978540\n11167000\nNA\n    Australia\n1965\n3.4055728\n5.980893\n11388000\nNA\n    Australia\n1966\n3.2934132\n2.381966\n11651000\nNA\n    Australia\n1967\n3.4782609\n6.303650\n11799000\nNA\n    Australia\n1968\n2.5210084\n5.095103\n12009000\nNA\n    Australia\n1969\n3.2786885\n7.043526\n12263000\nNA\n  \n  \n  \n\n\n\n\n\n\n\n6.3.2 Communicating summary statistics\nWe can use datasummary() from modelsummary to create tables of summary statistics from our dataset.\n\nlibrary(modelsummary)\n\n\nAttaching package: 'modelsummary'\n\n\nThe following object is masked from 'package:gt':\n\n    escape_latex\n\nworld_bank_data |> \n  datasummary_skim()\n\n\n \n  \n      \n    Unique (#) \n    Missing (%) \n    Mean \n    SD \n    Min \n    Median \n    Max \n       \n  \n \n\n  \n    year \n    61 \n    0 \n    1990.0 \n    17.6 \n    1960.0 \n    1990.0 \n    2020.0 \n     \n\n\n  \n  \n    inflation \n    238 \n    3 \n    6.1 \n    6.3 \n    −9.8 \n    4.3 \n    44.4 \n     \n\n\n  \n  \n    gdp_growth \n    220 \n    10 \n    4.2 \n    3.7 \n    −11.1 \n    3.9 \n    13.9 \n     \n\n\n  \n  \n    population \n    244 \n    0 \n    304177482.9 \n    380093166.9 \n    10276477.0 \n    147817291.5 \n    1380004385.0 \n     \n\n\n  \n  \n    unemployment_rate \n    104 \n    52 \n    6.0 \n    1.9 \n    1.2 \n    5.7 \n    10.9 \n     \n\n\n  \n\n\n\n\n\nBy default, datasummary() summarizes the ‘numeric’ variables, but we can ask for the ‘categorical’ variables (Table 6.9). Additionally we can add cross-references in the same way as kable(), that is, include a title and then cross-reference the name of the R chunk.\n\nworld_bank_data |> \n  datasummary_skim(type = \"categorical\")\n\n\nTable 6.9:  Summary of categorical economic indicator variables for four countries \n \n  \n    country \n    N \n    % \n  \n \n\n  \n    Australia \n    61 \n    25.0 \n  \n  \n    Ethiopia \n    61 \n    25.0 \n  \n  \n    India \n    61 \n    25.0 \n  \n  \n    United States \n    61 \n    25.0 \n  \n\n\n\n\n\n\nWe can create a table that shows the correlation between variables using datasummary_correlation() (Table 6.10).\n\nworld_bank_data |> \n  datasummary_correlation()\n\n\nTable 6.10:  Correlation between the economic indicator variables for four countries (Australia, Ethiopia, India, and the US) \n \n  \n      \n    year \n    inflation \n    gdp_growth \n    population \n    unemployment_rate \n  \n \n\n  \n    year \n    1 \n    . \n    . \n    . \n    . \n  \n  \n    inflation \n    .00 \n    1 \n    . \n    . \n    . \n  \n  \n    gdp_growth \n    .10 \n    .00 \n    1 \n    . \n    . \n  \n  \n    population \n    .24 \n    .07 \n    .15 \n    1 \n    . \n  \n  \n    unemployment_rate \n    −.13 \n    −.14 \n    −.31 \n    −.35 \n    1 \n  \n\n\n\n\n\n\nWe typically need a table of descriptive statistics that we could add to our paper (Table 6.11). This contrasts with Table 6.9) which would likely not be included in a paper. We can add a note about the source of the data using ‘notes’.\n\ndatasummary_balance(formula = ~country,\n                    data = world_bank_data,\n                    notes = \"Data source: World Bank.\")\n\n\nTable 6.11:  Descriptive statistics for the inflation and GDP dataset \n \n\n\nAustralia (N=61)\nEthiopia (N=61)\nIndia (N=61)\nUnited States (N=61)\n\n  \n      \n    Mean \n    Std. Dev. \n    Mean  \n    Std. Dev.  \n    Mean   \n    Std. Dev.   \n    Mean    \n    Std. Dev.    \n  \n \n\n  \n    year \n    1990.0 \n    17.8 \n    1990.0 \n    17.8 \n    1990.0 \n    17.8 \n    1990.0 \n    17.8 \n  \n  \n    inflation \n    4.7 \n    3.8 \n    8.7 \n    10.4 \n    7.4 \n    5.0 \n    3.7 \n    2.8 \n  \n  \n    gdp_growth \n    3.4 \n    1.8 \n    5.9 \n    6.4 \n    5.0 \n    3.3 \n    2.9 \n    2.2 \n  \n  \n    population \n    17244215.9 \n    4328625.6 \n    55662437.9 \n    27626912.1 \n    888774544.9 \n    292997809.4 \n    255028733.1 \n    45603604.8 \n  \n  \n    unemployment_rate \n    6.8 \n    1.7 \n    2.6 \n    0.9 \n    3.5 \n    1.4 \n    6.0 \n    1.6 \n  \n\n\n Data source: World Bank.\n\n\n\n\n\n\n\n6.3.3 Display regression results\nFinally, one common reason for needing a table is to report regression results. We will do this using modelsummary() from modelsummary (Arel-Bundock 2021a).\n\nfirst_model <- lm(formula = gdp_growth ~ inflation, \n                  data = world_bank_data)\n\nmodelsummary(first_model)\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    4.157 \n  \n  \n     \n    (0.352) \n  \n  \n    inflation \n    −0.002 \n  \n  \n     \n    (0.041) \n  \n  \n    Num.Obs. \n    218 \n  \n  \n    R2 \n    0.000 \n  \n  \n    R2 Adj. \n    −0.005 \n  \n  \n    AIC \n    1195.1 \n  \n  \n    BIC \n    1205.3 \n  \n  \n    Log.Lik. \n    −594.554 \n  \n  \n    F \n    0.002 \n  \n\n\n\n\n\nWe can put a variety of different of different models together (Table 6.12).\n\nsecond_model <- lm(formula = gdp_growth ~ inflation + country, \n                  data = world_bank_data)\n\nthird_model <- lm(formula = gdp_growth ~ inflation + country + population, \n                  data = world_bank_data)\n\nmodelsummary(list(first_model, second_model, third_model))\n\n\nTable 6.12:  Explaining GDP as a function of inflation \n \n  \n      \n    Model 1 \n    Model 2 \n    Model 3 \n  \n \n\n  \n    (Intercept) \n    4.157 \n    3.728 \n    3.668 \n  \n  \n     \n    (0.352) \n    (0.495) \n    (0.494) \n  \n  \n    inflation \n    −0.002 \n    −0.075 \n    −0.072 \n  \n  \n     \n    (0.041) \n    (0.041) \n    (0.041) \n  \n  \n    countryEthiopia \n     \n    2.872 \n    2.716 \n  \n  \n     \n     \n    (0.757) \n    (0.758) \n  \n  \n    countryIndia \n     \n    1.854 \n    −0.561 \n  \n  \n     \n     \n    (0.655) \n    (1.520) \n  \n  \n    countryUnited States \n     \n    −0.524 \n    −1.176 \n  \n  \n     \n     \n    (0.646) \n    (0.742) \n  \n  \n    population \n     \n     \n    0.000 \n  \n  \n     \n     \n     \n    (0.000) \n  \n  \n    Num.Obs. \n    218 \n    218 \n    218 \n  \n  \n    R2 \n    0.000 \n    0.110 \n    0.123 \n  \n  \n    R2 Adj. \n    −0.005 \n    0.093 \n    0.102 \n  \n  \n    AIC \n    1195.1 \n    1175.7 \n    1174.5 \n  \n  \n    BIC \n    1205.3 \n    1196.0 \n    1198.2 \n  \n  \n    Log.Lik. \n    −594.554 \n    −581.844 \n    −580.266 \n  \n  \n    F \n    0.002 \n    6.587 \n    5.939 \n  \n\n\n\n\n\n\nWe can adjust the number of significant digits (Table 6.13).\n\nmodelsummary(list(first_model, second_model, third_model),\n             fmt = 1)\n\n\nTable 6.13:  Two models of GDP as a function of inflation \n \n  \n      \n    Model 1 \n    Model 2 \n    Model 3 \n  \n \n\n  \n    (Intercept) \n    4.2 \n    3.7 \n    3.7 \n  \n  \n     \n    (0.4) \n    (0.5) \n    (0.5) \n  \n  \n    inflation \n    0.0 \n    −0.1 \n    −0.1 \n  \n  \n     \n    (0.0) \n    (0.0) \n    (0.0) \n  \n  \n    countryEthiopia \n     \n    2.9 \n    2.7 \n  \n  \n     \n     \n    (0.8) \n    (0.8) \n  \n  \n    countryIndia \n     \n    1.9 \n    −0.6 \n  \n  \n     \n     \n    (0.7) \n    (1.5) \n  \n  \n    countryUnited States \n     \n    −0.5 \n    −1.2 \n  \n  \n     \n     \n    (0.6) \n    (0.7) \n  \n  \n    population \n     \n     \n    0.0 \n  \n  \n     \n     \n     \n    (0.0) \n  \n  \n    Num.Obs. \n    218 \n    218 \n    218 \n  \n  \n    R2 \n    0.000 \n    0.110 \n    0.123 \n  \n  \n    R2 Adj. \n    −0.005 \n    0.093 \n    0.102 \n  \n  \n    AIC \n    1195.1 \n    1175.7 \n    1174.5 \n  \n  \n    BIC \n    1205.3 \n    1196.0 \n    1198.2 \n  \n  \n    Log.Lik. \n    −594.554 \n    −581.844 \n    −580.266 \n  \n  \n    F \n    0.002 \n    6.587 \n    5.939"
  },
  {
    "objectID": "06-static_communication.html#maps",
    "href": "06-static_communication.html#maps",
    "title": "6  Static communication",
    "section": "6.4 Maps",
    "text": "6.4 Maps\nIn many ways maps can be thought of as another type of graph, where the x-axis is latitude, the y-axis is longitude, and there is some outline or a background image. We have seen this type of set-up are used to this type of set-up, for instance, in the ggplot2 setting, this is quite familiar.\n\nggplot() +\n  geom_polygon( # First draw an outline\n    data = some_data, \n    aes(x = latitude, \n        y = longitude,\n        group = group\n        )) +\n  geom_point( # Then add points of interest\n    data = some_other_data, \n    aes(x = latitude, \n        y = longitude)\n    )\n\nAnd while there are some small complications, for the most part it is as straight-forward as that. The first step is to get some data. There is some geographic data built into ggplot2, and there is additional information in the ‘world.cities’ dataset from maps.\n\nlibrary(maps)\n\nfrance <- map_data(map = \"france\")\n\nhead(france)\n\n      long      lat group order region subregion\n1 2.557093 51.09752     1     1   Nord      <NA>\n2 2.579995 51.00298     1     2   Nord      <NA>\n3 2.609101 50.98545     1     3   Nord      <NA>\n4 2.630782 50.95073     1     4   Nord      <NA>\n5 2.625894 50.94116     1     5   Nord      <NA>\n6 2.597699 50.91967     1     6   Nord      <NA>\n\nfrench_cities <- \n  world.cities |>\n  filter(country.etc == \"France\")\n\nhead(french_cities)\n\n             name country.etc    pop   lat long capital\n1       Abbeville      France  26656 50.12 1.83       0\n2         Acheres      France  23219 48.97 2.06       0\n3            Agde      France  23477 43.33 3.46       0\n4            Agen      France  34742 44.20 0.62       0\n5 Aire-sur-la-Lys      France  10470 50.64 2.39       0\n6 Aix-en-Provence      France 148622 43.53 5.44       0\n\n\nWith that information in hand, we can then create a map of France that shows the larger cities. We use geom_polygon() from ggplot2 to draw shapes by connecting points within groups. And coord_map() adjusts for the fact that we are making a 2D map to represent a world that is 3D.)\n\nggplot() +\n  geom_polygon(data = france,\n               aes(x = long,\n                   y = lat,\n                   group = group),\n               fill = \"white\", \n               colour = \"grey\") +\n  coord_map() +\n  geom_point(aes(x = french_cities$long, \n                 y = french_cities$lat),\n             alpha = 0.3,\n             color = \"black\") +\n  theme_classic() +\n  labs(x = \"Longitude\",\n       y = \"Latitude\")\n\n\n\n\nAs is often the case with R, there are many different ways to get started creating static maps. We have seen how they can be built using only ggplot2, but ggmap brings additional functionality (Kahle and Wickham 2013).\nThere are two essential components to a map:\n\na border or background image (sometimes called a tile); and\nsomething of interest within that border, or on top of that tile.\n\nIn ggmap, we use an open-source option for our tile, Stamen Maps. And we use plot points based on latitude and longitude.\n\n6.4.1 Australian polling places\nIn Australia people go to specific locations, called booths, to vote. These booths have latitudes and longitudes and so we can plot these. One reason we may like to do this is to notice patterns over geographies.\nTo get started we need to get a tile. We are going to use ggmap to get a tile from Stamen Maps, which builds on OpenStreetMap (openstreetmap.org). The main argument to this function is to specify a bounding box. This requires two latitudes - one for the top of the box and one for the bottom of the box - and two longitudes - one for the left of the box and one for the right of the box. It can be useful to use Google Maps, or an alternative, to find the values of these that you need. The bounding box provides the coordinates of the edges that you are interested in. In this case we have provided it with coordinates such that it will be centered around Canberra, Australia, which is a small city that was created for the purposes of being the capital.\n\nlibrary(ggmap)\n\nbbox <- c(left = 148.95, bottom = -35.5, right = 149.3, top = -35.1)\n\nOnce you have defined the bounding box, then the function get_stamenmap() will get the tiles in that area. The number of tiles that it needs to get depends on the zoom, and the type of tiles that it gets depends on the maptype. We have used a black-and-white type of map but the helpfile specifies others. At this point we can the map to maps to ggmap() and it will plot the tile! It will be actively downloading these tiles, and so it needs an internet connection.\n\ncanberra_stamen_map <- get_stamenmap(bbox, zoom = 11, maptype = \"toner-lite\")\n\nggmap(canberra_stamen_map)\n\n\n\n\nOnce we have a map then we can use ggmap() to plot it. Now we want to get some data that we plot on top of our tiles. We will just plot the location of the polling places, based on which ‘division’ it is. This is available here. The Australian Electoral Commission (AEC) is the official government agency that is responsible for elections in Australia.\n\n# Read in the booths data for each year\nbooths <-\n  readr::read_csv(\n    \"https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload-24310.csv\",\n    skip = 1,\n    guess_max = 10000\n  )\n\nhead(booths)\n\n# A tibble: 6 × 15\n  State DivisionID DivisionNm PollingPlaceID PollingPlaceTypeID PollingPlaceNm  \n  <chr>      <dbl> <chr>               <dbl>              <dbl> <chr>           \n1 ACT          318 Bean                93925                  5 Belconnen BEAN …\n2 ACT          318 Bean                93927                  5 BLV Bean PPVC   \n3 ACT          318 Bean                11877                  1 Bonython        \n4 ACT          318 Bean                11452                  1 Calwell         \n5 ACT          318 Bean                 8761                  1 Chapman         \n6 ACT          318 Bean                 8763                  1 Chisholm        \n# … with 9 more variables: PremisesNm <chr>, PremisesAddress1 <chr>,\n#   PremisesAddress2 <chr>, PremisesAddress3 <chr>, PremisesSuburb <chr>,\n#   PremisesStateAb <chr>, PremisesPostCode <chr>, Latitude <dbl>,\n#   Longitude <dbl>\n\n\nThis dataset is for the whole of Australia, but as we are just going to plot the area around Canberra we filter to that and only to booths that are geographic (the AEC has various options for people who are in hospital, or not able to get to a booth, etc, and these are still ‘booths’ in this dataset).\n\n# Reduce the booths data to only rows with that have latitude and longitude\nbooths_reduced <-\n  booths |>\n  filter(State == \"ACT\") |> \n  select(PollingPlaceID, DivisionNm, Latitude, Longitude) |> \n  filter(!is.na(Longitude)) |> # Remove rows that do not have a geography\n  filter(Longitude < 165) # Remove Norfolk Island\n\nNow we can use ggmap in the same way as before to plot our underlying tiles, and then build on that using geom_point() to add our points of interest.\n\nggmap(canberra_stamen_map,\n      extent = \"normal\",\n      maprange = FALSE) +\n  geom_point(data = booths_reduced,\n             aes(x = Longitude,\n                 y = Latitude,\n                 colour = DivisionNm),) +\n  scale_color_brewer(name = \"2019 Division\", palette = \"Set1\") +\n  coord_map(\n    projection = \"mercator\",\n    xlim = c(attr(map, \"bb\")$ll.lon, attr(map, \"bb\")$ur.lon),\n    ylim = c(attr(map, \"bb\")$ll.lat, attr(map, \"bb\")$ur.lat)\n  ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())\n\n\n\n\nWe may like to save the map so that we do not have to draw it every time, and we can do that in the same way as any other graph, using ggsave().\n\nggsave(\"map.pdf\", width = 20, height = 10, units = \"cm\")\n\nFinally, the reason that we used Stamen Maps and OpenStreetMap is because it is open source, but we could have also used Google Maps. This requires you to first register a credit card with Google, and specify a key, but with low usage should be free. Using Google Maps, get_googlemap(), brings some advantages over get_stamenmap(), for instance it will attempt to find a placename, rather than needing to specify a bounding box.\n\n\n6.4.2 US troop deployment\nLet us see another example of a static map, this time using data on US military deployments from troopdata (Flynn 2021). We can access data about US overseas military bases back to the start of the Cold War using get_basedata().\n\ninstall.packages(\"troopdata\")\n\n\nlibrary(troopdata)\n\nbases <- get_basedata()\n\nhead(bases)\n\n# A tibble: 6 × 9\n  countryname ccode iso3c basename            lat   lon  base lilypad fundedsite\n  <chr>       <dbl> <chr> <chr>             <dbl> <dbl> <dbl>   <dbl>      <dbl>\n1 Afghanistan   700 AFG   Bagram AB          34.9  69.3     1       0          0\n2 Afghanistan   700 AFG   Kandahar Airfield  31.5  65.8     1       0          0\n3 Afghanistan   700 AFG   Mazar-e-Sharif     36.7  67.2     1       0          0\n4 Afghanistan   700 AFG   Gardez             33.6  69.2     1       0          0\n5 Afghanistan   700 AFG   Kabul              34.5  69.2     1       0          0\n6 Afghanistan   700 AFG   Herat              34.3  62.2     1       0          0\n\n\nWe will look at the locations of US military bases in: Germany, Japan, and Australia. The troopdata dataset already has latitude and longitude of the base. We will use that as our item of interest. The first step is to define a bounding box for each of country.\n\nlibrary(ggmap)\n\n# Based on: https://data.humdata.org/dataset/bounding-boxes-for-countries\nbbox_germany <-\n  c(\n    left = 5.867,\n    bottom = 45.967,\n    right = 15.033,\n    top = 55.133\n  )\n\nbbox_japan <-\n  c(\n    left = 127,\n    bottom = 30,\n    right = 146,\n    top = 45\n  )\n\nbbox_australia <-\n  c(\n    left = 112.467,\n    bottom = -45,\n    right = 155,\n    top = -9.133\n  )\n\nThen we need to get the tiles using get_stamenmap() from ggmap.\n\ngermany_stamen_map <-\n  get_stamenmap(bbox_germany, zoom = 6, maptype = \"toner-lite\")\n\njapan_stamen_map <-\n  get_stamenmap(bbox_japan, zoom = 6, maptype = \"toner-lite\")\n\naustralia_stamen_map <-\n  get_stamenmap(bbox_australia, zoom = 5, maptype = \"toner-lite\")\n\nAnd finally, we can bring it all together with maps show US military bases in Germany (Figure 6.33), Japan (Figure 6.34), and Australia (Figure 6.35).\n\nggmap(germany_stamen_map) +\n  geom_point(data = bases,\n             aes(x = lon,\n                 y = lat)\n             ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() \n\n\n\n\nFigure 6.33: Map of US military bases in Germany\n\n\n\n\n\nggmap(japan_stamen_map) +\n  geom_point(data = bases,\n             aes(x = lon,\n                 y = lat)\n             ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() \n\n\n\n\nFigure 6.34: Map of US military bases in Japan\n\n\n\n\n\nggmap(australia_stamen_map) +\n  geom_point(data = bases,\n             aes(x = lon,\n                 y = lat)\n             ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() \n\n\n\n\nFigure 6.35: Map of US military bases in Australia\n\n\n\n\n\n\n6.4.3 Geocoding\nTo this point we assumed that we already had geocoded data, which means that we have a latitude and longitude. If we only have place names, such as ‘Canberra, Australia’, ‘Ottawa, Canada’, ‘Accra, Ghana’, ‘Quito, Ecuador’ are just names, they do not actually inherently have a location. To plot them we need to get a latitude and longitude for them. The process of going from names to coordinates is called geocoding.\nThere are a range of options to geocode data in R, but tidygeocoder is especially useful (Cambon and Belanger 2021). We first need a dataframe of locations.\n\nplace_names <-\n  tibble(\n    city = c('Canberra', 'Ottawa', 'Accra', 'Quito'),\n    country = c('Australia', 'Canada', 'Ghana', 'Ecuador')\n  )\n\nplace_names\n\n# A tibble: 4 × 2\n  city     country  \n  <chr>    <chr>    \n1 Canberra Australia\n2 Ottawa   Canada   \n3 Accra    Ghana    \n4 Quito    Ecuador  \n\n\n\nlibrary(tidygeocoder)\n\nplace_names <-\n  geo(city = place_names$city,\n      country = place_names$country,\n      method = 'osm')\n\nplace_names\n\n# A tibble: 4 × 4\n  city     country       lat    long\n  <chr>    <chr>       <dbl>   <dbl>\n1 Canberra Australia -35.3   149.   \n2 Ottawa   Canada     45.4   -75.7  \n3 Accra    Ghana       5.56   -0.201\n4 Quito    Ecuador    -0.220 -78.5  \n\n\nAnd we can now plot and label these cities (Figure 6.36).\n\nworld <- map_data(map = \"world\")\n\nggplot() +\n  geom_polygon(\n    data = world,\n    aes(x = long,\n        y = lat,\n        group = group),\n    fill = \"white\",\n    colour = \"grey\"\n  ) +\n  coord_map(ylim = c(47,-47)) +\n  geom_point(aes(x = place_names$long,\n                 y = place_names$lat),\n             color = \"black\") +\n  geom_text(aes(\n    x = place_names$long,\n    y = place_names$lat,\n    label = place_names$city\n  ),\n  nudge_y = -5,) +\n  theme_classic() +\n  labs(x = \"Longitude\",\n       y = \"Latitude\")\n\n\n\n\nFigure 6.36: Map of Accra, Canberra, Ottawa, and Quito after geocoding to obtain their locations"
  },
  {
    "objectID": "06-static_communication.html#exercises-and-tutorial",
    "href": "06-static_communication.html#exercises-and-tutorial",
    "title": "6  Static communication",
    "section": "6.5 Exercises and tutorial",
    "text": "6.5 Exercises and tutorial\n\n6.5.1 Exercises\n\nAssume tidyverse and datasauRus are installed and loaded. What would be the outcome of the following code? datasaurus_dozen |> filter(dataset == \"v_lines\") |> ggplot(aes(x=x, y=y)) + geom_point()\n\nFour vertical lines\nFive vertical lines\nThree vertical lines\nTwo vertical lines\n\nAssume tidyverse and the ‘beps’ dataset have been installed and loaded. What change should be made to the following to make the bars for the different parties be next to each other rather than on top of each other? beps |> ggplot(mapping = aes(x = age, fill = vote)) + geom_bar()\n\nposition = \"side_by_side\"\nposition = \"dodge\"\nposition = \"adjacent\"\nposition = \"closest\"\n\nWhich theme should be used to remove the solid lines along the x and y axes?\n\ntheme_minimal()\ntheme_classic()\ntheme_bw()\ntheme_dark()\n\nAssume tidyverse and the ‘beps’ dataset have been installed and loaded. What should be added to ‘labs()’ to change the text of the legend? beps |> ggplot(mapping = aes(x = age, fill = vote)) + geom_bar() + theme_minimal() + labs(x = \"Age of respondent\", y = \"Number of respondents\")\n\ncolor = \"Voted for\"\nlegend = \"Voted for\"\nscale = \"Voted for\"\nfill = \"Voted for\"\n\nWhich palette from scale_colour_brewer() is divergent?\n\n‘Accent’\n‘RdBu’\n‘GnBu’\n‘Set1’\n\nWhich geom should be used to make a scatter plot?\n\ngeom_smooth()\ngeom_point()\ngeom_bar()\ngeom_dotplot()\n\nWhich of these would result in the largest number of bins?\n\ngeom_histogram(binwidth = 5)\ngeom_histogram(binwidth = 2)\n\nIf there is a dataset that contains the heights of 100 birds each from one of three different species. If we are interested in understanding the distribution of these heights, then in a paragraph or two, please explain which type of graph should be used and why?\nAssume the dataset and columns exist. Would this code work? data |> ggplot(aes(x = col_one)) |> geom_point() (pick one)?\n\nYes\nNo\n\nWhich geom should be used to plot categorical data (pick one)?\n\ngeom_bar()\ngeom_point()\ngeom_abline()\ngeom_boxplot()\n\nWhy are boxplots often inappropriate (pick one)?\n\nThey hide the full distribution of the data.\nThey are hard to make.\nThey are ugly.\nThe mode is clearly displayed.\n\nWhich of the following, if any, are elements of the layered grammar of graphics (Wickham 2010) (select all that apply)?\n\nA default dataset and set of mappings from variables to aesthetics.\nOne or more layers, with each layer having one geometric object, one statistical transformation, one position adjustment, and optionally, one dataset and set of aesthetic mappings.\nColors that enable the reader to understand the main point.\nA coordinate system.\nThe facet specification.\nOne scale for each aesthetic mapping used.\n\nWhich function from modelsummary is used to create a table of descriptive statistics?\n\ndatasummary_descriptive()\ndatasummary_skim()\ndatasummary_crosstab()\ndatasummary_balance()\n\n\n\n\n6.5.2 Tutorial\nUsing R Markdown, please create a graph using ggplot2 and a map using ggmap and add explanatory text to accompany both. Be sure to include cross-references and captions, etc. This should take one to two pages for each of them.\nThen, for the graph, please reflect on Vanderplas, Cook, and Hofmann (2020) and add a few paragraphs about the different options that you considered that the graph more effective. (If you’ve not now got at least two pages about your graph you’ve likely written too little.)\nAnd finally, for the map, please reflect on the following quote from Heather Krause: ‘maps only show people who aren’t invisible to the makers’ as well as Chapter 3 from D’Ignazio and Klein (2020) and add a few paragraphs related to this. (Again, if you’ve not now got at least two pages about your map you’ve likely written too little.)\nPlease submit a PDF.\n\n\n\n\n\nArel-Bundock, Vincent. 2021a. Modelsummary: Summary Tables and Plots for Statistical Models and Data: Beautiful, Customizable, and Publication-Ready. https://CRAN.R-project.org/package=modelsummary.\n\n\n———. 2021b. WDI: World Development Indicators and Other World Bank Data. https://CRAN.R-project.org/package=WDI.\n\n\nArnold, Jeffrey B. 2021. Ggthemes: Extra Themes, Scales and Geoms for ’Ggplot2’. https://CRAN.R-project.org/package=ggthemes.\n\n\nCambon, Jesse, and Christopher Belanger. 2021. “Tidygeocoder: Geocoding Made Easy.” Zenodo. https://doi.org/10.5281/zenodo.3981510.\n\n\nCleveland, William. 1994. The Elements of Graphing Data. 2nd ed. Hobart Press.\n\n\nD’Ignazio, Catherine, and Lauren F Klein. 2020. Data Feminism. Mit Press.\n\n\nFlynn, Michael. 2021. Troopdata: Tools for Analyzing Cross-National Military Deployment and Basing Data. https://CRAN.R-project.org/package=troopdata.\n\n\nFox, John, and Robert Andersen. 2006. “Effect Displays for Multinomial and Proportional-Odds Logit Models.” Sociological Methodology 36 (1): 225–55.\n\n\nFranconeri, Steven L, Lace M Padilla, Priti Shah, Jeffrey M Zacks, and Jessica Hullman. 2021. “The Science of Visual Data Communication: What Works.” Psychological Science in the Public Interest 22 (3): 110–61.\n\n\nFriendly, Michael, and Howard Wainer. 2021. A History of Data Visaulization and Graphic Communication. 1st ed. Harvard University Press.\n\n\nGarnier, Simon, Ross, Noam, Rudis, Robert, Camargo, et al. 2021. viridis - Colorblind-Friendly Color Maps for r. https://doi.org/10.5281/zenodo.4679424.\n\n\nGelfand, Sharla. 2020. Opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nHealy, Kieran. 2018. Data Visualization. Princeton University Press.\n\n\nIannone, Richard, Joe Cheng, and Barret Schloerke. 2020. Gt: Easily Create Presentation-Ready Display Tables. https://CRAN.R-project.org/package=gt.\n\n\nKahle, David, and Hadley Wickham. 2013. “Ggmap: Spatial Visualization with Ggplot2.” The R Journal 5 (1): 144–61. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf.\n\n\nKuznets, Simon. 1941. National Income and Its Composition, 1919-1938. National Bureau of Economic Research.\n\n\nLocke, Steph, and Lucy D’Agostino McGowan. 2018. datasauRus: Datasets from the Datasaurus Dozen. https://CRAN.R-project.org/package=datasauRus.\n\n\nNeuwirth, Erich. 2014. RColorBrewer: ColorBrewer Palettes. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nOECD. 2014. “The Essential Macroeconomic Aggregates.” In Understanding National Accounts, 13–46. OECD. https://doi.org/10.1787/9789264214637-2-en.\n\n\nPedersen, Thomas Lin. 2020. Patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nPhillips, Alban W. 1958. “The Relation Between Unemployment and the Rate of Change of Money Wage Rates in the United Kingdom, 1861-1957.” Economica 25 (100): 283–99.\n\n\nRudis, Bob. 2020. Hrbrthemes: Additional Themes, Theme Components and Utilities for ’Ggplot2’. https://CRAN.R-project.org/package=hrbrthemes.\n\n\nVanderplas, Susan, Dianne Cook, and Heike Hofmann. 2020. “Testing Statistical Charts: What Makes a Good Graph?” Annual Review of Statistics and Its Application 7: 61–88.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWeissgerber, Tracey L, Natasa M Milic, Stacey J Winham, and Vesna D Garovic. 2015. “Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm.” PLoS Biology 13 (4): e1002128.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n\n\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. https://r4ds.had.co.nz/.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed. Springer.\n\n\nXie, Yihui. 2021. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nZhu, Hao. 2020. kableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra."
  },
  {
    "objectID": "07-interactive_communication.html",
    "href": "07-interactive_communication.html",
    "title": "7  Interactive communication",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "07-interactive_communication.html#introduction",
    "href": "07-interactive_communication.html#introduction",
    "title": "7  Interactive communication",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nBooks and papers have been the primary mediums for communication for thousands of years. But with the rise of computers, and especially the internet, in recent decades, these static approaches have been complemented with interactive approaches. Fundamentally, the internet is about making files available others. If we additionally allow them to do something with what we make available, then we need to take a variety of additional aspects into consideration.\nIn this chapter we begin by covering how to create and publish a website. This serves as a place to host a portfolio of work. After that we cover adding interaction to maps and graphs, which are two that nicely lend themselves to this."
  },
  {
    "objectID": "07-interactive_communication.html#making-a-website",
    "href": "07-interactive_communication.html#making-a-website",
    "title": "7  Interactive communication",
    "section": "7.2 Making a website",
    "text": "7.2 Making a website\nA website is a critical part of communication. For instance, it is a place to make a portfolio of work publicly available. One way to make a website is to use blogdown (Xie, Dervieux, and Hill 2021). blogdown is a package that allows you to make websites, not just blogs, notwithstanding its name, largely within R Studio. It builds on ‘Hugo’, which is a popular general framework for making websites. blogdown enables us to freely and quickly get a website up-and-running. It is easy to add content from time-to-time. And it integrates with R Markdown, which makes it easy to share work. But blogdown is brittle. Because it is so dependent on Hugo, features that work today may not work tomorrow. Also, owners of Hugo templates can update them at any time, without thought to existing users. blogdown is a good option if we know what we are doing, or have a specific use-case, or style, in mind. But two other alternatives are better starting points.\nThe first is distill (Allaire et al. 2021). Again, this is an R package that wraps around another framework, in this case ‘Distill’. But in contrast to Hugo, Distill is more focused on common needs in data science, and is also only maintained by one group, so it may be considered a more stable choice. That said, the default distill site is a little plain looking. As such, following Presmanes Hill (2021a), we will pair it with a third option, postcards (Kross 2021).\nThe third option, and the one that we will start with, is postcards (Kross 2021). This is a tailored solution that creates simple biographical websites that look great. Having set-up GitHub in R Studio, it is literally possible to have a postcards website online in five minutes.\n\n7.2.1 Postcards\nBegin by installing postcards, with install.packages('postcards') and then creating a new project for the website (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘Postcards Website’). We can then pick a name and location for the project, and select a postcards theme. In this case, we can start with ‘trestles’ but this can be changed later. Click the option to ‘Open in new session’ and then create the project.\nThat will open a new file and we can now build the site by clicking ‘Knit’. This will result in a one-page website (Figure 7.1).\n\n\n\nFigure 7.1: Example of a website made with postcards using the ‘trestles’ theme\n\n\nWe can now update the basic content such as name, bio and links, to match our own (Figure 7.2).\n\n\n\nFigure 7.2: Example of Trestles website with updated details\n\n\nAfter the details are personalized, we can push it to GitHub which will act as a host for our website. By default, GitHub would try to build the site, which we do not want, so we need to first add a hidden file to turn that off, by running this in the console:\n\nfile.create('.nojekyll')\n\nThen, assuming GitHub was set-up in Chapter @ref(reproducible-workflows), we can use usethis (Wickham and Bryan 2020) to get our newly created project onto GitHub. We use use_git() to initialize a Git repository, and then use_github() pushes it to GitHub.\n\nlibrary(usethis)\nuse_git()\nuse_github()\n\nThe project will then be on GitHub. We can use GitHub pages to host it: ‘Settings -> Pages’ and then change the source to ‘main’ or ‘master’, depending on your settings. GitHub will let you know the address that you can share to visit your site.\n\n\n7.2.2 Distill\nWe will now use distill (Allaire et al. 2021) to build additional infrastructure around our postcards site, following Presmanes Hill (2021a). After that we will explore some of the aspects of distill that make it a nice choice, and mention some of the trade-offs. First, we install distill with install.packages('distill'), and again, create a new project for the website (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘Distill Blog’).\nWe can then pick a name and location for the project, and set a title. Select ‘Configure for GitHub Pages’ and also ‘Open in a new session’. These options can be changed ex post. It should look something like Figure 7.3.\n\n\n\nFigure 7.3: Example settings for setting up distill\n\n\nAt this point we can click ‘Build Website’ in the Build tab, and we should see the default website (Figure 7.4).\n\n\n\nFigure 7.4: Example of default distill website\n\n\nAgain, now we need to update it to reflect our own details. The default for a ‘Distill Blog’ is that the blog is the homepage. We can change that to use a postcards page as the homepage. First we change the name of ‘index.Rmd’ to ‘blog.Rmd’ and then create a new ‘trestles’ page:\n\npostcards::create_postcard(file = \"index.Rmd\", template = \"trestles\")\n\nThe trestles page will open, and we need to add the following line in the yaml file: site: distill::distill_website. In Figure 7.5 it was added at line 16, and then we can rebuild the website.\n\n\n\nFigure 7.5: Updating the yaml to change the homepage\n\n\nWe can make the same changes to the default content as earlier, for instance, updating the links, image, and bio. The advantage of using distill is that we now have additional pages, not just a one-page website, and we also have a blog. By default, we have an ‘about’ page, but some other pages that may be useful, depending on the particular use-case, include: ‘research’, ‘teaching’, ‘talks’, ‘projects’, ‘software’, and ‘datasets’. As an example, we will add and edit a page called ‘software’ using distill::create_article(file = 'software').\nThat will create and open an R Markdown document. To add it to the website, open ’_site.yml’ and then add a line to the ‘navbar’ (Figure 7.6). After this is done then we can rebuild the website, and the ‘software’ page will have been added.\n\n\n\nFigure 7.6: Adding another page to the website\n\n\nWe can continue with this process until we are happy with the website. For instance, we may want to add a blog. To do this we follow the same pattern as before, but with ‘blog’ instead of ‘software’.\nWhen we are happy with our website, we can push it to GitHub and then use GitHub Pages to host it, in the same way that we did with the postcards site.\nUsing distill is a good option when we need a multi-page website, but still want a fairly controlled environment. There are many options that can be changed, and Presmanes Hill (2021a) is a good starting point, in addition to the distill homepage: https://rstudio.github.io/distill/.\nThat said, distill is opinionated. While it is a great option, if we want something a little more flexible then blogdown might be a better option.\n\n\n7.2.3 Blogdown\nUsing blogdown (Xie, Dervieux, and Hill 2021) is more work than Google sites or Squarespace. It requires a little more knowledge than using a basic Wordpress site. And if we need to customize absolutely every aspect of the website, or need everything to be ‘just so’ then blogdown may not be a good option. But blogdown allows a variety and level of expression that is not possible with distill. Presmanes Hill (2021b) and Xie, Thomas, and Presmanes Hill (2021) are useful for learning more about blogdown.\nFirst we need to install blogdown with install.packages(\"blogdown\"). And then we create a new project for the website (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘Website using blogdown’). At this point we can set a name and location, and also select ‘Open in a new session’ (Figure 7.7).\n\n\n\nFigure 7.7: Example settings for setting up blogdown\n\n\nWe can click ‘Build Website’ from the ‘Build’ pane, but then an extra step is needed; we need to serve the site with blogdown:::serve_site(). After this, the site will show in the ‘Viewer’ pane (Figure 7.8).\n\n\n\nFigure 7.8: Serving default blogdown site\n\n\nThe default website is now being ‘served’ locally. This means that changes we make will be reflected in the website that we see in the Viewer pane. To see the website in a web browser, click ‘Show in new window’ on the top left of the Viewer. That will open the website using the address that R Studio also provides.\nWe now want to update the content, starting with the ‘About’ section. To do that we go to ‘content -> about.md’ and modify or add content. One nice aspect of blogdown is that it will automatically reload the content when we save, and so changes should appear immediately We could modify other aspects also. For instance, we could change the logo, by adding a square image to ‘public/images/’ and then changing the call to ‘logo.png’ in ‘config.yaml’. When we are happy with it, we can make our website public in the same way as we did for postcards.\nOne advantage of using blogdown is that it allows us to use Hugo templates. This provides a large number of beautifully crafted websites. To pick a theme we go to the Hugo themes page: https://themes.gohugo.io. There are hundreds of different themes. In general, most of them can be made to work with blogdown, but sometimes it can be a bit of a hassle.\nOne nice option is Apéro: https://hugo-apero-docs.netlify.app. We can specify the use of this theme as part of creating a new site (‘File’ -> ‘New Project’ -> ‘New Directory’ -> ‘Website using blogdown’). At this point, in addition to setting the name and location, we can specify a theme. Specifically, in the ‘Hugo theme’ field, we can specify a GitHub username and repository, which in this case is ‘hugo-apero/apero’ (Figure 7.9).\n\n\n\nFigure 7.9: Using the Apéro theme"
  },
  {
    "objectID": "07-interactive_communication.html#interactive-maps",
    "href": "07-interactive_communication.html#interactive-maps",
    "title": "7  Interactive communication",
    "section": "7.3 Interactive maps",
    "text": "7.3 Interactive maps\nThe nice thing about interactive maps is that we can let our user decide what they are interested in. For instance, in the case of a map, some people will be interested in, say, Toronto, while others will be interested in Chennai or even Auckland. But it would be difficult to present a map that focused on all of those, so an interactive map is a way to allow users to focus on what they want.\nThat said, it is important to be cognizant of what we are doing when we build maps, and more broadly, what is being done at scale to enable us to be able to build our own maps. For instance, with regard to Google, McQuire (2019) says:\n\nGoogle began life in 1998 as a company famously dedicated to organising the vast amounts of data on the Internet. But over the last two decades its ambitions have changed in a crucial way. Extracting data such as words and numbers from the physical world is now merely a stepping-stone towards apprehending and organizing the physical world as data. Perhaps this shift is not surprising at a moment when it has become possible to comprehend human identity as a form of (genetic) ‘code’. However, apprehending and organizing the world as data under current settings is likely to take us well beyond Heidegger’s ‘standing reserve’ in which modern technology enframed ‘nature’ as productive resource. In the 21st century, it is the stuff of human life itself—from genetics to bodily appearances, mobility, gestures, speech, and behaviour—that is being progressively rendered as productive resource that can not only be harvested continuously but subject to modulation over time.\n\nDoes this mean that we should not use or build interactive maps? Of course not. But it is important to be aware of the fact that this is a frontier, and the boundaries of appropriate use are still being determined. Indeed, the literal boundaries of the maps themselves are being consistently determined and updated. The move to digital maps, compared with physical printed maps, means that it is possible for different users to be presented with different realities. For instance, ‘…Google routinely takes sides in border disputes. Take, for instance, the representation of the border between Ukraine and Russia. In Russia, the Crimean Peninsula is represented with a hard-line border as Russian-controlled, whereas Ukrainians and others see a dotted-line border. The strategically important peninsula is claimed by both nations and was violently seized by Russia in 2014, one of many skirmishes over control’ Bensinger (2020).\n\n7.3.1 Leaflet\nWe can use leaflet (Cheng, Karambelkar, and Xie 2021) to make interactive maps. The essentials are similar to ggmap (Kahle and Wickham 2013), but there are many additional aspects beyond that. We can redo the US military deployments map from Chapter @ref(static-communication) that used troopdata (Flynn 2021). The advantage with an interactive map is that we can plot all the bases and allow the user to focus on which area they want, in comparison with Chapter @ref(static-communication) where we just picked a few particular countries.\nIn the same way as a graph in ggplot2 begins with ggplot(), a map in leaflet begins with leaflet(). Here we can specify data, and other options such as width and height. After this, we add ‘layers’ in the same way that we added them in ggplot2. The first layer that we add is a tile, using addTiles(). In this case, the default is from OpenStreeMap. After that we add markers with addMarkers() to show the location of each base (Figure 7.10).\n\nlibrary(leaflet)\nlibrary(tidyverse)\nlibrary(troopdata)\n\nbases <- get_basedata()\n\n# Some of the bases include unexpected characters which we need to address\nEncoding(bases$basename) <- 'latin1'\n\nleaflet(data = bases) |>\n  addTiles() |>  # Add default OpenStreetMap map tiles\n  addMarkers(lng = bases$lon, \n             lat = bases$lat, \n             popup = bases$basename,\n             label = bases$countryname)\n\n\n\n\nFigure 7.10: Interactive map of US bases\n\n\n\nThere are two new arguments, compared with ggmap. The first is ‘popup’, which is the behavior that occurs when the user clicks on the marker. In this case, the name of the base is provided. The second is ‘label’, which is what happens when the user hovers on the marker. In this case it is the name of the country.\nWe can try another example, this time of the amount spent building those bases. We will introduce a different type of marker here, which is circles. This will allow us to use different colors for the outcomes of each type. There are four possible outcomes: “More than $100,000”, “More than $10,000”, “More than $1,000”, “$1,000 or less” (Figure 7.11).\n\nbuild <- \n  get_builddata(startyear = 2008, endyear = 2019) |>\n  filter(!is.na(lon)) |>\n  mutate(\n    cost = case_when(\n      spend_construction > 100000 ~ \"More than $100,000\",\n      spend_construction > 10000 ~ \"More than $10,000\",\n      spend_construction > 1000 ~ \"More than $1,000\",\n      TRUE ~ \"$1,000 or less\"\n      )\n    )\n\npal <-\n  colorFactor(\"Dark2\", domain = build$cost |> unique())\n\nleaflet() |>\n  addTiles() |>  # Add default OpenStreetMap map tiles\n  addCircleMarkers(\n    data = build,\n    lng = build$lon,\n    lat = build$lat,\n    color = pal(build$cost),\n    popup = paste(\n      \"<b>Location:</b>\",\n      as.character(build$location),\n      \"<br>\",\n      \"<b>Amount:</b>\",\n      as.character(build$spend_construction),\n      \"<br>\"\n    )\n  ) |>\n  addLegend(\n    \"bottomright\",\n    pal = pal,\n    values = build$cost |> unique(),\n    title = \"Type\",\n    opacity = 1\n  )\n\n\n\n\nFigure 7.11: Interactive map of US bases with colored circules to indicate spend\n\n\n\n\n\n7.3.2 Mapdeck\nmapdeck (Cooley 2020) is based on WebGL. This means the web browser will do a lot of work for us. This enables us to accomplish things with mapdeck that leaflet struggles with, such as larger datasets.\nTo this point we have used ‘stamen maps’ as our underlying tile, but mapdeck uses ‘Mapbox’: https://www.mapbox.com/. This requires registering an account and obtaining a token. This is free and only needs to be done once. Once we have that token we add it to our R environment (the details of this process are covered in Chapter @ref(gather-data) by running usethis::edit_r_environ(), which will open a text file. There we can add our Mapbox secret token.\n\nMAPBOX_TOKEN = 'PUT_YOUR_MAPBOX_SECRET_HERE'\n\nWe then save this ‘.Renviron’ file, and restart R (‘Session’ -> ‘Restart R’).\nHaving obtained a token, we can create a plot of our base spend data from earlier (Figure 7.12).\n\nlibrary(mapdeck)\n\nmapdeck(style = mapdeck_style('dark')\n        ) |>\n  add_scatterplot(\n    data = build, \n    lat = \"lat\", \n    lon = \"lon\", \n    layer_id = 'scatter_layer',\n    radius = 10,\n    radius_min_pixels = 5,\n    radius_max_pixels = 100,\n    tooltip = \"location\"\n  )\n\n\n\n\nFigure 7.12: Interactive map of US bases using Mapdeck"
  },
  {
    "objectID": "07-interactive_communication.html#shiny",
    "href": "07-interactive_communication.html#shiny",
    "title": "7  Interactive communication",
    "section": "7.4 Shiny",
    "text": "7.4 Shiny\nshiny (Chang et al. 2021) is a way of making interactive web applications using R. It is fun, but fiddly. Here we are going to step through one way to take advantage of shiny. Which is to quickly add some interactivity to our graphs. We will return to shiny in more detail in Chapter @ref(deploying-models).\nWe are going to make an interactive graph based on the ‘babynames’ dataset from babynames (Wickham 2019). First, we will build a static version (Figure 7.13).\n\nlibrary(babynames)\nlibrary(tidyverse)\n\ntop_five_names_by_year <-\n  babynames |>\n  group_by(year, sex) |>\n  arrange(desc(n)) |>\n  slice_head(n = 5)\n\ntop_five_names_by_year |>\n  ggplot(aes(x = n, fill = sex)) +\n  geom_histogram(position = \"dodge\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(x = \"Babies with that name\",\n       y = \"Occurances\",\n       fill = \"Sex\")\n\n\n\n\nFigure 7.13: Popular baby names\n\n\n\n\nWe can see the most popular boys names tend to be more clustered, compared with the most-popular girls names, which may be more spread out. However, one thing that we might be interested in is how the effect of the ‘bins’ parameter shapes what we see. We might like to use interactivity to explore different values.\nTo get started, create a new shiny app (‘File -> New File -> Shiny Web App’). Give it a name, such as ‘not_my_first_shiny’ and then leave all the other options as the default. A new file ‘app.R’ will open and we click ‘Run app’ to see what it looks like.\nNow replace the content in that file, ‘app.R’, with the content below, and then again click ‘Run app’\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui <- fluidPage(\n  # Application title\n  titlePanel(\"Count of names for five most popular names each year.\"),\n  \n  # Sidebar with a slider input for number of bins\n  sidebarLayout(sidebarPanel(\n    sliderInput(\n      inputId = \"number_of_bins\",\n      label = \"Number of bins:\",\n      min = 1,\n      max = 50,\n      value = 30\n    )\n  ),\n  \n  # Show a plot of the generated distribution\n  mainPanel(plotOutput(\"distPlot\")))\n)\n\n# Define server logic required to draw a histogram\nserver <- function(input, output) {\n  output$distPlot <- renderPlot({\n    # Draw the histogram with the specified number of bins\n    top_five_names_by_year |>\n      ggplot(aes(x = n, fill = sex)) +\n      geom_histogram(position = \"dodge\", bins = input$number_of_bins) +\n      theme_minimal() +\n      scale_fill_brewer(palette = \"Set1\") +\n      labs(x = \"Babies with that name\",\n           y = \"Occurances\",\n           fill = \"Sex\")\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\nWe have just build an interactive graph where the number of bins can be changed. It should look like Figure 7.14.\n\n\n\nFigure 7.14: Example of Shiny app where the user controls the number of bins"
  },
  {
    "objectID": "07-interactive_communication.html#exercises-and-tutorial",
    "href": "07-interactive_communication.html#exercises-and-tutorial",
    "title": "7  Interactive communication",
    "section": "7.5 Exercises and tutorial",
    "text": "7.5 Exercises and tutorial\n\n7.5.1 Exercises\n\nBased on Presmanes Hill (2021a), are posts in distill re-built automatically (pick one)?\n\nNo\nYes\nDepends on settings\n\nBased on Lovelace, Nowosad, and Muenchow (2019), please explain in a paragraph or two, what is the difference between vector data and raster data in the context of geographic data?\nBased on Wickham (2021), shiny uses:\n\nObject-oriented programming\nFunctional programming\nReactive programming\n\nIn a paragraph or two, why is it important to have a website?\nWhich of the following are packages that we could use to make a website (select all that apply)?\n\ndistill\nblogdown\npostcards\nhugo\n\nLooking at the help file for postcards, which of the following are themes that we could use (select all that apply)?\n\njolla\njolla-blue\njolla-red\ntrestles\nmjolnir\nonofre\nsolana\n\nWhich function should we use to stop GitHub itself from trying to build our site instead of just serving it (pick one)?\n\nfile.create('.nojekyll')\nfile.remove('.nojekyll')\nfile.create('.jekyll')\nfile.remove('.jekyll')\n\nWhich argument to addMarkers() is used to specify the behavior that occurs after a marker is clicked (pick one)?\n\nlayerId\nicon\npopup\nlabel\n\n\n\n\n7.5.2 Tutorial\nThe catalyst for this tutorial was work by Mauricio Vargas Sepúlveda (‘Pachá’) and Andrew Whitby.\nPlease obtain data on the ethnic origins and number of victims of Auschwitz. Then use shiny to create an interactive graph and an interactive table. These should show the number of people murdered by nationality/category and should allow the user to specify the groups they are interested in seeing data for. Publish them. Then, based on the themes brought up in Bouie (2022), discuss your work in at least two pages. Submit a PDF created using R Markdown, and ensure that it contains a link to your app and the GitHub repo that contains all code and data.\n\n\n7.5.3 Paper\nAt about this point, Paper Two (Appendix @ref(paper-two)) would be appropriate.\n\n\n\n\n\nAllaire, JJ, Rich Iannone, Alison Presmanes Hill, and Yihui Xie. 2021. Distill: ’R Markdown’ Format for Scientific and Technical Writing.\n\n\nBensinger, Greg. 2020. Google Redraws the Borders on Maps Depending on Who’s Looking. Washington Post.\n\n\nBouie, Jamelle. 2022. We Still Can’t See American Slavery for What It Was.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2021. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny.\n\n\nCheng, Joe, Bhaskar Karambelkar, and Yihui Xie. 2021. Leaflet: Create Interactive Web Maps with the JavaScript ’Leaflet’ Library. https://CRAN.R-project.org/package=leaflet.\n\n\nCooley, David. 2020. Mapdeck: Interactive Maps Using ’Mapbox GL JS’ and ’Deck.gl’. https://CRAN.R-project.org/package=mapdeck.\n\n\nFlynn, Michael. 2021. Troopdata: Tools for Analyzing Cross-National Military Deployment and Basing Data. https://CRAN.R-project.org/package=troopdata.\n\n\nKahle, David, and Hadley Wickham. 2013. “Ggmap: Spatial Visualization with Ggplot2.” The R Journal 5 (1): 144–61. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf.\n\n\nKross, Sean. 2021. Postcards: Create Beautiful, Simple Personal Websites. https://CRAN.R-project.org/package=postcards.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with r. CRC Press.\n\n\nMcQuire, Scott. 2019. “One Map to Rule Them All? Google Maps as Digital Technical Object.” Communication and the Public 4 (2): 150–65.\n\n\nPresmanes Hill, Alison. 2021a. M-F-E-O: postcards + distill. https://alison.rbind.io/post/2020-12-22-postcards-distill/.\n\n\n———. 2021b. Up & Running with Blogdown in 2021.\n\n\nWickham, Hadley. 2019. Babynames: US Baby Names 1880-2017. https://CRAN.R-project.org/package=babynames.\n\n\n———. 2021. Mastering Shiny.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2020. Usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\nXie, Yihui, Christophe Dervieux, and Alison Presmanes Hill. 2021. Blogdown: Create Blogs and Websites with r Markdown. https://github.com/rstudio/blogdown.\n\n\nXie, Yihui, Amber Thomas, and Alison Presmanes Hill. 2021. Blogdown: Creating Websites with r Markdown."
  },
  {
    "objectID": "08-farm.html",
    "href": "08-farm.html",
    "title": "8  Farm data",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "08-farm.html#introduction",
    "href": "08-farm.html#introduction",
    "title": "8  Farm data",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nAs we think about our world and telling stories about it, one of the most difficult aspects is to reduce the beautiful complexity of it into a dataset that we can use. We need to know what we are giving up when we do this. Often, we are interested in understanding the implications of some dataset, making forecasts based on it, or using that dataset to make claims about the broader world. Regardless of how we turn our world into data, we will only ever have a sample of the data that we need. Statistics provides formal approaches that we use to keep these issues front of mind.\nIn this chapter we first introduce statistical notions around sampling to provide a framework that we use to guide our data gathering. We then discuss censuses."
  },
  {
    "objectID": "08-farm.html#sampling-essentials",
    "href": "08-farm.html#sampling-essentials",
    "title": "8  Farm data",
    "section": "8.2 Sampling essentials",
    "text": "8.2 Sampling essentials\nStatistics is at the heart of telling stories with data. Statisticians have spent considerable time and effort thinking about the properties that various samples of data will have and how they enable us to speak to implications for the broader population.\nLet us say that we have some data. For instance, a particular toddler goes to sleep at 6:00pm every night. We might be interested to know whether that bedtime is common among all toddlers, or if we have an unusual toddler. We only have one toddler so our ability to use his bedtime to speak about all toddlers is limited.\nOne approach would be to talk to friends who also have toddlers. And then talk to friends of friends. How many friends, and friends of friends, do we have to ask because we can begin to feel comfortable speaking about some underlying truth of toddler bedtime?\nWu and Thompson (2020, 3) describe statistics as ‘the science of how to collect and analyze data and draw statements and conclusions about unknown populations.’ Here ‘population’ refers to some infinite group that we can never know exactly, but that we can use the probability distributions of random variables to describe the characteristics of. Another way to say this is that statistics involves getting some data and trying to say something sensible based on it.\nSome of the critical terminology includes:\n\n‘Target population’: The collection of all items about which we would like to speak.\n‘Sampling frame’: A list of all the items from the target population that we could get data about.\n‘Sample’: The items from the sampling frame that we get data about.\n\nA target population is a finite set of labelled items, of size \\(N\\). For instance, we could hypothetically add a label to all the books in the world: ‘Book 1’, ‘Book 2’, ‘Book 3’, …, ‘Book \\(N\\)’. There is a difference between use of the term population here, and that of everyday usage. For instance, one sometimes hears those who work with census data say that they do not need to worry about sampling because they have the whole population of the country. This is a conflation of the terms, as what they have is the sample gathered by the census of the population of a country.\nIt can be difficult to define a target population. For instance, say we have been asked to find out about the consumption habits of hipsters. How can we define that target population? If someone regularly eats avocado toast, but has never drunk bullet coffee, then are they in the population? Some aspects that we might be interested in are formally defined to an extent that is not always commonly realized. For instance, whether an area is classified as rural is often formally defined by a country’s statistical agency. But other aspects are less clear. For instance, how do we classify someone as a ‘smoker’? If a 15-year-old has had 100 cigarettes over their lifetime, then we need to treat them differently than if they have had none. But if a 90-year-old has had 100 cigarettes over their lifetime, then are they likely to different to a 90-year-old who has had none? At what age, and number of cigarettes do these answers change?\nConsider if we want to speak to the titles of all the books ever written. Our target population is all books ever written. But it is almost impossible for us to imagine that we could get information about the title of a book that was written in the nineteenth century, but that the author locked in their desk and never told anyone about. One sampling frame could be all books in the Library of Congress Online Catalog, another could be the 25 million that were digitized by Google (Somers 2017). And then finally, our sample may be the tens of thousands that are available through Project Gutenberg, and that we can access using gutenbergr (Robinson 2021).\nTo consider another example, consider wanting to speak of the attitudes of all Brazilians who live in Germany. The target population is all Brazilians who live in Germany. One possible source of information would be Facebook and so in that case, the sampling frame might be all Brazilians who live in Germany who have Facebook. And then our sample be might all Brazilians who live in Germany who have Facebook who we can gather data about. The target population and the sampling frame will be different because not all Brazilians who live in Germany will have Facebook. And the sampling frame will be different to the sample because we will likely not be able to gather data about all Brazilians who live in Germany and have Facebook.\n\n8.2.1 Sampling in Dublin and Reading\nTo be clearer, we will consider two examples: a 1798 count of the number of inhabitants of Dublin, Ireland (Whitelaw 1905), and a 1912 count of working-class households in Reading, England (Bowley 1913).\nIn 1798 the Reverend James Whitelaw conducted a survey of Dublin, Ireland, to count its population. Whitelaw (1905) describes how population estimates had a wide variation, for instance the estimated size of London at the time ranged from 128,570 to 300,000. Reverend Whitelaw expected that the Lord Mayor of Dublin could compel the person in charge of each house to affix a list of the inhabitants of that house to the door, and then Reverend Whitelaw could simply use this.\nInstead, he found that the lists were ‘frequently illegible, and generally short of the actual number by a third, or even one-half’. And so instead he recruited assistants, and they went door-to-door making their own counts. The resulting estimates are particularly informative (Figure 8.1). And the total population of Dublin in 1798 was estimated at 182,370.\n\n\n\nFigure 8.1: Extract of the results that Reverend Whitelaw found in 1798\n\n\nOne aspect worth noticing is that Reverend Whitelaw includes information about class. It is difficult to know how that was determined, but it played a large role in the data collection. Reverend Whitelaw describes how the houses of ‘the middle and upper classes always contained some individual who was competent to the task [of making a list]’. But that ‘among the lower class, which forms the great mass of the population of this city, the case was very different’. It is difficult to know how Reverend Whitelaw could have known that the upper and middle classes were not representing their number, while the lower class was. It is also difficult to imagine Reverend Whitelaw going into the houses of the upper class and counting their number, as he and his assistants did for the lower classes. As always, the issue of defining the target population is a difficult one, and it seems that there may have been slightly different approaches to each class.\nA little over one hundred years later, Bowley (1913) was interested in counting the number of working-class households in Reading, England. Bowley selects the sample using the following procedure (Bowley 1913, 672):\n\nOne building in ten was marked throughout the local directory in alphabetical order of streets, making about 1,950 in all. Of those about 300 were marked as shops, factories, institutions and non-residential buildings, and about 300 were found to be indexed among Principal Residents, and were so marked. The remaining 1,350 were working-class houses, and a number of volunteers set out to visit every one of these… [I]t was decided to take only one house in 20, rejecting the incomplete information as to the intermediate tenths. The visitors were instructed never to substitute another house for that marked, however difficult it proved to get information, or whatever the type of house.\n\nBowley (1913) continues that they ended up with information about 622 working-class households. And, having judged, based on the census that there were about 18,000 households in Reading, Bowley (1913) applies ‘[t]he multiplier twenty-one… to all the sample data to give estimates for the whole of Reading.’ Bowley (1913) explains that the reasonableness of the estimates depends ‘not on its proportion to the whole, but on its own magnitude, if the conditions of random sampling are secured, as it is believed they have been in this inquiry’. Bowley is, for instance, able to furnish information about the rent paid per week (Figure 8.2).\n\n\n\nFigure 8.2: Extract of the results that Bowley found about rent paid by the working-class in Reading, England\n\n\n\n\n8.2.2 Probabilistic sampling\nHaving identified a target population and a sampling frame, we need to distinguish between probability and non-probability sampling, which Neyman (1934) describes as ‘random sampling’ and ‘purposive selection’:\n\n‘Probability sampling’: Every unit in the sampling frame has some, known, chance of being sampled and the specific sample is obtained randomly based on these chances. Note that these chances do not necessarily need to be same for each unit.\n‘Non-probability sampling’: Units from the sampling frame are sampled based on convenience, quotas, judgement, or other non-random processes.\n\nOften the difference between probability and non-probability sampling is one of degree. For instance, we cannot often forcibly obtain data and so there is almost always an aspect of volunteering. Even when there are penalties for not providing data, such as the case for completing a census form in many countries, it is difficult for even a government to force people to fill it out completely or truthfully. One reason that the Randomized Control Trial revolution, discussed in Chapter @ref(hunt-data), was needed was due to a lack of probability sampling. The most important aspect to be clear about with probability sampling is the role of uncertainty. This allows us to make claims about the population, based on our sample, with known amounts of error. The trade-off is that probability sampling is often expensive and difficult.\nTo add some more specificity to our discussion, following Lohr (2019, 27) it may help to consider the numbers 1 to 100 and let us define that as our target population. With simple random sampling, every unit has the same chance of being included. In this case it is 20 per cent. We would expect to have around 20 units in our sample, or around 1 in 5 compared with our target population.\n\nlibrary(tidyverse)\n\nset.seed(853)\n\nillustrative_sampling <-\n  tibble(unit = 1:100,\n         simple_random_sampling = \n           sample(\n             x = c(\"Included\", \"Not included\"),\n             size = 100,\n             replace = TRUE,\n             prob = c(0.2, 0.8)\n             ))\n\nillustrative_sampling\n\n# A tibble: 100 × 2\n    unit simple_random_sampling\n   <int> <chr>                 \n 1     1 Not included          \n 2     2 Not included          \n 3     3 Not included          \n 4     4 Not included          \n 5     5 Not included          \n 6     6 Not included          \n 7     7 Not included          \n 8     8 Not included          \n 9     9 Not included          \n10    10 Not included          \n# … with 90 more rows\n\n\nWith systematic sampling, as was used by Bowley (1913), we proceed by selecting some value, say 5. We randomly pick a starting point in units 1 to 5, say 3. And we then include every fifth unit. That starting point is usually randomly selecting.\n\nset.seed(853)\n\nstarting_point <- sample(x = c(1:5), \n                         size = 1)\n\nillustrative_sampling <-\n  illustrative_sampling |>\n  mutate(systematic_sampling = \n           if_else(row_number() %in% seq.int(from = starting_point, \n                                             to = 100, \n                                             by = 5), \n                   \"Included\", \n                   \"Not included\")\n         )\n\nillustrative_sampling\n\n# A tibble: 100 × 3\n    unit simple_random_sampling systematic_sampling\n   <int> <chr>                  <chr>              \n 1     1 Not included           Included           \n 2     2 Not included           Not included       \n 3     3 Not included           Not included       \n 4     4 Not included           Not included       \n 5     5 Not included           Not included       \n 6     6 Not included           Included           \n 7     7 Not included           Not included       \n 8     8 Not included           Not included       \n 9     9 Not included           Not included       \n10    10 Not included           Not included       \n# … with 90 more rows\n\n\nWhen we consider our population, it will typically have some grouping. This may be as straight-forward as a country having states, provinces, counties, or statistical districts; a university having faculties and departments; and humans having age-groups. A stratified structure is one in which we can divide the population into mutually exclusive and collectively exhaustive sub-populations, or strata.\nWe use stratification to help with the efficiency of sampling or with the balance of the survey. For instance, the population of the US is around 335 million, with 40 million being in California, while Wyoming as around half a million. So even a survey of 10,000 responses would only expect to have 15 responses from Wyoming, which could make inference about Wyoming difficult. We could use stratification to ensure there are 200 responses from each of the 50 US states. We would use random sampling within each state to select the person about whom data will be gathered.\nIn our case, we will stratify our illustration, we will consider that our strata are the 10s, that is, 1 to 10 is one stratum, 11 to 20 is another, and so on. We will use simple random sampling within these strata to select two units.\n\nset.seed(853)\n\nselected_within_strata <-\n  illustrative_sampling |>\n  mutate(strata = (row_number() - 1) %/% 10) |>\n  group_by(strata) |>\n  slice_sample(n = 2) |>\n  pull(unit)\n\nillustrative_sampling <-\n  illustrative_sampling |>\n  mutate(\n    stratified_sampling = if_else(\n      row_number() %in% selected_within_strata,\n      \"Included\",\n      \"Not included\"\n    )\n  )\n\nillustrative_sampling\n\n# A tibble: 100 × 4\n    unit simple_random_sampling systematic_sampling stratified_sampling\n   <int> <chr>                  <chr>               <chr>              \n 1     1 Not included           Included            Included           \n 2     2 Not included           Not included        Not included       \n 3     3 Not included           Not included        Not included       \n 4     4 Not included           Not included        Not included       \n 5     5 Not included           Not included        Not included       \n 6     6 Not included           Included            Not included       \n 7     7 Not included           Not included        Not included       \n 8     8 Not included           Not included        Not included       \n 9     9 Not included           Not included        Included           \n10    10 Not included           Not included        Not included       \n# … with 90 more rows\n\n\nAnd finally, we can also take advantage of some clusters that may exist in our dataset. Like strata, clusters are collectively exhaustive and mutually exclusive. Our examples from earlier, of states, departments, and age-groups remain valid as clusters. However, it is our intentions toward these groups that is different. Specific, with cluster sampling, we do not intend to collect data from every cluster, whereas with stratified sampling we do. With stratified sampling we look at every stratum and conduct simple random sampling within each strata to select the sample. With cluster sampling we conduct simple random sampling to select clusters of interest. We can then either sample every unit in those selected clusters or use simple random sampling, within the selected clusters, to select units. That all said, this difference can become less clear in practice, especially ex post.\nIn our case, we will cluster our illustration again based on the 10s. We will use simple random sampling to select two clusters for which we will use the entire cluster.\n\nset.seed(853)\n\nselected_clusters <- \n  sample(x = c(0:9),\n         size = 2)\n\nillustrative_sampling <-\n  illustrative_sampling |>\n  mutate(cluster = (row_number() - 1) %/% 10, \n         cluster_sampling = if_else(\n           cluster %in% selected_clusters,\n           \"Included\",\n           \"Not included\"\n           )\n         ) |> \n  select(-cluster)\n\nillustrative_sampling\n\n# A tibble: 100 × 5\n    unit simple_random_sampl… systematic_samp… stratified_samp… cluster_sampling\n   <int> <chr>                <chr>            <chr>            <chr>           \n 1     1 Not included         Included         Included         Included        \n 2     2 Not included         Not included     Not included     Included        \n 3     3 Not included         Not included     Not included     Included        \n 4     4 Not included         Not included     Not included     Included        \n 5     5 Not included         Not included     Not included     Included        \n 6     6 Not included         Included         Not included     Included        \n 7     7 Not included         Not included     Not included     Included        \n 8     8 Not included         Not included     Not included     Included        \n 9     9 Not included         Not included     Included         Included        \n10    10 Not included         Not included     Not included     Included        \n# … with 90 more rows\n\n\nAt this point we can illustrate the differences between our approaches (Figure 8.3).\n\nnew_labels <- c(simple_random_sampling = \"Simple random sampling\", \n                systematic_sampling = \"Systematic sampling\",\n                stratified_sampling = \"Stratified sampling\",\n                cluster_sampling = \"Cluster sampling\")\n\nillustrative_sampling_long <- \n  illustrative_sampling |>\n  pivot_longer(\n    cols = c(\n      simple_random_sampling,\n      systematic_sampling,\n      stratified_sampling,\n      cluster_sampling),\n    names_to = \"sampling_method\",\n    values_to = \"in_sample\"\n  ) |>\n  mutate(sampling_method = factor(sampling_method,\n                                  levels = c(\"simple_random_sampling\",\n                                             \"systematic_sampling\",\n                                             \"stratified_sampling\",\n                                             \"cluster_sampling\"))\n         ) \n\nillustrative_sampling_long |>\n  filter(in_sample == \"Included\") |>\n  ggplot(aes(x = unit, y = in_sample)) +\n  geom_point() +\n  facet_wrap(vars(sampling_method),\n             dir = \"v\",\n             ncol = 1,\n             labeller = labeller(sampling_method = new_labels)\n             ) +\n  theme_minimal() +\n  labs(x = \"Unit\",\n       y = \"Is included in sample\") +\n  theme(axis.text.y = element_blank())\n\n\n\n\nFigure 8.3: Illustrative example of simple random sampling, systematic sampling, stratified sampling, and cluster sampling over the numbers from 1 to 100\n\n\n\n\nHaving established our sample, we typically want to use it to make claims about the population. Neyman (1934, 561) goes further and says that ‘[o]bviously the problem of the representative method is par excellence the problem of statistical estimation. We are interested in characteristics of a certain population, such \\(\\pi\\), which it is either impossible or at least very difficult to study in detail, and we try to estimate these characteristics basing our judgment on the sample.’\nIn particular, we would typically be interested to estimate a population mean and variance.\n\nScaling up can be used when we are interested in using a count from our sample to imply some total count for the population. We saw this in Bowley (1913) where the ratio of the number of households in the sample, compared with the number of households known from the census, is 21, and this information is used to scale up the sample.\nTo consider an example, perhaps we were interested in the sum of the numbers from 1 to 100. We know that our samples are of size 20, and so need to be scaled up five times (Table 8.1).\n\nillustrative_sampling_long |>\n  filter(in_sample == \"Included\") |>\n  group_by(sampling_method) |>\n  summarize(sum_from_sample = sum(unit)) |>\n  mutate(scaled_by_five = sum_from_sample * 5) |>\n  knitr::kable(\n    col.names = c(\"Sampling method\", \"Sum of sample\", \"Implied population sum\"),\n    format.args = list(big.mark = \",\")\n  )\n\n\n\nTable 8.1: Sum of the numbers in each sample, and implied sum of population\n\n\nSampling method\nSum of sample\nImplied population sum\n\n\n\n\nsimple_random_sampling\n840\n4,200\n\n\nsystematic_sampling\n970\n4,850\n\n\nstratified_sampling\n979\n4,895\n\n\ncluster_sampling\n910\n4,550\n\n\n\n\n\n\nThe actual sum of the population is 5,050. We can obtain this using a trick, attributed to Euler, who noticed that the sum of 1 to any number can be quickly obtained by finding the middle number and then multiplying that by one plus the number. So, in this case, it 50*101. Alternatively we can use R: sum(1:100).\nOur estimate of the population sum, based on the scaling, are especially revealing. The closest is stratified sample, closely followed by systematic sampling. Cluster sampling is a little over 10 per cent off, while simple random sampling is a little further away. To get close, it is important that our sampling method gets as many of the higher values as possible. And so stratified and systematic sampling, both of which ensured that we had unit from the larger numbers did particularly well. The performance of cluster and simple random sampling would depend on the particular clusters, and units, selected. In this case, stratified and systematic sampling ensured that our estimate of the sum of the population, would not be too far away from the actual population sum.\nThis approach has a long history. For instance, Adolphe Quetelet, the nineteenth century astronomer, mathematician, statistician, and sociologist proposed one. Stigler (1986, 163) describes how by 1826 Quetelet had become involved in the statistical bureau, and they were planning for a census. Quetelet argued that births and deaths were well known, but migration was not. He proposed an approach based on counts in specific geographies, which could then be scaled up to the whole country. The criticism of the plan focused on the difficulty of selecting appropriate geographies, which we saw also in our example of cluster sampling. The criticism was reasonable, and even today, some two hundred years later, something that we should keep front of mind, (Stigler 1986):\n\nHe [Quetelet] was acutely aware of the infinite number of factors that could affect the quantities he wished to measure, and he lacked the information that could tell him which were indeed important. He… was reluctant to group together as homogenous, data that he had reason to believe was not… To be aware of a myriad of potentially important factors, without knowing which are truly important and how their effect may be felt, is often to fear the worst’…. He [Quetelet] could not bring himself to treat large regions as homogeneous, [and so] he could not think of a single rate as applying to a large area\n\nWe are able to do this scaling up when we know the population total, but if we do not know that, or we have concerns around the precision of that approach then we may use a ratio estimator.\nRatio estimators also have a long history. For instance, in 1802 they were used by Pierre-Simon Laplace to estimate the total population of France, based on the ratio of the number of registered births, which was known throughout the country, to the number of inhabitants, which was only know for certain communes. He calculated this ratio for the three communes, and then scaled it, based on knowing the number of births across the whole country to produce an estimate of the population of France (Lohr 2019).\n\n\n\n\n\n\n\n\nIn particular, a ratio estimator of some population parameter is the ratio of two means. For instance, we may have some information on the number of hours that a toddler sleeps overnight, \\(x\\), and the number of hours their parents sleep overnight \\(y\\) over a 30 day period.\n\nset.seed(853)\n\nsleep <- \n  tibble(\n    toddler_sleep = sample(x = c(2:14), size = 30, replace = TRUE),\n    difference = sample(x = c(0:3), size = 30, replace = TRUE),\n    parent_sleep = toddler_sleep - difference\n  )\n\nsleep\n\n# A tibble: 30 × 3\n   toddler_sleep difference parent_sleep\n           <int>      <int>        <int>\n 1            10          1            9\n 2            11          0           11\n 3            14          2           12\n 4             2          2            0\n 5             6          1            5\n 6            14          2           12\n 7             3          0            3\n 8             5          2            3\n 9             4          3            1\n10             4          1            3\n# … with 20 more rows\n\n\nAnd the average of each is:\n\nsleep |> \n  summarize(toddler_sleep_average = mean(toddler_sleep),\n            parent_sleep_average = mean(parent_sleep))\n\n# A tibble: 1 × 2\n  toddler_sleep_average parent_sleep_average\n                  <dbl>                <dbl>\n1                  6.17                  4.9\n\n\nThen the ratio estimate of the proportion of sleep that a parent gets compared with their toddler is:\n\\[\\hat{B} = \\frac{\\bar{y}}{\\bar{x}} = \\frac{4.9}{6.16} \\approx 0.8\\]\n\n\n\nWhile acknowledging that it is a spectrum, much of statistics was developed based on probability sampling. But a considerable amount of modern sampling is done using non-probability sampling. A common approach is to use Facebook and other advertisements to recruit a panel of respondents in exchange for compensation. This panel is then the group that is sent various surveys as necessary. But think for a moment about the implications of this. For instance, what type of people are likely to respond to such an advertisement? Is the richest person in the world likely to respond? Are especially young or especially old people likely to respond? In some cases, it is possible to do a census. Nation-states typically do one every five to ten years. But there is a reason that it is only nation states that do them—they are expensive, time-consuming, and surprisingly, they are sometimes not as accurate as we may hope because of how general they need to be.\n\n\n8.2.3 Non-probability samples\nNon-probability samples have an important role to play because they are typically cheaper and quicker to obtain than probability samples. Further, as we have discussed, the difference between probability and non-probability samples is sometimes one of degree, rather than dichotomy. In any case, non-probability samples are legitimate and appropriate for some tasks provided one is clear about the trade-offs and ensure transparency (Baker et al. 2013).\nConvenience sampling involves gathering data from a sample that is easy to access. For instance, one often asks one’s friends and family to fill out a survey as a way of testing it before more wide-scale distribution. If instead we were to analyze such a sample, then we would likely be using convenience sampling.\nThe main issue with convenience sampling is that it is unlikely to be able to speak to much of a broader population than those who filled out the survey. There are also tricky ethical considerations, and typically a lack of anonymity which may further bias the results. On the other hand, it can be useful to cheaply get a quick sense of a situation while rolling out sampling approaches likely to be more broadly useful.\nQuota sampling occurs when we have strata, but we do not use random sampling within those strata to select the unit. For instance, if we again stratified the US based on state, but then instead of ensuring that everyone in Wyoming had the chance to be chosen for that stratum, just picked people at Jackson Hole. Again, there are some advantages to this approach, especially in terms of speed and cost, but the resulting sample is likely biased in various ways.\nAs the saying goes, birds of a feather flock together. And we can take advantage of that in our sampling. Although Handcock and Gile (2011) describe various uses before this, and it is notoriously difficult to define attribution in multidisciplinary work, snowball sampling is nicely defined by Goodman (1961). Following Goodman (1961), to conduct snowball sampling, we first draw a random sample from the sampling frame. Each of these is asked to name \\(k\\) others also in the sample population, but not in that initial draw, and these form the ‘first stage’. Each individual in the first stage is then similarly asked to name \\(k\\) others who are also in the sample population, but again not in the random draw or the first stage, and these form the ‘second stage’. We need to have specified the number of stages, \\(s\\), and also \\(k\\) ahead of time.\nRespondent-driven sampling was developed by Heckathorn (1997) to focus on hidden populations, which are those for which: 1) there is no sampling frame and 2) being known to be in the sampling population could have a negative effect. For instance, we could imagine various countries in which it would be difficult to sample from the gay population or those who have had abortions because it is illegal. Respondent-driven sampling differs from snowball sampling in two ways: 1) In addition to compensation for their own response, as is the case with snowball sampling, respondent-driven sampling typically also involves compensation for recruiting others. 2) Respondents are not asked to provide information about others to the investigator, but instead recruit them into the study. Selection into the sample occurs not from sampling frame, but instead from the networks of those already in the sample (Salganik and Heckathorn 2004).\nHaving established the foundations of sampling, which should remain front of mind, we turn to describe some approaches to gathering data. These will largely represent convenience samples."
  },
  {
    "objectID": "08-farm.html#censuses",
    "href": "08-farm.html#censuses",
    "title": "8  Farm data",
    "section": "8.3 Censuses",
    "text": "8.3 Censuses\nThere are a variety of sources of data that have been produced for the purposes of being used as datasets. One thinks here especially of censuses. Whitby (2020, 30–31) provides an enthralling overview, describing how the earliest censuses that we have written suggestions of are from China’s Yellow River valley, and that they were used for more than just purposes of taxation and conscription. Whitby (2020) also highlights the links between censuses and religion, quoting from Book of Luke ‘In those days Caesar Augustus issued a decree that a census should be taken of the entire Roman world’, which led to David and Mary travelling to Bethlehem. The\nTaxation was a substantial motivator for censuses. Jones (1953) describes how census records survive that ‘were probably engraved in the late third or early fourth century A.D., when Diocletian and his colleagues and successors are known to have been active in carrying out censuses to serve as the basis of their new system of taxation’. And detailed records of this sort have been abused. For instance, Luebke and Milton (1994) say how ‘(t)he Nazi regime gathered its information with two relatively conventional tools of modern administration: the national census and police registration’.\nAnother source of data deliberately put together to be a dataset include economic conditions such as unemployment, inflation, and GDP. Interestingly, Rockoff (2019) describes how these economic statistics were not actually developed by the federal government, even though federal governments typically eventually took over that role. Typically, these sources of data are put together by governments. They have the powers of the state behind them which enables them to be thorough in a way that other datasets cannot be, and similarly bring a specific perspective. That is not to say that census data are unimpeachable, and common errors include under- and over-enumeration, as well as misreporting [steckel1991quality].\nAnother, similarly, large and established source of data are from long-running large surveys. These are conducted on a regular basis, and while not usually directly conducted by the government, they are usually funded, one way or another, by the government. For instance, here we often think of electoral surveys, such as the Canadian Election Study, which has run in association with every federal election since 1965, and similarly the British Election Study which has been associated with every general election since 1964.\nFinally, there has been a large push toward open data in government. While the term has become contentious because of how it has occurred in practice, the underlying principle—that the government should make available the data that is has—is undeniable. In this chapter we cover these datasets, which we term ‘farmed data’. They are typically fairly nicely put together and the work of collecting, preparing and cleaning these datasets has typically been done. They are also, usually, conducted on a known release cycle. For instance, most developed countries release unemployment and inflation dataset on a monthly basis, GDP on a quarterly basis, and a census every five to ten years.\nWhile these datasets have always been useful, they were developed for a time when much analysis was conducted without the use of scripts and programming languages. A cottage industry of R package development has sprung up around making it easier to get these datasets into R. In this chapter we cover a few that are especially useful.\nIt is important to recognize that data are not neutral. Thinking clearly about who is included in the dataset, and who is systematically excluded, is critical. As Crawford (2021, 121) says:\n\nThe way data is understood, captured, classified, and named is fundamentally an act of world-making and containment…. The myth of data collection as a benevolent practice… has obscured its operations of power, protecting those who profit most while avoiding responsibility for its consequences.\n\nAt this point, it is worth briefly discussing the role of sex and gender in survey research, following Kennedy et al. (2020). Sex is based on biological attributes, while gender is socially constructed. We are likely interested in the effect of gender on our dependent variable. Moving away from a non-binary concept of gender, in terms of official statistics, is only something that has happened recently. As a researcher one of the problems of insisting on a binary is that, as Kennedy et al. (2020, 2) say ‘…when measuring gender with simply two categories, there is a failure to capture the unique experiences of those who do not identify as either male or female, or for those whose gender does not align with their sex classification.’. A researcher has a variety of ways of proceeding, and Kennedy et al. (2020) discuss these based on: ethics, accuracy, practicality, and flexibility. However, ‘there is no single good solution that can be applied to all situations. Instead, it is important to recognize that there is a compromise between ethical concerns, statistical concerns, and the most appropriate decision will be reflective of this’ [p. 16]. The most important consideration is to ensure appropriate ‘respect and consideration for the survey respondent’.\n\n8.3.1 Canada\nThe first census in Canada was conducted in 1666. There were 3,215 inhabitants that were counted, and the census asked about age, sex, marital status, and occupation (Statistics Canada 2017). In association with Confederation, in 1867 a decennial census was required so that political representatives could be allocated for the new Parliament. Regular censuses have occurred since then, the most recent in 2021.\nWe can explore some data on languages spoken in Canada from the 2016 Census using canlang (Timbers 2020). This package is not yet available on CRAN, and so we install it from GitHub, using devtools (Wickham, Hester, and Chang 2020).\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"ttimbers/canlang\")\n\nWe will start with the ‘can_lang’ dataset, which provides the number of Canadians who use that language for 214 languages.\n\nlibrary(tidyverse)\nlibrary(canlang)\n\ncan_lang\n\n# A tibble: 214 × 6\n   category          language mother_tongue most_at_home most_at_work lang_known\n   <chr>             <chr>            <dbl>        <dbl>        <dbl>      <dbl>\n 1 Aboriginal langu… Aborigi…           590          235           30        665\n 2 Non-Official & N… Afrikaa…         10260         4785           85      23415\n 3 Non-Official & N… Afro-As…          1150          445           10       2775\n 4 Non-Official & N… Akan (T…         13460         5985           25      22150\n 5 Non-Official & N… Albanian         26895        13135          345      31930\n 6 Aboriginal langu… Algonqu…            45           10            0        120\n 7 Aboriginal langu… Algonqu…          1260          370           40       2480\n 8 Non-Official & N… America…          2685         3020         1145      21930\n 9 Non-Official & N… Amharic          22465        12785          200      33670\n10 Non-Official & N… Arabic          419890       223535         5585     629055\n# … with 204 more rows\n\n\nWe can quickly see the top-10 most common languages to have as mother tongue.\n\ncan_lang |>\n  slice_max(mother_tongue, n = 10) |>\n  select(language, mother_tongue)\n\n# A tibble: 10 × 2\n   language                     mother_tongue\n   <chr>                                <dbl>\n 1 English                           19460850\n 2 French                             7166700\n 3 Mandarin                            592040\n 4 Cantonese                           565270\n 5 Punjabi (Panjabi)                   501680\n 6 Spanish                             458850\n 7 Tagalog (Pilipino, Filipino)        431385\n 8 Arabic                              419890\n 9 German                              384040\n10 Italian                             375635\n\n\nWe could combine two datasets together ‘region_lang’ and ‘region_data’, to see if the five most-common languages differ between the largest region, Toronto, and the smallest, Belleville.\n\nregion_lang |>\n  left_join(region_data, by = \"region\") |>\n  slice_max(c(population)) |>\n  slice_max(mother_tongue, n = 5) |>\n  select(region, language, mother_tongue, population) |>\n  mutate(prop = mother_tongue / population)\n\n# A tibble: 5 × 5\n  region  language          mother_tongue population   prop\n  <chr>   <chr>                     <dbl>      <dbl>  <dbl>\n1 Toronto English                 3061820    5928040 0.516 \n2 Toronto Cantonese                247710    5928040 0.0418\n3 Toronto Mandarin                 227085    5928040 0.0383\n4 Toronto Punjabi (Panjabi)        171225    5928040 0.0289\n5 Toronto Italian                  151415    5928040 0.0255\n\nregion_lang |>\n  left_join(region_data, by = \"region\") |>\n  slice_min(c(population)) |>\n  slice_max(mother_tongue, n = 5) |>\n  select(region, language, mother_tongue, population) |>\n  mutate(prop = mother_tongue / population)\n\n# A tibble: 5 × 5\n  region     language mother_tongue population    prop\n  <chr>      <chr>            <dbl>      <dbl>   <dbl>\n1 Belleville English          93655     103472 0.905  \n2 Belleville French            2675     103472 0.0259 \n3 Belleville German             635     103472 0.00614\n4 Belleville Dutch              600     103472 0.00580\n5 Belleville Spanish            350     103472 0.00338\n\n\nWe can see a considerable difference between the proportions, with a little over 50 per cent of those in Toronto having English as their mother tongue, while that is the case for around 90 per cent of those in Belleville.\nIn general, data from Canadian censuses are not as easily available as in other countries. Statistics Canada, which is the government agency that is responsible for the census and other official statistics freely provides a Individuals File from the 2016 census as a Public Use Microdata File (PUMF), but only in response to a request. And while it is a 2.7 per cent sample from the 2016 census, this PUMF provides limited detail.\nAnother way to access data from the Canadian census is to use cancensus (von Bergmann, Shkolnik, and Jacobs 2021). This package can be installed from CRAN. It requires an API key, which can be requested by creating an account and then going to ‘edit profile’. The package has a helper function set_api_key(\"ADD_YOUR_API_KEY_HERE\", install = TRUE) that makes it easier to add the API key to an ‘.Renviron’ file, in the same way that we did in Chapter @ref(gather-data).\nWe can use get_census() to get census data. We need to specify a census of interest, and a variety of other arguments. For instance, we could get data from the 2016 census about Ontario, which is the largest Canadian province by population.\n\nlibrary(tidyverse)\nlibrary(cancensus)\n\nontario_population <- \n  get_census(dataset = \"CA16\",\n             level = \"Regions\",\n             vectors = \"v_CA16_1\", \n             regions = list(PR=c('35')\n                            )\n             )\n\n\nDownloading: 120 B     \nDownloading: 120 B     \nDownloading: 120 B     \nDownloading: 120 B     \nDownloading: 120 B     \nDownloading: 120 B     \n\nontario_population\n\n# A tibble: 0 × 0\n\n\nData from the 1996, 2001, 2006, 2011, and 2016 censuses are available, and list_census_datasets() provides the metadata that we need to provide to get_census() to access these. Data are available based on a variety of regions, and list_census_regions() provides the metadata that we need. And finally, list_census_vectors() provides the metadata about the variables that available.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.3.2 USA\nThe requirement for a US Census is included in the US Constitution, and decent, though clunky, access is provided. But the US is in the envious situation where there is usually a better approach than going through the national statistical agency of IPUMS. IPUMS provides access to a wide range of datasets, including international census microdata. In the specific case of the US, the American Community Survey (ACS) is a survey that is comparable to the questions asked on many censuses, but it is that are available on an annual basis, compared with a census which could be quite out-of-date by the time the data are available. It ends up with millions of responses each year. Although the ACS is smaller than a census, the advantage is that it is available on a more timely basis. We will access the ACS through IPUMS.\nGo to IPUMS, then ‘IPUMS USA’, and select ‘get data’ (Figure 8.4).\n\n\n\nFigure 8.4: The IPUMS homepage, with IPUMS USA shown in the top left box\n\n\nWe are interested in a sample, so go to ‘SELECT SAMPLE’, and un-select ‘Default sample from each year’ and instead select ‘2019 ACS’ and then ‘SUBMIT SAMPLE SELECTIONS’ (Figure 8.5).\n\n\n\nFigure 8.5: Selecting a sample from IPUMS USA and specifying interest in the 2019 ACS\n\n\n\n\n\n\n\nWe might be interested in data based on state. So we would begin by looking at ‘HOUSEHOLD’ variables and selecting ‘GEOGRAPHIC’ (Figure 8.6).\n\n\n\nFigure 8.6: Specifying that we are interested in the state\n\n\nWe add ‘STATEICP’ to our ‘cart’ by clicking the plus, which will then turn into a tick (Figure 8.7).\n\n\n\nFigure 8.7: Adding STATEICP to our cart\n\n\nWe might then be interested in data on a ‘PERSON’ basis, for instance, ‘DEMOGRAPHIC’ variables such as ‘AGE’, which we should add to our cart. Still on a ‘PERSON’ basis, we might be interested in ‘INCOME’, for instance, ‘Total personal income’ ‘INCTOT’ and we could add that to our cart (Figure 8.8).\n\n\n\nFigure 8.8: Adding additional demographic variables that are available on an individual basis\n\n\nWhen we are done, we can ‘VIEW CART’, and then ‘CREATE DATA EXTRACT’ (Figure 8.9). At this point there are two aspects that we likely want to change:\n\nChange the ‘DATA FORMAT’ from dat to csv (Figure 8.10).\nCustomize the sample size as we likely do not need three million responses, and could just change it to, say, 500,000 (Figure 8.11).\n\n\n\n\nFigure 8.9: Beginning the checkout process\n\n\n\n\n\nFigure 8.10: Specifying that we are interested in CSV files\n\n\n\n\n\nFigure 8.11: Reducing the sample size from three million responses to half a million\n\n\nFinally, we want to include a descriptive name for the extract, for instance, ‘2022-02-06: Income based on state and age’, which specifies the date we made the extract and what is in the extract. After that we can ‘SUBMIT EXTRACT’.\nWe will be asked to log in or create an account, and after doing that will be able to submit the request. IPUMS will email when the extract is available, after which we can download it and read it into R in the usual way. It is critical that we cite this dataset when we use it (Ruggles et al. 2021).\n\nlibrary(tidyverse)\nipums_extract <- read_csv(\"usa_00010.csv\")\n\nipums_extract\n\n\n\n# A tibble: 6 × 4\n   YEAR STATEICP   AGE INCTOT\n  <dbl>    <dbl> <dbl>  <dbl>\n1  2019       41    39   9000\n2  2019       41    35   9300\n3  2019       41    39  60000\n4  2019       41    32  14400\n5  2019       41    21      0\n6  2019       41    61  11100\n\n\nIncredibly, full count, that is the entire census, data are available through IPUMS for the US censuses conducted on: 1850, 1860, 1870, 1880, 1900, 1910, 1920, 1930, and 1940. Most of the 1890 census records were destroyed due to a fire in 1921. 1 per cent samples are available for these years, and through to 1990. And then ACS data are available from 2000."
  },
  {
    "objectID": "08-farm.html#exercises-and-tutorial",
    "href": "08-farm.html#exercises-and-tutorial",
    "title": "8  Farm data",
    "section": "8.4 Exercises and tutorial",
    "text": "8.4 Exercises and tutorial\n\n8.4.1 Exercises\n\nPlease identify three other sources of data that you are interested in and describe where are they available (please include a link or code)?\nPlease focus on one of those sources. What steps do you have to go through in order to get a dataset that can be analyzed in R?\nLet us say you take a job at RBC (a Canadian bank) and they already have some quantitative data for you to use. What are some questions that you should explore when deciding whether that data will be useful to you?\nWrite three points (you are welcome to use dot points) about why government data may be especially useful?\nPlease pick a government of interest and find their inflation statistics. To what extent do you know about how these data were gathered?\nWith reference to Chen et al. (2019) and Martinez (2019) to what extent do you think we can trust government statistics? Please mention at least three governments in your answer.\nThe 2021 census in Canada asked, firstly, ‘What was this person’s sex at birth? Sex refers to sex assigned at birth. Male/Female’, and then ‘What is this person’s gender? Refers to current gender which may be different from sex assigned at birth and may be different from what is indicated on legal documents. Male/Female/Or please specify this person’s gender (space for a typed or handwritten answer)’. With reference to Statistics Canada (2020), please discuss the extent to which you think this is an appropriate way for census to have proceeded. You are welcome to discuss the case of a different country if you are more familiar with that.\nPretend that we have conducted a survey of everyone in Canada, where we asked for age, sex, and gender. Your friend claims that there is no need to worry about uncertainty ‘because we have the whole population’. Is your friend right or wrong, and why?\n\n\n\n8.4.2 Tutorial\nUse IPUMS to access the ACS. Download some data that are of interest and write a two-to-three page paper analyzing it.\n\n\n\n\n\nBaker, Reg, J. Michael Brick, Nancy A. Bates, Mike Battaglia, Mick P. Couper, Jill A. Dever, Krista J. Gile, and Roger Tourangeau. 2013. “Summary Report of the AAPOR Task Force on Non-probability Sampling.” Journal of Survey Statistics and Methodology 1 (2): 90–143. https://doi.org/10.1093/jssam/smt008.\n\n\nBowley, Arthur Lyon. 1913. “Working-Class Households in Reading.” Journal of the Royal Statistical Society 76 (7): 672–701.\n\n\nChen, Wei, Xilu Chen, Chang-Tai Hsieh, and Zheng Song. 2019. “A Forensic Examination of China’s National Accounts.” National Bureau of Economic Research.\n\n\nCrawford, Kate. 2021. Atlas of AI. Yale University Press.\n\n\nGoodman, Leo A. 1961. “Snowball Sampling.” The Annals of Mathematical Statistics, 148–70.\n\n\nHandcock, Mark S, and Krista J Gile. 2011. “Comment: On the Concept of Snowball Sampling.” Sociological Methodology 41 (1): 367–71.\n\n\nHeckathorn, Douglas D. 1997. “Respondent-Driven Sampling: A New Approach to the Study of Hidden Populations.” Social Problems 44 (2): 174–99.\n\n\nJones, Arnold HM. 1953. “Census Records of the Later Roman Empire.” The Journal of Roman Studies 43 (1-2): 49–64.\n\n\nKennedy, Lauren, Katharine Khanna, Daniel Simpson, and Andrew Gelman. 2020. “Using Sex and Gender in Survey Adjustment.” https://arxiv.org/abs/2009.14401.\n\n\nLohr, Sharon L. 2019. Sampling: Design and Analysis. CRC Press.\n\n\nLuebke, David Martin, and Sybil Milton. 1994. “Locating the Victim: An Overview of Census-Taking, Tabulation Technology, and Persecution in Nazi Germany.” IEEE Annals of the History of Computing 16 (3): 25.\n\n\nMartinez, Luis R. 2019. “How Much Should We Trust the Dictator’s GDP Growth Estimates?” Available at SSRN 3093296.\n\n\nNeyman, Jerzy. 1934. “On the Two Different Aspects of the Representative Method: The Method of Stratified Sampling and the Method of Purposive Selection.” Journal of the Royal Statistical Society 97 (4): 558–625.\n\n\nRobinson, David. 2021. Gutenbergr: Download and Process Public Domain Works from Project Gutenberg. https://CRAN.R-project.org/package=gutenbergr.\n\n\nRockoff, Hugh. 2019. “On the Controversies Behind the Origins of the Federal Economic Statistics.” Journal of Economic Perspectives 33 (1): 147–64.\n\n\nRuggles, Steven, Sarah Flood, Sophia Foster, Ronald Goeken, Jose Pacas, Megan Schouweiler, and Matthew Sobek. 2021. “IPUMS USA: Version 11.0.” Minneapolis, MN: IPUMS. https://doi.org/10.18128/D010.V11.0.\n\n\nSalganik, Matthew J, and Douglas D Heckathorn. 2004. “Sampling and Estimation in Hidden Populations Using Respondent-Driven Sampling.” Sociological Methodology 34 (1): 193–240.\n\n\nSomers, James. 2017. “Torching the Modern-Day Library of Alexandria.” The Atlantic 20.\n\n\nStatistics Canada. 2017. “Guide to the Census of Population, 2016.” Statistics Canada. https://www12.statcan.gc.ca/census-recensement/2016/ref/98-304/98-304-x2016001-eng.pdf.\n\n\n———. 2020. “Sex at Birth and Gender: Technical Report on Changes for the 2021 Census.” Statistics Canada. https://www12.statcan.gc.ca/census-recensement/2021/ref/98-20-0002/982000022020002-eng.pdf.\n\n\nStigler, Stephen. 1986. The History of Statistics. Harvard University Press.\n\n\nTimbers, Tiffany. 2020. Canlang: Canadian Census Language Data. https://ttimbers.github.io/canlang/.\n\n\nvon Bergmann, Jens, Dmitry Shkolnik, and Aaron Jacobs. 2021. Cancensus: R Package to Access, Retrieve, and Work with Canadian Census Data and Geography. https://mountainmath.github.io/cancensus/.\n\n\nWhitby, Andrew. 2020. The Sum of the People. Basic Books.\n\n\nWhitelaw, James. 1905. An Essay on the Population of Dublin. Being the Result of an Actual Survey Taken in 1798, with Great Care and Precision, and Arranged in a Manner Entirely New. Graisberry; Campbell.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Jim Hester, and Winston Chang. 2020. Devtools: Tools to Make Developing r Packages Easier. https://CRAN.R-project.org/package=devtools.\n\n\nWu, Changbao, and Mary E Thompson. 2020. Sampling Theory and Practice. Springer."
  },
  {
    "objectID": "09-gather.html",
    "href": "09-gather.html",
    "title": "9  Gather data",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "09-gather.html#introduction",
    "href": "09-gather.html#introduction",
    "title": "9  Gather data",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nIn this chapter we first go through a variety of approaches for gathering data, including the use of APIs and semi-structured data, such as JSON and XML, web scraping, converting PDFs, and using optical character recognition, especially to obtain text data."
  },
  {
    "objectID": "09-gather.html#apis",
    "href": "09-gather.html#apis",
    "title": "9  Gather data",
    "section": "9.2 APIs",
    "text": "9.2 APIs\nIn everyday language, and for our purposes, an Application Programming Interface (API) is a situation in which someone has set up specific files on their computer such that we can follow their instructions to get them. For instance, when we use a gif on Slack, Slack asks Giphy’s server for the appropriate gif, Giphy’s server gives that gif to Slack and then Slack inserts it into your chat. The way in which Slack and Giphy interact is determined by Giphy’s API. More strictly, an API is just an application that runs on a server that we access using the HTTP protocol.\nWe focus on using APIs for gathering data. And so, with that focus, an API is a website that is set-up for another computer to be able to access, rather than a person. For instance, we could go to Google Maps: https://www.google.com/maps. And we could then scroll and click and drag to center the map on Canberra, Australia. Or we could paste this into the browser: https://www.google.com/maps/@-35.2812958,149.1248113,16z. We just used the Google Maps API, and the result should be a map similar to Figure 9.1).\n\n\n\nFigure 9.1: Example of Google Maps, as at 29 January 2022\n\n\nThe advantage of using an API is that the data provider specifies exactly the data that they are willing to provide, and the terms under which they will provide it. These terms may include aspects such as rate limits (i.e. how often we can ask for data), and what we can do with the data, for instance, we might not be allowed to use it for commercial purposes, or to republish it. Additionally, because the API is being provided specifically for us to use it, it is less likely to be subject to unexpected changes or legal issues. Because of this it is ethically and legally clear that when an API is available we should try to use it rather than web scraping.\nWe will now go through a few case studies of using APIs. In the first we deal directly with an API using httr (Wickham 2019b). In the second we access data from Twitter using rtweet (Kearney 2019). And in the third we access data from Spotify using spotifyr (Thompson et al. 2020). Developing comfort with gathering data through APIs enables access to exciting datasets. For instance, Wong (2020) use the Facebook Political Ad API to gather all 218,100 of the Trump 2020 campaign ads to better understand the campaign.\n\n9.2.1 Case study: Gathering data from arXiv, NASA, and Dataverse\nWe use GET() from httr (Wickham 2019b) to obtain data from an API directly. This will try to get some specific data and the main argument is ‘url’. In a way, this is very similar to the earlier Google Maps example. In that example, the specific information that we were interested in was a map.\nIn this case study we will use an API provided by arXiv: https://arxiv.org. arXiv is an online repository for academic papers before they go through peer-review, and these are typically referred to as ‘pre-prints’. After installing and loading httr, we use GET() to ask arXiv to obtain some information about the pre-print of Alexander and Alexander (2021).\n\nlibrary(httr)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ tidyr   1.2.0     ✓ stringr 1.4.0\n✓ readr   2.1.1     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(xml2)\n\narxiv <-\n  GET(\"http://export.arxiv.org/api/query?id_list=2111.09299\")\n\nstatus_code(arxiv)\n\n[1] 200\n\n\nWe can use status_code() to check whether we received an error from the server. And assuming we received something back from the server, we can use content() to display the information. In this case we have received XML formatted data, which we can read using read_xml() from xml2 (Wickham, Hester, and Ooms 2021). XML is a semi-formatted structure, and it can be useful to start by having a look at it using html_structure().\n\ncontent(arxiv) |>\n  read_xml() |>\n  html_structure()\n\n<feed [xmlns]>\n  <link [href, rel, type]>\n  <title [type]>\n    {text}\n  <id>\n    {text}\n  <updated>\n    {text}\n  <totalResults [xmlns:opensearch]>\n    {text}\n  <startIndex [xmlns:opensearch]>\n    {text}\n  <itemsPerPage [xmlns:opensearch]>\n    {text}\n  <entry>\n    <id>\n      {text}\n    <updated>\n      {text}\n    <published>\n      {text}\n    <title>\n      {text}\n    <summary>\n      {text}\n    <author>\n      <name>\n        {text}\n    <author>\n      <name>\n        {text}\n    <comment [xmlns:arxiv]>\n      {text}\n    <link [href, rel, type]>\n    <link [title, href, rel, type]>\n    <primary_category [term, scheme, xmlns:arxiv]>\n    <category [term, scheme]>\n\n\nOr we might be interested to create a dataset based on extracting various aspects of this XML tree. For instance, we might be interested to look at the ‘entry’, which is the eighth item, and in particular to obtain the title and the URL, which are the fourth and ninth items, respectively, within entry.\n\ndata_from_arxiv <-\n  tibble(\n    title = content(arxiv) |>\n      read_xml() |>\n      xml_child(search = 8) |>\n      xml_child(search = 4) |>\n      xml_text(),\n    link = content(arxiv) |>\n      read_xml() |>\n      xml_child(search = 8) |>\n      xml_child(search = 9) |>\n      xml_attr(\"href\")\n  )\ndata_from_arxiv\n\n# A tibble: 1 × 2\n  title                                                                    link \n  <chr>                                                                    <chr>\n1 \"The Increased Effect of Elections and Changing Prime Ministers on Topi… http…\n\n\nEach day NASA provides the Astronomy Picture of the Day (APOD) through its APOD API. We can again use GET() to obtain the URL for the photo on particular dates and then display it.\n\nNASA_APOD_20211226 <-\n  GET(\"https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY&date=2021-12-26\")\n\nNASA_APOD_20190719 <-\n  GET(\"https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY&date=2019-07-19\")\n\nExamining the returned data using content(), we can see that we are provided with various fields, such as date, title, explanation, and a URL. And we can provide that URL to include_graphics() from knitr to display it (Figure 9.2)).\n\ncontent(NASA_APOD_20211226)$date\n\n[1] \"2021-12-26\"\n\ncontent(NASA_APOD_20211226)$title\n\n[1] \"James Webb Space Telescope over Earth\"\n\ncontent(NASA_APOD_20211226)$explanation\n\n[1] \"There's a big new telescope in space. This one, the James Webb Space Telescope (JWST), not only has a mirror over five times larger than Hubble's in area, but can see better in infrared light. The featured picture shows JWST high above the Earth just after being released by the upper stage of an Ariane V rocket, launched yesterday from French Guiana. Over the next month, JWST will move out near the Sun-Earth L2 point where it will co-orbit the Sun with the Earth. During this time and for the next five months, JWST will unravel its segmented mirror and an array of sophisticated scientific instruments -- and test them. If all goes well, JWST will start examining galaxies across the universe and planets orbiting stars across our Milky Way Galaxy in the summer of 2022.   APOD Gallery: Webb Space Telescope Launch\"\n\ncontent(NASA_APOD_20211226)$url\n\n[1] \"https://apod.nasa.gov/apod/image/2112/JwstLaunch_Arianespace_1080.jpg\"\n\ncontent(NASA_APOD_20190719)$date\n\n[1] \"2019-07-19\"\n\ncontent(NASA_APOD_20190719)$title\n\n[1] \"Tranquility Base Panorama\"\n\ncontent(NASA_APOD_20190719)$explanation\n\n[1] \"On July 20, 1969 the Apollo 11 lunar module Eagle safely touched down on the Moon. It landed near the southwestern corner of the Moon's Mare Tranquillitatis at a landing site dubbed Tranquility Base. This panoramic view of Tranquility Base was constructed from the historic photos taken from the lunar surface. On the far left astronaut Neil Armstrong casts a long shadow with Sun is at his back and the Eagle resting about 60 meters away ( AS11-40-5961). He stands near the rim of 30 meter-diameter Little West crater seen here to the right ( AS11-40-5954). Also visible in the foreground is the top of the camera intended for taking stereo close-ups of the lunar surface.\"\n\ncontent(NASA_APOD_20190719)$url\n\n[1] \"https://apod.nasa.gov/apod/image/1907/apollo11TranquilitybasePan600h.jpg\"\n\n\n\n\n\nFigure 9.2: Photo of the James Webb Space Telescope over Earth and another of Tranquility Base obtained from the NASA APOD API\n\n\nFinally, another common API response in semi-structured form is JSON. We can parse JSON with jsonlite (Ooms 2014). A Dataverse is a web application that makes it easier to share dataset. We can use an API go query a demonstration dataverse. For instance we might be interested in datasets related to politics.\n\nlibrary(jsonlite)\n\npolitics_datasets <- fromJSON(\"https://demo.dataverse.org/api/search?q=politics\")\n\nWe can also look at the dataset using View(politics_datasets), which allows us to expand the tree based on what we are interested in and even get the code that we need to focus on different aspects by hovering on the item and then clicking the icon with the green arrow (Figure 9.3)).\n\n\n\nFigure 9.3: Example of hovering over an JSON element, ‘items’, where the icon with a green arrow can be clicked on to get the code that would focus on that element\n\n\nThis tells us how to obtain the dataset of interest.\n\nas_tibble(politics_datasets[[\"data\"]][[\"items\"]])\n\n# A tibble: 1 × 6\n  name                    type      url      identifier description published_at\n  <chr>                   <chr>     <chr>    <chr>      <chr>       <chr>       \n1 China Archive Dataverse dataverse https:/… china-arc… Introducti… 2016-12-09T…\n\n\n\n\n9.2.2 Case study: Gathering data from Twitter\nTwitter is a rich source of text and other data. The Twitter API is the way in which Twitter asks that we gather these data. And rtweet (Kearney 2019) is built around this API and allows us to interact with it in ways that are similar to using any other R package. Initially, we can use the Twitter API with just a regular Twitter account.\nBegin by installing and loading rtweet and tidyverse. We then need to authorize rtweet and we start that process by calling a function from the package, for instance get_favorites() which will return a tibble of a user’s favorites. This will open a browser, and we then log into a regular Twitter account (Figure 9.4)).\n\nlibrary(rtweet)\nlibrary(tidyverse)\n\n\nget_favorites(user = \"RohanAlexander\")\n\n\n\n\nFigure 9.4: rtweet authorisation page\n\n\nOnce the application is authorized, then we can use get_favorites() to actually get the favorites of a user and save them.\n\nrohans_favorites <- get_favorites(\"RohanAlexander\")\n\nsaveRDS(rohans_favorites, \"rohans_favorites.rds\")\n\nWe could then look at some recent favorites, keeping in mind that they may be different depending on when they are being accessed.\n\nrohans_favorites |> \n  arrange(desc(created_at)) |> \n  slice(1:10) |> \n  select(screen_name, text)\n\n# A tibble: 10 × 2\n   screen_name text                                                             \n   <chr>       <chr>                                                            \n 1 EconAndrew  \"How much better are the investment opportunities available to t…\n 2 simonpcouch \"There's a new release of #rstats broom up on CRAN as of last ni…\n 3 MineDogucu  \"🚨 New manuscript🚨\\n📕 Content and Computing Outline of Two Un…\n 4 reid_nancy  \"Latest issue. From the intro: \\\"... it has been a great privile…\n 5 tjmahr      \"bathing is good, folks\"                                         \n 6 andrewheiss \"finished hand washing that load in the bathtub and am now the w…\n 7 monkmanmh   \"@CMastication https://t.co/3Eh0mLy44v\"                          \n 8 eplusgg     \"Stares from Ontario https://t.co/swzYhaptF9\"                    \n 9 ryancbriggs \"Same. https://t.co/C9pNpXO0F9\"                                  \n10 flynnpolsci \"I’m not great at coming up with assignments for intro courses b…\n\n\nWe can use search_tweets() to search for tweets about a particular topic. For instance, we could look at tweets using a hashtag commonly associated with R: ‘#rstats’.\n\nrstats_tweets <- search_tweets(\n  q = \"#rstats\",\n  include_rts = FALSE\n)\n\nsaveRDS(rstats_tweets, \"rstats_tweets.rds\")\n\n\nrstats_tweets |> \n  select(screen_name, text) |> \n  head()\n\n# A tibble: 6 × 2\n  screen_name     text                                                          \n  <chr>           <chr>                                                         \n1 SuccessAnalytiX \"The Science of Success \\n\\nhttps://t.co/xLM2OrqHBd\\n\\n#BigDa…\n2 babycoin_dev    \"BabyCoin (BABY)\\n\\nGUI wallet v2.05 =&gt; https://t.co/CFNtp…\n3 rstatsdata      \"#rdata #rstats: Yield of 6 barley varieties at 18 locations …\n4 PDH_SciTechNews \"#Coding Arm Puts Security Architecture to the Test With New …\n5 PDH_SciTechNews \"#Coding Network Engineer: Skills, Roles &amp; Responsibiliti…\n6 PDH_SciTechNews \"#Coding CockroachDB Strengthens Change Data Capture - iProgr…\n\n\nOther useful functions that can be used include get_friends() to get all the accounts that a user follows, and get_timelines() to get a user’s recent tweets. Registering as a developer enables access to more API functionality.\nWhen using APIs, even when they are wrapped in an R package, in this case rtweet, it is important to read the terms under which access is provided. The Twitter API docs are surprisingly readable, and the developer policy is especially clear: https://developer.twitter.com/en/developer-terms/policy. To see how easy it is to violate the terms under which an API provider makes data available, consider that we saved the tweets that we downloaded. If we were to push these to GitHub, then it is possible that we may have accidentally stored sensitive information if there happened to be some in the tweets. Twitter is also explicit about asking those that use their API to be especially careful about sensitive information and not matching Twitter users with off-Twitter folks. Again, the documentation around these restricted uses is clear and readable: https://developer.twitter.com/en/developer-terms/more-on-restricted-use-cases.\n\n\n9.2.3 Case study: Gathering data from Spotify\nFor the final case study, we will use spotifyr (Thompson et al. 2020), which is a wrapper around the Spotify API. Install install.packages('spotifyr') and load the package.\n\nlibrary(spotifyr)\n\nTo access the Spotify API, we need a Spotify Developer Account: https://developer.spotify.com/dashboard/. This will require logging in with a Spotify account and then accepting the Developer Terms (Figure 9.5).\n\n\n\nFigure 9.5: Spotify Developer Account Terms agreement page\n\n\nContinuing with the registration process, in our case, we ‘do not know’ what we are building and so Spotify requires us to use a non-commercial agreement. To use the Spotify API we need a ‘Client ID’ and a ‘Client Secret’. These are things that we want to keep to ourselves because anyone with the details could use our developer account as though they were us. One way to keep these details secret with a minimum of hassle is to keep them in our ‘System Environment’. In this way, when we push to GitHub they should not be included. (We followed this process without explanation in Chapter @ref(interactive-communication) when we used mapdeck.) We will use usethis (Wickham and Bryan 2020) to modify our System Environment. In particular, there is a file called ‘.Renviron’ which we will open using edit_r_environ() and add our ‘Client ID’ and ‘Client Secret’ to.\n\nlibrary(usethis)\n\nedit_r_environ()\n\nWhen we run edit_r_environ(), our ‘.Renviron’ file will open and we can add our ‘Spotify Client ID’ and ‘Client Secret’. It is important to use the same names, because spotifyr will look in our environment for keys with those specific names.\n\nSPOTIFY_CLIENT_ID = 'PUT_YOUR_CLIENT_ID_HERE'\nSPOTIFY_CLIENT_SECRET = 'PUT_YOUR_SECRET_HERE'\n\nSave the ‘.Renviron’ file, and then restart R (‘Session’ -> ‘Restart R’). We can now use our ‘Spotify Client ID’ and ‘Client Secret’ as needed. And functions that require those details as arguments will work without them being explicitly specified again. We will get and save some information about Radiohead, the English rock band, using get_artist_audio_features(). One of the required arguments is authorization, but as that is set, by default, to look at the ‘.Renviron’ file, we do not need to specify it here.\n\nradiohead <- get_artist_audio_features('radiohead')\nsaveRDS(radiohead, \"radiohead.rds\")\n\n\nradiohead <- readRDS(\"radiohead.rds\")\n\nThere is a variety of information available based on songs. We might be interested to see whether their songs are getting longer over time (Figure 9.6)).\n\nradiohead |> \n  select(artist_name, track_name, album_name) |> \n  head()\n\n  artist_name                    track_name   album_name\n1   Radiohead Everything In Its Right Place KID A MNESIA\n2   Radiohead                         Kid A KID A MNESIA\n3   Radiohead           The National Anthem KID A MNESIA\n4   Radiohead   How to Disappear Completely KID A MNESIA\n5   Radiohead                   Treefingers KID A MNESIA\n6   Radiohead                    Optimistic KID A MNESIA\n\n\n\nlibrary(lubridate)\n\nradiohead |> \n  mutate(album_release_date = ymd(album_release_date)) |> \n  ggplot(aes(x = album_release_date, y = duration_ms)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Album release date\",\n       y = \"Duration of song (ms)\"\n       ) \n\n\n\n\nFigure 9.6: Length of each Radiohead song, over time, as gathered from Spotify\n\n\n\n\nOne interesting variable provided by Spotify about each song is ‘valence’. The Spotify documentation describe this as a measure between 0 and 1 that signals the ‘the musical positiveness’ of the track with higher values being more positive. Further details are available at the documentation: https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features. We might be interested to compare valence over time between a few artists, for instance, the American rock band The National, and the American singer Taylor Swift.\nFirst, we need to gather the data.\n\ntaylor_swift <- get_artist_audio_features('taylor swift')\nthe_national <- get_artist_audio_features('the national')\n\nsaveRDS(taylor_swift, \"taylor_swift.rds\")\nsaveRDS(the_national, \"the_national.rds\")\n\nThen we can bring them together and make the graph (Figure 9.7)). This appears to show that while Taylor Swift and Radiohead have largely maintained their level of valence overtime, The National has decreased theirs.\n\nthree_artists <-\n  rbind(taylor_swift, the_national, radiohead) |>\n  select(artist_name, album_release_date, valence) |>\n  mutate(album_release_date = ymd(album_release_date))\n\nthree_artists |>\n  ggplot(aes(x = album_release_date,\n             y = valence,\n             color = artist_name)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth() +\n  theme_minimal() +\n  labs(x = \"Album release date\",\n       y = \"Valence\",\n       color = \"Artist\") +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 9.7: Comparing valence, over time, for Radiohead, Taylor Swift, and The National\n\n\n\n\nHow amazing that we live in a world that all that information is available with very little effort or cost. And having gathered the data, there is a lot that could be done. For instance, Pavlik (2019) uses an expanded dataset to classify musical genres and The Economist (2022) looks at how language is associated with music streaming on Spotify. Our ability to gather such data enables us to answer questions that had to be considered experimentally in the past, for instance Salganik, Dodds, and Watts (2006) had to use experimental data rather than the real data we are able to access. But at the same time, it is worth thinking about what valence is purporting to represent. Little information is available in the Spotify documentation about how this is being created. And it is doubtful that one number can completely represent how positive a song is."
  },
  {
    "objectID": "09-gather.html#web-scraping",
    "href": "09-gather.html#web-scraping",
    "title": "9  Gather data",
    "section": "9.3 Web scraping",
    "text": "9.3 Web scraping\nWeb scraping is a way to get data from websites. Rather than going to a website using a browser the copy and pasting, we write code that does it for us. This opens a lot of data to us, but on the other hand, it is not typically data that is being made available for these purposes. This means that it is important to be respectful of it. While generally not illegal, the specifics about the legality of web scraping depend on jurisdictions and the specifics of what we are doing, and so it is also important to be mindful of this. While our use would rarely be commercially competitive, of particular concern is the conflict between the need for our work to be reproducible with the need to respect terms of service that may disallow data republishing (Luscombe, Dick, and Walby 2021). And finally, web scraping imposes a cost on the website host, and so it is important to reduce this to the extent possible.\nThat all said, web scraping is an invaluable source of data. But they are typically datasets that can be created as a by-product of someone trying to achieve another aim. For instance, a retailer may have a website with their products and their prices. That has not been created deliberately as a source of data, but we can scrape it to create a dataset. As such, the following principles are useful to guide web scraping.\n\nAvoid it. Try to use an API wherever possible.\nAbide by their desires. Some websites have a ‘robots.txt’ file that contains information about what they are comfortable with scrapers doing, for instance ‘https://www.google.com/robots.txt’.\nReduce the impact.\n\nFirstly, slow down the scraper, for instance, rather than having it visit the website every second, slow it down using sys.sleep(). If we only need a few hundred files, then why not just have it visit the website a few times a minute, running in the background overnight?\nSecondly, consider the timing of when we run our scraper. For instance, if we are scraping a retailer then maybe we should set our script to run from 10pm through to the morning, when fewer customers are likely using the site. Similarly, if it is a government website and they have a big monthly release, then it might be polite to avoid that day.\n\nTake only what is needed. For instance, we do not need to scrape the entire of Wikipedia if all we need is the names of the ten largest cities in Croatia. This reduces the impact on the website, and allows us to more easily justify our actions.\nOnly scrape once. This means we should save everything as we go so that we do not have to re-collect data. Similarly, once we have the data, we should keep that separate and not modify it. Of course, if we need data over time then we will need to go back, but this is different to needlessly re-scraping a page.\nDo not republish the pages that were scraped. (This contrasts with datasets that we create from it.)\nTake ownership and ask permission if possible. At a minimum level all scripts should have our contact details in them. Depending on the circumstances, it may be worthwhile asking for permission before you scrape.\n\nWeb scraping is possible by taking advantage of the underlying structure of a webpage. We use patterns in the HTML/CSS to get the data that we want. To look at the underlying HTML/CSS we can either:\n\nopen a browser, right-click, and choose something like ‘Inspect’; or\nsave the website and then open it with a text editor rather than a browser.\n\nHTML/CSS is a markup language comprised of matching tags. If we want text to be bold, then we would use something like:\n\n<b>My bold text</b>\n\nSimilarly, if we want a list then we start and end the list, as well as each item.\n\n<ul>\n  <li>Learn webscraping</li>\n  <li>Do data science</li>\n  <li>Proft</li>\n</ul>\n\nWhen scraping we will search for these tags.\nTo get started, we can pretend that we obtained some HTML from a website, and that we want to get the name from it. We can see that the name is in bold, so we want to focus on that feature and extract it.\n\nwebsite_extract <- \"<p>Hi, I’m <b>Rohan</b> Alexander.</p>\"\n#| echo: true\n\nWe will use read_html() from rvest (Wickham 2019c) to read in the data.\n\n# install.packages(\"rvest\")\n#| echo: true\nlibrary(rvest)\n\nrohans_data <- read_html(website_extract)\n\nrohans_data\n\n{html_document}\n<html>\n[1] <body><p>Hi, I’m <b>Rohan</b> Alexander.</p></body>\n\n\nThe language used by rvest to look for tags is ‘node’, so we focus on bold nodes. By default html_nodes() returns the tags as well. We can focus on the text that they contain, with html_text().\n\nrohans_data |> \n#| echo: true\n  html_nodes(\"b\")\n\n{xml_nodeset (1)}\n[1] <b>Rohan</b>\n\nfirst_name <- \n  rohans_data |> \n  html_nodes(\"b\") |>\n  html_text()\n\nfirst_name\n\n[1] \"Rohan\"\n\n\n\n9.3.1 Case study: Web scraping book information\nIn this case study we will scrape a list of books from: https://rohanalexander.com/bookshelf.html. We will then clean the data and look at the distribution of the first letters of author surnames. It is slightly more complicated than the example above, but the underlying approach is the same: download the website, look for the nodes of interest, extract the information, clean it.\nWe use rvest (Wickham 2019c) to download a website, and to then navigate the html to find the aspects that we are interested in. And we use tidyverse to clean the dataset. We first need to go to the website and then save a local copy.\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(xml2)\n\nbooks_data <- read_html(\"https://rohanalexander.com/bookshelf.html\")\n\nwrite_html(books_data, \"raw_data.html\") \n\nNow we need to navigate the HTML to get the aspects that we want, and to then put them into some sensible structure. We will start with trying to get the data into a tibble as quickly as possible because this will allow us to more easily use dplyr verbs and tidyverse functions.\n\nbooks_data <- read_html(\"inputs/my_website/raw_data.html\")\n#| echo: true\n\n\nbooks_data\n\n{html_document}\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"\" xml:lang=\"\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n\\n<!--radix_placeholder_front_matter-->\\n\\n<script id=\"distill-fr ...\n\n\nTo get the data into a tibble we first need to identify the data that we are interested in using html tags. If we look at the website then we need to focus on list items (Figure 9.8). And we can look at the source, focusing particularly on looking for a list (Figure 9.9).\n\n\n\nFigure 9.8: Books website as displayed\n\n\n\n\n\nFigure 9.9: HTML for the top of the books website and the list of books\n\n\n\n\n\n\n\nThe tag for a list item is ‘li’, so we can use that to focus on the list.\n\ntext_data <- \n  books_data |>\n  html_nodes(\"li\") |>\n  html_text()\n\nall_books <- \n  tibble(books = text_data)\n\nhead(all_books)\n\n# A tibble: 6 × 1\n  books                     \n  <chr>                     \n1 Academic                  \n2 Non-fiction               \n3 Fiction                   \n4 Cookbooks                 \n5 Want to buy               \n6 Best books that I read in:\n\n\nWe now need to clean the data. First we want to separate the title and the author using separate() and then clean up the author and title columns.\n\nall_books <-\n  all_books |>\n  slice(7:nrow(all_books)) |>\n  separate(books, into = c(\"author\", \"title\"), sep = \", ‘\")\n\nall_books <-\n  all_books |>\n  separate(title, into = c(\"title\", \"debris\"), sep = \"’.\") |>\n  select(-debris) |>\n  mutate(author = str_remove_all(author, \"^, \"),\n         author = str_squish(author),\n         title = str_remove(title, \"“\"),\n         title = str_remove(title, \"^-\")\n         )\n\nhead(all_books)\n\n# A tibble: 6 × 2\n  author                               title                                    \n  <chr>                                <chr>                                    \n1 Bryant, John, and Junni L. Zhang     Bayesian Demographic Estimation and Fore…\n2 Chan, Ngai Hang                      Time Series                              \n3 Clark, Greg                          The Son Also Rises                       \n4 Duflo, Esther                        Expérience, science et lutte contre la p…\n5 Foster, Ghani, Jarmin, Kreuter, Lane Big Data and Social Science              \n6 Francois Chollet with JJ Allaire     Deep Learning with R                     \n\n\n\n\n#| echo: true          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are some at the end that we need to get rid of because they are from a ‘best of’.\n\nall_books <- \n  all_books |> \n  slice(1:142) |>\n  filter(author != \"‘150 Years of Stats Canada!’.\")\n\nFinally, we could make a table of the distribution of the first letter of the names (Table 9.1).\n\nall_books |> \n  mutate(\n    first_letter = str_sub(author, 1, 1)\n    ) |> \n  group_by(first_letter) |> \n  count() |>\n  knitr::kable(\n    col.names = c(\"First letter\", \"Number of times\"),\n    booktabs = TRUE, \n    linesep = \"\"\n    )\n\n\n\nTable 9.1: Distribution of first letter of author names in a collection of books\n\n\nFirst letter\nNumber of times\n\n\n\n\n⭐\n12\n\n\nA\n6\n\n\nB\n8\n\n\nC\n13\n\n\nD\n7\n\n\nE\n5\n\n\nF\n6\n\n\nG\n13\n\n\nH\n6\n\n\nI\n3\n\n\nJ\n1\n\n\nK\n3\n\n\nl\n1\n\n\nL\n4\n\n\nM\n8\n\n\nN\n2\n\n\nO\n4\n\n\nP\n7\n\n\nR\n3\n\n\nS\n12\n\n\nT\n6\n\n\nW\n9\n\n\nY\n1\n\n\nZ\n1\n\n\n\n\n\n\n\n\n9.3.2 Case study: Web scraping UK Prime Ministers from Wikipedia\nIn this case study we are interested in how long UK prime ministers lived, based on the year that they were born. We will scrape data from Wikipedia using rvest (Wickham 2019c), clean it, and then make a graph. Every time we scrape a website things will change. Each scrape will largely be bespoke, even if we can borrow some code from earlier projects. It is completely normal to feel frustrated at times. It helps to begin with an end in mind.\nTo that end, we can start by generating some simulated data. Ideally, we want a table that has a row for each prime minister, a column for their name, and a column each for the birth and death years. If they are still alive, then that death year can be empty. We know that birth and death years should be somewhere between 1700 and 1990, and that death year should be larger than birth year. Finally, we also know that the years should be integers, and the names should be characters. So, we want something that looks roughly like this:\n\nlibrary(babynames)\n\nset.seed(853)\n\nsimulated_dataset <-\n  tibble(\n    prime_minister = sample(\n      x = babynames |> filter(prop > 0.01) |>\n        select(name) |> unique() |> unlist(),\n      size = 10,\n      replace = FALSE\n    ),\n    birth_year = sample(\n      x = c(1700:1990),\n      size = 10,\n      replace = TRUE\n    ),\n    years_lived = sample(\n      x = c(50:100),\n      size = 10,\n      replace = TRUE\n    ),\n    death_year = birth_year + years_lived\n  ) |>\n  select(prime_minister, birth_year, death_year, years_lived) |>\n  arrange(birth_year)\n\nOne of the advantages of generating a simulated dataset is that if we are working in groups then one person can start making the graph, using the simulated dataset, while the other person gathers the data. In terms of a graph, we are aiming for something like Figure 9.10.\n\n\n\nFigure 9.10: Sketch of planned graph showing how long UK prime ministers lived\n\n\nWe are starting with a question that is of interest, which how long each UK prime minister lived. As such, we need to identify a source of data While there are plenty of data sources that have the births and deaths of each prime minister, we want one that we can trust, and as we are going to be scraping, we want one that has some structure to it. The Wikipedia page about UK prime ministers fits both these criteria: https://en.wikipedia.org/wiki/List_of_prime_ministers_of_the_United_Kingdom. As it is a popular page the information is more likely to be correct, and the data are available in a table.\nWe load rvest and then download the page using read_html(). Saving it locally provides us with a copy that we need for reproducibility in case the website changes, and also means that we do not have to keep visiting the website. But it is likely not our property, and so this is typically not something that should be necessarily redistributed.\n\nlibrary(rvest)\nlibrary(tidyverse)\n\n\nraw_data <-\n  read_html(\"https://en.wikipedia.org/wiki/List_of_prime_ministers_of_the_United_Kingdom\")\nwrite_html(raw_data, \"pms.html\")\n\nAs with the earlier case study we are looking for patterns in the HTML that we can use to help us get closer to the data that we want. This is an iterative process and requires a lot of trial and error. Even simple examples will take time.\nOne tool that may help is the SelectorGadget: https://rvest.tidyverse.org/articles/articles/selectorgadget.html. This allows us to pick and choose the elements that we want, and then gives us the input to give to html_nodes() (Figure 9.11))\n\n\n\nFigure 9.11: Using the Selector Gadget to identify the tag, as at 13 March 2020.\n\n\n\n# Read in our saved data\nraw_data <- read_html(\"pms.html\")\n\n\n# We can parse tags in order\nparse_data_selector_gadget <- \n  raw_data |> \n  html_nodes(\"td:nth-child(3)\") |> \n  html_text()\n\nhead(parse_data_selector_gadget)\n\n[1] \"\\nSir Robert Walpole(1676–1745)\\n\"                       \n[2] \"\\nSpencer Compton1st Earl of Wilmington(1673–1743)\\n\"    \n[3] \"\\nHenry Pelham(1694–1754)\\n\"                             \n[4] \"\\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\\n\"\n[5] \"\\nWilliam Cavendish4th Duke of Devonshire(1720–1764)\\n\"  \n[6] \"\\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\\n\"\n\n\nIn this case there is are a few blank lines that we will need to filter away.\n\nparsed_data <-\n  tibble(raw_text = parse_data_selector_gadget) |>\n  filter(raw_text != \"—\\n\") |>\n  filter(\n    !raw_text %in% c(\n      \"\\n1868\\n\",\n      \"\\n1874\\n\",\n      \"\\n1880\\n\",\n      \"\\n1885\\n\",\n      \"\\n1892\\n\",\n      \"\\n1979\\n\",\n      \"\\n1997\\n\",\n      \"\\n2010\\n\"\n    )\n  ) |>\n  filter(\n    !raw_text %in% c(\n      \"\\nNational Labour\\n\",\n      \"\\nWilliam Pulteney1st Earl of Bath(1684–1764)\\n\",\n      \"\\nJames Waldegrave2nd Earl Waldegrave(1715–1763)\\n\",\n      \"\\nEdward VII\\n\\n\\n1901–1910\\n\\n\", \n      \"\\nGeorge V\\n\\n\\n1910–1936\\n\\n\"\n    )\n  )\n\nhead(parsed_data)\n\n# A tibble: 6 × 1\n  raw_text                                                  \n  <chr>                                                     \n1 \"\\nSir Robert Walpole(1676–1745)\\n\"                       \n2 \"\\nSpencer Compton1st Earl of Wilmington(1673–1743)\\n\"    \n3 \"\\nHenry Pelham(1694–1754)\\n\"                             \n4 \"\\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\\n\"\n5 \"\\nWilliam Cavendish4th Duke of Devonshire(1720–1764)\\n\"  \n6 \"\\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\\n\"\n\n\nNow that we have the parsed data, we need to clean it to match what we wanted. In particular we want a names column, as well as columns for birth year and death year. We will use separate() to take advantage of the fact that it looks like the dates are distinguished by brackets.\n\ninitial_clean <- \n  parsed_data |> \n  mutate(raw_text = str_remove_all(raw_text, \"\\n\")) |>\n  separate(raw_text, \n            into = c(\"Name\", \"not_name\"), \n            sep = \"\\\\(\",\n            remove = FALSE) |> # The remove = FALSE option here means that we \n  # keep the original column that we are separating.\n  separate(not_name, \n            into = c(\"Date\", \"all_the_rest\"), \n            sep = \"\\\\)\",\n            remove = FALSE)\n\nhead(initial_clean)\n\n# A tibble: 6 × 5\n  raw_text                                     Name  not_name Date  all_the_rest\n  <chr>                                        <chr> <chr>    <chr> <chr>       \n1 Sir Robert Walpole(1676–1745)                Sir … 1676–17… 1676… \"\"          \n2 Spencer Compton1st Earl of Wilmington(1673–… Spen… 1673–17… 1673… \"\"          \n3 Henry Pelham(1694–1754)                      Henr… 1694–17… 1694… \"\"          \n4 Thomas Pelham-Holles1st Duke of Newcastle(1… Thom… 1693–17… 1693… \"\"          \n5 William Cavendish4th Duke of Devonshire(172… Will… 1720–17… 1720… \"\"          \n6 Thomas Pelham-Holles1st Duke of Newcastle(1… Thom… 1693–17… 1693… \"\"          \n\n\nFinally, we need to clean up the columns.\n\ninitial_clean <- \n initial_clean |> \n  separate(col = Name, \n           into = c(\"Name\", \"Title\"),\n           sep = \"[[:digit:]]\",\n           extra = \"merge\",\n           fill = \"right\") |>\n  separate(col = Name, \n           into = c(\"Name\", \"Title\"),\n           sep = \"MP for\",\n           extra = \"merge\",\n           fill = \"right\") |>\n  mutate(Name = str_remove(Name, \"\\\\[b\\\\]\"))\n\nhead(initial_clean)\n\n# A tibble: 6 × 6\n  raw_text                               Name  Title not_name Date  all_the_rest\n  <chr>                                  <chr> <chr> <chr>    <chr> <chr>       \n1 Sir Robert Walpole(1676–1745)          Sir … <NA>  1676–17… 1676… \"\"          \n2 Spencer Compton1st Earl of Wilmington… Spen… <NA>  1673–17… 1673… \"\"          \n3 Henry Pelham(1694–1754)                Henr… <NA>  1694–17… 1694… \"\"          \n4 Thomas Pelham-Holles1st Duke of Newca… Thom… <NA>  1693–17… 1693… \"\"          \n5 William Cavendish4th Duke of Devonshi… Will… <NA>  1720–17… 1720… \"\"          \n6 Thomas Pelham-Holles1st Duke of Newca… Thom… <NA>  1693–17… 1693… \"\"          \n\n\n\ncleaned_data <- \n  initial_clean |> \n  select(Name, Date) |> \n  separate(Date, into = c(\"Birth\", \"Died\"), sep = \"–\", remove = FALSE) |> # The \n  # PMs who have died have their birth and death years separated by a hyphen, but \n  # we need to be careful with the hyphen as it seems to be a slightly odd type of \n  # hyphen and we need to copy/paste it.\n  mutate(Birth = str_remove_all(Birth, \"born\"),\n         Birth = str_trim(Birth)\n         ) |> # Alive PMs have slightly different format\n  select(-Date) |> \n  mutate(Name = str_remove(Name, \"\\n\")) |> # Remove some html tags that remain\n  mutate_at(vars(Birth, Died), ~as.integer(.)) |> # Change birth and death to integers\n  mutate(Age_at_Death = Died - Birth) |>  # Add column of the number of years they lived\n  distinct() # Some of the PMs had two goes at it.\n\nhead(cleaned_data)\n\n# A tibble: 6 × 4\n  Name                 Birth  Died Age_at_Death\n  <chr>                <int> <int>        <int>\n1 Sir Robert Walpole    1676  1745           69\n2 Spencer Compton       1673  1743           70\n3 Henry Pelham          1694  1754           60\n4 Thomas Pelham-Holles  1693  1768           75\n5 William Cavendish     1720  1764           44\n6 John Stuart           1713  1792           79\n\n\nOur dataset looks similar to the one that we said we wanted at the start (Table 9.2).\n\ncleaned_data |> \n  knitr::kable(\n    col.names = c(\"Prime Minister\", \"Birth year\", \"Death year\", \"Age at death\"),\n    booktabs = TRUE, \n    linesep = \"\"\n    )\n\n\n\nTable 9.2: UK Prime Ministers, by how old they were when they died\n\n\nPrime Minister\nBirth year\nDeath year\nAge at death\n\n\n\n\nSir Robert Walpole\n1676\n1745\n69\n\n\nSpencer Compton\n1673\n1743\n70\n\n\nHenry Pelham\n1694\n1754\n60\n\n\nThomas Pelham-Holles\n1693\n1768\n75\n\n\nWilliam Cavendish\n1720\n1764\n44\n\n\nJohn Stuart\n1713\n1792\n79\n\n\nGeorge Grenville\n1712\n1770\n58\n\n\nCharles Watson-Wentworth\n1730\n1782\n52\n\n\nWilliam Pitt the Elder\n1708\n1778\n70\n\n\nAugustus FitzRoy\n1735\n1811\n76\n\n\nFrederick NorthLord North\n1732\n1792\n60\n\n\nWilliam Petty\n1737\n1805\n68\n\n\nWilliam Cavendish-Bentinck\n1738\n1809\n71\n\n\nWilliam Pitt the Younger\n1759\n1806\n47\n\n\nHenry Addington\n1757\n1844\n87\n\n\nWilliam Grenville\n1759\n1834\n75\n\n\nSpencer Perceval\n1762\n1812\n50\n\n\nRobert Jenkinson\n1770\n1828\n58\n\n\nGeorge Canning\n1770\n1827\n57\n\n\nF. J. Robinson\n1782\n1859\n77\n\n\nArthur Wellesley\n1769\n1852\n83\n\n\nCharles Grey\n1764\n1845\n81\n\n\nWilliam Lamb\n1779\n1848\n69\n\n\nSir Robert Peel\n1788\n1850\n62\n\n\nLord John Russell\n1792\n1878\n86\n\n\nEdward Smith-Stanley\n1799\n1869\n70\n\n\nGeorge Hamilton-Gordon\n1784\n1860\n76\n\n\nHenry John Temple\n1784\n1865\n81\n\n\nJohn Russell\n1792\n1878\n86\n\n\nBenjamin Disraeli\n1804\n1881\n77\n\n\nWilliam Ewart Gladstone\n1809\n1898\n89\n\n\nRobert Gascoyne-Cecil\n1830\n1903\n73\n\n\nArchibald Primrose\n1847\n1929\n82\n\n\nArthur Balfour\n1848\n1930\n82\n\n\nSir Henry Campbell-Bannerman\n1836\n1908\n72\n\n\nH. H. Asquith\n1852\n1928\n76\n\n\nDavid Lloyd George\n1863\n1945\n82\n\n\nBonar Law\n1858\n1923\n65\n\n\nStanley Baldwin\n1867\n1947\n80\n\n\nRamsay MacDonald\n1866\n1937\n71\n\n\nNeville Chamberlain\n1869\n1940\n71\n\n\nWinston Churchill\n1874\n1965\n91\n\n\nClement Attlee\n1883\n1967\n84\n\n\nSir Winston Churchill\n1874\n1965\n91\n\n\nSir Anthony Eden\n1897\n1977\n80\n\n\nHarold Macmillan\n1894\n1986\n92\n\n\nSir Alec Douglas-Home\n1903\n1995\n92\n\n\nHarold Wilson\n1916\n1995\n79\n\n\nEdward Heath\n1916\n2005\n89\n\n\nJames Callaghan\n1912\n2005\n93\n\n\nMargaret Thatcher\n1925\n2013\n88\n\n\nJohn Major\n1943\nNA\nNA\n\n\nTony Blair\n1953\nNA\nNA\n\n\nGordon Brown\n1951\nNA\nNA\n\n\nDavid Cameron\n1966\nNA\nNA\n\n\nTheresa May\n1956\nNA\nNA\n\n\nBoris Johnson\n1964\nNA\nNA\n\n\n\n\n\n\nAt this point we would like to make a graph that illustrates how long each prime minister lived. If they are still alive then we would like them to run to the end, but we would like to color them differently.\n\ncleaned_data |> \n  mutate(still_alive = if_else(is.na(Died), \"Yes\", \"No\"),\n         Died = if_else(is.na(Died), as.integer(2022), Died)) |> \n  mutate(Name = as_factor(Name)) |> \n  ggplot(aes(x = Birth, \n             xend = Died,\n             y = Name,\n             yend = Name, \n             color = still_alive)) +\n  geom_segment() +\n  labs(x = \"Year of birth\",\n       y = \"Prime minister\",\n       color = \"PM is alive\",\n       title = \"How long each UK Prime Minister lived, by year of birth\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n9.3.3 Case study: Downloading multiple files\nConsidering text as data is exciting and opens up a lot of different research questions. Many guides assume that we already have a nicely formatted text dataset, but that is rarely actually the case. In this case study we will download files from a few different pages. While we have already seen two examples of web scraping, those were focused on just one page, whereas we often need many. Here we will focus on this iteration. We will use download.file() to do the download, and purrr (Henry and Wickham 2020) to apply this function across multiple sites.\nThe Reserve Bank of Australia (RBA) is Australia’s central bank and sets monetary policy. It has responsibility for setting the cash rate, which is the interest rate used for loans between banks. This interest rate is an especially important one, and has a large impact on the other interest rates in the economy. Four times a year – February, May, August, and November – the RBA publishes a statement on monetary policy, and these are available as PDFs. In this example we will download the four statements published in 2021.\nFirst we set-up a dataframe that has the information that we need.\n\nlibrary(tidyverse)\n\nstatements_of_interest <- \n  tibble(\n    address = c(\"https://www.rba.gov.au/publications/smp/2021/nov/pdf/00-overview.pdf\",\n                \"https://www.rba.gov.au/publications/smp/2021/aug/pdf/00-overview.pdf\",\n                \"https://www.rba.gov.au/publications/smp/2021/may/pdf/00-overview.pdf\",\n                \"https://www.rba.gov.au/publications/smp/2021/feb/pdf/00-overview.pdf\"\n                ),\n    local_save_name = c(\n      \"2021-11.pdf\",\n      \"2021-08.pdf\",\n      \"2021-05.pdf\",\n      \"2021-02.pdf\"\n    )\n  )\n\nstatements_of_interest\n\n# A tibble: 4 × 2\n  address                                                        local_save_name\n  <chr>                                                          <chr>          \n1 https://www.rba.gov.au/publications/smp/2021/nov/pdf/00-overv… 2021-11.pdf    \n2 https://www.rba.gov.au/publications/smp/2021/aug/pdf/00-overv… 2021-08.pdf    \n3 https://www.rba.gov.au/publications/smp/2021/may/pdf/00-overv… 2021-05.pdf    \n4 https://www.rba.gov.au/publications/smp/2021/feb/pdf/00-overv… 2021-02.pdf    \n\n\nThen we can apply the function download.files() to these four\nThen we can write a function that will download the file, let us know that it was downloaded, wait a polite amount of time, and then go get the next file.\n\nvisit_download_and_wait <-\n  function(the_address_to_visit, where_to_save_it_locally) {\n    \n    download.file(url = the_address_to_visit,\n                  destfile = where_to_save_it_locally\n                  )\n\n    print(paste(\"Done with\", the_address_to_visit, \"at\", Sys.time()))\n    \n    Sys.sleep(sample(5:10, 1))\n  }\n\nWe now apply that function to our list of URLs.\n\nwalk2(statements_of_interest$address,\n      statements_of_interest$local_save_name,\n      ~visit_download_and_wait(.x, .y))\n\nThe result is that we have downloaded these four PDFs and saved them to our computer. In the next section we will build on this to discuss getting information from these PDFs."
  },
  {
    "objectID": "09-gather.html#pdfs",
    "href": "09-gather.html#pdfs",
    "title": "9  Gather data",
    "section": "9.4 PDFs",
    "text": "9.4 PDFs\nIn contrast to an API, a PDF is usually only produced for human rather than computer consumption. The nice thing about PDFs is that they are static and constant. And it is nice that they make data available at all. But the trade-off is that:\n\nIt is not overly useful to do larger-scale statistical analysis.\nWe do not know how the PDF was put together so we do not know whether we can trust it.\nWe cannot manipulate the data to get results that we are interested in.\n\nIndeed, sometimes governments publish data as PDFs because they do not actually want us to be able to analyze it. Being able to get data from PDFs opens up a large number of datasets.\nThere are two important aspects to keep in mind when approaching a PDF with a mind to extracting data from it:\n\nBegin with an end in mind. Planning and then literally sketching out what we want from a final dataset/graph/paper stops us wasting time and keeps us focused.\nStart simple, then iterate. The quickest way to make a complicated model is often to first build a simple model and then complicate it. Start with just trying to get one page of the PDF working or even just one line. Then iterate from there.\n\nWe will start by walking through several examples and then go through a case study where we will gather data on US Total Fertility Rate, by state.\nFigure 9.12 is a PDF that consists of just the first sentence from Jane Eyre taken from Project Gutenberg (Bronte 1847).\n\n\n\nFigure 9.12: First sentence of Jane Eyre\n\n\nIf assume that it was saved as ‘first_example.pdf’, then we can pdftools (Ooms 2019a) to get the text from this one-page PDF into R.\n\nlibrary(pdftools)\nlibrary(tidyverse)\n\nfirst_example <- pdf_text(\"first_example.pdf\")\n\nfirst_example\n\nclass(first_example)\n\nWe can see that the PDF has been correctly read in, as a character vector.\nWe will now try a slightly more complicated example that consists of the first few paragraphs of Jane Eyre (Figure 9.13). Also notice that now we have the chapter heading as well.\n\n\n\nFigure 9.13: First few paragraphs of Jane Eyre\n\n\nWe use the same function as before.\n\nsecond_example <- pdftools::pdf_text(\"second_example.pdf\")\n\nsecond_example\n\nclass(second_example)\n\nAgain, we have a character vector. The end of each line is signaled by ‘\\n’, but other than that it looks pretty good. Finally, we consider the first two pages.\n\nthird_example <- pdftools::pdf_text(\"third_example.pdf\")\n\nthird_example\n\nclass(third_example)\n\nNotice that the first page is the first element of the character vector, and the second page is the second element. As we are most familiar with rectangular data, we will try to get it into that format as quickly as possible. And then we can use our regular tidyverse functions to deal with it.\nFirst we want to convert the character vector into a tibble. At this point we may like to add page numbers as well.\n\njane_eyre <- tibble(raw_text = third_example,\n                    page_number = c(1:2))\n\nWe then want to separate the lines so that each line is an observation. We can do that by looking for ‘\\n’ remembering that we need to escape the backslash as it is a special character.\n\njane_eyre <- \n  separate_rows(jane_eyre, raw_text, sep = \"\\\\n\", convert = FALSE)\njane_eyre\n\n# A tibble: 93 × 2\n   raw_text                                                          page_number\n   <chr>                                                                   <int>\n 1 \"CHAPTER I\"                                                                 1\n 2 \"There was no possibility of taking a walk that day. We had been…           1\n 3 \"leafless shrubbery an hour in the morning; but since dinner (Mr…           1\n 4 \"company, dined early) the cold winter wind had brought with it …           1\n 5 \"penetrating, that further out-door exercise was now out of the …           1\n 6 \"\"                                                                          1\n 7 \"I was glad of it: I never liked long walks, especially on chill…           1\n 8 \"coming home in the raw twilight, with nipped fingers and toes, …           1\n 9 \"chidings of Bessie, the nurse, and humbled by the consciousness…           1\n10 \"Eliza, John, and Georgiana Reed.\"                                          1\n# … with 83 more rows\n\n\n\n9.4.1 Case-study: Gathering data on the US Total Fertility Rate\nThe US Department of Health and Human Services Vital Statistics Report provides information about the total fertility rate (the average number of births per woman if women experience the current age-specific fertility rates throughout their reproductive years) for each state for nineteen years. The US persists in only making this data available in PDFs, which hinders research. But we can use the approaches above to get the data into a nice dataset.\nFor instance, in the case of the year 2000 the table that we are interested in is on page 40 of a PDF that is available at https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf. The column of interest is labelled: “Total fertility rate” (Figure 9.14).\n\n\n\nFigure 9.14: Example Vital Statistics Report, from 2000\n\n\nThe first step when getting data out of a PDF is to sketch out what we eventually want. A PDF typically contains a lot of information, and so it is handy to be very clear about what you need. This helps keep you focused, and prevents scope creep, but it is also helpful when thinking about data checks. We literally write down on paper what we have in mind. In this case, what is needed is a table with a column for state, year and TFR (Figure 9.15).\n\n\n\nFigure 9.15: Planned dataset of TFR for each year and US state\n\n\nThere are 19 different PDFs, and we are interested in a particular column in a particular table in each of them. Unfortunately, there is nothing magical about what is coming. This first step requires working out the link for each, and the page and column name that is of interest. In the end, this looks like this.\n\nlibrary(gt)\n\nsummary_tfr_dataset |> \n  select(year, page, table, column_name, url) |> \n  gt()\n\n\n\n\n  \n  \n    \n      year\n      page\n      table\n      column_name\n      url\n    \n  \n  \n    2000\n40\n10\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf\n    2001\n41\n10\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr51/nvsr51_02.pdf\n    2002\n46\n10\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr52/nvsr52_10.pdf\n    2003\n45\n10\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr54/nvsr54_02.pdf\n    2004\n52\n11\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr55/nvsr55_01.pdf\n    2005\n52\n11\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr56/nvsr56_06.pdf\n    2006\n49\n11\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr57/nvsr57_07.pdf\n    2007\n41\n11\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr58/nvsr58_24.pdf\n    2008\n43\n12\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr59/nvsr59_01.pdf\n    2009\n43\n12\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr60/nvsr60_01.pdf\n    2010\n42\n12\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr61/nvsr61_01.pdf\n    2011\n40\n12\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr62/nvsr62_01.pdf\n    2012\n38\n12\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr62/nvsr62_09.pdf\n    2013\n37\n12\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64_01.pdf\n    2014\n38\n12\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64_12.pdf\n    2015\n42\n12\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66_01.pdf\n    2016\n29\n8\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67_01.pdf\n    2016\n30\n8\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67_01.pdf\n    2017\n23\n12\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67_08-508.pdf\n    2017\n24\n12\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67_08-508.pdf\n    2018\n23\n12\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr68/nvsr68_13-508.pdf\n  \n  \n  \n\n\n\n\nThe first step is to get some code that works for one of them. I’ll step through the code in a lot more detail than normal because we’re going to use these pieces a lot.\nWe will choose the year 2000. We first download and save the PDF using download.file().\n\ndownload.file(url = summary_tfr_dataset$url[1], \n              destfile = \"year_2000.pdf\")\n\n\n# INTERNAL\n\ndownload.file(url = summary_tfr_dataset$url[1], \n              destfile = \"inputs/pdfs/dhs/year_2000.pdf\")\n\nWe then read the PDF in as a character vector using pdf_text() from pdftools. And then convert it to a tibble, so that we can use familiar verbs on it.\n\nlibrary(pdftools)\ndhs_2000 <- pdf_text(\"year_2000.pdf\")\n\n\n\n\n\ndhs_2000 <- tibble(raw_data = dhs_2000)\n\nhead(dhs_2000)\n\n# A tibble: 6 × 1\n  raw_data                                                                      \n  <chr>                                                                         \n1 \"Volume 50, Number 5                                                         …\n2 \"2   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n\\n\\…\n3 \"                                                                            …\n4 \"4   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n\\n\\…\n5 \"                                                                            …\n6 \"6   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n\\n …\n\n\nGrab the page that is of interest (remembering that each page is a element of the character vector, hence a row in the tibble).\n\ndhs_2000 <- \n  dhs_2000 |> \n  slice(summary_tfr_dataset$page[1])\n\nhead(dhs_2000)\n\n# A tibble: 1 × 1\n  raw_data                                                                      \n  <chr>                                                                         \n1 \"40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022\\n…\n\n\nNow we want to separate the rows.\n\ndhs_2000 <- \n  dhs_2000 |> \n  separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE)\n\nhead(dhs_2000)\n\n# A tibble: 6 × 1\n  raw_data                                                                      \n  <chr>                                                                         \n1 \"40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022\"  \n2 \"\"                                                                            \n3 \"Table 10. Number of births, birth rates, fertility rates, total fertility ra…\n4 \"United States, each State and territory, 2000\"                               \n5 \"[By place of residence. Birth rates are live births per 1,000 estimated popu…\n6 \"estimated in each area; total fertility rates are sums of birth rates for 5-…\n\n\nNow we are searching for patterns that we can use. Let us look at the first ten lines of content.\n\ndhs_2000[13:22,]\n\n# A tibble: 10 × 1\n   raw_data                                                                     \n   <chr>                                                                        \n 1 \"                                  State                                    …\n 2 \"                                                                           …\n 3 \"                                                                           …\n 4 \"\"                                                                           \n 5 \"\"                                                                           \n 6 \"United States 1 ......................................................     …\n 7 \"\"                                                                           \n 8 \"Alabama ...............................................................    …\n 9 \"Alaska ................................................................... …\n10 \"Arizona .................................................................  …\n\n\nIt does not get much better than this:\n\nWe have dots separating the states from the data.\nWe have a space between each of the columns.\n\nSo we can now separate this in to separate columns. First we want to match on when there is at least two dots (remembering that the dot is a special character and so needs to be escaped).\n\ndhs_2000 <- \n  dhs_2000 |> \n  separate(col = raw_data, \n           into = c(\"state\", \"data\"), \n           sep = \"\\\\.{2,}\", \n           remove = FALSE,\n           fill = \"right\"\n           )\n\nhead(dhs_2000)\n\n# A tibble: 6 × 3\n  raw_data                                                           state data \n  <chr>                                                              <chr> <chr>\n1 \"40 National Vital Statistics Report, Vol. 50, No. 5, Revised May… \"40 … <NA> \n2 \"\"                                                                 \"\"    <NA> \n3 \"Table 10. Number of births, birth rates, fertility rates, total … \"Tab… <NA> \n4 \"United States, each State and territory, 2000\"                    \"Uni… <NA> \n5 \"[By place of residence. Birth rates are live births per 1,000 es… \"[By… <NA> \n6 \"estimated in each area; total fertility rates are sums of birth … \"est… <NA> \n\n\nWe get the expected warnings about the top and the bottom as they do not have multiple dots. (Another option here is to use pdf_data() which would allow us to use location rather than delimiters.)\nWe can now separate the data based on spaces. There is an inconsistent number of spaces, so we first squish any example of more than one space into just one.\n\ndhs_2000 <- \n  dhs_2000 |>\n  mutate(data = str_squish(data)) |> \n  tidyr::separate(col = data, \n           into = c(\"number_of_births\", \n                    \"birth_rate\", \n                    \"fertility_rate\", \n                    \"TFR\", \n                    \"teen_births_all\", \n                    \"teen_births_15_17\", \n                    \"teen_births_18_19\"), \n           sep = \"\\\\s\", \n           remove = FALSE\n           )\n\nhead(dhs_2000)\n\n# A tibble: 6 × 10\n  raw_data          state data  number_of_births birth_rate fertility_rate TFR  \n  <chr>             <chr> <chr> <chr>            <chr>      <chr>          <chr>\n1 \"40 National Vit… \"40 … <NA>  <NA>             <NA>       <NA>           <NA> \n2 \"\"                \"\"    <NA>  <NA>             <NA>       <NA>           <NA> \n3 \"Table 10. Numbe… \"Tab… <NA>  <NA>             <NA>       <NA>           <NA> \n4 \"United States, … \"Uni… <NA>  <NA>             <NA>       <NA>           <NA> \n5 \"[By place of re… \"[By… <NA>  <NA>             <NA>       <NA>           <NA> \n6 \"estimated in ea… \"est… <NA>  <NA>             <NA>       <NA>           <NA> \n# … with 3 more variables: teen_births_all <chr>, teen_births_15_17 <chr>,\n#   teen_births_18_19 <chr>\n\n\nThis is all looking fairly great. The only thing left is to clean up.\n\ndhs_2000 <- \n  dhs_2000 |> \n  select(state, TFR) |> \n  slice(13:69) |> \n  mutate(year = 2000)\n\ndhs_2000\n\n# A tibble: 57 × 3\n   state                                                             TFR    year\n   <chr>                                                             <chr> <dbl>\n 1 \"                                  State                        … <NA>   2000\n 2 \"                                                               … <NA>   2000\n 3 \"                                                               … <NA>   2000\n 4 \"\"                                                                <NA>   2000\n 5 \"\"                                                                <NA>   2000\n 6 \"United States 1 \"                                                2,13…  2000\n 7 \"\"                                                                <NA>   2000\n 8 \"Alabama \"                                                        2,02…  2000\n 9 \"Alaska \"                                                         2,43…  2000\n10 \"Arizona \"                                                        2,65…  2000\n# … with 47 more rows\n\n\nAnd we’re done for that year. Now we want to take these pieces, put them into a function and then run that function over all 19 years.\nThe first part is downloading each of the 19 PDFs that we need. We are going to build on the code that we used before. That code was:\n\ndownload.file(url = summary_tfr_dataset$url[1], destfile = \"year_2000.pdf\")\n\nTo modify this we need:\n\nTo have it iterate through each of the lines in the dataset that contains our CSVs (i.e. where it says 1, we want 1, then 2, then 3, etc.).\nWhere it has a filename, we need it to iterate through our desired filenames (i.e. year_2000, then year_2001, then year_2002, etc).\nWe would like for it to do all of this in a way that is a little robust to errors. For instance, if one of the URLs is wrong or the internet drops out then we would like it to just move onto the next PDF, and then warn us at the end that it missed one, not to stop. (This does not really matter because it is only 19 files, but it is easy to find oneself doing this for thousands of files).\n\nWe will draw on purrr for this (Henry and Wickham 2020).\n\nlibrary(purrr)\n\nsummary_tfr_dataset <- \n  summary_tfr_dataset |> \n  mutate(pdf_name = paste0(\"dhs/year_\", year, \".pdf\"))\n\n\n\n\n\nwalk2(\n  summary_tfr_dataset$url,\n  summary_tfr_dataset$pdf_name,\n  safely( ~ download.file(.x , .y))\n)\n\nHere we take download.file() and pass it two arguments: .x and .y. Then walk2() applies that function to the inputs that we give it, in this case the URLs columns is the .x and the pdf_names column is the .y. Finally, safely() means that if there are any failures then it just moves onto the next file instead of throwing an error.\nWe now have each of the PDFs saved and we can move onto getting the data from them.\nNow we need to get the data from the PDFs. As before, we are going to build on the code that we used before. That code (overly condensed) was:\n\ndhs_2000 <- pdftools::pdf_text(\"year_2000.pdf\")\n\ndhs_2000 <-\n  tibble(raw_data = dhs_2000) |>\n  slice(summary_tfr_dataset$page[1]) |>\n  separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE) |>\n  separate(\n    col = raw_data,\n    into = c(\"state\", \"data\"),\n    sep = \"\\\\.{2,}\",\n    remove = FALSE\n  ) |>\n  mutate(data = str_squish(data)) |>\n  separate(\n    col = data,\n    into = c(\n      \"number_of_births\",\n      \"birth_rate\",\n      \"fertility_rate\",\n      \"TFR\",\n      \"teen_births_all\",\n      \"teen_births_15_17\",\n      \"teen_births_18_19\"\n    ),\n    sep = \"\\\\s\",\n    remove = FALSE\n  ) |>\n  select(state, TFR) |>\n  slice(13:69) |>\n  mutate(year = 2000)\n\ndhs_2000\n\nThe first thing that we want to iterate is the argument to pdf_text(), then the number in in slice() will also need to change (that is doing the work to get only the page that we are interested in).\nTwo aspects are hardcoded, and these may need to be updated. In particular:\n\nThe separate only works if each of the tables has the same columns in the same order; and\nthe slice (which restricts the data to just the states) only works in this case.\n\nFinally, we add the year only at the end, whereas we would need to bring that up earlier in the process.\nWe will start by writing a function that will go through all the files, grab the data, get the page of interest, and then expand the rows. We will then use pmap_dfr() from purrr to apply that function to all of the PDFs and to output a tibble.\n\nget_pdf_convert_to_tibble <- function(pdf_name, page, year){\n#| echo: true\n  \n  dhs_table_of_interest <- \n    tibble(raw_data = pdftools::pdf_text(pdf_name)) |> \n    slice(page) |> \n    separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE) |> \n    separate(col = raw_data, \n             into = c(\"state\", \"data\"), \n             sep = \"[�|\\\\.]\\\\s+(?=[[:digit:]])\", \n             remove = FALSE) |> \n    mutate(\n      data = str_squish(data),\n      year_of_data = year)\n\n  print(paste(\"Done with\", year))\n  \n  return(dhs_table_of_interest)\n}\n\nraw_dhs_data <- purrr::pmap_dfr(summary_tfr_dataset |> select(pdf_name, page, year),\n                                get_pdf_convert_to_tibble)\n\n[1] \"Done with 2000\"\n[1] \"Done with 2001\"\n[1] \"Done with 2002\"\n[1] \"Done with 2003\"\n[1] \"Done with 2004\"\n[1] \"Done with 2005\"\n[1] \"Done with 2006\"\n[1] \"Done with 2007\"\n[1] \"Done with 2008\"\n[1] \"Done with 2009\"\n[1] \"Done with 2010\"\n[1] \"Done with 2011\"\n[1] \"Done with 2012\"\n[1] \"Done with 2013\"\n[1] \"Done with 2014\"\n[1] \"Done with 2015\"\n[1] \"Done with 2016\"\n[1] \"Done with 2016\"\n[1] \"Done with 2017\"\n[1] \"Done with 2017\"\n[1] \"Done with 2018\"\n\nhead(raw_dhs_data)\n\n# A tibble: 6 × 4\n  raw_data                                              state data  year_of_data\n  <chr>                                                 <chr> <chr>        <dbl>\n1 \"40 National Vital Statistics Report, Vol. 50, No. 5… \"40 … 50, …         2000\n2 \"\"                                                    \"\"    <NA>          2000\n3 \"Table 10. Number of births, birth rates, fertility … \"Tab… <NA>          2000\n4 \"United States, each State and territory, 2000\"       \"Uni… <NA>          2000\n5 \"[By place of residence. Birth rates are live births… \"[By… <NA>          2000\n6 \"estimated in each area; total fertility rates are s… \"est… <NA>          2000\n\n\nNow we need to clean up the state names and then filter on them.\n\nstates <- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \n#| echo: true\n            \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \n            \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n            \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \n            \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \n            \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \n            \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \n            \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \n            \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \n            \"Wyoming\", \"District of Columbia\")\n\nraw_dhs_data <- \n  raw_dhs_data |> \n  mutate(state = str_remove_all(state, \"\\\\.\"),\n         state = str_remove_all(state, \"�\"),\n         state = str_remove_all(state, \"\\u0008\"),\n         state = str_replace_all(state, \"United States 1\", \"United States\"),\n         state = str_replace_all(state, \"United States1\", \"United States\"),\n         state = str_replace_all(state, \"United States 2\", \"United States\"),\n         state = str_replace_all(state, \"United States2\", \"United States\"),\n         state = str_replace_all(state, \"United States²\", \"United States\"),\n         ) |> \n  mutate(state = str_squish(state)) |> \n  filter(state %in% states)\n\nhead(raw_dhs_data)\n\n# A tibble: 6 × 4\n  raw_data                                              state data  year_of_data\n  <chr>                                                 <chr> <chr>        <dbl>\n1 Alabama ............................................… Alab… 63,2…         2000\n2 Alaska .............................................… Alas… 9,97…         2000\n3 Arizona ............................................… Ariz… 85,2…         2000\n4 Arkansas ...........................................… Arka… 37,7…         2000\n5 California .........................................… Cali… 531,…         2000\n6 Colorado ...........................................… Colo… 65,4…         2000\n\n\nThe next step is to separate the data and get the correct column from it. We are going to separate based on spaces once it is cleaned up.\n\nraw_dhs_data <- \n#| echo: true\n  raw_dhs_data |> \n  mutate(data = str_remove_all(data, \"\\\\*\")) |> \n  separate(data, into = c(\"col_1\", \"col_2\", \"col_3\", \"col_4\", \"col_5\", \n                          \"col_6\", \"col_7\", \"col_8\", \"col_9\", \"col_10\"), \n           sep = \" \",\n           remove = FALSE)\nhead(raw_dhs_data)\n\n# A tibble: 6 × 14\n  raw_data     state data  col_1 col_2 col_3 col_4 col_5 col_6 col_7 col_8 col_9\n  <chr>        <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n1 Alabama ...… Alab… 63,2… 63,2… 14.4  65.0  2,02… 62.9  37.9  97.3  <NA>  <NA> \n2 Alaska ....… Alas… 9,97… 9,974 16.0  74.6  2,43… 42.4  23.6  69.4  <NA>  <NA> \n3 Arizona ...… Ariz… 85,2… 85,2… 17.5  84.4  2,65… 69.1  41.1  111.3 <NA>  <NA> \n4 Arkansas ..… Arka… 37,7… 37,7… 14.7  69.1  2,14… 68.5  36.7  114.1 <NA>  <NA> \n5 California … Cali… 531,… 531,… 15.8  70.7  2,18… 48.5  28.6  75.6  <NA>  <NA> \n6 Colorado ..… Colo… 65,4… 65,4… 15.8  73.1  2,35… 49.2  28.6  79.8  <NA>  <NA> \n# … with 2 more variables: col_10 <chr>, year_of_data <dbl>\n\n\nWe can now grab the correct column.\n\ntfr_data <- \n  raw_dhs_data |> \n  mutate(TFR = if_else(year_of_data < 2008, col_4, col_3)) |> \n  select(state, year_of_data, TFR) |> \n  rename(year = year_of_data)\n\nhead(tfr_data)\n\n# A tibble: 6 × 3\n  state       year TFR    \n  <chr>      <dbl> <chr>  \n1 Alabama     2000 2,021.0\n2 Alaska      2000 2,437.0\n3 Arizona     2000 2,652.5\n4 Arkansas    2000 2,140.0\n5 California  2000 2,186.0\n6 Colorado    2000 2,356.5\n\n\nFinally, we need to convert the case.\n\nhead(tfr_data)\n\n# A tibble: 6 × 3\n  state       year TFR    \n  <chr>      <dbl> <chr>  \n1 Alabama     2000 2,021.0\n2 Alaska      2000 2,437.0\n3 Arizona     2000 2,652.5\n4 Arkansas    2000 2,140.0\n5 California  2000 2,186.0\n6 Colorado    2000 2,356.5\n\ntfr_data <- \n  tfr_data |> \n  mutate(TFR = str_remove_all(TFR, \",\"),\n         TFR = as.numeric(TFR))\n\nhead(tfr_data)\n\n# A tibble: 6 × 3\n  state       year   TFR\n  <chr>      <dbl> <dbl>\n1 Alabama     2000 2021 \n2 Alaska      2000 2437 \n3 Arizona     2000 2652.\n4 Arkansas    2000 2140 \n5 California  2000 2186 \n6 Colorado    2000 2356.\n\n\nAnd run some checks.\n\ntfr_data$state |> unique() |> length() == 51\n\n[1] TRUE\n\ntfr_data$year |> unique() |> length() == 19\n\n[1] TRUE\n\n\nIn particular we want for there to be 51 states and for there to be 19 years.\nAnd we are done (Table 9.3)!\n\ntfr_data |>\n  slice(1:10) |>\n  knitr:: kable(\n    col.names = c(\"State\", \"Year\", \"TFR\"),\n    digits = 0,\n    booktabs = TRUE, \n    linesep = \"\",\n    format.args = list(big.mark = \",\")\n  )\n\n\n\nTable 9.3: First ten rows of a dataset of TFR by US state, 2000-2019\n\n\nState\nYear\nTFR\n\n\n\n\nAlabama\n2,000\n2,021\n\n\nAlaska\n2,000\n2,437\n\n\nArizona\n2,000\n2,652\n\n\nArkansas\n2,000\n2,140\n\n\nCalifornia\n2,000\n2,186\n\n\nColorado\n2,000\n2,356\n\n\nConnecticut\n2,000\n1,932\n\n\nDelaware\n2,000\n2,014\n\n\nDistrict of Columbia\n2,000\n1,976\n\n\nFlorida\n2,000\n2,158\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.4.2 Optical Character Recognition\nAll of the above is predicated on having a PDF that is already ‘digitized’. But what if it is images? In that case we need to first use Optical Character Recognition (OCR) using tesseract (Ooms 2019b). This is a R wrapper around the Tesseract open-source OCR engine.\nLet us see an example with a scan from the first page of Jane Eyre (Figure 9.16).\n\n\n\nFigure 9.16: Scan of first page of Jane Eyre\n\n\n\nlibrary(tesseract)\n\ntext <- tesseract::ocr(here::here(\"jane_scan.png\"), engine = tesseract(\"eng\"))\ncat(text)"
  },
  {
    "objectID": "09-gather.html#exercises-and-tutorial",
    "href": "09-gather.html#exercises-and-tutorial",
    "title": "9  Gather data",
    "section": "9.5 Exercises and tutorial",
    "text": "9.5 Exercises and tutorial\n\n9.5.1 Exercises\n\nWhat are some types of probability sampling, and in what circumstances might you want to implement them (write two or three pages)?\nThere have been some substantial political polling ‘misses’ in recent years (Trump and Brexit come to mind). To what extent do you think non-response bias was the cause of this (write a page or two, being sure to ground your writing with citations)?\nIt seems like a lot of businesses have closed since the pandemic. To investigate this, we walk along some blocks downtown and count the number of businesses that are closed and open. To decide which blocks to walk, we open a map, start at the lake, and then pick every 10th street. This type of sampling is (pick one)?\n\nCluster sampling.\nSystematic sampling.\nStratified sampling.\nConvenience sampling.\n\nPlease name some reasons why you may wish to use cluster sampling (select all)?\n\nBalance in responses.\nAdministrative convenience.\nEfficiency in terms of money.\nUnderlying systematic concerns.\nEstimation of sub-populations.\n\nPlease consider Beaumont, 2020, ‘Are probability surveys bound to disappear for the production of official statistics?’. With reference to that paper, do you think that probability surveys will disappear, and why or why not (please write a paragraph or two)?\nIn your own words, what is an API (write a paragraph or two)?\nFind two APIs and discuss how you could use them to tell interesting stories (write a paragraph or two for each)?\nFind two APIs that have an R packages written around them. How could you use these to tell interesting stories (write a paragraph or two)?\nWhat is the main argument to httr::GET() (pick one)?\n\n‘url’\n‘website’\n‘domain’\n‘location’\n\nWhat are three reasons why we should be respectful when getting scraping data from websites (write a paragraph or two)?\nWhat features of a website do we typically take advantage of when we parse the code (pick on)?\n\nHTML/CSS mark-up.\nCookies.\nFacebook beacons.\nCode comments.\n\nWhat are three advantages and three disadvantages of scraping compared with using an API (write a paragraph or two)?\nWhat are three delimiters that could be useful when trying to bring order to the PDF that you read in as a character vector (write a paragraph or two)?\nWhich of the following, used as part of a regular expression, would match a full stop (hint: see the ‘strings’ cheat sheet) (pick one)?\n\n‘.’\n‘.’\n‘\\.’\n‘\\.’\n\nName three reasons for sketching out what you want before starting to try to extract data from a PDF (write a paragraph or two for each)?\nWhat are three checks that we might like to use for demographic data, such as the number of births in a country in a particular year (write a paragraph or two for check)?\nWhat are three checks that we might like to use for economic data, such as GDP for a particular country in a particular year (write a paragraph or two for check)?\nWhat does the purrr package do (select all that apply)?\n\nEnhances R’s functional programming toolkit.\nMakes loops easier to code and read.\nChecks the consistency of datasets.\nIdentifies issues in data structures and proposes replacements.\n\nWhich of these are functions from the purrr package (select all that apply)?\n\nmap()\nwalk()\nrun()\nsafely()\n\nWhat are some principles to follow when scraping (select all that apply)?\n\nAvoid it if possible\nFollow the site’s guidance\nSlow down\nUse a scalpel not an axe.\n\n\nWhat is a robots.txt file (pick one)?\n\nThe instructions that Frankenstein followed.\nNotes that web scrapers should follow when scraping.\n\nWhat is the html tag for an item in list (pick one)?\n\nli\nbody\nb\nem\n\nWhich function should we use if we have the following text data: ‘rohan_alexander’ in a column called ‘names’ and want to split it into first name and surname based on the underbar (pick one)?\n\nseparate()\nslice()\nspacing()\ntext_to_columns()\n\n\n\n\n9.5.2 Tutorial\nPlease redo the web scraping example, but for one of: Australia, Canada, India, or New Zealand.\nPlan, gather, and clean the data, and then use it to create a similar table to the one created above. Write a few paragraphs about your findings. Then write a few paragraphs about the data source, what you gathered, and how you went about it. What took longer than you expected? When did it become fun? What would you do differently next time you do this? Your submission should be at least two pages and likely more.\nPlease submit a link to a PDF produced using R Markdown that includes a link to the GitHub repo.\n\n\n\n\n\nAlexander, Rohan, and Monica Alexander. 2021. “The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament Between 1901 and 2018.” https://arxiv.org/abs/2111.09299.\n\n\nBronte, Charlotte. 1847. Jane Eyre. https://www.gutenberg.org/files/1260/1260-h/1260-h.htm.\n\n\nCirone, Alexandra, and Arthur Spirling. 2021. “Turning History into Data: Data Collection, Measurement, and Inference in HPE.” Journal of Historical Political Economy 1 (1): 127–54. https://doi.org/10.1561/115.00000005.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. http://www.jstatsoft.org/v40/i03/.\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nJohnson, Kaneesha R. 2021. “Two Regimes of Prison Data Collection.” Harvard Data Science Review, July. https://doi.org/10.1162/99608f92.72825001.\n\n\nKearney, Michael W. 2019. “Rtweet: Collecting and Analyzing Twitter Data.” Journal of Open Source Software 4 (42): 1829. https://doi.org/10.21105/joss.01829.\n\n\nLuscombe, Alex, Kevin Dick, and Kevin Walby. 2021. “Algorithmic Thinking in the Public Interest: Navigating Technical, Legal, and Ethical Hurdles to Web Scraping in the Social Sciences.” Quality & Quantity, 1–22.\n\n\nOoms, Jeroen. 2014. “The Jsonlite Package: A Practical and Consistent Mapping Between JSON Data and r Objects.” arXiv:1403.2805 [Stat.CO]. https://arxiv.org/abs/1403.2805.\n\n\n———. 2018a. Pdftools: Text Extraction, Rendering and Converting of PDF Documents. https://CRAN.R-project.org/package=pdftools.\n\n\n———. 2018b. Tesseract: Open Source OCR Engine. https://CRAN.R-project.org/package=tesseract.\n\n\n———. 2019a. Pdftools: Text Extraction, Rendering and Converting of PDF Documents. https://CRAN.R-project.org/package=pdftools.\n\n\n———. 2019b. Tesseract: Open Source OCR Engine. https://CRAN.R-project.org/package=tesseract.\n\n\nPavlik, Kaylin. 2019. “Understanding + Classifying Genres Using Spotify Audio Features.” https://www.kaylinpavlik.com/classifying-songs-genres/.\n\n\nSalganik, Matthew J, Peter Sheridan Dodds, and Duncan J Watts. 2006. “Experimental Study of Inequality and Unpredictability in an Artificial Cultural Market.” Science 311 (5762): 854–56.\n\n\nThe Economist. 2022. What Spotify Data Show about the Decline of English. https://www.economist.com/interactives/graphic-detail/2022/01/29/what-spotify-data-show-about-the-decline-of-english.\n\n\nThompson, Charlie, Josiah Parry, Donal Phipps, and Tom Wolff. 2020. Spotifyr: R Wrapper for the ’Spotify’ Web API. http://github.com/charlie86/spotifyr.\n\n\nWickham, Hadley. 2019a. Babynames: US Baby Names 1880-2017. https://CRAN.R-project.org/package=babynames.\n\n\n———. 2019b. Httr: Tools for Working with URLs and HTTP. https://CRAN.R-project.org/package=httr.\n\n\n———. 2019c. Rvest: Easily Harvest (Scrape) Web Pages. https://CRAN.R-project.org/package=rvest.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2020. Usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\nWickham, Hadley, Jim Hester, and Jeroen Ooms. 2021. Xml2: Parse XML. https://CRAN.R-project.org/package=xml2.\n\n\nWong, Julia Carrie. 2020. One Year Inside Trump’s Monumental Facebook Campaign."
  },
  {
    "objectID": "10-hunt.html",
    "href": "10-hunt.html",
    "title": "10  Hunt data",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "10-hunt.html#experiments-and-randomized-controlled-trials",
    "href": "10-hunt.html#experiments-and-randomized-controlled-trials",
    "title": "10  Hunt data",
    "section": "10.1 Experiments and randomized controlled trials",
    "text": "10.1 Experiments and randomized controlled trials\n\n10.1.1 Introduction\nRonald Fisher, the twentieth century statistician, and Francis Galton, the nineteenth century statistician, are the intellectual grandfathers of much of the work that we cover in this chapter. In some cases it is directly their work, in other cases it is work that built on their contributions. Both men believed in eugenics, amongst other things that are generally reprehensible. In the same way that art history must acknowledge, say Caravaggio as a murderer, while also considering his work and influence, so to must statistics and the data sciences more generally concern themselves with this past, at the same time as we try to build a better future.\nThis chapter is about experiments. This is a situation in which we can explicitly control and vary that which we are interested in. The advantage of this is that identification should be clear. There is a treatment group that is subject to that which we are interested in, and a control group that is not. These are randomly split before treatment. And so, if they end up different then it must be because of the treatment. Unfortunately, life is rarely so smooth. Arguing about how similar the treatment and control groups were tends to carry on indefinitely. Our ability to speak to whether we have measured the effect of the treatment, affects our ability to speak to what effect that treatment could have.\n\nIn this chapter we cover experiments, especially constructing treatment and control groups, and appropriately considering their results. We discuss some aspects of ethical behaviour in experiments through reference to the abhorrent Tuskegee Syphilis Study and ECMO. And we go through the Oregon Health Insurance Experiment as a case study. We then turn to A/B testing, which is extensively used in industry, and consider a case study based on Upworthy data. Finally, we go through actually implementing a survey using Google Forms. \n\n\n10.1.2 Motivation and notation\nProfessional sports are a big deal in North America. Consider the situation of someone who moves to San Francisco in 2014, such that as soon as they moved the Giants win the World Series and the Golden State Warriors begin a historic streak of World Championships. They move to Chicago, and immediately the Cubs win the World Series for the first time in a hundred years. They move to Massachusetts, where the Patriots win the Super Bowl again, and again, and again. And finally, they move to Toronto, where the Raptors immediately win the World Championship. Should a city pay them to move, or could municipal funds be better spent elsewhere?\nOne way to get at the answer would be to run an experiment. Make a list of the North American cities with major sports teams, and then roll a dice and send them to live there for a year. With enough lifetimes, we could work it out. The fundamental issue is that we cannot both live in a city and not live in a city. This is the fundamental problem of causal inference: a person cannot be both treated and untreated. Experiments and randomized controlled trials are circumstances in which we try to randomly allocate some treatment, to have a belief that everything else was the same (or at least ignorable). The framework that we use to formalize the situation is the Neyman-Rubin model (Holland 1986).\nA treatment, \\(t\\), will often be a binary variable that is either 0 or 1. It is 0 if the person, \\(i\\), is not treated, which is to say they are in the control group, and 1 if they are treated. We will typically have some outcome, \\(Y_i\\), of interest for that person, and that could be binary, multinomial, or continuous. For instance, it could be vote choice, in which case we could measure whether the person is: ‘Conservative’ or ‘Not Conservative’; which party they support, say: ‘Conservative’, ‘Liberal’, ‘Democratic’, ‘Green’; or maybe a probability of support.\nA treatment is causal if \\((Y_i|t=0) \\neq (Y_i|t=1)\\). That is to say, that the outcome for person \\(i\\), given they were not treated, is different to their outcome given they were treated. If we could both treat and control the one individual at the one time, then we would know that it was only the treatment that had caused any change in outcome, as there is no other factor that could explain it. But the fundamental problem of causal inference is that we cannot both treat and control the one individual at the one time. So when we want to know the effect of the treatment, we need to compare it with a counterfactual. The counterfactual is what would have happened if the individual were not treated. As it turns out, this means one way to think of causal inference is as a missing data problem, where we are missing the counterfactual.\nAs we cannot compared treatment and control in one individual, we instead compare the average of two groups—all those treated and all those not. We are looking to estimate the counterfactual at a group level because of the impossibility of doing it at an individual level. Making this trade-off allows us to move forward but comes at the cost of certainty. We must instead rely on randomization, probabilities, and expectations.\nWe usually consider a default of there being no effect and we look for evidence that would cause us to change our mind. As we are interested in what is happening in groups, we turn to expectations, and notions of probability to express ourselves. Hence, we will make claims that talk, on average. Maybe wearing fun socks really does make you have a lucky day, but on average, across the group, it is probably not the case.\nIt is worth pointing out that we do not just have to be interested in the average effect. We may consider the median, or variance, or whatever. Nonetheless, if we were interested in the average effect, then one way to proceed would be to:\n\ndivide the dataset into two—treated and not treated—and have a binary effect column;\nsum the column, then divide it by the length of the column; and\nthen look at the ratio.\n\nThis is an estimator, touched on in Chapter @ref(on-writing), which is a way of putting together a guess of something of interest. The estimand is the thing of interest, in this case the average effect, and the estimate is whatever our guess turns out to be. We can simulate data to illustrate the situation.\n\nlibrary(tidyverse)\n\nset.seed(853)\n\ntreatment_control <- \n  tibble(\n    binary_effect = sample(x = c(0, 1), size = 10, replace = TRUE)\n  )\n\ntreatment_control\n\n# A tibble: 10 × 1\n   binary_effect\n           <dbl>\n 1             0\n 2             1\n 3             1\n 4             0\n 5             0\n 6             0\n 7             0\n 8             0\n 9             1\n10             1\n\n\n\nestimate <-\n  sum(treatment_control$binary_effect) / length(treatment_control$binary_effect)\nestimate\n\n[1] 0.4\n\n\nMore broadly, to tell causal stories we need to bring together both theory and a detailed knowledge of what we are interested in (Cunningham 2021, 4). In Chapter @ref(gather-data) we discussed gathering data that we observed about the world. In this chapter we are going to be more active about turning the world into the data that we need. As the researcher we will decide what to measure and how, and we will need to define what we are interested in and what we are not. We will be active participants in the data generating process. That is, if we want to use this data, then as researchers we must go out and hunt it, if you like.\n\n\n10.1.3 Randomization\nCorrelation can be enough in some settings, but in order to be able to make forecasts when things change, and the circumstances are slightly different we need to understand causation. The key is the counterfactual: what would have happened in the absence of the treatment. Ideally, we could keep everything else constant, randomly divide the world into two groups, and treat one and not the other. Then we can be pretty confident that any difference between the two groups is due to that treatment. The reason for this is that if we have some population and we randomly select two groups from it, then our two groups (provided they are both big enough) should have the same characteristics as the population. Randomized controlled trials (RCTs) and A/B testing attempt to get us as close to this ‘gold standard’ as we can hope. When we, and others such as Athey and Imbens (2017), use language like gold standard to refer to these approaches, we do not mean to imply that they are perfect. Just that they can be better than most of the other options.\nWhat we hope to be able to do is to find treatment and control groups that are the same, but for the treatment. This means that establishing the control group is critical because when we do that, we establish the counterfactual. We might be worried about, say, underlying trends, which is one issue with a before-and-after comparison, or selection bias, which could occur when we allow self-selection. Either of these issues could result in biased estimators. We use randomization to go some way to addressing these.\nTo get started, we simulate a population, and then randomly sample from it. We will set it up so that half the population likes blue, and the other half likes white. And further, if someone likes blue then they almost surely prefer dogs, but if they like white then they almost surely prefer cats. The approach of heavily using simulation is a critical part of the workflow advocated in this book. This is because we know roughly what the outcomes should be from the analysis of simulated data. Whereas if we go straight to analyzing the real data then we do not know if unexpected outcomes are due to our own analysis errors, or actual results. Another good reason it is useful to take this approach of simulation is that when you are working in teams the analysis can get started before the data collection and cleaning is completed. That simulation will also help the collection and cleaning team think about tests they should run on their data.\n\nset.seed(853)\n\nnumber_of_people <- 5000\n\npopulation <-\n  tibble(\n    person = c(1:number_of_people),\n    favorite_color = sample(\n      x = c(\"Blue\", \"White\"),\n      size  = number_of_people,\n      replace = TRUE\n    )\n  ) |>\n  mutate(\n    prefers_dogs_to_cats =\n      if_else(favorite_color == \"Blue\", \"Yes\", \"No\"),\n    noise = sample(1:10, size = 1),\n    prefers_dogs_to_cats =\n      if_else(\n        noise <= 8,\n        prefers_dogs_to_cats,\n        sample(c(\"Yes\", \"No\"), \n               size = 1)\n        )\n  ) |> \n  select(-noise)\n\n\npopulation\n\n# A tibble: 5,000 × 3\n   person favorite_color prefers_dogs_to_cats\n    <int> <chr>          <chr>               \n 1      1 Blue           Yes                 \n 2      2 White          No                  \n 3      3 White          No                  \n 4      4 Blue           Yes                 \n 5      5 Blue           Yes                 \n 6      6 Blue           Yes                 \n 7      7 Blue           Yes                 \n 8      8 Blue           Yes                 \n 9      9 White          No                  \n10     10 White          No                  \n# … with 4,990 more rows\n\npopulation |>\n  group_by(favorite_color) |> \n  count()\n\n# A tibble: 2 × 2\n# Groups:   favorite_color [2]\n  favorite_color     n\n  <chr>          <int>\n1 Blue            2547\n2 White           2453\n\n\nWe will now construct a frame, assuming that we have a frame that contains 80 per cent of the population.\n\nset.seed(853)\n\nframe <-\n  population |>\n  mutate(\n    in_frame = sample(\n      x = c(0, 1),\n      size  = number_of_people,\n      replace = TRUE,\n      prob = c(0.2, 0.8)\n  )) |>\n  filter(in_frame == 1)\n\nframe |>\n  group_by(favorite_color) |> \n  count()\n\n# A tibble: 2 × 2\n# Groups:   favorite_color [2]\n  favorite_color     n\n  <chr>          <int>\n1 Blue            2023\n2 White           1980\n\n\nFor now, we will set aside dog or cat preferences and focus on creating treatment and control groups on the basis of favorite color only.\n\nset.seed(853)\n\nsample <-\n  frame |>\n  select(-prefers_dogs_to_cats) |>\n  mutate(group = sample(\n    x = c(\"Treatment\", \"Control\"),\n    size  = nrow(frame),\n    replace = TRUE\n  ))\n\nWhen we look at the mean for the two groups, we can see that the proportions that prefer blue or white are very similar to what we specified (?tbl-bluetowhite).\n\nsample |>\n  group_by(group, favorite_color) |>\n  count() |>\n  ungroup() |>\n  group_by(group) |>\n  mutate(prop = n / sum(n)) |>\n  knitr::kable(\n    col.names = c(\"Group\", \"Preferred color\", \"Number\", \"Proportion\"),\n    digits = 2,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\n\n\nProportion of the treatment and control group that prefer blue or white \n\n\nGroup\nPreferred color\nNumber\nProportion\n\n\n\n\nControl\nBlue\n987\n0.50\n\n\nControl\nWhite\n997\n0.50\n\n\nTreatment\nBlue\n1036\n0.51\n\n\nTreatment\nWhite\n983\n0.49\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe randomized based on favorite color. But we should also find that we took dog or cat preferences along at the same time and will have a ‘representative’ share of people who prefer dogs to cats. Why should that happen when we have not randomized on these variables? Let’s start by looking at our dataset (Table 10.1).\n\nsample |> \n  left_join(frame |> select(person, prefers_dogs_to_cats), \n            by = \"person\") |>\n  group_by(group, prefers_dogs_to_cats) |> \n  count() |>\n  ungroup() |>\n  group_by(group) |>\n  mutate(prop = n / sum(n)) |>\n  knitr::kable(\n    col.names = c(\"Group\", \"Prefers dogs to cats\", \"Number\", \"Proportion\"),\n    digits = 2,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\n\n\n\nTable 10.1: Proportion of the treatment and control group that prefer dogs or cats\n\n\nGroup\nPrefers dogs to cats\nNumber\nProportion\n\n\n\n\nControl\nNo\n997\n0.50\n\n\nControl\nYes\n987\n0.50\n\n\nTreatment\nNo\n983\n0.49\n\n\nTreatment\nYes\n1036\n0.51\n\n\n\n\n\n\nIt is exciting to have a representative share on ‘unobservables’. In this case, we do ‘observe’ them—to illustrate the point—but we did not select on them. We get this because the variables were correlated. But it will break down in several ways that we will discuss. It also assumes large enough groups. For instance, if we considered specific dog breeds, instead of dogs as an entity, we may not find ourselves in this situation. To check that the two groups are the same we look to see if we can identify a difference between the two groups based on observables. In this case we looked at the mean, but we could look at other aspects as well.\nThis all brings us to Analysis of Variation (ANOVA). ANOVA was introduced by Fisher while he was working on statistical problems in agriculture. This is less unexpected than it may seem as historically agricultural research has been closely tied to statistical innovation. We mention ANOVA here because of its importance historically, but it is a variant of linear regression which we cover in some detail in Chapter @ref(ijalm). Further, in general, we would usually not use ANOVA day-to-day. There is nothing wrong with it in the right circumstances. But it is more than a hundred years old and the number of modern use-case where it is the best option is small.\nIn any case, we approach ANOVA with the expectation that the groups are from the same distribution and could conduct it using aov(). In this case, we would fail to reject our default hypothesis that the samples are the same.\n\n\n10.1.4 Treatment and control\nIf the treated and control groups are the same in all ways and remain that way, but for the treatment, then we have internal validity, which is to say that our control will work as a counterfactual and our results can speak to a difference between the groups in that study. Internal validity means that our estimates of the effect of the treatment are speaking to the treatment and not some other aspect. They mean that we can use our results to make claims about what happened in the experiment.\nIf the group to which we applied our randomization were representative of the broader population, and the experimental set-up were fairly similar to outside conditions, then we further could have external validity. That would mean that the difference we find does not just apply in our own experiment, but also in the broader population. External validity means that we can use our experiment to make claims about what would happen outside the experiment. It is randomization that has allowed that to happen.\nBut this means we need randomization twice. Firstly, into the group that was subject to the experiment, and then secondly, between treatment and control. How do we think about this randomization, and to what extent does it matter?\nWe are interested in the effect of being treated. It may be that we charge different prices, which would be a continuous treatment variable, or that we compare different colors on a website, which would be a discrete treatment variable. Either way, we need to make sure that all the groups are otherwise the same. How can we be convinced of this? One way is to ignore the treatment variable and to examine all other variables, looking for whether we can detect a difference between the groups based on any other variables. For instance, if we are conducting an experiment on a website, then are the groups roughly similar in terms of, say:\n\nMicrosoft and Apple users?\nSafari, Chrome, and Firefox users?\nMobile and desktop users?\nUsers from certain locations?\n\nFurther, are the groups representative of the broader population? These are all threats to the validity of our claims.\nBut if done properly, that is if the treatment is truly independent, then we can estimate the average treatment effect (ATE). In a binary treatment variable setting this is:\n\\[\\mbox{ATE} = \\mathbb{E}[Y|t=1] - \\mathbb{E}[Y|t=0].\\]\nThat is, the difference between the treated group, \\(t = 1\\), and the control group, \\(t = 0\\), when measured by the expected value of the outcome, \\(Y\\). The ATE becomes the difference between the two expectations.\nTo illustrate this concept, we first simulate some data that shows a difference of one between the treatment and control groups.\n\nset.seed(853)\n\nate_example <- tibble(person = c(1:1000),\n                      was_treated = sample(\n                        x = c(\"Yes\", \"No\"),\n                        size  = 1000,\n                        replace = TRUE\n                      ))\n\n# Make outcome a bit more likely if treated.\nate_example <-\n  ate_example |>\n  rowwise() |>\n  mutate(outcome = if_else(\n    was_treated == \"No\",\n    rnorm(n = 1, mean = 5, sd = 1),\n    rnorm(n = 1, mean = 6, sd = 1)\n  ))\n\nWe can see the difference, which we simulated to be one, between the two groups in Figure (Figure 10.1). And we can compute the average between the groups and then the difference to see also that we get back the result that we put in (Table 10.2).\n\nate_example |>\n  ggplot(aes(x = outcome,\n             fill = was_treated)) +\n  geom_histogram(position = \"dodge\",\n                 binwidth = 0.2) +\n  theme_minimal() +\n  labs(x = \"Outcome\",\n       y = \"Number of people\",\n       fill = \"Person was treated\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\nFigure 10.1: Simulated data showing a difference between the treatment and control group\n\n\n\n\n\nate_example |>\n  group_by(was_treated) |>\n  summarize(mean = mean(outcome)) |>\n  pivot_wider(names_from = was_treated, values_from = mean) |>\n  mutate(difference = Yes - No) |>\n  knitr::kable(\n    col.names = c(\"Average for treated\", \"Average for not treated\", \"Difference\"),\n    digits = 2,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\n\n\n\nTable 10.2: Average difference between the treatment and control groups for data simulated to have an average difference of one\n\n\nAverage for treated\nAverage for not treated\nDifference\n\n\n\n\n5\n6.06\n1.06\n\n\n\n\n\n\nUnfortunately, there is often a difference between simulated data and reality. For instance, an experiment cannot run for too long otherwise people may be treated many times, or become inured to the treatment; but it cannot be too short otherwise we cannot measure longer term outcomes. We cannot have a ‘representative’ sample across every facet of a population, but if not, then the treatment and control will be different. Practical difficulties may make it difficult to follow up with certain groups and so we end up with a biased collection. Some questions to explore when working with real experimental data include:\n\nHow are the participants being selected into the frame for consideration?\nHow are they being selected for treatment? We would hope this is being done randomly, but this term is applied to a variety of situations. Additionally, early ‘success’ can lead to pressure to treat everyone, especially in medical settings.\nHow is treatment being assessed?\nTo what extent is random allocation ethical and fair? Some argue that shortages mean it is reasonable to randomly allocate, but that may depend on how linear the benefits are. It may also be difficult to establish definitions, and the power imbalance between those making these decisions and those being treated should be considered.\n\nBias and other issues are not the end of the world. But we need to think about them carefully. In a well-known example, Abraham Wald, the twentieth century Hungarian mathematician, was given data on the planes that came back to Britain after being shot at in WW2. The question was where to place the armor. One option was to place it over the bullet holes. Wald recognized that there is a selection effect here—these are the planes that made it back to be examined—so those holes did not necessarily need the armor. Arguably armor would be better placed where there were no bullet holes.\nFor instance, how would the results of a survey about the difficulty of a university course differ if only students who completed the course were surveyed, and not those who dropped out? While we should work to try to make our dataset as good as possible, it may be possible to use the model to control for some of the bias. For instance, if there was a variable that was correlated with say, attrition, then it could be added to the model either by-itself, or as an interaction. Similarly, if there was correlation between the individuals. For instance, if there was some ‘hidden variable’ that we did not know about that meant some individuals were correlated, then we could use ‘wider’ standard errors. This needs to be done carefully and we discuss this further in Chapter @ref(causality). That said, if such issues can be anticipated, then it can be better to change the experiment. For instance, perhaps it would be possibly to stratify by that hidden variable.\n\n\n\n10.1.5 Fisher’s tea party\nFisher introduced an experiment designed to see if a person can distinguish between a cup of tea where the milk was added first, or last. We begin by preparing eight cups of tea: four with milk added first and the other four with milk added last. We then randomize the order of all eight cups. We tell the taster, whom we will call ‘Ian’, about the experimental set-up: there are eight cups of tea, four of each type, he will be given cups of tea in a random order, and the task is to group them into two groups.\nOne of the nice aspects of this experiment is that we can do it ourselves. There are a few things to be careful of in practice, including that: the quantities of milk and tea and consistent; the groups are marked in some way that the taster cannot see; and the order is randomized.\nAnother nice aspect of this experiment is that we can calculate the chance that Ian is able to randomly get the groupings correct. To decide if his groupings were likely to have occurred at random, we need to calculate the probability this could happen. First, we count the number of successes out of the four that were chosen. Fisher (1935, 14) says there are: \\({8 \\choose 4} = \\frac{8!}{4!(8-4)!}=70\\) possible outcomes.\nWe are asking Ian to group the cups, not to identify which is which, and so there are two ways for him to be perfectly correct. He could either correctly identify all the ones that were milk-first (one outcome out of 70) or correctly identify all the ones that were tea-first (one outcome out of 70). This means the probability of this event is: \\(\\frac{2}{70} \\approx 0.028\\) or about 3 per cent.\nAs Fisher (1935, 15) makes clear, this now becomes a judgement call. We need to consider the weight of evidence that we require before we accept the groupings did not occur by chance and that Ian was well-aware of what he was doing. We need to decide what evidence it takes for us to be convinced. If there is no possible evidence that would dissuade us from the view that we held coming into the experiment, say, that there is no difference between milk-first and tea-first, then what is the point of doing an experiment? We would expect that if Ian got it completely right, then most would accept that he was able to tell the difference.\nWhat if he is almost perfect? By chance, there are 16 ways for a person to be ‘off-by-one’. Either Ian thinks there was one cup that was milk-first when it was tea-first—there are, \\({4 \\choose 1} = 4\\), four ways this could happen—or he thinks there was one cup that was tea-first when it was milk-first—again, there are, \\({4 \\choose 1}\\) = 4, four ways this could happen. These outcomes are independent, so the probability is \\(\\frac{4\\times 4}{70} \\approx 0.228\\). And so on. Given there is an almost 23 per cent chance of being off-by-one just be randomly grouping the teacups, this outcome probably would not convince us that Ian can tell the difference between tea-first and milk-first.\nWhat we are looking for, in order to claim something is experimentally demonstrable is the results of not just it being shown once, but instead we know the features of an experiment where such a result is reliably found (Fisher 1935, 16). We are looking to thoroughly interrogate our data and our experiments, and to think precisely about the analysis methods we are using. Rather than searching for meaning in constellations of stars, we want to make it as easy as possible for others to reproduce our work. It is only in that way that our conclusions stand a chance of holding up in the long-term.\n\n\n10.1.6 Informed consent and the need for an experiment\nOne of the foundations of ethical experimental practice is informed consent and ensuring that an experiment is actually needed. We will now detail two cases where human life was potentially lost due to these issues. One issue with experiments in medical settings is that the weight of evidence is measured in lost lives. Ethical practice in experiments develops because of the many people who may have unnecessarily lost their life due to experiments. Two cases that have dramatically informed practice are the Tuskegee Syphilis Study and ECMO.\nThe Tuskegee Syphilis Study is an infamous medical trial that began in 1932. As part of this experiment 400 Black Americans with syphilis, and a control group without, were not given appropriate treatment, nor even told they had syphilis (in the case of the treatment group), well after a standard treatment for syphilis was established and widely available sometime between the mid-1940s and early 1950s (Brandt 1978; Alsan and Wanamaker 2018). Like the treatment group, the control group were also given non-effective drugs. These financially-poor Black Americans in the US South were identified and offered compensation including ‘hot meals, the guise of treatment, and burial payments’ (Alsan and Wanamaker 2018). The men were not actually treated for syphilis (Brandt 1978; Alsan and Wanamaker 2018). The men were not told they were part of an experiment (Brandt 1978; Alsan and Wanamaker 2018). Further, extensive work was undertaken to ensure the men would not receive treatment from anywhere including writing to local doctors, the local health department, and, incredibly, after some of the men were drafted and told to immediately get treatment, the draft board complied with a request to have the men excluded from treatment (Brandt 1978, 25). By the time the study was stopped in 1972, more than half of the men were deceased and many of deaths were from syphilis-related causes (Alsan and Wanamaker 2018).\nThe effect of the Tuskegee Syphilis Study was felt not just by the men in the study, but more broadly. Alsan and Wanamaker (2018) found that it is associated with a decrease in life expectancy at age 45 of up to 1.5 years for Black men. In response the US established requirements for Institutional Review Boards and President Clinton made a formal apology in 1997. Brandt (1978, 27) says:\n\nIn retrospect the Tuskegee Study revealed more about the pathology of racism than the pathology of syphilis; more about the nature of scientific inquiry than the nature of the disease process… [T]he notion that science is a value-free discipline must be rejected. The need for greater vigilance in assessing the specific ways in which social values and attitudes affect professional behavior is clearly indicated.\n\nTurning to the evaluation of extracorporeal membrane oxygenation (ECMO), J. H. Ware (1989) describes how they viewed ECMO as a possible treatment for persistent pulmonary hypertension in newborns (PPHN). They enrolled 19 patients and used conventional medical therapy on ten of them, and ECMO on nine of them. It was found that six of the ten in the control group survived while all in the treatment group survived. J. H. Ware (1989) used randomized consent whereby only the parents of infants randomly selected to be treated with ECMO were asked to consent.\nJ. H. Ware (1989) are concerned with ‘equipoise’, by which they refer to a situation in which there is genuine uncertainty about whether the treatment is more effective than existing procedures. They further note that in medical settings even if there is initial equipoise it could be undermined if the treatment is found to be effective early in the study. J. H. Ware (1989) describe how after the results of these first 19 patients, randomization stopped and only ECMO was used. The recruiters and those treating the patients were initially not told that randomization had stopped. It was decided that this complete allocation to ECMO would continue ‘until either the 28th survivor or the 4th death was observed’. After 19 of 20 additional patients survived ECMO the trial was terminated. So, the actual result of the experiment was divided into two phases: in the first there was randomized use of ECMO, and in the second only ECMO was used.\nOne approach in these settings is a ‘randomized play-the-winner’ rule following Wei and Durham (1978). Treatment is still randomized, but the weight of probability shifts with each successful treatment to make treatment more likely and there is some stopping rule. Berry (1989) argues that the stopping rule in the case of J. H. Ware (1989) occurred before the study started and that there was no need for the study at all because equipoise never existed. Berry (1989) re-visit the literature mentioned by J. H. Ware (1989) and find extensive evidence that ECMO was known to be effective. Berry (1989) points out that there is almost never complete consensus and so one could almost always argue for the existence of equipoise even in the face of a substantial weight of evidence. Berry (1989) further criticizes J. H. Ware (1989) for the use of randomized consent because of the potential that there may have been different outcomes for the infants subject to conventional therapy had their parents known there were other options. And instead, Berry (1989) argues for the need for comprehensive patient registries, enabling the analysis of large datasets.\nWhile the Tuskegee Syphilis Study and ECMO may seem quite far from our present circumstances, Dr Monica Alexander, Assistant Professor, University of Toronto explains that while it may be illegal to do this exact research these days, it does not mean that unethical research does not still happen. We see it all the time in machine learning applications in health and other areas. While we are not meant to explicitly discriminate and we are meant to get consent, it does not mean that we cannot implicitly discriminate without any type of buy-in at all. For instance, Obermeyer et al. (2019) describes how US health care systems use algorithms to score the severity of how sick a patient is. They show that for the same score, ‘Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses’ and that if Black patients were scored in the same way as White patients, then they would receive considerably more help than they do now. They find that the discrimination occurs because the algorithm is based on health care costs, rather than sickness. But because access to healthcare is unequally distributed between Black and White patients, the algorithm, however inadvertently, perpetuates racial bias.\n\n\n10.1.7 Case study: The Oregon Health Insurance Experiment\nIn the US, unlike many developed countries, basic health insurance is not necessarily available to all residents even those on low incomes. The Oregon Health Insurance Experiment involved low-income adults in Oregon, a state in the north-west of the US, from 2008 to 2010 (Finkelstein et al. 2012).\nOregon funded 10,000 places in the state-run Medicaid program, which provides health insurance for people with low incomes. A lottery was used to allocate these places and was judged fair because it was expected, correctly as it turned out, that demand for places would exceed the supply. People had a month to sign up to enter the draw. Then a lottery was used to determine which of the 89,824 individuals who signed up would be allowed to apply for Medicaid.\nThe draws were conducted over a six-month period and those who were selected had the opportunity to sign up. 35,169 individuals were selected (the household of those who actually won the draw was given the opportunity) but only 30 per cent of them completed the paperwork and were eligible (typically they earned too much). The insurance lasted indefinitely. This random allocation of insurance allowed the researchers to understand the effect of health insurance.\nThe reason that this random allocation is important is that it is not usually possible to compare those with and without insurance because the type of people that sign up to get health insurance differ to those who do not. That decision is ‘confounded’ with other variables and results in a selection effect.\nAs the opportunity to apply for health insurance was randomly allocated, the researchers were able to evaluate the health and earnings of those who received health insurance and compare them to those who did not. To do this they used administrative data, such as hospital discharge data, credit reports that were matched to 68.5 per cent of lottery participants, and mortality records, which will be uncommon. Interestingly this collection of data is fairly restrained and so they included a survey conducted via mail.\nThe specifics of this are not important, and we will have more to say in Chapter @ref(ijalm), but they use a statistical model, ?eq-oregon, to analyze the results (Finkelstein et al. 2012):\n\\[\\begin{equation}\ny_{ihj} = \\beta_0 + \\beta_1\\mbox{Lottery} + X_{ih}\\beta+2 + V_{ih}\\beta_3 + \\epsilon_{ihj} {#eq-oregon}\n\\end{equation}\\]\nEquation @ref(eq:oregon) explains various \\(j\\) outcomes (such as health) for an individual \\(i\\) in household \\(h\\) as a function of an indicator variable as to whether household \\(h\\) was selected by the lottery. Hence, ‘(t)he coefficient on Lottery, \\(\\beta_1\\), is the main coefficient of interest, and gives the average difference in (adjusted) means between the treatment group (the lottery winners) and the control group (those not selected by the lottery).’\nTo complete the specification of Equation @ref(eq:oregon), \\(X_{ih}\\) is a set of variables that are correlated with the probability of being treated. These adjust for that impact to a certain extent. An example of that is the number of individuals in a household. And finally, \\(V_{ih}\\) is a set of variables that are not correlated with the lottery. These variables include demographics, hospital discharge and lottery draw.\nAs has been found in earlier studies such as Brook et al. (1984), Finkelstein et al. (2012) found that, the treatment group was 25 per cent more likely to have insurance than the control group. The treatment group used more health care including both primary and preventive care as well as hospitalizations but had lower out-of-pocket medical expenditures. More generally, the treatment group reported better physical and mental health."
  },
  {
    "objectID": "10-hunt.html#ab-testing",
    "href": "10-hunt.html#ab-testing",
    "title": "10  Hunt data",
    "section": "10.2 A/B testing",
    "text": "10.2 A/B testing\nThe past decade has probably seen the most experiments ever run by several orders of magnitude with the extensive use of A/B testing on websites. Large tech companies typically have extensive infrastructure for these experiments, and they term them A/B tests because of the comparison of two groups: one that gets treatment A and the other that either gets treatment B or does not see any change (Salganik 2018, 185). Every time you are online you are probably subject to tens, hundreds, or potentially thousands, of different A/B tests. If you use apps like TikTok then this could run to the tens of thousands. While, at their heart, they are still just surveys that result in data that need to be analysed, they have several interesting features that we will discuss.\nFor instance, Kohavi, Tang, and Xu (2020, 3) discusses the example of Microsoft’s search engine Bing where they increased the amount of content displayed by their ads. The change triggered an alert that usually signaled a bug in the billing. But there was no bug, it was instead the case that revenue would increase by 12 per cent, or around $100 million annually in the US, without any significant trade-off being measured.\nWe use the term A/B test to strictly refer to the situation in which we are primarily implementing an experiment through a technology stack about something that is primarily of the internet, for instance a change to a website or similar. While at their heart they are just experiments, A/B testing has a range of specific concerns. There is something different about doing tens of thousands of small experiments all the time, compared with our normal experimental set-up of conducting one experiment over the course of months. Additionally, tech firms have such distinct cultures that it can be difficult to shift toward an experimental set-up. Sometimes it can be easier to experiment by not delivering, or delaying, a change that has been decided to create a control group rather than a treatment group (Salganik 2018, 188). Often the most difficult aspect of A/B testing and conducting experiments more generally, is not the statistics, it’s the politics.\nThe first aspect of concern is the delivery of the A/B test (Kohavi, Tang, and Xu 2020, 153–61). In the case of an experiment, it is usually clear how it is being delivered. For instance, we may have the person come to a doctor’s clinic and then inject them with either a drug or a placebo. But in the case of A/B testing, it is less obvious. For instance, should it be run ‘server-side’, meaning to make a change to a website, or ‘client-side’, meaning to change an app. This decision affects our ability to both conduct the experiment and to gather data from it.\nIn the case of the effect of conducting the experiment, it is relatively easy and normal to update a website all the time. This means that small changes can be easily implemented if the experiment is conducted server-side. But in the case of a client-side implementation of an app, then conducting an experiment becomes a bigger deal. For instance, the release may need to go through an app store, and this usually does not happen all the time. Instead, it would need to be part of a regular release cycle. There is also a selection concern because some users will not update their app and there is the possibility that they are different to those that do regularly update the app.\nTurning to the effect of the delivery decision on our ability to gather data from the experiment. Again, server-side is less of a big deal because we get the data anyway as part of the user interacting with the website. But in the case of an app, the user may use the app offline or with limited data upload, which then requires a data transmission protocol or caching, but this then could affect user experience, especially as some phones place limits are various aspects.\nThe effect of all this is that we need to plan. For instance, results are unlikely to be available the day after a change to an app, whereas they are likely available the day after a change to a website. Further, we may need to consider our results in the context of different devices and platforms, potentially using, say, multilevel regression which will be covered in Chapter @ref(ijalm).\nThe second aspect of concern is ‘instrumentation’ or the method of measurement (Kohavi, Tang, and Xu 2020, 162 - 165). When we conduct a traditional experiment then we might, for instance, ask respondents to fill out a survey. But this is usually not done with A/B testing. One approach is to put a cookie on the user’s device, but different users will clear these at different rates. Another approach is to use a beacon, such as forcing the user to download a tiny image from our server, so that we know when they have completed some action. For instance, this is a commonly used approach to know when a user has opened an email. There are practical concerns around when the beacon loads, for instance, if it is before the main content loads then the user experience may be worse, but if it is after then our sample may be biased.\nThe third aspect of concern is what are we randomizing over (Kohavi, Tang, and Xu 2020, 162 - 165). In the case of traditional experiments, this is usually clear and it is often a person, but sometimes various groups of people. But in the case of A/B testing it can be less clear. For instance, are we randomizing over the page, the session, or the user?\nTo think about this, let us consider color. For instance, say we are interested in whether we should change our logo from red to blue on the homepage. If we are randomizing at the page level, then if the user goes to some other page of our website, and then back to the homepage the logo could be back to red. If we are randomizing at the session level, then while it could be blue while they use the website this time, if they close it and come back then it could be red. Finally, if we are randomizing at a user level then possibly it would always be red for one used, but always blue for another.\nThe extent to which this matters depends on a trade-off between consistency and importance. For instance, if we are A/B testing product prices then consistency is likely a feature. But if we are A/B testing background colors then consistency might not be as important. On the other hand, if we are A/B testing the position of a log-in button then it might be important that we not move that around too much for the one user, but between users it might matter less.\nInterestingly, in A/B testing, as in traditional experiments, we are concerned that our treatment and control groups are the same, but for the treatment. In the case of traditional experiments, we satisfy ourselves of this by making conducting analysis on the basis of the data that we have after the experiment is conducted. That is usually all we can do because it would be weird to treat or control both groups. But in the case of A/B testing, the pace of experimentation allows us to randomly create the treatment and control groups, and then check, before we subject the treatment group to the treatment, that the groups are the same. For instance, if we were to show each group the same website, then we would expect the same outcomes across the two groups. If we found different outcomes then we would know that we may have a randomization issue (Taddy 2019, 129).\nOne of the interesting aspects of A/B testing is that we are usually running them not because we desperately care about the specific outcome, but because that feeds into some other measure that we care about. For instance, do we care whether the website is quite-dark-blue or slightly-dark-blue? Probably not, but we probably care a lot about the company share price. But then what if picking the best blue comes at a cost to the share price? That example is a bit contrived, so let us pretend that we work at a food delivery app and we are concerned with driver retention. Say we do some A/B tests and find that drivers are always more likely to be retained when they can deliver food to the customer faster. Our finding is that faster is better, for driver retention, always. But one way to achieve faster deliveries, is for them to not put the food into a hot box that would maintain the food’s temperature. Something like that might save 30 seconds, which is significant on a 10-15 minute delivery. Unfortunately, although we would decide to encourage that that on the basis of A/B tests designed to optimize driver-retention, such a decision would likely make the customer experience worse. If customers receive cold food, when it is meant to be hot, then they may stop using the app, which would ultimately be very bad for the business.\nThis trade-off may be obvious when we run the driver experiment if we were to look at customer complaints. It is possible that on a small team we would be exposed to those tickets, but on a larger team we may not be. Ensuring that A/B tests are not resulting in false optimization is especially important. And not something that we typically have to worry about in normal experiments.\n\n10.2.1 Case study: Upworthy\nThe trouble with much of A/B testing is that it is done by firms and so we typically do not have datasets that we can use. But Matias et al. (2019) provide access to a dataset of A/B tests from Upworthy, a clickbait media website that used A/B testing to optimize their content. Fitts (2014) provides more background information about Upworthy. And the datasets of A/B tests are available: https://osf.io/jd64p/.\nWe can look at what the dataset looks like, and get a sense for it by looking at the names and an extract.\n\nupworthy <- read_csv(\"https://osf.io/vy8mj/download\")\n\n\nupworthy |> \n  names()\n\n [1] \"...1\"                 \"created_at\"           \"updated_at\"          \n [4] \"clickability_test_id\" \"excerpt\"              \"headline\"            \n [7] \"lede\"                 \"slug\"                 \"eyecatcher_id\"       \n[10] \"impressions\"          \"clicks\"               \"significance\"        \n[13] \"first_place\"          \"winner\"               \"share_text\"          \n[16] \"square\"               \"test_week\"           \n\nupworthy |> \n  head()\n\n# A tibble: 6 × 17\n   ...1 created_at          updated_at          clickability_test_id     excerpt\n  <dbl> <dttm>              <dttm>              <chr>                    <chr>  \n1     0 2014-11-20 06:43:16 2016-04-02 16:33:38 546d88fb84ad38b2ce000024 Things…\n2     1 2014-11-20 06:43:44 2016-04-02 16:25:54 546d88fb84ad38b2ce000024 Things…\n3     2 2014-11-20 06:44:59 2016-04-02 16:25:54 546d88fb84ad38b2ce000024 Things…\n4     3 2014-11-20 06:54:36 2016-04-02 16:25:54 546d902c26714c6c44000039 Things…\n5     4 2014-11-20 06:54:57 2016-04-02 16:31:45 546d902c26714c6c44000039 Things…\n6     5 2014-11-20 06:55:07 2016-04-02 16:25:54 546d902c26714c6c44000039 Things…\n# … with 12 more variables: headline <chr>, lede <chr>, slug <chr>,\n#   eyecatcher_id <chr>, impressions <dbl>, clicks <dbl>, significance <dbl>,\n#   first_place <lgl>, winner <lgl>, share_text <chr>, square <chr>,\n#   test_week <dbl>\n\n\nIt is also useful to look at the documentation for the dataset. This describes the structure of the dataset, which is that there are packages within tests. A package is a collection of headlines and images that were shown randomly to different visitors to the website, as part of a test. A test can include many packages. Each row in the dataset is a package and the test that it is part of is specified by the ‘clickability_test_id’ column.\nThere are a variety of variables. We will focus on:\n\n‘created_at’,\n‘clickability_test_id’ so that we can create comparison groups,\n‘headline’,\n‘impressions’ which is the number of people that saw the package, and\n‘clicks’ which is the number that clicked on that package.\n\nWithin each batch of tests, we are interested in the effect of the varied headlines on impressions and clicks.\n\nupworthy_restricted <- \n  upworthy |> \n  select(created_at, clickability_test_id, headline, impressions, clicks)\n\nhead(upworthy_restricted)\n\n# A tibble: 6 × 5\n  created_at          clickability_test_id     headline       impressions clicks\n  <dttm>              <chr>                    <chr>                <dbl>  <dbl>\n1 2014-11-20 06:43:16 546d88fb84ad38b2ce000024 They're Being…        3052    150\n2 2014-11-20 06:43:44 546d88fb84ad38b2ce000024 They're Being…        3033    122\n3 2014-11-20 06:44:59 546d88fb84ad38b2ce000024 They're Being…        3092    110\n4 2014-11-20 06:54:36 546d902c26714c6c44000039 This Is What …        3526     90\n5 2014-11-20 06:54:57 546d902c26714c6c44000039 This Is What …        3506    120\n6 2014-11-20 06:55:07 546d902c26714c6c44000039 This Is What …        3380     98\n\n\nWe will focus on the text contained in headlines, and look at whether headlines that asked a question got more clicks than those that did not. We want to remove the effect of different images and so will focus on those tests that have the same image. To identify whether a headline asks a question, we search for a question mark. Although there are more complicated constructions that we could use, this is enough to get started.\n\nupworthy_restricted <-\n  upworthy_restricted |>\n  mutate(asks_question = str_detect(string = headline, pattern = \"\\\\?\"))\n\nupworthy_restricted |> \n  count(asks_question)\n\n# A tibble: 2 × 2\n  asks_question     n\n  <lgl>         <int>\n1 FALSE         19130\n2 TRUE           3536\n\n\nFor every test, and for every picture, we want to know whether asking a question affected the number of clicks.\n\nto_question_or_not_to_question <- \n  upworthy_restricted |> \n  group_by(clickability_test_id, asks_question) |> \n  summarize(ave_clicks = mean(clicks)) |> \n  ungroup()\n\n`summarise()` has grouped output by 'clickability_test_id'. You can override\nusing the `.groups` argument.\n\nlook_at_differences <- \n  to_question_or_not_to_question |> \n  pivot_wider(id_cols = clickability_test_id,\n              names_from = asks_question,\n              values_from = ave_clicks) |> \n  rename(ave_clicks_not_question = `FALSE`,\n         ave_clicks_is_question = `TRUE`) |> \n  filter(!is.na(ave_clicks_not_question)) |>\n  filter(!is.na(ave_clicks_is_question)) |> \n  mutate(difference_in_clicks = ave_clicks_is_question - ave_clicks_not_question)\n\nlook_at_differences$difference_in_clicks |> mean()\n\n[1] -4.890435\n\n\nWe find that in general, having a question in the headline may slightly decrease the number of clicks on a headline, although if there is an effect it does not appear to be very large (Figure 10.2).\n\n\n\n\n\nFigure 10.2: Comparison of the average number of clicks when a headline contains a question mark or not"
  },
  {
    "objectID": "10-hunt.html#implementing-surveys",
    "href": "10-hunt.html#implementing-surveys",
    "title": "10  Hunt data",
    "section": "10.3 Implementing surveys",
    "text": "10.3 Implementing surveys\nThere are many ways to implement surveys. For instance, there are dedicated survey platforms such as Survey Monkey and Qualtrics. In general, the focus of those platforms is on putting together the survey form and they expect that we already have contact details for the sample of interest. Some other platforms, such as Mechanical Turk and Prolific, focus on providing that audience, and we can then ask that audience to do more than just take a survey. While that is useful, it usually comes with higher costs. Finally, platforms such as Facebook also provide the ability to run a survey. One especially common approach, because it is free, is to use Google Forms.\nTo create a survey with Google Forms, sign into your Google Account, go to Google Drive, and then click ‘New’ then ‘Google Form’. By default, the form is largely empty (Figure 10.3), and we should add a title and description.\n\n\n\nFigure 10.3: The default view when a new Google Form is created contains many empty fields\n\n\nBy default, an multiple-choice question is included, and we can update the content of this by clicking in the question field. Helpfully, there are often suggestions that can help provide the options. We can make the question required by toggling (Figure 10.4).\n\n\n\nFigure 10.4: Updating the multiple-choice question that is included by default\n\n\nWe can add another question, by clicking on the plus with a circle around it, and select different types of question, for instance, ‘Short answer’, ‘Checkboxes’, or ‘Linear scale’ (Figure 10.5). It can be especially useful to use ‘Short answer’ for aspect such as name and email address, checkboxes and linear scale to understand preferences.\n\n\n\nFigure 10.5: Different options for questions include short answer, checkboxes, and linear scale\n\n\nWhen we are happy with our survey, we make like to preview it ourselves, by clicking on the icon that looks like an eye. After checking it that way, we can click on ‘Send’. Usually it is especially useful to use the second option, which is to send via a link, and it can be handy to shorten the URL (Figure 10.6)).\n\n\n\nFigure 10.6: There are a variety of ways to share the survey, and one helpful one is to get a link with a short URL\n\n\nAfter you share your survey, results will accrue in the ‘Responses’ tab and it can be especially useful to create a spreadsheet to view these responses, by clicking on the ‘Sheets’ icon. After you have collected enough responses then you can turn off ‘Accepting responds’ (Figure 10.7).\n\n\n\nFigure 10.7: Responses show up alongside the survey and it can be helpful to add those to a separate spreadsheet"
  },
  {
    "objectID": "10-hunt.html#exercises-and-tutorial",
    "href": "10-hunt.html#exercises-and-tutorial",
    "title": "10  Hunt data",
    "section": "10.4 Exercises and tutorial",
    "text": "10.4 Exercises and tutorial\n\n10.4.1 Exercises\n\nIn your own words, what is the role of randomization in constructing a counterfactual (write two or three paragraphs)?\nWhat is external validity (pick one)?\n\nFindings from an experiment hold in that setting.\nFindings from an experiment hold outside that setting.\nFindings from an experiment that has been repeated many times.\nFindings from an experiment for which code and data are available.\n\nWhat is internal validity (pick one)?\n\nFindings from an experiment hold in that setting.\nFindings from an experiment hold outside that setting.\nFindings from an experiment that has been repeated many times.\nFindings from an experiment for which code and data are available.\n\nIf we have a dataset named ‘netflix_data’, with the columns ‘person’ and ‘tv_show’ and ‘hours’, (person is a character class uniqueID for every person, tv_show is a character class name of a tv show, and hours is double expressing the number of hours that person watched that tv show). Could you please write some code that would randomly assign people into one of two groups? The data looks like this:\n\n\nlibrary(tidyverse)\nnetflix_data <- \n  tibble(person = c(\"Rohan\", \"Rohan\", \"Monica\", \"Monica\", \"Monica\", \n                    \"Patricia\", \"Patricia\", \"Helen\"),\n         tv_show = c(\"Broadchurch\", \"Duty-Shame\", \"Broadchurch\", \"Duty-Shame\", \n                     \"Shetland\", \"Broadchurch\", \"Shetland\", \"Duty-Shame\"),\n         hours = c(6.8, 8.0, 0.8, 9.2, 3.2, 4.0, 0.2, 10.2)\n         )\n\n\nIn the context of randomization, what does stratification mean to you (write a paragraph or two)?\nHow could you check that your randomization had been done appropriately (write two or three paragraphs)?\nIdentify three companies that conduct A/B testing commercially and write one paragraph for each of them about how they work and the trade-offs involved.\nPretend that you work as a junior analyst for a large consulting firm. Further, pretend that your consulting firm has taken a contract to put together a facial recognition model for the Canada Border Services Agency’s Inland Enforcement branch. Taking a page or two, please discuss your thoughts on this matter. What would you do and why?\nWhat is an estimate (pick one)?\n\nA rule for calculating an estimate of a given quantity based on observed data.\nThe quantity of interest.\nThe result.\nUnknown numbers that determine a statistical model.\n\nWhat is an estimator (pick one)?\n\nA rule for calculating an estimate of a given quantity based on observed data.\nThe quantity of interest.\nThe result.\nUnknown numbers that determine a statistical model.\n\nWhat is an estimand (pick one)?\n\nA rule for calculating an estimate of a given quantity based on observed data.\nThe quantity of interest.\nThe result.\nUnknown numbers that determine a statistical model.\n\nWhat is a parameter (pick one)?\n\nA rule for calculating an estimate of a given quantity based on observed data.\nThe quantity of interest.\nThe result.\nUnknown numbers that determine a statistical model.\n\nJ. Ware (1989, 298) mentions ‘a randomized play the winner design’. What is it?\nJ. Ware (1989, 299) mentions ‘adaptive randomization’. What is it, in your own words?\nJ. Ware (1989, 299) mentions ‘randomized-consent’. He continues that it was ‘attractive in this setting because a standard approach to informed consent would require that parents of infants near death be approached to give informed consent for an invasive surgical procedure that would then, in some instances, not be administered. Those familiar with the agonizing experience of having a child in a neonatal intensive care unit can appreciate that the process of obtaining informed consent would be both frightening and stressful to parents’. To what extent do you agree with this position, especially given, as Ware (1989), p. 305, mentions ‘the need to withhold information about the study from parents of infants receiving CMT’?\nJ. Ware (1989, 300) mentions ‘equipoise’. In your own words, please define and discuss it, using an example from your own experience.\n\n\n\n10.4.2 Tutorial\nPlease build a website using postcards (Kross 2021). Add Google Analytics. Deploy it using Netlify. Change some aspect of the website, add a different tracker, and push it to a new branch. Then use Netlify to conduct an A/B test. Write a one-to-two page paper about what you did and what you found.\n\n\n10.4.3 Paper\nAt about this point, Paper Three (Appendix @ref(paper-three)) would be appropriate.\n\n\n\n\n\nAlsan, Marcella, and Marianne Wanamaker. 2018. “Tuskegee and the Health of Black Men.” The Quarterly Journal of Economics 133 (1): 407–55.\n\n\nAthey, Susan, and Guido W Imbens. 2017. “The State of Applied Econometrics: Causality and Policy Evaluation.” Journal of Economic Perspectives 31 (2): 3–32.\n\n\nBerry, Donald A. 1989. “[Investigating Therapies of Potentially Great Benefit: ECMO]: Comment: Ethics and ECMO.” Statistical Science 4 (4): 306–10.\n\n\nBrandt, Allan M. 1978. “Racism and Research: The Case of the Tuskegee Syphilis Study.” Hastings Center Report, 21–29.\n\n\nBrook, Robert H, John E Ware, William H Rogers, Emmett B Keeler, Allyson Ross Davies, Cathy D Sherbourne, George A Goldberg, Kathleen N Lohr, Patricia Camp, and Joseph P Newhouse. 1984. “The Effect of Coinsurance on the Health of Adults: Results from the RAND Health Insurance Experiment.”\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. Yale Press.\n\n\nFinkelstein, Amy, Sarah Taubman, Bill Wright, Mira Bernstein, Jonathan Gruber, Joseph P Newhouse, Heidi Allen, Katherine Baicker, and Oregon Health Study Group. 2012. “The Oregon Health Insurance Experiment: Evidence from the First Year.” The Quarterly Journal of Economics 127 (3): 1057–1106.\n\n\nFisher, Ronald. 1935. The Design of Experiments. Oliver; Boyd.\n\n\nFitts, Alexis Sobel. 2014. “The King of Content: How Upworthy Aims to Alter the Web, and Could End up Altering the World.” Columbia Journalism Review. https://archives.cjr.org/feature/the_king_of_content.php.\n\n\nFry, Hannah. 2020. “Big Tech Is Testing You.” The New Yorker, 61–65.\n\n\nGertler, Paul J, Sebastian Martinez, Patrick Premand, Laura B Rawlings, and Christel MJ Vermeersch. 2016. Impact Evaluation in Practice. The World Bank.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association 81 (396): 945–60.\n\n\nKohavi, Ron, Diane Tang, and Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical Guide to a/b Testing. Cambridge University Press.\n\n\nKross, Sean. 2021. Postcards: Create Beautiful, Simple Personal Websites. https://CRAN.R-project.org/package=postcards.\n\n\nMatias, J. Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles Ebersole. 2019. “The Upworthy Research Archive.” https://upworthy.natematias.com.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53.\n\n\nSalganik, Matthew. 2018. Bit by Bit: Social Research in the Digital Age. Princeton University Press.\n\n\nStolberg, Michael. 2006. “Inventing the Randomized Double-Blind Trial: The Nuremberg Salt Test of 1835.” Journal of the Royal Society of Medicine 99 (12): 642–43.\n\n\nTaddy, Matt. 2019. Business Data Science. McGraw Hill.\n\n\nWare, James. 1989. “Investigating Therapies of Potentially Great Benefit: ECMO.” Statistical Science, no. 4: 298–306.\n\n\nWare, James H. 1989. “Investigating Therapies of Potentially Great Benefit: ECMO.” Statistical Science 4 (4): 298–306.\n\n\nWei, LJ, and S Durham. 1978. “The Randomized Play-the-Winner Rule in Medical Trials.” Journal of the American Statistical Association 73 (364): 840–43.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "11-clean_and_prepare.html",
    "href": "11-clean_and_prepare.html",
    "title": "11  Clean and prepare",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "11-clean_and_prepare.html#introduction",
    "href": "11-clean_and_prepare.html#introduction",
    "title": "11  Clean and prepare",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\n\n“Well, Lyndon, you may be right and they may be every bit as intelligent as you say,” said Rayburn, “but I’d feel a whole lot better about them if just one of them had run for sheriff once.”\nSam Rayburn’s reaction to Lyndon Johnson’s enthusiasm about Kennedy’s incoming cabinet, as quoted in The Best and the Brightest (Halberstam 1972, 41).\n\nIn earlier chapters we have done some data cleaning and preparation, and in this chapter we will put in place more formal approaches. To a large extent, the role of data cleaning and preparation is so great that the only people that we can trust understand their data, are those that who have cleaned it. And, paradoxically, often those that do the cleaning and preparation are often those that trust it the least. At some point in every data science workflow, those doing the modelling should get their hands dirty with data cleaning. To clean and prepare data is to make many decisions, some of which may have important effects on our results.\nFor a long time, data cleaning and preparation was largely overlooked. We now realize that was a mistake. It has been difficult to trust results in disciplines that apply statistics. The reproducibility crisis, which started in psychology but has now extended to many other fields in the physical and social sciences, has brought to light issues such as p-value ‘hacking’, researcher degrees of freedom, file-drawer issues, and even data and results fabrication (Gelman and Loken 2013). Steps are now being put in place to address these. But, there has been relatively little focus on the data gathering, cleaning, and preparation aspects of applied statistics, despite evidence that decisions made during these steps greatly affect statistical results (Huntington-Klein et al. 2020). In this chapter we focus on these issues.\nWhile the statistical practices that underpin data science are themselves correct and robust when applied to simulated datasets, data science is typically not conducted with these types of datasets. For instance, data scientists are interested in ‘messy, unfiltered, and possibly unclean data—tainted by heteroskedasticity, complex dependence and missingness patterns—that until recently were avoided in polite conversations between more traditional statisticians’ (Craiu 2019). Big data does not resolve this issue, and may even exacerbate it, for instance ‘without taking data quality into account, population inferences with Big Data are subject to a Big Data Paradox: the more the data, the surer we fool ourselves’ (Meng 2018). It is important to note that the issues that are found in much applied statistics research are not necessarily associated with researcher quality, or their biases (Silberzahn et al. 2018). Instead, they are a result of the environment within which data science is conducted. This chapter provides an approach and tools to explicitly think about this work.\nGelman and Vehtari (2020) writing about the most important statistical ideas of the past 50 years say that each of them enabled new ways of thinking about data analysis and they brought into the tent of statistics, approaches that ‘had been considered more a matter of taste or philosophy’. The focus on data cleaning and preparation in this chapter is analogous, insofar, as it represents a codification, or bringing inside the tent, of aspects that are typically, incorrectly, considered those of taste rather than statistics.\nThe workflow that we advocate is:\n\nSave the raw data.\nBegin with an end in mind.\nExecute that plan on a small sample.\nWrite tests and documentation.\nIterate the plan.\nGeneralize the execution.\nUpdate tests and documentation.\n\nWe will need a variety of skills to be effective, but this is the very stuff of statistical sciences. The approach needed is some combination of dogged and sensible. Perfect is very much the enemy of good enough when it comes to data cleaning. And to be specific, it is better to have 90 per cent of the data cleaned and prepared, and to start exploring that, before deciding whether it is worth the effort to clean and prepare the remaining 10 per cent because that remainder will likely take an awful lot of time and effort.\nAll data regardless of whether they were obtained from hunting, gathering, or farming, will have issues and it is critical that we have approaches that can deal with a variety of issues, and more importantly, understand how it might affect our modelling (Van den Broeck et al. 2005). To clean data is to analyze data. This is because the process forces us to make choices about what we value in our results (Au 2020)."
  },
  {
    "objectID": "11-clean_and_prepare.html#workflow",
    "href": "11-clean_and_prepare.html#workflow",
    "title": "11  Clean and prepare",
    "section": "11.2 Workflow",
    "text": "11.2 Workflow\n\n11.2.1 Save a copy of the raw data\nThe first step is to save the raw data into a separate, local, folder. It is important to save this raw data, to the extent that is possible, because it establishes the foundation for reproducibility (Wilson et al. 2017). If we are obtaining our data from a third-party, such as a government website, then we have no control over whether they will continue to host that data, whether they will update it, and the address at which it will be available. We also want to reduce the burden that we impose on their servers, by saving a local copy.\nHaving locally saved the raw data we must maintain it in that state, and not modify it. As we begin to clean and prepare it, we instead create another dataset. Maintaining the initial, raw, state of the dataset, and using scripts to create the dataset that we are interested in analyzing, ensures that our entire workflow is reproducible.\n\n\n11.2.2 Begin with an end in mind\nPlanning the end state or forcing yourself to begin with an end in mind is important for a variety of reasons. As with scraping data, it helps us to be proactive about scope-creep, but with data cleaning it additionally forces us to really think about what we want the final dataset to look like.\nThe first step is to sketch the dataset that we are interested in. The key features of the sketch will be aspects such as the names of the columns, their class, and the possible range of values. For instance, we might be interested in the populations of US states. In which case our sketch might look like Figure 11.1.\n\n\n\nFigure 11.1: Planned dataset of US states and their populations\n\n\nIn this case, the sketch forces us to decide whether we want full names or abbreviations for the state names, and that the population has been measured in millions. The process of sketching this end-point has forced us to make decisions early on, and be clear about our desired end state.\nWe then implement that using code to simulate data. Again, this process forces us to think about what reasonable values look like in our dataset because we are literally forced to decide which functions to use. Thinking carefully about the membership of each column here, for instance if the column is meant to be ‘gender’ then values such as ‘male’, ‘female’, ‘other’, and ‘unknown’ may be expected, but a number such as ‘1,000’ would likely be unexpected. It also forces us to be explicit about variable names because we have to assign the outputs of those functions to a variable. For instance, we could simulate some data for the population data.\n\nlibrary(tidyverse)\n\nset.seed(853)\n\nsimulated_tfr <- \n  tibble(\n    state = state.name,\n    population = runif(n = 50, min = 0, max = 50) |> round(digits = 2)\n  )\n\nsimulated_tfr\n\n# A tibble: 50 × 2\n   state       population\n   <chr>            <dbl>\n 1 Alabama          18.0 \n 2 Alaska            6.01\n 3 Arizona          24.2 \n 4 Arkansas         15.8 \n 5 California        1.87\n 6 Colorado         20.2 \n 7 Connecticut       6.54\n 8 Delaware         12.1 \n 9 Florida           7.9 \n10 Georgia           9.44\n# … with 40 more rows\n\n\nOur purpose, during data cleaning and preparation, is to then bring our raw data close to that plan. Ideally, we would plan so that the desired end-state of our dataset is ‘tidy data’, which was introduced in Chapter @ref(r-essentials).\n\n\n\n\n\n\n\n\n\n\n\n\n11.2.3 Start small\nHaving thoroughly planned we can turn to the raw data that we are dealing with. Usually, regardless of what the raw data look like, we want to manipulate them into a rectangular dataset as quickly as possible. This allows us to use our family dplyr verbs and tidyverse approaches. For instance, let us assume that we are starting with some .txt file.\nThe first step is to look for regularities in the dataset. We are wanting to end up with tabular data, which means that we need some type of delimiter to distinguish different columns. Ideally this might be features such as a comma, a semicolon, a tab, a double space, or a line break.\n\nAlabama, 5\nAlaska, 0.7\nArizona, 7\nArkansas, 3\nCalifornia, 40\n\nIn worse cases there may be some regular feature of the dataset that we can take advantage of. For instance, sometimes various text is repeated.\n\nState is Alabama and population is 5 million.\nState is Alaska and population is 0.7 million.\nState is Arizona and population is 7 million.\nState is Arkansas and population is 3 million.\nState is California and population is 40 million.\n\nIn this case, although we do not have a traditional delimiter we can use the regularity of ‘State is’ and ’ and population is ’ to get what we need. A more difficult case is when we do not have line breaks.\n\nAlabama 5 Alaska 0.7 Arizona 7 Arkansas 3 California 40\n\nOne way to approach this is to take advantage of the different classes and values that we are looking for. For instance, in this case, we know that we are after US states, so there are only 50 possible options, and we could use the existence of these as a delimiter. We could also use the fact that population is a number here, and so split based on a space followed by a number.\nWe will now go through the process of converting this last example into tidy data using tidyr (Wickham 2021).\n\nraw_data <-\n  c('Alabama 5 Alaska 0.7 Arizona 7 Arkansas 3 California 40')\n\ndata_as_tibble <-\n  tibble(raw = raw_data)\n\ntidy_data <-\n  data_as_tibble |>\n  separate(col = raw,\n           into = letters[1:5],\n           sep = \"(?<=[[:digit:]]) \") |>\n  pivot_longer(cols = letters[1:5],\n               names_to = \"drop_me\",\n               values_to = \"separate_me\") |>\n  separate(col = separate_me,\n           into = c('state', 'population'),\n           sep = \" (?=[[:digit:]])\") |>\n  select(-drop_me)\n\ntidy_data\n\n# A tibble: 5 × 2\n  state      population\n  <chr>      <chr>     \n1 Alabama    5         \n2 Alaska     0.7       \n3 Arizona    7         \n4 Arkansas   3         \n5 California 40        \n\n\n\n\n11.2.4 Write tests and documentation\n\nHaving established a rectangular dataset, albeit a messy one, we should begin to look at the classes that we have. We do not necessarily want to fix the classes at this point, because that can result in us losing data. But we look at the class to see what it is, and then compare it to our simulated dataset to see where it needs to get to. We note the columns where it is different.\nBefore changing the class and before going onto more bespoke issues, we should deal with some of the common issues in each class. Some common issues are:\n\nCommas and other punctuation, such as denomination signs in columns that should be numeric.\nInconsistent formatting of dates, such as ‘December’ and ‘Dec’ and ‘12’.\nUnexpected characters, especially in unicode, which may not display consistently.\n\nTypically, we want to fix anything immediately obvious. For instance, remove commas that have been used to group digits in currencies. However, the situation will typically quickly become dire. What we need to do is to look at the membership of each group, and then triage what we will fix. We should probably make the decision of how to triage based on what is likely to have the largest impact. That usually means starting with the counts, sorting in descending order, and then dealing with each as they come.\nWhen the tests of membership are passed, then finally we can change the class, and run all the tests again. We are adapting this idea from the software development approach of unit testing. Tests are crucial because the enable us to understand whether software (or in this case data) is fit for purpose (Wilson 2021).\nLet us run through an example with a collection of strings, some of which are slightly wrong. This type of output is typical of OCR, which often gets most of the way there, but not quite.\n\nmessy_string <-\n  c('Patricia, Ptricia, PatricIa, Patncia, PatricIa, Patricia, Patricia, Patric1a, Patricia , 8atricia')\n\nmessy_string\n\n[1] \"Patricia, Ptricia, PatricIa, Patncia, PatricIa, Patricia, Patricia, Patric1a, Patricia , 8atricia\"\n\n\nAs before, we first want to get this into a rectangular dataset.\n\nmessy_data <- \n  tibble(names = messy_string) |> \n  separate_rows(names, sep = \", \") \n\nmessy_data\n\n# A tibble: 10 × 1\n   names      \n   <chr>      \n 1 \"Patricia\" \n 2 \"Ptricia\"  \n 3 \"PatricIa\" \n 4 \"Patncia\"  \n 5 \"PatricIa\" \n 6 \"Patricia\" \n 7 \"Patricia\" \n 8 \"Patric1a\" \n 9 \"Patricia \"\n10 \"8atricia\" \n\n\nWe now need decide which of these errors we are going to fix. To help us decide which are most important, we will create a count.\n\nmessy_data |> \n  count(names, sort = TRUE)\n\n# A tibble: 7 × 2\n  names           n\n  <chr>       <int>\n1 \"Patricia\"      3\n2 \"PatricIa\"      2\n3 \"8atricia\"      1\n4 \"Patncia\"       1\n5 \"Patric1a\"      1\n6 \"Patricia \"     1\n7 \"Ptricia\"       1\n\n\nThe most common element is the correct one, which is great. The next one - ‘PatricIa’ - looks like the ‘i’ has been incorrectly capitalized, and the one after that - ‘8atricia’ - is distinguished by an ‘8’ instead of a ‘P’. Let us quickly fix these issues and then redo the count.\n\nmessy_data <- \n  messy_data |> \n  mutate(names = str_replace_all(names, 'PatricIa', 'Patricia'),\n         names = str_replace_all(names, '8atricia', 'Patricia')\n         )\n\nmessy_data |> \n  count(names, sort = TRUE)\n\n# A tibble: 5 × 2\n  names           n\n  <chr>       <int>\n1 \"Patricia\"      6\n2 \"Patncia\"       1\n3 \"Patric1a\"      1\n4 \"Patricia \"     1\n5 \"Ptricia\"       1\n\n\nAlready this is much better and 60 per cent of the values are correct, compared with earlier where it was 30 per cent. There are two more obvious errors - ‘Ptricia’ and ‘Patncia’ - with the first missing an ‘a’ and the second having an ‘n’ where the ‘ri’ should be. Again, we can quickly update and fix those.\n\nmessy_data <- \n  messy_data |> \n  mutate(names = str_replace_all(names, 'Ptricia', 'Patricia'),\n         names = str_replace_all(names, 'Patncia', 'Patricia')\n         )\n\nmessy_data |> \n  count(names, sort = TRUE)\n\n# A tibble: 3 × 2\n  names           n\n  <chr>       <int>\n1 \"Patricia\"      8\n2 \"Patric1a\"      1\n3 \"Patricia \"     1\n\n\nWe have achieved an 80 per cent fix with not too much effort. The two remaining issues are more subtle. The first - ‘Patric1a’ - has occurred because the ‘i’ has been incorrectly coded as an ‘1’. In some fonts this will show up, but in others it will be more difficult to see. This is a common issue, especially with OCR, and something to be aware of. The second - ‘Patricia’ - is similarly subtle and is occurring because there is a trailing space. Again, trailing and leading spaces are a common issue and we can address them with str_trim(). After we fix these two remaining issues then we will have all entries corrected.\n\ncleaned_data <- \n  messy_data |> \n  mutate(names = str_replace_all(names, 'Patric1a', 'Patricia'),\n         names = str_trim(names, side = c(\"right\"))\n         )\n\ncleaned_data |> \n  count(names, sort = TRUE)\n\n# A tibble: 1 × 2\n  names        n\n  <chr>    <int>\n1 Patricia    10\n\n\nWe have been doing the tests in our head in this example. We know that we are hoping for ‘Patricia’. But we can start to document this test as well. One way is to look to see if values other than ‘Patricia’ exist in the dataset.\n\ncheck_me <- \n  cleaned_data |> \n  filter(names != \"Patricia\")\n\nif (nrow(check_me) > 0) {\n  print(\"Still have values that are not Patricia!\")\n}\n\nWe can make things a little more imposing by stopping our code execution if the condition is not met with stopifnot(). To use that we define a condition that we would like met. We could implement this type of check throughout our code. For instance, if we expected there to be a certain number of rows in the dataset, or for a certain column to have various properties, such as being an integer, or a factor.\n\nstopifnot(nrow(check_me) == 0)\n\nWe can use stopifnot() to ensure that our script is working as expected as it goes through. Another way that is especially used\nAnother way to write tests for our dataset is to use testthat (Wickham 2011). Although developed for testing packages, we can use the same functionality to test our datasets. For instance, we can use expect_length() to check the length of a dataset and could use expect_equal() to check the content.\n\nlibrary(testthat)\n\nexpect_length(check_me, 1)\nexpect_equal(class(cleaned_data$names), \"character\")\nexpect_equal(unique(cleaned_data$names), \"Patricia\")\n\nIf the tests pass then nothing happens, and if the tests fail then the script will stop.\n\n\n11.2.5 Iterate, generalize and update\nWe could now iterate the plan. In this most recent case, we started with 10 entries. There is no reason that we could not increase this to 100 or even 1,000. We may need to generalize the cleaning procedures and tests. But eventually we would start to being the dataset into some sort of order."
  },
  {
    "objectID": "11-clean_and_prepare.html#case-study-kenya-census",
    "href": "11-clean_and_prepare.html#case-study-kenya-census",
    "title": "11  Clean and prepare",
    "section": "11.3 Case study: Kenya census",
    "text": "11.3 Case study: Kenya census\n\n11.3.1 Gather and clean data\nTo make this all more clear, let us gather, clean, and prepare some data from the 2019 Kenyan census. The distribution of population by age, sex, and administrative unit from the 2019 Kenyan census can be downloaded here. While this format as a PDF makes it easy to look up a particular result, it is not overly useful if we want to model the data. In order to be able to do that, we need to convert a PDF of Kenyan census results of counts, by age and sex, by county and sub-county, into a tidy dataset that can be analyzed. We will use janitor (Firke 2020), pdftools (Ooms 2019), tidyverse (Wickham et al. 2019), and stringi (Gagolewski 2020).\n\nlibrary(janitor)\nlibrary(pdftools)\nlibrary(purrr)\nlibrary(tidyverse)\nlibrary(stringi)\n\nAnd we can then download1 and read in the PDF of the 2019 Kenyan census. When we have a PDF and want to read the content into R, then pdf_text() from pdftools (Ooms 2019) is useful. It works well for many recently produced PDFs because the content is text which it can extract. But if the PDF is an image, then pdf_text() will not work. Instead, the PDF will first need to go through OCR, which was covered in Chapter @ref(gather-data).\n\ndownload.file(\n  \"https://www.knbs.or.ke/download/2019-kenya-population-and-housing-census-volume-iii-distribution-of-population-by-age-sex-and-administrative-units/?wpdmdl=5729&refresh=620561f1ce3ad1644519921\", \n  \"2019_Kenya_census.pdf\",\n  mode=\"wb\")\n\nall_content <- pdf_text(\"2019_Kenya_census.pdf\")\n\nWe can see an example page of the PDF of the 2019 Kenyan census (Figure 11.2).\n\n\n\nFigure 11.2: Example page from the 2019 Kenyan census\n\n\nThe first challenge is to get the dataset into a format that we can more easily manipulate. We will consider each page of the PDF and extract the relevant parts. To do this, we first write a function, and then apply it to each page.\n\n# The function is going to take an input of a page\nget_data <- function(i){\n  # i = 467\n  # Just look at the page of interest\n  # Based on Bob Rudis: https://stackoverflow.com/a/47793617\n  just_page_i <- stri_split_lines(all_content[[i]])[[1]] \n  \n  just_page_i <- just_page_i[just_page_i != \"\"]\n  \n  # Grab the name of the location\n  area <- just_page_i[3] |> str_squish()\n  area <- str_to_title(area)\n  \n  # Grab the type of table\n  type_of_table <- just_page_i[2] |> str_squish()\n  \n  # Get rid of the top matter\n  # Manually for now, but could create some rules if needed\n  just_page_i_no_header <- just_page_i[5:length(just_page_i)] \n  \n  # Get rid of the bottom matter\n  # Manually for now, but could create some rules if needed\n  just_page_i_no_header_no_footer <- just_page_i_no_header[1:62] \n  \n  # Convert into a tibble\n  demography_data <- tibble(all = just_page_i_no_header_no_footer)\n  \n  # Split columns\n  demography_data <-\n    demography_data |>\n    mutate(all = str_squish(all)) |> # Any space more than two spaces is reduced\n    mutate(all = str_replace(all, \"10 -14\", \"10-14\")) |> # One specific issue\n    mutate(all = str_replace(all, \"Not Stated\", \"NotStated\")) |> # And another\n    separate(col = all,\n             into = c(\"age\", \"male\", \"female\", \"total\", \"age_2\", \"male_2\", \"female_2\", \"total_2\"),\n             sep = \" \", # Works fine because the tables are nicely laid out\n             remove = TRUE,\n             fill = \"right\",\n             extra = \"drop\"\n    )\n  \n  # They are side by side at the moment, need to append to bottom\n  demography_data_long <-\n    rbind(demography_data |> select(age, male, female, total),\n          demography_data |>\n            select(age_2, male_2, female_2, total_2) |>\n            rename(age = age_2, male = male_2, female = female_2, total = total_2)\n    )\n  \n  # There is one row of NAs, so remove it\n  demography_data_long <- \n    demography_data_long |> \n    remove_empty(which = c(\"rows\"))\n  \n  # Add the area and the page\n  demography_data_long$area <- area\n  demography_data_long$table <- type_of_table\n  demography_data_long$page <- i\n  \n  rm(just_page_i,\n     i,\n     area,\n     type_of_table,\n     just_page_i_no_header,\n     just_page_i_no_header_no_footer,\n     demography_data)\n  \n  return(demography_data_long)\n}\n\nWe now have a function that does what we need to each page of the PDF. We will use map_dfr() from purrr (Henry and Wickham 2020) to apply that function to each page, and then combine all the outputs into one tibble.\n\n# Run through each relevant page and get the data\npages <- c(30:513)\nall_tables <- map_dfr(pages, get_data)\nrm(pages, get_data)\n\n\nall_tables\n\n# A tibble: 59,532 × 7\n   age   male    female  total     area    table                            page\n   <chr> <chr>   <chr>   <chr>     <chr>   <chr>                           <int>\n 1 Total 610,257 598,046 1,208,303 Mombasa Table 2.3: Distribution of Pop…    30\n 2 0     15,111  15,009  30,120    Mombasa Table 2.3: Distribution of Pop…    30\n 3 1     15,805  15,308  31,113    Mombasa Table 2.3: Distribution of Pop…    30\n 4 2     15,088  14,837  29,925    Mombasa Table 2.3: Distribution of Pop…    30\n 5 3     14,660  14,031  28,691    Mombasa Table 2.3: Distribution of Pop…    30\n 6 4     14,061  13,993  28,054    Mombasa Table 2.3: Distribution of Pop…    30\n 7 0-4   74,725  73,178  147,903   Mombasa Table 2.3: Distribution of Pop…    30\n 8 5     13,851  14,023  27,874    Mombasa Table 2.3: Distribution of Pop…    30\n 9 6     12,889  13,216  26,105    Mombasa Table 2.3: Distribution of Pop…    30\n10 7     13,268  13,203  26,471    Mombasa Table 2.3: Distribution of Pop…    30\n# … with 59,522 more rows\n\n\nHaving got it into a rectangular format, we now need to clean the dataset to make it useful.\nThe first step is to make the numbers into actual numbers, rather than characters. Before we can convert the type, we need to remove anything that is not a number otherwise that cell will be converted into an NA. We first identify any values that are not numbers so that we can remove them.\n\n# Need to convert male, female, and total to integers\n# First find the characters that should not be in there\nall_tables |> \n  select(male, female, total) |>\n  mutate_all(~str_remove_all(., \"[:digit:]\")) |> \n  mutate_all(~str_remove_all(., \",\")) |>\n  mutate_all(~str_remove_all(., \"_\")) |>\n  mutate_all(~str_remove_all(., \"-\")) |> \n  distinct()\n\n# A tibble: 3 × 3\n  male  female total\n  <chr> <chr>  <chr>\n1 \"\"    \"\"     \"\"   \n2 \"Aug\" \"\"     \"\"   \n3 \"Jun\" \"\"     \"\"   \n\n# We clearly need to remove \",\", \"_\", and \"-\". \n# This also highlights a few issues on p. 185 that need to be manually adjusted\n# https://twitter.com/RohanAlexander/status/1244337583016022018\nall_tables$male[all_tables$male == \"23-Jun\"] <- 4923\nall_tables$male[all_tables$male == \"15-Aug\"] <- 4611\n\nWhile we could use janitor here, it is worthwhile at least first looking at what is going on because sometimes there is odd stuff that janitor (and other packages) will deal with, but not in a way that we want. In this case, the Kenyan government used Excel or similar, and this has converted two entries into dates. If we just took the numbers from the column then we would have 23 and 15 here, but by inspecting the column we can use Excel to reverse the process and enter the correct values of 4,923 and 4,611, respectively.\nHaving identified everything that needs to be removed, we can do the actual removal and convert our character column of numbers to integers.\n\nall_tables <-\n  all_tables |>\n  mutate_at(vars(male, female, total), ~str_remove_all(., \",\")) |>\n  mutate_at(vars(male, female, total), ~str_replace(., \"_\", \"0\")) |>\n  mutate_at(vars(male, female, total), ~str_replace(., \"-\", \"0\")) |>\n  mutate_at(vars(male, female, total), ~as.integer(.))\n\nall_tables\n\n# A tibble: 59,532 × 7\n   age     male female   total area    table                                page\n   <chr>  <int>  <int>   <int> <chr>   <chr>                               <int>\n 1 Total 610257 598046 1208303 Mombasa Table 2.3: Distribution of Populat…    30\n 2 0      15111  15009   30120 Mombasa Table 2.3: Distribution of Populat…    30\n 3 1      15805  15308   31113 Mombasa Table 2.3: Distribution of Populat…    30\n 4 2      15088  14837   29925 Mombasa Table 2.3: Distribution of Populat…    30\n 5 3      14660  14031   28691 Mombasa Table 2.3: Distribution of Populat…    30\n 6 4      14061  13993   28054 Mombasa Table 2.3: Distribution of Populat…    30\n 7 0-4    74725  73178  147903 Mombasa Table 2.3: Distribution of Populat…    30\n 8 5      13851  14023   27874 Mombasa Table 2.3: Distribution of Populat…    30\n 9 6      12889  13216   26105 Mombasa Table 2.3: Distribution of Populat…    30\n10 7      13268  13203   26471 Mombasa Table 2.3: Distribution of Populat…    30\n# … with 59,522 more rows\n\n\nThe next thing to clean is the areas. We know that there are 47 counties in Kenya, and a large number of sub-counties. The Kenyan government purports to provide a list on pages 19 to 22 of the PDF (document pages 7 to 10). But this list is not complete, and there are a few minor issues that we will deal with later. In any case, we first need to fix a few inconsistencies.\n\n# Fix some area names\nall_tables$area[all_tables$area == \"Taita/ Taveta\"] <- \"Taita/Taveta\"\nall_tables$area[all_tables$area == \"Elgeyo/ Marakwet\"] <- \"Elgeyo/Marakwet\"\nall_tables$area[all_tables$area == \"Nairobi City\"] <- \"Nairobi\"\n\nKenya has 47 counties, each of which has sub-counties. The PDF has them arranged as the county data then the sub-counties, without designating which is which. We can use the names, to a certain extent, but in a handful of cases, there is a sub-county that has the same name as a county, so we need to first fix that.\nThe PDF is made-up of three tables. So we can first get the names of the counties based on those final two tables and then reconcile them to get a list of the counties.\n\nall_tables$table |> \n  table()\n\n\nTable 2.3: Distribution of Population by Age, Sex*, County and Sub- County \n                                                                     48216 \n      Table 2.4a: Distribution of Rural Population by Age, Sex* and County \n                                                                      5535 \n      Table 2.4b: Distribution of Urban Population by Age, Sex* and County \n                                                                      5781 \n\n\n\nlist_counties <- \n  all_tables |> \n  filter(table %in% c(\"Table 2.4a: Distribution of Rural Population by Age, Sex* and County\",\n                      \"Table 2.4b: Distribution of Urban Population by Age, Sex* and County\")\n         ) |> \n  select(area) |> \n  distinct()\n\nlist_counties\n\n# A tibble: 47 × 1\n   area        \n   <chr>       \n 1 Kwale       \n 2 Kilifi      \n 3 Tana River  \n 4 Lamu        \n 5 Taita/Taveta\n 6 Garissa     \n 7 Wajir       \n 8 Mandera     \n 9 Marsabit    \n10 Isiolo      \n# … with 37 more rows\n\n\nAs we hoped, there are 47 of them. But before we can add a flag based on those names, we need to deal with the sub-counties that share their name. We will do this based on the page, then looking it up and deciding which is the county page and which is the sub-county page.\n\nall_tables |> \n  filter(table == \"Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County\") |> \n  filter(area %in% c(\"Busia\",\n                     \"Garissa\",\n                     \"Homa Bay\",\n                     \"Isiolo\",\n                     \"Kiambu\",\n                     \"Machakos\",\n                     \"Makueni\",\n                     \"Samburu\",\n                     \"Siaya\",\n                     \"Tana River\",\n                     \"Vihiga\",\n                     \"West Pokot\")\n         ) |> \n  select(area, page) |> \n  distinct()\n\n# A tibble: 24 × 2\n   area        page\n   <chr>      <int>\n 1 Samburu       42\n 2 Tana River    53\n 3 Tana River    56\n 4 Garissa       65\n 5 Garissa       69\n 6 Isiolo        98\n 7 Isiolo       100\n 8 Machakos     149\n 9 Machakos     154\n10 Makueni      159\n# … with 14 more rows\n\n\nNow we can add the flag for whether the area is a county, and adjust for the ones that are troublesome,\n\nall_tables <- \n  all_tables |> \n  mutate(area_type = if_else(area %in% list_counties$area, \"county\", \"sub-county\"))\n\nall_tables <- \n  all_tables |> \n  mutate(area_type = case_when(\n    area == \"Samburu\" & page == 42 ~ \"sub-county\",\n    area == \"Tana River\" & page == 56 ~ \"sub-county\",\n    area == \"Garissa\" & page == 69 ~ \"sub-county\",\n    area == \"Isiolo\" & page == 100 ~ \"sub-county\",\n    area == \"Machakos\" & page == 154 ~ \"sub-county\",\n    area == \"Makueni\" & page == 164 ~ \"sub-county\",\n    area == \"Kiambu\" & page == 213 ~ \"sub-county\",\n    area == \"West Pokot\" & page == 233 ~ \"sub-county\",\n    area == \"Vihiga\" & page == 333 ~ \"sub-county\",\n    area == \"Busia\" & page == 353 ~ \"sub-county\",\n    area == \"Siaya\" & page == 360 ~ \"sub-county\",\n    area == \"Homa Bay\" & page == 375 ~ \"sub-county\",\n    TRUE ~ area_type\n    )\n  )\n\nrm(list_counties)\n\nall_tables\n\n# A tibble: 59,532 × 8\n   age     male female   total area    table                      page area_type\n   <chr>  <int>  <int>   <int> <chr>   <chr>                     <int> <chr>    \n 1 Total 610257 598046 1208303 Mombasa Table 2.3: Distribution …    30 county   \n 2 0      15111  15009   30120 Mombasa Table 2.3: Distribution …    30 county   \n 3 1      15805  15308   31113 Mombasa Table 2.3: Distribution …    30 county   \n 4 2      15088  14837   29925 Mombasa Table 2.3: Distribution …    30 county   \n 5 3      14660  14031   28691 Mombasa Table 2.3: Distribution …    30 county   \n 6 4      14061  13993   28054 Mombasa Table 2.3: Distribution …    30 county   \n 7 0-4    74725  73178  147903 Mombasa Table 2.3: Distribution …    30 county   \n 8 5      13851  14023   27874 Mombasa Table 2.3: Distribution …    30 county   \n 9 6      12889  13216   26105 Mombasa Table 2.3: Distribution …    30 county   \n10 7      13268  13203   26471 Mombasa Table 2.3: Distribution …    30 county   \n# … with 59,522 more rows\n\n\nHaving dealt with the areas, we can deal with the ages. First, we need to fix some clear errors.\n\ntable(all_tables$age) |> head()\n\n\n    0   0-4     1    10 10-14 10-19 \n  484   484   484   484   482     1 \n\nunique(all_tables$age) |> head()\n\n[1] \"Total\" \"0\"     \"1\"     \"2\"     \"3\"     \"4\"    \n\n# Looks like there should be 484, so need to follow up on some:\nall_tables$age[all_tables$age == \"NotStated\"] <- \"Not Stated\"\nall_tables$age[all_tables$age == \"43594\"] <- \"5-9\"\nall_tables$age[all_tables$age == \"43752\"] <- \"10-14\"\nall_tables$age[all_tables$age == \"9-14\"] <- \"5-9\"\nall_tables$age[all_tables$age == \"10-19\"] <- \"10-14\"\n\nThe census has done some of the work of putting together age-groups for us, but we want to make it easy to just focus on the counts by single-year-age. As such we will add a flag as to the type of age it is: an age group, such as “ages 0 to 5”, or a single age, such as “1”.\n\nall_tables$age_type <-\n  if_else(str_detect(all_tables$age, c(\"-\")), \"age-group\", \"single-year\")\nall_tables$age_type <-\n  if_else(str_detect(all_tables$age, c(\"Total\")),\n          \"age-group\",\n          all_tables$age_type)\n\nAt the moment, age is a character variable. We have a decision to make here. We do not want it to be a character variable (because it will not graph properly), but we do not want it to be numeric, because there is total and 100+ in there. For now, we will just make it into a factor, and at least that will be able to be nicely graphed.\n\nall_tables$age <- as_factor(all_tables$age)\n\n\n\n11.3.2 Check data\nHaving gathered and cleaned the data, we would like to run a few checks. Given the format of the data, we can check that ‘total’ is the sum of ‘male’ and ‘female’. (While we would prefer to use different groupings, this is what the Kenyan government collected and makes available.)\n\nfollow_up <- \n  all_tables |> \n  mutate(check_sum = male + female,\n         totals_match = if_else(total == check_sum, 1, 0)\n         ) |> \n  filter(totals_match == 0)\n\nAnd we can adjust the one that looks to be wrong.\n\n# There is just one that looks wrong\nall_tables$male[all_tables$age == \"10\" & all_tables$page == 187] <- as.integer(1)\n\nrm(follow_up)\n\nThe Kenyan census provides different tables for the total of each county and sub-county; and then within each county, for the number in an urban area in that county, and the number in a urban area in that county. Some counties only have an urban count, but we would like to make sure that the sum of rural and urban counts equals the total count. This requires pivoting the data from long to wide.\nFirst, we construct different tables for each of the three.\n\n# Table 2.3\ntable_2_3 <- all_tables |> \n  filter(table == \"Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County\")\ntable_2_4a <- all_tables |> \n  filter(table == \"Table 2.4a: Distribution of Rural Population by Age, Sex* and County\")\ntable_2_4b <- all_tables |> \n  filter(table == \"Table 2.4b: Distribution of Urban Population by Age, Sex* and County\")\n\nThen having constructed the constituent parts, we can join then based on age, area, and whether it is a county.\n\nboth_2_4s <-\n#| echo: true\n  full_join(\n    table_2_4a,\n    table_2_4b,\n    by = c(\"age\", \"area\", \"area_type\"),\n    suffix = c(\"_rural\", \"_urban\")\n  )\n\nall <-\n  full_join(\n    table_2_3,\n    both_2_4s,\n    by = c(\"age\", \"area\", \"area_type\"),\n    suffix = c(\"_all\", \"_\")\n  )\n\nall <-\n  all |>\n  mutate(\n    page = glue::glue(\n      'Total from p. {page}, rural from p. {page_rural}, urban from p. {page_urban}'\n    )\n  ) |>\n  select(\n    -page,\n    -page_rural,\n    -page_urban,-table,\n    -table_rural,\n    -table_urban,-age_type_rural,\n    -age_type_urban\n  )\n\nrm(both_2_4s, table_2_3, table_2_4a, table_2_4b)\n\nWe can now check that the sum of rural and urban is the same as the total.\n\nfollow_up <- \n  all |> \n  mutate(total_from_bits = total_rural + total_urban,\n         check_total_is_rural_plus_urban = if_else(total == total_from_bits, 1, 0),\n         total_from_bits - total) |> \n  filter(check_total_is_rural_plus_urban == 0)\n\nhead(follow_up)\n\n# A tibble: 3 × 16\n  age        male female  total area  area_type age_type male_rural female_rural\n  <fct>     <int>  <int>  <int> <chr> <chr>     <chr>         <int>        <int>\n1 Not Sta…     31     10     41 Naku… county    single-…          8            6\n2 Total    434287 441379 875666 Bomet county    age-gro…     420119       427576\n3 Not Sta…      3      2      5 Bomet county    single-…          2            1\n# … with 7 more variables: total_rural <int>, male_urban <int>,\n#   female_urban <int>, total_urban <int>, total_from_bits <int>,\n#   check_total_is_rural_plus_urban <dbl>, `total_from_bits - total` <int>\n\nrm(follow_up)\n\nThere are just a few, but as they only have a difference of 1, we will just move on.\nFinally, we want to check that the single age counts sum to the age-groups.\n\nfollow_up <- \n  all |> \n  mutate(groups = case_when(age %in% c(\"0\", \"1\", \"2\", \"3\", \"4\", \"0-4\") ~ \"0-4\",\n                            age %in% c(\"5\", \"6\", \"7\", \"8\", \"9\", \"5-9\") ~ \"5-9\",\n                            age %in% c(\"10\", \"11\", \"12\", \"13\", \"14\", \"10-14\") ~ \"10-14\",\n                            age %in% c(\"15\", \"16\", \"17\", \"18\", \"19\", \"15-19\") ~ \"15-19\",\n                            age %in% c(\"20\", \"21\", \"22\", \"23\", \"24\", \"20-24\") ~ \"20-24\",\n                            age %in% c(\"25\", \"26\", \"27\", \"28\", \"29\", \"25-29\") ~ \"25-29\",\n                            age %in% c(\"30\", \"31\", \"32\", \"33\", \"34\", \"30-34\") ~ \"30-34\",\n                            age %in% c(\"35\", \"36\", \"37\", \"38\", \"39\", \"35-39\") ~ \"35-39\",\n                            age %in% c(\"40\", \"41\", \"42\", \"43\", \"44\", \"40-44\") ~ \"40-44\",\n                            age %in% c(\"45\", \"46\", \"47\", \"48\", \"49\", \"45-49\") ~ \"45-49\",\n                            age %in% c(\"50\", \"51\", \"52\", \"53\", \"54\", \"50-54\") ~ \"50-54\",\n                            age %in% c(\"55\", \"56\", \"57\", \"58\", \"59\", \"55-59\") ~ \"55-59\",\n                            age %in% c(\"60\", \"61\", \"62\", \"63\", \"64\", \"60-64\") ~ \"60-64\",\n                            age %in% c(\"65\", \"66\", \"67\", \"68\", \"69\", \"65-69\") ~ \"65-69\",\n                            age %in% c(\"70\", \"71\", \"72\", \"73\", \"74\", \"70-74\") ~ \"70-74\",\n                            age %in% c(\"75\", \"76\", \"77\", \"78\", \"79\", \"75-79\") ~ \"75-79\",\n                            age %in% c(\"80\", \"81\", \"82\", \"83\", \"84\", \"80-84\") ~ \"80-84\",\n                            age %in% c(\"85\", \"86\", \"87\", \"88\", \"89\", \"85-89\") ~ \"85-89\",\n                            age %in% c(\"90\", \"91\", \"92\", \"93\", \"94\", \"90-94\") ~ \"90-94\",\n                            age %in% c(\"95\", \"96\", \"97\", \"98\", \"99\", \"95-99\") ~ \"95-99\",\n                            TRUE ~ \"Other\")\n         ) |> \n  group_by(area_type, area, groups) |> \n  mutate(group_sum = sum(total, na.rm = FALSE),\n         group_sum = group_sum / 2,\n         difference = total - group_sum) |> \n  ungroup() |> \n  filter(age == groups) |> \n  filter(total != group_sum) \n\nhead(follow_up)\n\n# A tibble: 6 × 16\n  age    male female total area       area_type age_type male_rural female_rural\n  <fct> <int>  <int> <int> <chr>      <chr>     <chr>         <int>        <int>\n1 0-4       1      5     6 Mt. Kenya… sub-coun… age-gro…         NA           NA\n2 5-9       1      2     3 Mt. Kenya… sub-coun… age-gro…         NA           NA\n3 10-14     6      0     6 Mt. Kenya… sub-coun… age-gro…         NA           NA\n4 15-19     9      1    10 Mt. Kenya… sub-coun… age-gro…         NA           NA\n5 20-24    21      4    25 Mt. Kenya… sub-coun… age-gro…         NA           NA\n6 25-29    59      9    68 Mt. Kenya… sub-coun… age-gro…         NA           NA\n# … with 7 more variables: total_rural <int>, male_urban <int>,\n#   female_urban <int>, total_urban <int>, groups <chr>, group_sum <dbl>,\n#   difference <dbl>\n\nrm(follow_up)\n\nMt. Kenya Forest, Aberdare Forest, Kakamega Forest are all slightly dodgy. It does not seem to be in the documentation, but it looks like the Kenyan government has apportioned these between various countries. This is understandable, and unlikely to be a big deal, so, again, we will just move on.\n\n\n11.3.3 Tidy-up\nNow that we are confident that everything is looking good, we can just convert it to tidy format. This will make it easier to work with.\n\nall <-\n  all |>\n  rename(male_total = male,\n         female_total = female,\n         total_total = total) |>\n  pivot_longer(\n    cols = c(\n      male_total,\n      female_total,\n      total_total,\n      male_rural,\n      female_rural,\n      total_rural,\n      male_urban,\n      female_urban,\n      total_urban\n    ),\n    names_to = \"type\",\n    values_to = \"number\"\n  ) |>\n  separate(\n    col = type,\n    into = c(\"gender\", \"part_of_area\"),\n    sep = \"_\"\n  ) |>\n  select(area, area_type, part_of_area, age, age_type, gender, number)\n\nhead(all)\n\n# A tibble: 6 × 7\n  area    area_type part_of_area age   age_type  gender  number\n  <chr>   <chr>     <chr>        <fct> <chr>     <chr>    <int>\n1 Mombasa county    total        Total age-group male    610257\n2 Mombasa county    total        Total age-group female  598046\n3 Mombasa county    total        Total age-group total  1208303\n4 Mombasa county    rural        Total age-group male        NA\n5 Mombasa county    rural        Total age-group female      NA\n6 Mombasa county    rural        Total age-group total       NA\n\n\nThe original purpose of cleaning this dataset was to make a table for Alexander and Alkema (2021). Just to bring this all together, we could make a graph of single-year counts, by gender, for Nairobi (Figure 11.3)).\n\nmonicas_dataset <- \n  all |> \n  filter(area_type == \"county\") |> \n  filter(part_of_area == \"total\") |>\n  filter(age_type == \"single-year\") |> \n  select(area, age, gender, number)\n\nhead(monicas_dataset)\n\n# A tibble: 6 × 4\n  area    age   gender number\n  <chr>   <fct> <chr>   <int>\n1 Mombasa 0     male    15111\n2 Mombasa 0     female  15009\n3 Mombasa 0     total   30120\n4 Mombasa 1     male    15805\n5 Mombasa 1     female  15308\n6 Mombasa 1     total   31113\n\n\n\nmonicas_dataset |>\n  filter(area == \"Nairobi\") |>\n  filter(gender != \"total\") |>\n  ggplot(aes(x = age, y = number, fill = gender)) +\n  geom_col(aes(x = age, y = number, fill = gender), position = \"dodge\") +\n  scale_y_continuous(labels = scales::comma) +\n  scale_x_discrete(breaks = c(seq(from = 0, to = 99, by = 5), \"100+\")) +\n  theme_classic()+\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(y = \"Number\",\n       x = \"Age\",\n       fill = \"Gender\",\n       caption = \"Data source: 2019 Kenya Census\")\n\n\n\n\nFigure 11.3: Distribution of age and gender in Nairobi in 2019, based on Kenyan census\n\n\n\n\nA variety of features are clear from Figure 11.3, including age-heaping, a slight difference in the ratio of male-female birth, and a substantial difference between ages 15 and 25."
  },
  {
    "objectID": "11-clean_and_prepare.html#checks-and-tests",
    "href": "11-clean_and_prepare.html#checks-and-tests",
    "title": "11  Clean and prepare",
    "section": "11.4 Checks and tests",
    "text": "11.4 Checks and tests\nRobert Caro, the biographer of Lyndon Johnson, spent years tracking down everyone connected to the 36th President of the United States. He went to far as to live in Texas Hill Country for three years so that he could better understand where LBJ was from. When he heard a story that LBJ used to run to the Senate when he was a senator, he ran that route multiple times himself to try to understand why LBJ was running. Caro eventually understood it only when he ran the route as the sun was rising, just as LBJ had done, that the sun hits the Senate Rotunda in a particularly inspiring way (Caro 2019). This background work enabled him to uncover aspects that no one else knew. For instance, it turns out that LBJ almost surely stole his first election win as a Texas Senator (Caro 2019). We need to understand our data to this same extent. We must turn every page and go to every extreme.\nThe idea of negative space is well established in design. It refers to that which surrounds the subject. Sometimes negative space is used as an effect, for instance the logo of FedEx, an American logistics company, has negative space between the E and x that creates an arrow. In a similar way, we want to be cognizant of the data that we have, and the data that we do not have. We are worried that the data that we do not have somehow has meaning, potentially even to the extent of changing our conclusions. When we are cleaning data, we are looking for anomalies. We are interested in values that are in there that should not be, but also the opposite situation—values that are missing that should not be. There are four tools that we use to identify these situations: graphs, counts, green/red conditions, targets.\n\n11.4.1 Graphs\nGraphs are an invaluable tool when cleaning data, because they show each point in the dataset, in relation to the other points. They are especially useful for identifying when a value does not belong. For instance, if a value is expected to be numerical, but it is still a character then it will not plot and a warning will be displayed.\nGraphs will be especially useful for numerical data, but are still useful for text and categorical data. Let us pretend that we have a situation where we are interested in a person’s age, for some youth survey. We have the following data:\n\nraw_data <- \n  tibble(ages = c(11, 17, 22, 13, 21, 16, 16, 6, 16, 11, 150))\n\nraw_data |> \n  ggplot(aes(y = ages, x = 0)) +\n  geom_point()\n\n\n\n\nThe graph clearly shows the unexpected value of 150. The most likely explanation is that the data were incorrectly entered with a trailing 0, and should be 15. We can fix that, and document it, and then redo the graph, so as to see that everything seems more reasonable now.\n\n\n11.4.2 Counts\nWe want to focus on getting most of the data right. So we are interested in the counts of unique values. Hopefully a majority of the data are concentrated in the most common counts. But it can also be useful to invert it, and see what is especially uncommon. The extent to which we want to deal with these depends on what we need. Ultimately, each time we fix one we are getting very few additional observations, potentially even just one! Counts are especially useful with text or categorical data, but can be helpful with numerical as well.\nLet us see an example.\n\nraw_data <- \n  tibble(country = c('Australie', 'Austrelia', 'Australie', 'Australie', 'Aeustralia', 'Austraia', 'Australia', 'Australia', 'Australia', 'Australia'\n                  )\n         )\n\nraw_data |> \n  count(country, sort = TRUE)\n\n# A tibble: 5 × 2\n  country        n\n  <chr>      <int>\n1 Australia      4\n2 Australie      3\n3 Aeustralia     1\n4 Austraia       1\n5 Austrelia      1\n\n\nThe use of this count clearly identifies where we should spend our time - changing ‘Australie’ to ‘Australia’ would almost double our amount of usable data.\n\n\n11.4.3 Go/no-go\nSome things are so important that you require that your cleaned dataset have them. These are go/no-go conditions. They would typically come out of experience, expert knowledge, or the planning and simulation exercises. An example may be that there are no negative numbers in an age column, and no ages above 140.\nFor these we could specifically require that the condition is met. Other examples include:\n\nIf doing cross-country analysis, then a list of country names that we know should be in our dataset would be useful. Our no-go conditions would then be if there were: 1) values not in that list in our dataset, or, vice versa; 2) countries that we expected to be in there that were not.\n\nTo have a concrete example, let us consider if we were doing some analysis about the five largest counties in Kenya: ‘Nairobi’, ‘Kiambu’, ‘Nakuru’, ‘Kakamega’, ‘Bungoma’. Let us create that array first.\n\ncorrect_counties <- c('Nairobi', 'Kiambu', 'Nakuru', 'Kakamega', 'Bungoma')\n\nWe begin with the following dataset.\n\ntop_five_kenya <- \n  tibble(county = c('Nairobi', 'Nairob1', 'Nakuru', 'Kakamega', 'Nakuru', \n                      'Kiambu', 'Kiambru', 'Kabamega', 'Bun8oma', 'Bungoma')\n  )\n\ntop_five_kenya |> \n  count(county, sort = TRUE)\n\n# A tibble: 9 × 2\n  county       n\n  <chr>    <int>\n1 Nakuru       2\n2 Bun8oma      1\n3 Bungoma      1\n4 Kabamega     1\n5 Kakamega     1\n6 Kiambru      1\n7 Kiambu       1\n8 Nairob1      1\n9 Nairobi      1\n\n\nBased on the count we know that we have to fix some of them and there are two with numbers that are obvious fixes.\n\ntop_five_kenya <- \n  top_five_kenya |> \n  mutate(county = str_replace_all(county, 'Nairob1', 'Nairobi'),\n         county = str_replace_all(county, 'Bun8oma', 'Bungoma')\n  )\n\ntop_five_kenya |> \n  count(county, sort = TRUE)\n\n# A tibble: 7 × 2\n  county       n\n  <chr>    <int>\n1 Bungoma      2\n2 Nairobi      2\n3 Nakuru       2\n4 Kabamega     1\n5 Kakamega     1\n6 Kiambru      1\n7 Kiambu       1\n\n\nAt this point we can use our go/no-go conditions to decide whether we are finished or not.\n\ntop_five_kenya$county |> unique()\n\n[1] \"Nairobi\"  \"Nakuru\"   \"Kakamega\" \"Kiambu\"   \"Kiambru\"  \"Kabamega\" \"Bungoma\" \n\nif(all(top_five_kenya$county |> unique() == top_five_kenya)) {\n  \"Oh no\"\n}\nif(all(top_five_kenya==top_five_kenya$county |> unique()) ) {\n  \"Oh no\"\n}\n\nAnd so it is clear that we still have cleaning to do!\nWe may also find similar conditions from experts and those with experience in the particular field.\n\n\n\n\n\n\n\n\n\n\n\n11.4.4 Class\nIt is often said that American society is obsessed with money, while British society is obsessed with class. In the case of data cleaning and preparation we need to be British. Explicit checks of the class of variables are essential. Accidentally assigning the wrong class to a variable can have a large effect on subsequent analysis. In particular:\n\ncheck whether some value should be a number or a factor; and\ncheck that dates are correctly formatted.\n\nTo understand why it is important to be clear about whether a value is a number or a factor, consider the following situation:\n\nsome_data <- \n  tibble(response = c(1, 1, 0, 1, 0, 1, 1, 0, 0),\n         group = c(1, 2, 1, 1, 2, 3, 1, 2, 3)) |> \n  mutate(group_as_integer = as.integer(group),\n         group_as_factor = as.factor(group),\n         )\n\nLet us start with ‘group’ as an integer and look at a logistic regression.\n\nlm(response~group_as_integer, data = some_data) |> \n  summary()\n\n\nCall:\nlm(formula = response ~ group_as_integer, data = some_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -0.68  -0.52   0.32   0.32   0.64 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)\n(Intercept)        0.8400     0.4495   1.869    0.104\ngroup_as_integer  -0.1600     0.2313  -0.692    0.511\n\nResidual standard error: 0.5451 on 7 degrees of freedom\nMultiple R-squared:  0.064, Adjusted R-squared:  -0.06971 \nF-statistic: 0.4786 on 1 and 7 DF,  p-value: 0.5113\n\n\nNow we can try it as a factor. The interpretation of the variable is completely different.\n\nlm(response~group_as_factor, data = some_data) |> \n  summary()\n\n\nCall:\nlm(formula = response ~ group_as_factor, data = some_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7500 -0.3333  0.2500  0.2500  0.6667 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)  \n(Intercept)        0.7500     0.2826   2.654   0.0378 *\ngroup_as_factor2  -0.4167     0.4317  -0.965   0.3717  \ngroup_as_factor3  -0.2500     0.4895  -0.511   0.6278  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5652 on 6 degrees of freedom\nMultiple R-squared:  0.1375,    Adjusted R-squared:  -0.15 \nF-statistic: 0.4783 on 2 and 6 DF,  p-value: 0.6416\n\n\nAnother critical aspect is to check the dates. In particular we want to try to make it into the following format: YYYY-MM-DD. There are of course differences of opinion as to what is an appropriate date format in the broader world, and reasonable people can differ on whether 1 July 2010 or July 1, 2020, is better, but YYYY-MM-DD is the format that is generally most appropriate for data."
  },
  {
    "objectID": "11-clean_and_prepare.html#naming-things",
    "href": "11-clean_and_prepare.html#naming-things",
    "title": "11  Clean and prepare",
    "section": "11.5 Naming things",
    "text": "11.5 Naming things\n\nAn improved scanning software we developed identified gene name errors in 30.9% (3,436/11,117) of articles with supplementary Excel gene lists; a figure significantly higher than previously estimated. This is due to gene names being converted not just to dates and floating-point numbers, but also to internal date format (five-digit numbers).\nAbeysooriya et al. (2021)\n\nNames matter. The land on which much of this book was written is today named Toronto, which is within a country named Canada, but for a long time before was known as Turtle Island. While not common, these days people will sometimes still refer to themselves as being on Turtle Island. That tells us something about them, and our use of the name Canada tells them something about us. There is a big rock in the center of Australia. For a long time, it was called Uluru, then it was known as Ayers Rock. Today it has a dual name that combines both, and the choice of which name you use tells someone something about you. Even the British Royal Family recognize the power of names. In 1917 they changed from the House of Saxe-Coburg and Gotha to the House of Windsor, due to a feeling that the former was too Germanic given World War I was ongoing. Names matter in everyday life. And they matter in data science too.\nThe importance of names, and of ignoring existing claims through re-naming was clear in those cases, but we see it in data science as well. We need to be very careful when we name our datasets, our variables, and our functions. There is a tendency, these days, to call the variable ‘gender’ even though it may only have male and female, because we do not want to say the word ‘sex’. Tukey (1962) essentially defines what we today call data science, but it was popularized by folks in computer science in the 2010s who ignored, either deliberately or through ignorance, what came before them. The past ten years has been characteristic by the renaming of concepts that were well-established in the fields that computer science has recently expanded into. For instance, the use of binary variables in regression, sometimes called ‘dummy variables’, is called one-hot encoding in computer science. Like all fashions, this one will pass also. We most recently saw this through the 1980s through to early 2010s with economics. Economists described themselves as the ‘queen of the social sciences’ and self-described as imperialistic (Lazear 2000). We are now recognizing the costs of this imperialism in social sciences, and in the future we will look back and count the cost of computer science imperialism in data science. The key here is that no area of study is ever terra nullius, or nobody’s land. It is important to recognize, adopt, and use existing names, and practices.\nNames give places meaning, and by ignoring existing names, we ignore what has come before us. Kimmerer (2012, 34) describes how ‘Tahawus is the Algonquin name for Mount Marcy, the highest peak in the Adirondacks. It’s called Mount March to commemorate a governor who never set foot on those wild slopes.’ She continues that ‘[w]hen we call a place by name it is transformed from wilderness to homeland.’ She is talking with regard to physical places, but the same is true of our function names, our variable names and our dataset names. When we use gender instead of sex because we do not want to say sex in front of others, we ignore the preferences of those that provided data.\nIn addition to respecting the nature of the data, names need to satisfy two additional considerations:\n\nthey need to be machine readable, and\nthey need to be human readable.\n\nMachine readable names is an easier standard to meet, but usually means avoiding spaces and special characters. A space can be replaced with a underbar. Usually, special characters should just be removed because they can be inconsistent between different computers and languages. The names should also be unique within a dataset, and unique within a collection of datasets unless that particular column is being deliberately used as a key to join different datasets.\nAn especially useful function to use to get closer to machine readable names is janitor::clean_names() which is from the janitor package (Firke 2020). This deals with those issues mentioned above as well as a few others. We can see an example.\n\nbad_names_good_names <- \n  tibble(\n    'First' = c(1),\n    'second name has spaces' = c(1),\n    'weird#symbol' = c(1),\n    'InCoNsIsTaNtCaPs' = c(1)\n  )\n\nbad_names_good_names\n\n# A tibble: 1 × 4\n  First `second name has spaces` `weird#symbol` InCoNsIsTaNtCaPs\n  <dbl>                    <dbl>          <dbl>            <dbl>\n1     1                        1              1                1\n\nbad_names_good_names <- \n  bad_names_good_names |> \n  janitor::clean_names()\n  \nbad_names_good_names\n\n# A tibble: 1 × 4\n  first second_name_has_spaces weird_number_symbol in_co_ns_is_ta_nt_ca_ps\n  <dbl>                  <dbl>               <dbl>                   <dbl>\n1     1                      1                   1                       1\n\n\nHuman readable names require an additional layer. We need to consider other cultures and how they may interpret some of the names that we are using. We also need to consider different experience levels that subsequent users of your dataset may have. This is both in terms of experience with programming and statistics, but also experience with similar datasets. For instance, a column of ‘flag’ is often used to signal that a column contains data that needs to be followed up with or treated carefully in some way. An experienced analyst will know this, but a beginner will not. Try to use meaningful names wherever possible (Lin, Ali, and Wilson 2020). It has been found that shorter names may take longer to comprehend (Hofmeister, Siegmund, and Holt 2017), and so it is often useful to avoid abbreviations where possible.\nOne interesting feature of R is that in certain cases partial matching on names is possible. For instance:\n\nnever_use_partial_matching <- \n  data.frame(\n    my_first_name = c(1, 2),\n    another_name = c(\"wow\", \"great\")\n  )\n\nnever_use_partial_matching$my_first_name\n\n[1] 1 2\n\nnever_use_partial_matching$my\n\n[1] 1 2\n\n\nThis behavior is not possible within the tidyverse, for instance if data.frame were replaced with tibble in the above code. Partial matching should almost never be used. It makes it more difficult to understand code after a break, and for others to come to it fresh.\nRiederer (2020) advises using column names as contracts, through establishing a controlled vocabulary for column names. In this way, we would define a set of words that we can use in column names. In the controlled vocabulary of Riederer (2020) a column could start with an abbreviation for its class, then something specific to what it pertains to, and then various details. For instance, in the Kenyan data example earlier we have the following column names: “area”, “age”, “gender”, and “number”. If we were to use our column names as contracts, then these could be: “chr_area”, “fctr_group_age”, “chr_group_gender”, and “int_group_count”.\n\ncolumn_names_as_contracts <- \n  monicas_dataset |> \n  rename(\n    \"chr_area\" = \"area\",\n    \"fctr_group_age\" = \"age\",\n    \"chr_group_gender\" = \"gender\",\n    \"int_group_count\" = \"number\"\n  )\n\nWe can then use pointblank (Iannone and Vargas 2022) to set-up tests for us.\n\nlibrary(pointblank)\n\nagent <-\n  create_agent(tbl = column_names_as_contracts) |>\n  col_is_character(columns = vars(chr_area, chr_group_gender)) |>\n  col_is_factor(columns = vars(fctr_group_age)) |>\n  col_is_integer(columns = vars(int_group_count)) |>\n  col_vals_in_set(columns = chr_group_gender,\n                  set = c(\"male\", \"female\", \"total\")) |>\n  interrogate()\n\nagent\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Pointblank Validation\n    \n    \n      [2022-04-14|21:10:56]\ntibble\ncolumn_names_as_contracts\n\n    \n  \n  \n    \n      \n      \n      STEP\n      COLUMNS\n      VALUES\n      TBL\n      EVAL\n      UNITS\n      PASS\n      FAIL\n      W\n      S\n      N\n      EXT\n    \n  \n  \n    \n1\n\n                                                                                                                              c                        \n   col_is_character()\n\n\n\n  \n    ▮chr_area\n  \n\n\n—\n                                                            \n\n✓\n\n1\n11.00\n00.00\n—\n\n—\n\n—\n\n—\n\n    \n2\n\n                                                                                                                              c                        \n   col_is_character()\n\n\n\n  \n    ▮chr_group_gender\n  \n\n\n—\n                                                            \n\n✓\n\n1\n11.00\n00.00\n—\n\n—\n\n—\n\n—\n\n    \n3\n\n                                                                                                                              f                        \n   col_is_factor()\n\n\n\n  \n    ▮fctr_group_age\n  \n\n\n—\n                                                            \n\n✓\n\n1\n11.00\n00.00\n—\n\n—\n\n—\n\n—\n\n    \n4\n\n                                                                                                                              i                        \n   col_is_integer()\n\n\n\n  \n    ▮int_group_count\n  \n\n\n—\n                                                            \n\n✓\n\n1\n11.00\n00.00\n—\n\n—\n\n—\n\n—\n\n    \n5\n\n                                                                                                              \n   col_vals_in_set()\n\n\n\n  \n    ▮chr_group_gender\n  \n\n\nmale, female, total\n\n                                                            \n\n✓\n\n14K\n14K1.00\n00.00\n—\n\n—\n\n—\n\n—\n\n  \n  \n    \n      2022-04-14 21:10:56 EDT\n< 1 s\n2022-04-14 21:10:56 EDT"
  },
  {
    "objectID": "11-clean_and_prepare.html#exercises-and-tutorial",
    "href": "11-clean_and_prepare.html#exercises-and-tutorial",
    "title": "11  Clean and prepare",
    "section": "11.6 Exercises and tutorial",
    "text": "11.6 Exercises and tutorial\n\n11.6.1 Exercises\n\nIs the following an example of tidy data?\n\n\ntibble(name = c('Anne', 'Bethany', 'Stephen', 'William'),\n       age_group = c('18-29', '30-44', '45-60', '60+'),\n       )\n\n# A tibble: 4 × 2\n  name    age_group\n  <chr>   <chr>    \n1 Anne    18-29    \n2 Bethany 30-44    \n3 Stephen 45-60    \n4 William 60+      \n\n\n\nIf I am dealing with ages then what is the most likely class for the variable? [Select all that apply.]\n\ninteger\nmatrix\nnumeric\nfactor\n\n\n\n\n11.6.2 Tutorial\nWith regard to Jordan (2019), D’Ignazio and Klein (2020), Chapter 6, Au (2020), and other relevant work, to what extent do you think we should let the data speak for themselves? [Please write a page or two.]\n\n\n\n\n\nAbeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu, and Mark Ziemann. 2021. “Gene Name Errors: Lessons Not Learned.” PLOS Computational Biology 17 (7): 1–13. https://doi.org/10.1371/journal.pcbi.1008984.\n\n\nAlexander, Monica, and Leontine Alkema. 2021. “A Bayesian Cohort Component Projection Model to Estimate Adult Populations at the Subnational Level in Data-Sparse Settings.” https://arxiv.org/abs/2102.06121.\n\n\nAu, Randy. 2020. “Data Cleaning IS Analysis, Not Grunt Work.” Counting Stuff. https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt.\n\n\nCaro, Robert. 2019. Working. 1st ed. Knopf.\n\n\nCohn, Nate. 2016. We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results.\n\n\nCraiu, Radu V. 2019. “The Hiring Gambit: In Search of the Twofer Data Scientist.” Harvard Data Science Review 1 (1).\n\n\nD’Ignazio, Catherine, and Lauren F Klein. 2020. Data Feminism. Mit Press.\n\n\nFirke, Sam. 2020. Janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGagolewski, Marek. 2020. R Package Stringi: Character String Processing Facilities. http://www.gagolewski.com/software/stringi/.\n\n\nGelman, Andrew, and Eric Loken. 2013. “The Garden of Forking Paths: Why Multiple Comparisons Can Be a Problem, Even When There Is No ‘Fishing Expedition’ or ‘p-Hacking’ and the Research Hypothesis Was Posited Ahead of Time.” Department of Statistics, Columbia University 348.\n\n\nGelman, Andrew, and Aki Vehtari. 2020. “What Are the Most Important Statistical Ideas of the Past 50 Years?” arXiv Preprint arXiv:2012.00174.\n\n\nHalberstam, David. 1972. The Best and the Brightest. Random House.\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nHofmeister, Johannes, Janet Siegmund, and Daniel V. Holt. 2017. “Shorter Identifier Names Take Longer to Comprehend.” In 2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER), 217–27. https://doi.org/10.1109/SANER.2017.7884623.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey Bloem, Pralhad H Burli, Naibin Chen, et al. 2020. “The Influence of Hidden Researcher Decisions in Applied Microeconomics.”\n\n\nIannone, Richard, and Mauricio Vargas. 2022. Pointblank: Data Validation and Organization of Metadata for Local and Remote Tables. https://CRAN.R-project.org/package=pointblank.\n\n\nJordan, Michael I. 2019. “Artificial Intelligence—the Revolution Hasn’t Happened Yet.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.f06c6e61.\n\n\nKimmerer, Robin Wall. 2012. Braiding Sweetgrass. Milkweed Editions.\n\n\nLazear, Edward P. 2000. “Economic Imperialism.” The Quarterly Journal of Economics 115 (1): 99–146.\n\n\nLin, Sarah, Ibraheem Ali, and Greg Wilson. 2020. “Ten Quick Tips for Making Things Findable.” PLoS Computational Biology 16 (12): e1008469.\n\n\nMeng, Xiao-Li. 2018. “Statistical Paradises and Paradoxes in Big Data (i): Law of Large Populations, Big Data Paradox, and the 2016 US Presidential Election.” The Annals of Applied Statistics 12 (2): 685–726.\n\n\nOoms, Jeroen. 2019. Pdftools: Text Extraction, Rendering and Converting of PDF Documents. https://CRAN.R-project.org/package=pdftools.\n\n\nRiederer, Emily. 2020. “Column Names as Contracts.” https://emilyriederer.netlify.app/post/column-name-contracts/.\n\n\n———. 2022. Convo: Enables Conversations and Contracts Through Controlled Vocabulary Naming Conventions. https://github.com/emilyriederer/convo.\n\n\nSilberzahn, Raphael, Eric L Uhlmann, Daniel P Martin, Pasquale Anselmi, Frederik Aust, Eli Awtrey, Štěpán Bahnı́k, et al. 2018. “Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results.” Advances in Methods and Practices in Psychological Science 1 (3): 337–56.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\n\nVan den Broeck, Jan, Solveig Argeseanu Cunningham, Roger Eeckels, and Kobus Herbst. 2005. “Data Cleaning: Detecting, Diagnosing, and Editing Data Abnormalities.” PLoS Medicine 2 (10): e267.\n\n\nWickham, Hadley. 2011. “Testthat: Get Started with Testing.” The R Journal 3: 5–10. https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.\n\n\n———. 2017. Tidyverse: Easily Install and Load the ’Tidyverse’. https://CRAN.R-project.org/package=tidyverse.\n\n\n———. 2019. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2021. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. https://r4ds.had.co.nz/.\n\n\nWilson, Greg. 2021. Building Software Together. CRC Books.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): 1–20. https://doi.org/10.1371/journal.pcbi.1005510."
  },
  {
    "objectID": "12-store_and_share.html",
    "href": "12-store_and_share.html",
    "title": "12  Store and share",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "12-store_and_share.html#introduction",
    "href": "12-store_and_share.html#introduction",
    "title": "12  Store and share",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nAfter we have put together a dataset, an important part of being responsible is storing it appropriately and enabling easy retrieval. While it is certainly possible to be especially concerned about this, and entire careers are based on the storage and retrieval of data, to a certain extent, the baseline is not onerous. If we can get our dataset off our own computer, then we are much of the way there. Further confirming that someone else can retrieve it and use it, puts us much further than most.\nThat said, the FAIR principles are useful when we come to think more formally about data sharing and management. These are (Wilkinson et al. 2016):\n\nFindable. There is one, unchanging, identifier for the dataset and the dataset has high-quality descriptions and explanations.\nAccessible. Standardized approaches can be used to retrieve the data, and these are open and free, possibly with authentication, and that the metadata persists even if the dataset is removed.\nInteroperable. The dataset and its metadata use a broadly applicable language, and vocabulary.\nReusable. There is plenty of description of the dataset and the usage conditions are made clear along with provenance.\n\nIt is important to recognize that just because a dataset is FAIR, it is not necessarily an unbiased representation of the world. FAIR reflects whether a dataset is appropriately available, not whether it is appropriate.\nOne reason for the rise of data science is that humans are at the heart of it. And often the data that we are interested in directly concern humans. This means there is a tension between sharing the data to facilitate reproducibility and maintaining privacy. Medicine has developed approaches to this over a long time. And out of that we have seen Health Insurance Portability and Accountability Act (HIPAA) in the US, and the related General Data Protection Regulation (GDPR) in Europe. Our concerns in data science tend to be about personally identifying information (PII). We have a variety of ways to protect especially private information in our datasets, such as emails, including hashing. And sometimes we simulate data and distribute that instead of sharing the actual dataset. More recently, differential privacy is being implemented. But this is usually an inappropriate choice, for anything other than the most massive of datasets and ensures a level of privacy, only at the expense of population minorities.\nIn this chapter we will consider how we plan and organize our datasets to meet essential requirements. To a large extent we put these in place to make our own life easier when we come back to use our dataset later. We then go through putting our dataset on GitHub, building R packages for data, and finally depositing it in various archives. Then we consider documentation, and in particular focus on datasheets."
  },
  {
    "objectID": "12-store_and_share.html#plan",
    "href": "12-store_and_share.html#plan",
    "title": "12  Store and share",
    "section": "12.2 Plan",
    "text": "12.2 Plan\nThe storage and retrieval of information has a long history. This is especially connected with libraries which have existed since antiquity and have established protocols for deciding what information to store and what to discard, as well as its retrieval. One of the defining aspects of libraries is that deliberate curation and organization. The cataloging system ensures that books on similar topics are located close to each other, and there are typically also deliberate plans for ensuring the collection is up-to-date.\nVannevar Bush defines a ‘memex’ in 1945 as a device used to store books, records, and communications, in a way that supplements memory (Bush 1945). And the key is the indexing, or linking together of items. We can see this concept echoed in Tim Berners-Lee proposal for hypertext (Berners-Lee 1989), which led to the World Wide Web. This is the way that resources are identified. They are then transported over the Internet, using HTTP.\nAt its most fundamental, this is about storing and retrieving data. For instance, we make various files on our computer available to others. The internet is famously brittle, but when we are considering the storage and retrieval of our datasets we want to consider especially, for how long it is important that the data are stored and for whom (Michener 2015). For instance, if we want some data to be available for a decade and widely available then it becomes important to store data in open and persistent formats, such as CSVs (Hart et al. 2016). But if we are just using some data as part of an intermediate step, we have the raw data, and the scripts to create it, then it might be fine to not worry too much about such considerations.\nStoring raw data is important and there are many cases where raw data have revealed or hinted at fraud (Simonsohn 2013). Shared data also enhances the credibility of our work, by enabling others to verify it, and can lead to the generation of new knowledge as others use it to answer different questions (Christensen, Freese, and Miguel 2019). Finally, research that shares its data may be more highly cited (Christensen et al. 2019)."
  },
  {
    "objectID": "12-store_and_share.html#share-data",
    "href": "12-store_and_share.html#share-data",
    "title": "12  Store and share",
    "section": "12.3 Share data",
    "text": "12.3 Share data\n\n12.3.1 GitHub\nThe easiest place to store our datasets will be GitHub because that is already built into our workflow. For instance, when we push, our dataset becomes available. One great benefit of this is that, if we have set-up our workspace appropriately, then we likely store our raw data, and the tidy data, as well as the scripts that are needed to transform one to the other.\nAs an example of how we have stored some data, we can access the ‘raw_data.csv’ file from the ‘starter_folder’. To get the file that we pass to read_csv() we navigate to the file in GitHub, and then click ‘Raw’.\n\nlibrary(tidyverse)\n\nstarter_data <- read_csv(\"https://raw.githubusercontent.com/RohanAlexander/starter_folder/main/inputs/data/raw_data.csv\")\n\nstarter_data\n\n# A tibble: 1 × 3\n  first_col second_col third_col\n  <chr>     <chr>      <chr>    \n1 some      raw        data     \n\n\nOne issue with this is that the dataset does not have much documentation. While we can store and retrieve the dataset easily in this way, it lacks much explanation, a formal dictionary, and aspects such as a license that would bring our dataset closer to aligning with the FAIR principles.\n\n\n12.3.2 R Packages for data\nTo this point we have largely used R packages for their code, although we have seen a few that were focused on sharing data, for instance, Flynn (2021). We can build an R Package for our dataset and then add it to GitHub. This will make it easy to store and retrieve because we can obtain the dataset by loading the package. This will be the first R package that we build. In Chapter @ref(deploying-models), will return to R packages and use them to deploy models.\nTo get started, create a new package (‘File’ -> ‘New project’ -> ‘New Directory’ -> ‘R Package’). Give the package a name, such as ‘favcolordata’ and select ‘Open in new session’. Create a new folder called ‘data’. We will simulate a dataset that we will include.\n\nlibrary(tidyverse)\n\nset.seed(853)\n\ncolor_data <-\n    tibble(\n        name =\n            c(\"Edward\",\n              \"Helen\",\n              \"Hugo\",\n              \"Ian\",\n              \"Monica\",\n              \"Myles\",\n              \"Patricia\",\n              \"Roger\",\n              \"Rohan\",\n              \"Ruth\"\n            ),\n        fav_color =\n            sample(\n                x = c(\"Black\", \"White\", \"Rainbow\"),\n                size = 10,\n                replace = TRUE\n            )\n    )\n\nTo this point we have largely been trying to use CSV files for our datasets. To include our data in this R package, we will save our dataset in a particular format, ‘rda’, using save().\n\nsave(color_data, file=\"data/color_data.rda\")\n\nThen create an R file ‘data.R’ in the ‘R’ folder. This R file will only contain documentation using roxygen2 comments, which start with #', and we follow the documentation for troopdata closely.\n\n#' Favorite color of various people data\n#'\n#' @description \\code{favcolordata} returns a dataframe of the favorite color of various people.\n#' \n#' @return Returns a dataframe of the favorite color of various people.\n#' \n#' @docType data\n#'\n#' @usage data(color_data)\n#'\n#' @format An dataframe of individual-level observations with the following variables: \n#'\n#' \\describe{\n#' \\item{\\code{name}}{A character vector of individual names.}\n#' \\item{\\code{fav_color}}{A character vector of one of: black, white, rainbow.}\n#' }\n#'\n#' @keywords datasets\n#'\n#' @source \\url{https://www.tellingstorieswithdata.com/storing-and-retrieving-data.html}\n#'\n\"color_data\"\n\nFinally, we should add a README which provides a summary of all of this for someone coming to the project from the outside.\nAt this we can go to the ‘Build’ tab and then ‘Install and Restart’. After this happens, the package ‘favcolordata’, is loaded and the data can be accessed using ‘color_data’. If we were to push this to GitHub, then anyone would be able to install the package using devtools (Wickham, Hester, and Chang 2020) and then use our dataset.\n\ndevtools::install_github(\"RohanAlexander/favcolordata\")\n\nlibrary(favcolordata)\n\ncolor_data\n\nThis has addressed many of the issues that we faced earlier. For instance, we have included a README and a data dictionary of sorts in terms of the descriptions that we added. But if we were to try to put this package onto CRAN, then we might face some issues. For instance, the maximum size of a package is 5MB and we would quickly come up against that. We have also largely forced users to use R, and while there are considerable benefits of that, we may like to be more language agnostic (Tierney and Ram 2020).\n\n\n12.3.3 Depositing data\nWhile it is possible that a dataset will be cited if it is available through GitHub or an R package, this becomes more likely if the dataset is deposited somewhere. There are several reasons for this, but one is that it seems a bit more formal. Zenodo and Open Science Framework (OSF) are two that are commonly used. For instance, C. Carleton (2021) use Zenodo to share the dataset and analysis supporting W. C. Carleton, Campbell, and Collard (2021). Similarly Michael et al. (2021) use Zenodo to share the dataset that underpins Geuenich et al. (2021). \nAnother option is a dataverse, such as the Harvard Dataverse, which is a common requirement for journal publications. One nice aspect of this is that we can use dataverse (Kuriwaki, Beasley, and Leeper 2022) to retrieve the dataset as part of a reproducible workflow."
  },
  {
    "objectID": "12-store_and_share.html#documentation",
    "href": "12-store_and_share.html#documentation",
    "title": "12  Store and share",
    "section": "12.4 Documentation",
    "text": "12.4 Documentation\nDatasheets (Gebru et al. 2021) are an increasingly critical aspect of data science. Datasheets are basically nutrition labels for datasets. The process of creating them enables us to think more carefully about what we will feed our model. More importantly, they enable others to better understand what we fed our model. One important task is going back and putting together datasheets for datasets that are widely used. For instance, researchers went back and wrote a datasheet for one of the most popular datasets in computer science, and they found that around 30 per cent of the data were duplicated (Bandy and Vincent 2021).\nInstead of telling us how unhealthy various foods are, a datasheet tells us things like:\n\nWho put the dataset together?\nWho paid for the dataset to be created?\nHow complete is the dataset?\nWhat fields are present, and equally not present, for particular observations?\n\nSometimes we have done a lot of work to create a dataset. In that case, we may like to publish and share it on its own, for instance, Biderman, Bicheno, and Gao (2022) and Bandy and Vincent (2021). But typically a datasheet might live in an appendix to the paper, or be included in a file adjacent to the dataset.\nWe will put together a datasheet for the dataset that underpins Alexander and Hodgetts (2021). The text of the questions directly comes from Gebru et al. (2021). When we create datasheets for a dataset, especially a dataset that we did not put together ourselves, it is possible that the answer to some questions will simply be “Unknown”.\nMotivation\n\nFor what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\n\nThe dataset was created to enable analysis of Australian politicians. We were unable to find a publicly available dataset in a structured format that had the biographical and political information on Australian politicians that was needed for modelling.\n\nWho created the dataset (for example, which team, research group) and on behalf of which entity (for example, company, institution, organization)?\n\nRohan Alexander, while working at the Australian National University and the University of Toronto\n\nWho funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.\n\nNo direct funding was received for this project, but Rohan received a salary from University of Toronto.\n\nAny other comments?\n\nNo.\n\n\nComposition\n\nWhat do the instances that comprise the dataset represent (for example, documents, photos, people, countries)? Are there multiple types of instances (for example, movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\n\nEach row of the main dataset is an individual, and these then link to other datasets where each row refers to various information about that person.\n\nHow many instances are there in total (of each type, if appropriate)?\n\nA little more than 1700.\n\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (for example, geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (for example, to cover a more diverse range of instances, because instances were withheld or unavailable).\n\nAll individuals elected or appointed to the Australian Federal Parliament are in the dataset.\n\nWhat data does each instance consist of? “Raw” data (for example, unprocessed text or images) or features? In either case, please provide a description.\n\nEach instance consists of biographical information such as birthdate, or political information, such as political party membership.\n\nIs there a label or target associated with each instance? If so, please provide a description.\n\nYes there is a unique key comprising the surname and year of birth, with a few individuals needing additional demarcation.\n\nIs any information missing from individual instances? If so, please provide a description, explaining why this information is missing (for example, because it was unavailable). This does not include intentionally removed information, but might include, for example, redacted text.\n\nBirthdate is not available in all cases, especially earlier in the dataset.\n\nAre relationships between individual instances made explicit (for example, users’ movie ratings, social network links)? If so, please describe how these relationships are made explicit.\n\nYes, through the uniqueID.\n\nAre there recommended data splits (for example, training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.\n\nNo.\n\nAre there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.\n\nThere is some uncertainty about cabinet and ministries. For instance, different sources differ. There is also a little bit of uncertainty about birthdates.\n\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (for example, websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (that is, including the external resources as they existed at the time the dataset was created); c) are there any restrictions (for example, licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.\n\nSelf-contained.\n\nDoes the dataset contain data that might be considered confidential (for example, data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals’ non-public communications)? If so, please provide a description.\n\nNo, all data were gathered from public sources.\n\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.\n\nNo.\n\nDoes the dataset identify any sub-populations (for example, by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.\n\nYes, age and gender.\n\nIs it possible to identify individuals (that is, one or more natural persons), either directly or indirectly (that is, in combination with other data) from the dataset? If so, please describe how.\n\nYes, individuals are identified by name.\n\nDoes the dataset contain data that might be considered sensitive in any way (for example, data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.\n\nThe dataset contains sensitive information, such as political membership, however this is all public knowledge as they are federal politicians.\n\nAny other comments?\n\nNo.\n\n\nCollection process\n\nHow was the data associated with each instance acquired? Was the data directly observable (for example, raw text, movie ratings), reported by subjects (for example, survey responses), or indirectly inferred/derived from other data (for example, part-of-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\n\nThe data were gathered from the Australian Parliamentary Handbook in the first instance, and this was augmented with information from other parliaments, especially Victoria and New South Wales, and Wikipedia.\n\nWhat mechanisms or procedures were used to collect the data (for example, hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated?\n\nScraping and parsing using R.\n\nIf the dataset is a sample from a larger set, what was the sampling strategy (for example, deterministic, probabilistic with specific sampling probabilities)?\n\nThe dataset is not a sample.\n\nWho was involved in the data collection process (for example, students, crowdworkers, contractors) and how were they compensated (for example, how much were crowdworkers paid)?\n\nRohan Alexander. Paid as a post-doc and an assistant professor, although this was not tied to this specific project.\n\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (for example, recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.\n\nThree years, and then updated from time to time.\n\nWere any ethical review processes conducted (for example, by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.\n\nNo.\n\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (for example, websites)?\n\nThird parties in almost all cases.\n\nWere the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.\n\nNo.\n\nDid the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.\n\nNo.\n\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).\n\nConsent was not obtained.\n\nHas an analysis of the potential impact of the dataset and its use on data subjects (for example, a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.\n\nNo.\n\nAny other comments?\n\nNo.\n\n\nPreprocessing/cleaning/labeling\n\nWas any preprocessing/cleaning/labeling of the data done (for example, discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section.\n\nYes cleaning of the data was done.\n\nWas the “raw” data saved in addition to the preprocessed/cleaned/labeled data (for example, to support unanticipated future uses)? If so, please provide a link or other access point to the “raw” data.\n\nIn general, no. The scripts that got the data from the parliamentary handbook to CSV are not available. There are scripts that go through Wikipedia and check things and these are available.\n\nIs the software that was used to preprocess/clean/label the data available? If so, please provide a link or other access point.\n\nR was used.\n\nAny other comments?\n\nNo\n\n\nUses\n\nHas the dataset been used for any tasks already? If so, please provide a description.\n\nYes, a few papers about Australian politics, for instance, https://arxiv.org/abs/2111.09299.\n\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\n\nNo\n\nWhat (other) tasks could the dataset be used for?\n\nLinking with elections would be interesting.\n\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (for example, stereotyping, quality of service issues) or other risks or harms (for example, legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms?\n\nNo.\n\nAre there tasks for which the dataset should not be used? If so, please provide a description.\n\nNo.\n\nAny other comments?\n\nNo.\n\n\nDistribution\n\nWill the dataset be distributed to third parties outside of the entity (for example, company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.\n\nThe dataset is available through GitHub.\n\nHow will the dataset be distributed (for example, tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?\n\nGitHub for now, and eventually a deposit.\n\nWhen will the dataset be distributed?\n\nThe dataset is available now.\n\nWill the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/ or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.\n\nNo. MIT license.\n\nHave any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.\n\nNone that are known.\n\nDo any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.\n\nNone that are known.\n\nAny other comments?\n\nNo.\n\n\nMaintenance\n\nWho will be supporting/hosting/maintaining the dataset?\n\nRohan Alexander\n\nHow can the owner/curator/manager of the dataset be contacted (for example, email address)?\n\nrohan.alexander@utoronto\n\nIs there an erratum? If so, please provide a link or other access point.\n\nNo, the dataset is just updated.\n\nWill the dataset be updated (for example, to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (for example, mailing list, GitHub)?\n\nYes, roughly quarterly.\n\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (for example, were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.\n\nNo.\n\nWill older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.\n\nNo the dataset is just updated. Although a history is available through GitHub.\n\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.\n\nPull request on GitHub.\n\nAny other comments?\n\nNo"
  },
  {
    "objectID": "12-store_and_share.html#personally-identifying-information",
    "href": "12-store_and_share.html#personally-identifying-information",
    "title": "12  Store and share",
    "section": "12.5 Personally identifying information",
    "text": "12.5 Personally identifying information\nPersonally identifying information (PII) is that which enables us to link a row in our dataset with an actual person. For instance, email addresses are often PII, as are names and addresses. Interestingly, sometimes the combination of several variables, none of which are PII in and of themselves, can be PII. For instance, age is unlikely PII by itself, but age combined with city, education, and a few other variables could be. One concern is that this re-identification can occur across datasets. Another interesting aspect is that again while some variable may not be PII for almost everyone in the dataset, it can become PII in the extreme. For instance, if we have age, then there are many people of most ages, but there are fewer people in ages over 100 and it likely becomes PII. The same scenario happens with both income and wealth. One response to this is for data to be censored. For instance, we may record age between 0 and 100, and then group everyone over that into ‘101+’.\nZook et al. (2017) recommend considering whether data even need to be gathered in the first place. For instance, if a phone number is not absolutely required then it might be better to not gather it in the first place, rather than need to worry about protecting it before data dissemination.\nGDPR and HIPAA are two legal structures that govern data in Europe and the US, respectively. Due to the influence of these regions, they have a significant effect outside those regions also. GDPR concerns data generally, while HIPAA is focused on healthcare. GDPR applies to all personal data, which is defined as:\n\n‘personal data’ means any information relating to an identified or identifiable natural person (‘data subject’); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person;\nCouncil of European Union (2016), Article 4, ‘Definitions’\n\nHIPAA refers to the privacy of medical records in the US and codify the idea that the patient should have access to their medical records, and that only the patient should be able to authorize access to their medical records (Annas 2003). While it only applies to covered entities, it sets a standard that informs practice, yet is piecemeal given the variety of applications. For instance, a person’s social media posts about their health is generally not subject to it, nor is knowledge about a person’s location and how active they are, even though based on that information we may be able to get some idea of their health (Cohen and Mello 2018). Such data are hugely valuable (Ross 2022).\nThere are a variety of ways of protecting PII, while sharing data, that we will now go through.\n\n12.5.1 Hashing and salting\nA hash is a one-way transformation of data, such that the same input always provides the same output, but given the output, it is not reasonably possible to obtain the input. For instance, a function that doubled its input always gives the same output, for the same input, but is also easy to reverse.\nKnuth (1998, 514) relates an interesting etymology for ‘hash’ by first defining ‘to hash’ as relating to chop up or make a mess, and then explaining that hashing relates to scrambling the input and using this partial information to define the output. A collision is when different inputs map to the same output, and one feature of a good hashing algorithm is that collisions are reduced. One simple approach is to rely on the modulo operator. For instance, if we were interested in 10 different groupings for the integers 1 through to 10. A better approach would be for the number of groupings to be a prime number, such as 11 or 853.\n\nlibrary(tidyverse)\n\nhashing <- \n  tibble(ppi_data = c(1:10),\n         modulo_ten = ppi_data %% 10,\n         modulo_eleven = ppi_data %% 11,\n         modulo_eightfivethree = ppi_data %% 853)\n\nhashing\n\n# A tibble: 10 × 4\n   ppi_data modulo_ten modulo_eleven modulo_eightfivethree\n      <int>      <dbl>         <dbl>                 <dbl>\n 1        1          1             1                     1\n 2        2          2             2                     2\n 3        3          3             3                     3\n 4        4          4             4                     4\n 5        5          5             5                     5\n 6        6          6             6                     6\n 7        7          7             7                     7\n 8        8          8             8                     8\n 9        9          9             9                     9\n10       10          0            10                    10\n\n\nRather than worry about things ourselves, we can use various hash functions from openssl (Ooms 2021) including sha512() and md5().\n\nlibrary(openssl)\n\nopenssl_hashing <- \n  tibble(names =\n            c(\"Edward\",\n              \"Helen\",\n              \"Hugo\",\n              \"Ian\",\n              \"Monica\",\n              \"Myles\",\n              \"Patricia\",\n              \"Roger\",\n              \"Rohan\",\n              \"Ruth\"\n            )) |> \n  mutate(md5 = md5(names),\n         sha512 = sha512(names)\n  )\n\nopenssl_hashing\n\n# A tibble: 10 × 3\n   names    md5                              sha512                             \n   <chr>    <hash>                           <hash>                             \n 1 Edward   243f63354f4c1cc25d50f6269b844369 5759ada975e7cb87c28fe1b6ea5f6a1d51…\n 2 Helen    29e00d3659d1c5e75f99e892f0c1a1f1 6ee4156ca7e8e9a5acc42704363f35e278…\n 3 Hugo     1b3840b0b70d91c17e70014c8537dbba b1e441a54866906727d842e6a064ba8fb9…\n 4 Ian      245a58a5dc42397caf57bc06c2c0afd2 d3cf9cdaea6ffdfd8b8d143fe609bc35b9…\n 5 Monica   09084cc0cda34fd80bfa3cc0ae8fe3dc 84250b971b87728aa4ab24cec405864ed9…\n 6 Myles    fafdf519cb5877d4751b4cbe6f3f534a 4eae7c19d5c5d4ffffd8c2f97ed7df69f7…\n 7 Patricia 54a7b18f26374fc200ddedde0844f8ec e511593a2db805e51ed0bb74c96d0db652…\n 8 Roger    efc5c58b9a85926a31587140cbeb0220 f63ab236a5b0135f71c32ee39517001b21…\n 9 Rohan    02df8936eee3d4d2568857ed530671b2 5111e18391d41fbabd9005b85dc2413f2a…\n10 Ruth     8e06843ec162b74a7902867dd4bca8c8 d7e7d23b69e37208002cdfa7dc7a0fa6f6…\n\n\nWe could share either of these and be comfortable that, in general, it would be difficult for someone to use only that information to recover the names of our respondents. That is not to say that it is impossible. If we made a mistake, such as accidentally committing the original dataset to GitHub then they could be recovered. And of course, it is likely that various governments have the ability to reverse the cryptographic hashes used here.\nOne issue that remains is that anyone can take advantage of the key feature of hashing being that the same input always gets the same output, to test various options for inputs. For instance, they could, themselves try to has ‘Rohan’, and then noticing that the hash is the same as the one that we published in our dataset, know that data relates to that particular individual. We could try to keep our hashing approach secret, but that is difficult as there are only a few that was widely used. One approach is to add a salt that we keep secret. This slightly changes the input. For instance, we could add the salt ’_is_a_person’ to all our names and then hash that, although a large random number might be a better option. Provided the salt is not shared, then it would be difficult for most folks to reverse our approach in that way.\n\nopenssl_hashing_with_salt <- \n  tibble(names =\n            c(\"Edward\",\n              \"Helen\",\n              \"Hugo\",\n              \"Ian\",\n              \"Monica\",\n              \"Myles\",\n              \"Patricia\",\n              \"Roger\",\n              \"Rohan\",\n              \"Ruth\"\n            )\n         ) |> \n  mutate(names = paste0(names, \"_is_a_person\")) |> \n  mutate(md5 = md5(names),\n         sha512 = sha512(names)\n         )\n\nopenssl_hashing_with_salt\n\n# A tibble: 10 × 3\n   names                md5                              sha512                 \n   <chr>                <hash>                           <hash>                 \n 1 Edward_is_a_person   9845500d4070c0cbba7c6b81ed306027 e8ce00d98d5d2f22f7c2bf…\n 2 Helen_is_a_person    7e4a77b41fb6e108618f93fb9f47bae3 b066b72ebafcb77c765de7…\n 3 Hugo_is_a_person     b9b8c4e9870aca482cf062da4681b232 07c18ebb565cc86b84a523…\n 4 Ian_is_a_person      9b1ad8fbbc190c2e3ce74372029f9735 eb9928f62e7004aa0a6ef7…\n 5 Monica_is_a_person   50bb9dfffa926c855b830845ac61b659 ef429119529d22b72aedbd…\n 6 Myles_is_a_person    3635be5fe758ed1fc0d9c78fc0b26458 795dce5816cf5a070ac87c…\n 7 Patricia_is_a_person 4e4a5ed8842fd7caad320c3a92bf07db 1b44649bc13fa3c505a29c…\n 8 Roger_is_a_person    ea1d56e89771d8b0a7b5981324424ec2 2f753bc3f871d60889c6d5…\n 9 Rohan_is_a_person    3ab064d7f746fde604122d072fd4fa97 cc9c7c3d9da05da52f394e…\n10 Ruth_is_a_person     8b83f4285ac30a3efa5ede3636b7d687 6d329fde75a259b7de137e…\n\n\n\n\n12.5.2 Data simulation\nOne common approach to deal with the issue of being unable to share the actual data that underpins an analysis, is to use data simulation. We have used data simulation throughout this book toward the start of the workflow to help us to think more deeply about our dataset before we turn to it. We can use data simulation again at the end, to ensure that others cannot think about our actual dataset. The workflow advocated in this book makes this relatively straight-forward.\nThe approach is to understand the critical features of the dataset and the appropriate distribution. For instance, if our data were the ages of some population, then we may want to use the Poisson distribution and experiment with different parameters for lambda. Having simulated a dataset, we conduct our analysis using this simulated dataset and ensure that the results are broadly similar to when we use the real data. We can then release the simulated dataset along with our code.\nFor more nuanced situations, Koenecke and Varian (2020) recommend using the synthetic data vault (Patki, Wedge, and Veeramachaneni 2016) and then the use of Generative Adversarial Networks, such as implemented by Athey et al. (2021).\n\n\n12.5.3 Differential privacy\nDifferential privacy implements a mathematical definition of privacy, that means that even if datasets are combined, a certain level of privacy will be maintained. A dataset is differentially private to different levels, based on how much it changes when one person’s results are removed.\nA variant of differential privacy has recently been implemented by the US census. This has been shown to not universally protect respondent privacy, and yet it is expected to have a significant effect on redistricting (Kenny et al. 2021). Suriyakumar et al. (2021) found that such model will be disproportionately affected by large demographic groups. The implementation of differential privacy is expected to result in publicly available data that are unusable in the social sciences (Ruggles et al. 2019)."
  },
  {
    "objectID": "12-store_and_share.html#exercises-and-tutorial",
    "href": "12-store_and_share.html#exercises-and-tutorial",
    "title": "12  Store and share",
    "section": "12.6 Exercises and tutorial",
    "text": "12.6 Exercises and tutorial\n\n12.6.1 Exercises\n\nFollowing Wilkinson et al. (2016), which of the following are FAIR principles (please select all that apply)?\n\nFindable.\nApproachable.\nInteroperable.\nReusable.\nIntegrated.\nFungible.\nReduced.\nAccessible.\n\nPlease create an R package for a simulated dataset, push it to GitHub, and submit the link.\nPlease simulate some data, add it to a GitHub repository and then submit the link.\nAccording to Gebru et al. (2021), a datasheet should document a dataset’s (please select all that apply):\n\ncomposition.\nrecommended uses.\nmotivation.\ncollection process.\n\nDo you think that a person’s name is PII?\n\n\nYes.\nNo.\n\n\nUnder what circumstances do you think income is PII (please write a paragraph or two)?\nUsing openssl::md5() what is the hash of “Rohan” (pick one)?\n\n243f63354f4c1cc25d50f6269b844369\n09084cc0cda34fd80bfa3cc0ae8fe3dc\n02df8936eee3d4d2568857ed530671b2\n1b3840b0b70d91c17e70014c8537dbba\n\n\n\n\n12.6.2 Tutorial\nPlease identify a dataset you consider interesting and important, that does not have a datasheet (Gebru et al. 2021). As a reminder, datasheets accompany datasets and document ‘motivation, composition, collection process, recommended uses,’ among other aspects. Please put together a datasheet for this dataset. You are welcome to use the template here as a starting point. The datasheet should be completely contained in its own GitHub repository. Please submit a PDF.\n\n\n12.6.3 Paper\nAt about this point, Paper Four (Appendix @ref(paper-four)) would be appropriate.\n\n\n\n\n\n\n\n\n\nAlexander, Rohan, and Paul A. Hodgetts. 2021. AustralianPoliticians: Provides Datasets about Australian Politicians. https://CRAN.R-project.org/package=AustralianPoliticians.\n\n\nAnnas, George J. 2003. “HIPAA Regulations: A New Era of Medical-Record Privacy?” New England Journal of Medicine 348: 1486.\n\n\nAthey, Susan, Guido W Imbens, Jonas Metzger, and Evan Munro. 2021. “Using Wasserstein Generative Adversarial Networks for the Design of Monte Carlo Simulations.” Journal of Econometrics.\n\n\nBandy, Jack, and Nicholas Vincent. 2021. “Addressing \"Documentation Debt\" in Machine Learning Research: A Retrospective Datasheet for BookCorpus.” https://arxiv.org/abs/2105.05241.\n\n\nBerners-Lee, Timothy J. 1989. “Information Management: A Proposal.”\n\n\nBiderman, Stella, Kieran Bicheno, and Leo Gao. 2022. “Datasheet for the Pile.” https://arxiv.org/abs/2201.07311.\n\n\nBush, Vannevar. 1945. “As We May Think.” The Atlantic Monthly 176 (1): 101–8.\n\n\nCarleton, Chris. 2021. Wccarleton/Conflict-Europe: Acce (version v1.0.0). Zenodo. https://doi.org/10.5281/zenodo.4550688.\n\n\nCarleton, W Christopher, Dave Campbell, and Mark Collard. 2021. “A Reassessment of the Impact of Temperature Change on European Conflict During the Second Millennium CE Using a Bespoke Bayesian Time-Series Model.” Climatic Change 165 (1): 1–16.\n\n\nChristensen, Garret, Allan Dafoe, Edward Miguel, Don A Moore, and Andrew K Rose. 2019. “A Study of the Impact of Data Sharing on Article Citations Using Journal Policies as a Natural Experiment.” PLoS One 14 (12): e0225883.\n\n\nChristensen, Garret, Jeremy Freese, and Edward Miguel. 2019. Transparent and Reproducible Social Science Research. University of California Press.\n\n\nCohen, I. Glenn, and Michelle M. Mello. 2018. “HIPAA and Protecting Health Information in the 21st Century.” JAMA 320 (3): 231. https://doi.org/10.1001/jama.2018.5630.\n\n\nCouncil of European Union. 2016. “General Data Protection Regulation 2016/679.”\n\n\nFlynn, Michael. 2021. Troopdata: Tools for Analyzing Cross-National Military Deployment and Basing Data. https://CRAN.R-project.org/package=troopdata.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021. “Datasheets for Datasets.” Communications of the ACM 64 (12): 86–92.\n\n\nGeuenich, Michael J, Jinyu Hou, Sunyun Lee, Shanza Ayub, Hartland W Jackson, and Kieran R Campbell. 2021. “Automated Assignment of Cell Identity from Single-Cell Multiplexed Imaging and Proteomic Data.” Cell Systems 12 (12): 1173–86.\n\n\nHart, Edmund M, Pauline Barmby, David LeBauer, François Michonneau, Sarah Mount, Patrick Mulrooney, Timothée Poisot, Kara H Woo, Naupaka B Zimmerman, and Jeffrey W Hollister. 2016. “Ten Simple Rules for Digital Data Storage.” Public Library of Science San Francisco, CA USA.\n\n\nKenny, Christopher T., Shiro Kuriwaki, Cory McCartan, Evan Rosenman, Tyler Simko, and Kosuke Imai. 2021. “The Impact of the u.s. Census Disclosure Avoidance System on Redistricting and Voting Rights Analysis.” https://arxiv.org/abs/2105.14197.\n\n\nKnuth, Donald E. 1998. Art of Computer Programming, Volume 2: Seminumerical Algorithms. 2nd ed.\n\n\nKoenecke, Allison, and Hal Varian. 2020. “Synthetic Data Generation for Economists.” https://arxiv.org/abs/2011.01374.\n\n\nKröger, Jacob Leon, Milagros Miceli, and Florian Müller. 2021. “How Data Can Be Used Against People: A Classification of Personal Data Misuses.” Available at SSRN 3887097.\n\n\nKuriwaki, Shiro, Will Beasley, and Thomas J. Leeper. 2022. Dataverse: R Client for Dataverse 4+ Repositories.\n\n\nMichael, Geuenich, Hou Jinyu, Lee Sunyun, Ayub Shanza, Jackson Hartland, and Campbell Kieran. 2021. “Automated assignment of cell identity from single- cell multiplexed imaging and proteomic data.” Zenodo. https://doi.org/10.5281/zenodo.5156049.\n\n\nMichener, William K. 2015. “Ten Simple Rules for Creating a Good Data Management Plan.” PLoS Computational Biology 11 (10): e1004525. https://doi.org/https://doi.org/10.1371/journal.pcbi.1004525.\n\n\nOoms, Jeroen. 2021. Openssl: Toolkit for Encryption, Signatures and Certificates Based on OpenSSL. https://CRAN.R-project.org/package=openssl.\n\n\nPatki, Neha, Roy Wedge, and Kalyan Veeramachaneni. 2016. “The Synthetic Data Vault.” In 2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA), 399–410. IEEE.\n\n\nPaullada, Amandalynne, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna. 2021. “Data and Its (Dis)contents: A Survey of Dataset Development and Use in Machine Learning Research.” Patterns 2 (11): 100336. https://doi.org/10.1016/j.patter.2021.100336.\n\n\nRoss, Casey. 2022. “How a Decades-Old Database Became a Hugely Profitable Dossier on the Health of 270 Million Americans.” Stat, February. https://www.statnews.com/2022/02/01/ibm-watson-health-marketscan-data/.\n\n\nRuggles, Steven, Catherine Fitch, Diana Magnuson, and Jonathan Schroeder. 2019. “Differential Privacy and Census Data: Implications for Social and Economic Research.” In AEA Papers and Proceedings, 109:403–8.\n\n\nSimonsohn, Uri. 2013. “Just Post It: The Lesson from Two Cases of Fabricated Data Detected by Statistics Alone.” Psychological Science 24 (10): 1875–88.\n\n\nSuriyakumar, Vinith M., Nicolas Papernot, Anna Goldenberg, and Marzyeh Ghassemi. 2021. “Chasing Your Long Tails.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. ACM. https://doi.org/10.1145/3442188.3445934.\n\n\nTierney, Nicholas J, and Karthik Ram. 2020. “A Realistic Guide to Making Data Available Alongside Code to Improve Reproducibility.” https://arxiv.org/abs/2002.11626.\n\n\nWickham, Hadley, Jim Hester, and Winston Chang. 2020. Devtools: Tools to Make Developing r Packages Easier. https://CRAN.R-project.org/package=devtools.\n\n\nWilkinson, Mark D, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 1–9.\n\n\nZook, Matthew, Solon Barocas, Danah Boyd, Kate Crawford, Emily Keller, Seeta Peña Gangadharan, Alyssa Goodman, et al. 2017. “Ten Simple Rules for Responsible Big Data Research.” PLoS Computational Biology. Public Library of Science San Francisco, CA USA."
  },
  {
    "objectID": "13-eda.html",
    "href": "13-eda.html",
    "title": "13  Exploratory data analysis",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "13-eda.html#introduction",
    "href": "13-eda.html#introduction",
    "title": "13  Exploratory data analysis",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\n\nThe future of data analysis can involve great progress, the overcoming of real difficulties, and the provision of a great service to all fields of science and technology. Will it? That remains to us, to our willingness to take up the rocky road of real problems in preference to the smooth road of unreal assumptions, arbitrary criteria, and abstract results without real attachments. Who is for the challenge?\nTukey (1962, 64).\n\nExploratory data analysis is never finished, you just die. It is the active process of exploring and becoming familiar with our data. Like a farmer with their hands in the earth, we need to know every contour and aspect of our data. We need to know how it changes, what it shows, hides, and what are its limits. Exploratory data analysis (EDA) is the unstructured process of doing this.\nEDA is a means to an end. While it will inform the entire paper, especially the data section, it is not typically something that ends up in the final paper. The way to proceed is to make a separate R Markdown file, and add code as well as brief notes on-the-go. Do not delete previous code, just add to it. By the end of it we will have created a useful notebook that captures our exploration of the dataset. This is a document that will guide the subsequent analysis and modelling.\nEDA draws on a variety of skills and there are a lot of options for EDA (Staniak and Biecek 2019). Every tool should be considered. Look at the data and scroll through it. Make tables, plots, summary statistics, even some models. The key is to iterate, move quickly rather than perfectly, and come to a thorough understanding of the data.\nIn this chapter we will go through various examples of EDA including TTC subway delays, and Airbnb."
  },
  {
    "objectID": "13-eda.html#case-study-ttc-subway-delays",
    "href": "13-eda.html#case-study-ttc-subway-delays",
    "title": "13  Exploratory data analysis",
    "section": "13.2 Case study: TTC subway delays",
    "text": "13.2 Case study: TTC subway delays\nWe can use opendatatoronto (Gelfand 2020) and tidyverse (Wickham et al. 2019) to obtain data about the Toronto subway system, and especially the delays that have occurred. The idea for this case study comes from Monica Alexander.\n\n\n#| message: false #| warning: false\n\n#| echo: true      \nTo begin, we download the data on Toronto Transit Commission (TTC) subway delays in 2020. The data are available as a separate dataset for each month. We are interested in 2020, so we create a column that of the year, and then filter the resources to just those months that were in 2020. We download them using get_resource(), iterating through each month using map_dfr from purrr (Henry and Wickham 2020) which brings each of the twelve datasets together, and then save them. then save them.\n\nlibrary(janitor)\nlibrary(opendatatoronto)\nlibrary(tidyverse)\n\n# We know this unique key by looking the 'id' of the interest.\nttc_resources <- \n  list_package_resources(\"996cfe8d-fb35-40ce-b569-698d51fc683b\") |>\n  mutate(year = str_extract(name, \"20..?\")) |>  \n  filter(year == 2020)\n\nall_2020_ttc_data <- \n  map_dfr(ttc_resources$id, get_resource)\n\nall_2020_ttc_data <- clean_names(all_2020_ttc_data)\n\nwrite_csv(all_2020_ttc_data, \"all_2020_ttc_data.csv\")\n\nall_2020_ttc_data\n\nThe dataset has a variety of columns, and we can find out more about each of them by downloading the codebook. The reason for the delay is coded, and so we can also download the explanations. One particular variable of interest appears to be ‘min_delay’, which gives the extent of the delay in minutes.\n\n# Data codebook\ndelay_data_codebook <- get_resource(\"54247e39-5a7d-40db-a137-82b2a9ab0708\")\ndelay_data_codebook <- clean_names(delay_data_codebook)\nwrite_csv(delay_data_codebook, \"delay_data_codebook.csv\")\n\n# Explanation for delay codes\ndelay_codes <- get_resource(\"fece136b-224a-412a-b191-8d31eb00491e\")\ndelay_codes <- clean_names(delay_codes)\nwrite_csv(delay_codes, \"delay_codes.csv\")\n\nThere is no one way to explore a dataset while conducting EDA, but we are usually especially interested in:\n\nWhat should the variables look like? For instance, what is their type, what are the values, and what does the distribution of these look like?\nWhat aspects are surprising, both in terms of data that are there that we do not expect, such as outliers, but also in terms of data that we may expect here but do not have such as missing data.\nDeveloping a goal for our analysis. For instance, in this case, it might be understanding the factors such as stations and the time of day, that are associated with delays.\n\nIt is important to document all aspects as we go through and make note of anything surprising. We are looking to create a record of the steps and assumptions that we made as we were going because these will be important when we come to modelling.\n\n13.2.1 Checking data\nWe should check that the variables are what they say they are. If they are not, then we need to work out what to do, for instance, should we recode them, or even remove them? It is also important to ensure that the class of the variables is as we expect, for instance variables that should be a factor are a factor and those that should be a character are a character. And also that we do not accidentally have, say, factors as numbers, or vice versa. One way to do this is to use unique(), and another is to use table().\n\nunique(all_2020_ttc_data$day)\n\n[1] \"Wednesday\" \"Thursday\"  \"Friday\"    \"Saturday\"  \"Sunday\"    \"Monday\"   \n[7] \"Tuesday\"  \n\nunique(all_2020_ttc_data$line)\n\n [1] \"SRT\"                    \"YU\"                     \"BD\"                    \n [4] \"SHP\"                    \"YU/BD\"                  \"YU / BD\"               \n [7] \"999\"                    NA                       \"29 DUFFERIN\"           \n[10] \"95 YORK MILLS\"          \"35 JANE\"                \"YU-BD\"                 \n[13] \"BLOOR - DANFORTH\"       \"YU/BD LINE\"             \"YUS\"                   \n[16] \"YUS/BD\"                 \"40 JUNCTION-DUNDAS WES\" \"71 RUNNYMEDE\"          \n[19] \"BD/YU\"                  \"102 MARKHAM ROAD\"       \"YUS/DB\"                \n[22] \"YU & BD\"                \"SHEP\"                  \n\ntable(all_2020_ttc_data$day)\n\n\n   Friday    Monday  Saturday    Sunday  Thursday   Tuesday Wednesday \n     2174      2222      1867      1647      2353      2190      2329 \n\ntable(all_2020_ttc_data$line)\n\n\n      102 MARKHAM ROAD            29 DUFFERIN                35 JANE \n                     1                      1                      1 \n40 JUNCTION-DUNDAS WES           71 RUNNYMEDE          95 YORK MILLS \n                     1                      1                      1 \n                   999                     BD                  BD/YU \n                     2                   5473                      1 \n      BLOOR - DANFORTH                   SHEP                    SHP \n                     1                      1                    619 \n                   SRT                     YU                YU / BD \n                   644                   7620                     22 \n               YU & BD                  YU-BD                  YU/BD \n                     2                      1                    338 \n            YU/BD LINE                    YUS                 YUS/BD \n                     1                      2                      1 \n                YUS/DB \n                     1 \n\n\nIt is clear that we have likely issues in terms of the lines. Some of them have a clear re-code, but not all. One option would be to drop them, but we would need to think about whether these errors might be correlated with something that is of interest, because if they were then we may be dropping important information. There is usually no one right answer, because it will usually depend on what we are using the data for. We would note the issue, as we continued with EDA and then decide later about what to do. For now, we will remove all the lines that are not the ones that we know to be correct.\n\nall_2020_ttc_data <- \n  all_2020_ttc_data |> \n  filter(line %in% c(\"BD\", \"YU\", \"SHP\", \"SRT\"))\n\nExploring missing data could be a course in itself, but the presence, or lack, of missing values can haunt an analysis. To get started we could look at known-unknowns, which are the NAs for each variable. And vis_dat() and vis_miss() from visdat (Tierney 2017) can be useful to get a feel for how the missing values are distributed.\n\nlibrary(visdat)\n\nall_2020_ttc_data |>\n  summarise_all(list( ~ sum(is.na(.))))\n\n# A tibble: 1 × 10\n   date  time   day station  code min_delay min_gap bound  line vehicle\n  <int> <int> <int>   <int> <int>     <int>   <int> <int> <int>   <int>\n1     0     0     0       0     0         0       0  3272     0       0\n\nvis_dat(x = all_2020_ttc_data,\n         palette = \"cb_safe\"\n         )\n\n\n\nvis_miss(all_2020_ttc_data)\n\n\n\n\nIn this case we have many missing values in ‘bound’ and two in ‘line’. For these known-unknowns, we are interested in whether the they are missing at random. We want to, ideally, show that data happened to just drop out. This is unlikely, and so we are usually trying to look at what is systematic about how our data are missing.\nSometime data happen to be duplicated. If we did not notice this then our analysis would be wrong in ways that we’d not be able to consistently expect. There are a variety of ways to look for duplicated rows, but get_dupes() from janitor (Firke 2020) is especially useful.\n\nget_dupes(all_2020_ttc_data)\n\nNo variable names specified - using all columns.\n\n\n# A tibble: 37 × 11\n   date                time   day    station code  min_delay min_gap bound line \n   <dttm>              <time> <chr>  <chr>   <chr>     <dbl>   <dbl> <chr> <chr>\n 1 2020-02-10 00:00:00 06:00  Monday TORONT… MRO           0       0 <NA>  SRT  \n 2 2020-02-10 00:00:00 06:00  Monday TORONT… MRO           0       0 <NA>  SRT  \n 3 2020-02-10 00:00:00 06:00  Monday TORONT… MUO           0       0 <NA>  SHP  \n 4 2020-02-10 00:00:00 06:00  Monday TORONT… MUO           0       0 <NA>  SHP  \n 5 2020-03-10 00:00:00 23:00  Tuesd… YORK M… MUO           0       0 <NA>  YU   \n 6 2020-03-10 00:00:00 23:00  Tuesd… YORK M… MUO           0       0 <NA>  YU   \n 7 2020-03-26 00:00:00 13:20  Thurs… VAUGHA… MUNOA         3       6 S     YU   \n 8 2020-03-26 00:00:00 13:20  Thurs… VAUGHA… MUNOA         3       6 S     YU   \n 9 2020-03-26 00:00:00 18:32  Thurs… VAUGHA… MUNOA         3       6 S     YU   \n10 2020-03-26 00:00:00 18:32  Thurs… VAUGHA… MUNOA         3       6 S     YU   \n# … with 27 more rows, and 2 more variables: vehicle <dbl>, dupe_count <int>\n\n\nThis dataset has many duplicates. Again, we are interested in whether there is something systematic going on. Remembering that during EDA we are trying to quickly come to terms with a dataset, one way forward is to flag this as an issue to come back to and explore later, and to just remove duplicates for now using distinct().\n\nall_2020_ttc_data <- \n  all_2020_ttc_data |> \n  distinct()\n\nThe station names are a mess. We could try to quickly bring a little order to the chaos by just taking just the first word (or, the first two if it starts with ‘ST’).\n\nall_2020_ttc_data <-\n  all_2020_ttc_data |>\n  mutate(station_clean = if_else(str_starts(station, \"ST\"), \n                                 word(station, 1, 2), \n                                 word(station, 1)))\n\n\n\n13.2.2 Visualizing data\nWe need to see the data in its original state to understand it and we use bar charts, scatterplots, line plots and histograms extensively for this. During EDA we are not as concerned with whether the graph is aesthetically pleasing, but are instead trying to acquire a sense of the data as quickly as possible. We can start by looking at the distribution of ‘min_delay’, which is one outcome of interest.\n\nall_2020_ttc_data |>\n  ggplot(aes(x = min_delay)) + \n  geom_bar()\n\n\n\n\nThe largely empty graph suggests the presence of outliers. There are a variety of ways to try to understand what could be going on, but one quick way to proceed it to use a log, remembering that we would expect values of 0 to drop away.\n\nall_2020_ttc_data |>\n  ggplot(aes(x = min_delay)) + \n  geom_bar() +\n  scale_x_log10()\n\n\n\n\nThis initial exploration further hints at an issue that we might like to explore further. We will join this dataset with ‘delay_codes’ to understand what is going on.\n\nall_2020_ttc_data <-\n  all_2020_ttc_data |>\n  left_join(\n    delay_codes |>\n      rename(code = sub_rmenu_code, \n             code_desc = code_description_3) |>\n      select(code, code_desc),\n    by = \"code\"\n  )\n\nall_2020_ttc_data <-\n  all_2020_ttc_data |>\n  mutate(code_srt = ifelse(line == \"SRT\", code, \"NA\")) |>\n  left_join(\n    delay_codes |>\n      rename(code_srt = sub_rmenu_code, code_desc_srt = code_description_7) |>\n      select(code_srt, code_desc_srt),\n    by = \"code_srt\"\n  ) |>\n  mutate(\n    code = ifelse(code_srt == \"NA\", code, code_srt),\n    code_desc = ifelse(is.na(code_desc_srt), code_desc, code_desc_srt)\n  ) |>\n  select(-code_srt,-code_desc_srt)\n\nAnd so we can see that the 450 minute delay was due to ‘Transit Control Related Problems’, the 446 minute delay was due to ‘Miscellaneous Other’, they seem to be outliers, even among the outliers.\n\nall_2020_ttc_data |> \n  left_join(delay_codes |> \n              rename(code = sub_rmenu_code, code_desc = code_description_3) |>\n              select(code, code_desc)) |> \n  arrange(-min_delay) |> \n  select(date, time, station, line, min_delay, code, code_desc)\n\nJoining, by = c(\"code\", \"code_desc\")\n\n\n# A tibble: 14,335 × 7\n   date                time   station            line  min_delay code  code_desc\n   <dttm>              <time> <chr>              <chr>     <dbl> <chr> <chr>    \n 1 2020-02-13 00:00:00 05:30  ST GEORGE YUS STA… YU          450 TUCC  Transit …\n 2 2020-05-08 00:00:00 16:16  ST CLAIR STATION   YU          446 MUO   Miscella…\n 3 2020-01-22 00:00:00 05:57  KEELE STATION      BD          258 EUTR  Trucks   \n 4 2020-03-19 00:00:00 11:26  ROYAL YORK STATION BD          221 MUPR1 Priority…\n 5 2020-11-12 00:00:00 23:10  SHEPPARD STATION   YU          197 PUCSC Signal C…\n 6 2020-12-13 00:00:00 21:37  MCCOWAN STATION    SRT         167 PRSP  <NA>     \n 7 2020-12-04 00:00:00 16:23  MCCOWAN STATION    SRT         165 PRSP  <NA>     \n 8 2020-01-18 00:00:00 05:48  SCARBOROUGH RAPID… SRT         162 PRSL  <NA>     \n 9 2020-02-22 00:00:00 05:16  SPADINA TO OSGOODE YU          159 PUSWZ Work Zon…\n10 2020-09-03 00:00:00 14:35  CASTLE FRANK STAT… BD          150 MUPR1 Priority…\n# … with 14,325 more rows\n\n\n\n\n13.2.3 Groups of small counts\nAnother thing that we are looking for is various groupings of the data, especially where sub-groups may end up with only a small numbers of observations in them. This is because any analysis could be especially influenced by them. One quick way to do this is to group the data by a variable that is of interest, for instance ‘line’, using color.\n\nggplot(data = all_2020_ttc_data) + \n  geom_histogram(aes(x = min_delay, y = ..density.., fill = line), \n                 position = 'dodge', \n                 bins = 10) + \n  scale_x_log10()\n\n\n\n\nFigure 13.1: Density of the distribution of delay, in minutes\n\n\n\n\nFigure 13.1 uses density so that we can look at the the distributions more comparably, but we should also be aware of differences in frequency (Figure 13.2)). In this case, we will see that ‘SHP’ and ‘SRT’ have much smaller counts.\n\nggplot(data = all_2020_ttc_data) + \n  geom_histogram(aes(x = min_delay, fill = line), \n                 position = 'dodge', \n                 bins = 10) + \n  scale_x_log10()\n\n\n\n\nFigure 13.2: Frequency of the distribution of delay, in minutes\n\n\n\n\nTo group by another variable we can add facets (Figure 13.3).\n\nggplot(data = all_2020_ttc_data) + \n  geom_density(aes(x = min_delay, color = line), \n               bw = .08) + \n  scale_x_log10() + \n  facet_wrap(vars(day))\n\n\n\n\nFigure 13.3: Frequency of the distribution of delay, in minutes, by day\n\n\n\n\nWe can now plot the top five stations by mean delay.\n\nall_2020_ttc_data |> \n  group_by(line, station_clean) |> \n  summarise(mean_delay = mean(min_delay), n_obs = n()) |> \n  filter(n_obs>1) |> \n  arrange(line, -mean_delay) |> \n  slice(1:5) |> \n  ggplot(aes(station_clean, mean_delay)) + \n    geom_col() + \n    coord_flip() + \n    facet_wrap(vars(line), \n               scales = \"free_y\")\n\n`summarise()` has grouped output by 'line'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n13.2.4 Dates\nDates are often difficult to work with because they are so prone to having issues. For this reason, it is especially important to consider them during EDA. We could create a graph by week, to see if there is any seasonality. When using dates, lubridate (Grolemund and Wickham 2011) is especially useful. For instance, we can look at the average delay, of those that were delayed, by week drawing on week() to construct the weeks (Figure 13.4).\n\nlibrary(lubridate)\n\nall_2020_ttc_data |>\n  filter(min_delay > 0) |>\n  mutate(week = week(date)) |>\n  group_by(week, line) |>\n  summarise(mean_delay = mean(min_delay)) |>\n  ggplot(aes(week, mean_delay, color = line)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(line),\n              scales = \"free_y\"\n             )\n\n\n\n\nFigure 13.4: Average delay, in minutes, by week, for the Toronto subway\n\n\n\n\nNow let us look at the proportion of delays that were greater than 10 minutes (Figure 13.5).\n\nall_2020_ttc_data |> \n  mutate(week = week(date)) |> \n  group_by(week, line) |> \n  summarise(prop_delay = sum(min_delay>10)/n()) |> \n  ggplot(aes(week, prop_delay, color = line)) + \n    geom_point() + \n    geom_smooth() + \n      facet_wrap(vars(line),\n              scales = \"free_y\"\n             )\n\n\n\n\nFigure 13.5: Delays longer than ten minutes, by week, for the Toronto subway\n\n\n\n\nThese figures, tables, and analysis have no place in a final paper. Instead, they allow us to become comfortable with the data. We note aspects about each that stand out, as well as the warnings and any implications or aspects to return to.\n\n\n13.2.5 Relationships\nWe are also interested in looking at the relationship between two variables. Scatter plots are especially useful here for continuous variables, and are a good precursor to modeling. For instance, we may be interested in the relationship between the delay and the gap (Figure 13.6).\n\nall_2020_ttc_data |>\n  ggplot(aes(x = min_delay, y = min_gap)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\nFigure 13.6: Relationship between delay and gap for the Toronto subway in 2020\n\n\n\n\nThe relationship between categorical variables takes more work, but we could also, for instance, look at the top five reasons for delay by station. In particular, we may be interested in whether they differ, and how any difference could be modeled (Figure 13.7).\n\nall_2020_ttc_data |>\n  group_by(line, code_desc) |>\n  summarise(mean_delay = mean(min_delay)) |>\n  arrange(-mean_delay) |>\n  slice(1:5) |>\n  ggplot(aes(x = code_desc,\n             y = mean_delay)) +\n  geom_col() + \n  facet_wrap(vars(line), \n             scales = \"free_y\",\n             nrow = 4) +\n  coord_flip()\n\n\n\n\nFigure 13.7: Relationship between categorical variables for the Toronto subway in 2020\n\n\n\n\nPrincipal components analysis (PCA) is another powerful exploratory tool. It allows us to pick up potential clusters and outliers that can help to inform modeling. To see this, we can look at the types of delay by station. The delay categories are messy and there a lot of them, but as we are trying to come to terms with the dataset, we will just take the first word.\n\nall_2020_ttc_data <-\n  all_2020_ttc_data |>\n  mutate(code_red = case_when(\n    str_starts(code_desc, \"No\") ~ word(code_desc, 1, 2),\n    str_starts(code_desc, \"Operator\") ~ word(code_desc, 1, 2),\n    TRUE ~ word(code_desc, 1)\n  ))\n\nLet us also just restrict the analysis to causes that happen at least 50 times over 2019. To do the PCA, the dataframe also needs to be switched to wide format.\n\ndwide <-\n  all_2020_ttc_data |>\n  group_by(line, station_clean) |>\n  mutate(n_obs = n()) |>\n  filter(n_obs > 1) |>\n  group_by(code_red) |>\n  mutate(tot_delay = n()) |>\n  arrange(tot_delay) |>\n  filter(tot_delay > 50) |>\n  group_by(line, station_clean, code_red) |>\n  summarise(n_delay = n()) |>\n  pivot_wider(names_from = code_red, values_from = n_delay) |>\n  mutate_all(.funs = funs(ifelse(is.na(.), 0, .)))\n\n`summarise()` has grouped output by 'line', 'station_clean'. You can override using the `.groups` argument.\n`mutate_all()` ignored the following grouping variables:\nColumns `line`, `station_clean`\nUse `mutate_at(df, vars(-group_cols()), myoperation)` to silence the message.\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nPlease use a list of either functions or lambdas: \n\n  # Simple named list: \n  list(mean = mean, median = median)\n\n  # Auto named with `tibble::lst()`: \n  tibble::lst(mean, median)\n\n  # Using lambdas\n  list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\nNow we can quickly do some PCA.\n\ndelay_pca <- prcomp(dwide[,3:ncol(dwide)])\n\ndf_out <- as_tibble(delay_pca$x)\ndf_out <- bind_cols(dwide |> select(line, station_clean), df_out)\nhead(df_out)\n\n# A tibble: 6 × 32\n# Groups:   line, station_clean [6]\n  line  station_clean    PC1    PC2     PC3   PC4    PC5   PC6    PC7   PC8\n  <chr> <chr>          <dbl>  <dbl>   <dbl> <dbl>  <dbl> <dbl>  <dbl> <dbl>\n1 BD    BATHURST       10.9   17.2   2.04   12.9   4.60  -2.98  2.31   5.02\n2 BD    BAY            14.6    6.54  4.76   14.4   0.406  3.09 -0.144  4.68\n3 BD    BLOOR          22.8  -18.6  19.7    -7.37 -1.54  -8.60 -1.36   1.08\n4 BD    BLOOR-DANFORTH 23.4  -20.2  20.4    -4.85 -0.429 -6.77 -0.562  1.19\n5 BD    BROADVIEW       9.29  22.0  -0.0365  6.72  4.31   1.73  0.304  4.71\n6 BD    CASTLE         15.1    5.21  7.62   11.6  -1.17   2.77 -1.71   4.93\n# … with 22 more variables: PC9 <dbl>, PC10 <dbl>, PC11 <dbl>, PC12 <dbl>,\n#   PC13 <dbl>, PC14 <dbl>, PC15 <dbl>, PC16 <dbl>, PC17 <dbl>, PC18 <dbl>,\n#   PC19 <dbl>, PC20 <dbl>, PC21 <dbl>, PC22 <dbl>, PC23 <dbl>, PC24 <dbl>,\n#   PC25 <dbl>, PC26 <dbl>, PC27 <dbl>, PC28 <dbl>, PC29 <dbl>, PC30 <dbl>\n\n\nWe can plot the first two principal components, and add labels to some of the outlying stations.\n\nlibrary(ggrepel)\nggplot(df_out,aes(x=PC1,y=PC2,color=line )) + \n  geom_point() + \n  geom_text_repel(data = df_out |> filter(PC2>100|PC1<100*-1), \n                  aes(label = station_clean)\n                  )\n\n\n\n\nWe could also plot the factor loadings. We see some evidence that perhaps one is to do with the public, compared with another to do with the operator.\n\ndf_out_r <- as_tibble(delay_pca$rotation)\ndf_out_r$feature <- colnames(dwide[,3:ncol(dwide)])\n\ndf_out_r\n\n# A tibble: 30 × 31\n        PC1    PC2      PC3      PC4      PC5     PC6      PC7      PC8     PC9\n      <dbl>  <dbl>    <dbl>    <dbl>    <dbl>   <dbl>    <dbl>    <dbl>   <dbl>\n 1 -0.0279  0.125  -0.0576   0.0679  -0.0133   0.0307 -0.00449  0.134   -0.0472\n 2 -0.108   0.343  -0.150    0.205   -0.100    0.317  -0.116    0.252   -0.407 \n 3 -0.0196  0.0604 -0.0207   0.0195   0.0189   0.0574 -0.0803  -0.0467  -0.146 \n 4 -0.0244  0.0752 -0.0325  -0.0237   0.00121 -0.0263 -0.0137   0.0251   0.104 \n 5 -0.0128  0.0113 -0.00340 -0.00977 -0.0255   0.0186 -0.0645  -0.0552  -0.0302\n 6 -0.231   0.650  -0.309    0.245    0.222   -0.309   0.172   -0.0711   0.363 \n 7 -0.0871  0.233  -0.0904  -0.692   -0.311   -0.414  -0.216    0.00703 -0.116 \n 8 -0.00377 0.0193 -0.00201 -0.0140  -0.0424   0.0751 -0.146   -0.0712  -0.0203\n 9 -0.0167  0.120  -0.0367  -0.578    0.336    0.563   0.207    0.226    0.289 \n10 -0.0708  0.276  -0.118    0.116   -0.368    0.435  -0.178    0.0583  -0.0173\n# … with 20 more rows, and 22 more variables: PC10 <dbl>, PC11 <dbl>,\n#   PC12 <dbl>, PC13 <dbl>, PC14 <dbl>, PC15 <dbl>, PC16 <dbl>, PC17 <dbl>,\n#   PC18 <dbl>, PC19 <dbl>, PC20 <dbl>, PC21 <dbl>, PC22 <dbl>, PC23 <dbl>,\n#   PC24 <dbl>, PC25 <dbl>, PC26 <dbl>, PC27 <dbl>, PC28 <dbl>, PC29 <dbl>,\n#   PC30 <dbl>, feature <chr>\n\nggplot(df_out_r,aes(x=PC1,y=PC2,label=feature )) + geom_text_repel()\n\nWarning: ggrepel: 22 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps"
  },
  {
    "objectID": "13-eda.html#case-study-airbnb-listing-in-toronto",
    "href": "13-eda.html#case-study-airbnb-listing-in-toronto",
    "title": "13  Exploratory data analysis",
    "section": "13.3 Case study: Airbnb listing in Toronto",
    "text": "13.3 Case study: Airbnb listing in Toronto\n\nIn this case study we look at Airbnb listings in Toronto. The dataset is from Inside Airbnb (Cox 2021) and we will read it from their website, and then save a local copy. We can give read_csv() a link to where the dataset is and it will download it. This helps with reproducibility because the source is clear. But, as that link could change at any time, longer-term reproducibility, as well as wanting to minimize the effect on the Inside Airbnb servers, suggest that we should also save a local copy of the data and then use that. As the original data is not ours, we should not make that public without first getting written permission. The ‘guess_max’ option in read_csv helps us avoid having to specify the column types. Usually read_csv() takes a best guess at the column types based on the first few rows. But sometimes those first ones are misleading and so ‘guess_max’ forces it to look at a larger number of rows to try to work out what is going on.\n\nlibrary(tidyverse)\n\nairbnb_data <- \n  read_csv(\"http://data.insideairbnb.com/canada/on/toronto/2021-01-02/data/listings.csv.gz\", \n           guess_max = 20000)\n\nwrite_csv(airbnb_data, \"airbnb_data.csv\")\n\nairbnb_data\n\n\n13.3.1 Explore individual variables\nThere are a large number of columns, so we will just select a few.\n\nnames(airbnb_data) |> \n  length()\n\n[1] 74\n\nairbnb_data_selected <- \n  airbnb_data |> \n  select(host_id, \n         host_since, \n         host_response_time, \n         host_is_superhost, \n         host_listings_count,\n         host_total_listings_count,\n         host_neighbourhood, \n         host_listings_count, \n         neighbourhood_cleansed, \n         room_type, \n         bathrooms, \n         bedrooms, \n         price, \n         number_of_reviews, \n         has_availability, \n         review_scores_rating, \n         review_scores_accuracy, \n         review_scores_cleanliness, \n         review_scores_checkin, \n         review_scores_communication, \n         review_scores_location, \n         review_scores_value\n         )\n\nairbnb_data_selected\n\n# A tibble: 18,265 × 21\n   host_id host_since host_response_time host_is_superhost host_listings_count\n     <dbl> <date>     <chr>              <lgl>                           <dbl>\n 1    1565 2008-08-08 N/A                FALSE                               1\n 2   22795 2009-06-22 N/A                FALSE                               2\n 3   48239 2009-10-25 N/A                FALSE                               1\n 4   93825 2010-03-15 N/A                FALSE                               2\n 5  118124 2010-05-04 within a day       FALSE                               1\n 6   22795 2009-06-22 N/A                FALSE                               2\n 7  174063 2010-07-20 within an hour     TRUE                                3\n 8  183071 2010-07-28 within an hour     TRUE                                2\n 9  187320 2010-08-01 within a few hours TRUE                               13\n10  192364 2010-08-05 N/A                FALSE                               1\n# … with 18,255 more rows, and 16 more variables:\n#   host_total_listings_count <dbl>, host_neighbourhood <chr>,\n#   neighbourhood_cleansed <chr>, room_type <chr>, bathrooms <lgl>,\n#   bedrooms <dbl>, price <chr>, number_of_reviews <dbl>,\n#   has_availability <lgl>, review_scores_rating <dbl>,\n#   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>,\n#   review_scores_checkin <dbl>, review_scores_communication <dbl>, …\n\n\nFirst we might be interested in price. It is a character at the moment and so we need to convert it to a numeric. This is a common problem, and we need to be a little careful that it does not all just convert to NAs. In our case if we just force the price data to be a numeric then it will go to NA because there are a lot of characters where it is unclear what the numeric equivalent is, such as ‘$’. So we need to remove those characters first.\n\nairbnb_data_selected$price |> head()\n\n[1] \"$469.00\" \"$96.00\"  \"$64.00\"  \"$70.00\"  \"$45.00\"  \"$127.00\"\n\nairbnb_data_selected$price |> str_split(\"\") |> unlist() |> unique()\n\n [1] \"$\" \"4\" \"6\" \"9\" \".\" \"0\" \"7\" \"5\" \"1\" \"2\" \"3\" \"8\" \",\"\n\nairbnb_data_selected |> \n  select(price) |> \n  filter(str_detect(price, \",\"))\n\n# A tibble: 145 × 1\n   price    \n   <chr>    \n 1 $1,724.00\n 2 $1,000.00\n 3 $1,100.00\n 4 $1,450.00\n 5 $1,019.00\n 6 $1,000.00\n 7 $1,300.00\n 8 $2,142.00\n 9 $2,000.00\n10 $1,200.00\n# … with 135 more rows\n\nairbnb_data_selected <- \n  airbnb_data_selected |> \n  mutate(price = str_remove(price, \"\\\\$\"),\n         price = str_remove(price, \",\"),\n         price = as.integer(price)\n         )\n\nNow we can have a look at the distribution of prices (Figure 13.8).\n\nairbnb_data_selected |>\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")\n\n\n\n\nFigure 13.8: Distribution of prices of Toronto Airbnb rentals in January 2021\n\n\n\n\nIt is clear that there are outliers, so again we might like to consider it on the log scale (Figure 13.9).\n\nairbnb_data_selected |>\n  filter(price > 1000) |> \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\") +\n  scale_y_log10()\n\n\n\n\nFigure 13.9: Distribution of log prices of Toronto Airbnb rentals in January 2021\n\n\n\n\nIf we focus on prices that are less than $1,000 then we see that the majority of properties have a nightly price less than $250. Interestingly it looks like there is some bunching of prices, possible around numbers ending in zero or nine. Let us just zoom in on prices between $90 and $210, out of interest, but change the bins to be smaller (?fig-airbnbpricesbunch).\n\nairbnb_data_selected |>\n  filter(price < 1000) |> \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")\n\nairbnb_data_selected |>\n  filter(price > 90) |> \n  filter(price < 210) |> \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Number of properties\")\n\n\n\n\nFigure 13.10: Distribution of prices less than $1000 for Toronto Airbnb rentals in January 2021 shows bunching\n\n\n\n\n\n\n\nFigure 13.11: Distribution of prices less than $1000 for Toronto Airbnb rentals in January 2021 shows bunching\n\n\n\n\nFor now, we will just remove all prices that are more than $999.\n\nairbnb_data_selected <- \n  airbnb_data_selected |> \n  filter(price < 1000)\n\nSuperhosts are especially experienced Airbnb hosts, and we might be interested to learn more about them. For instance, a host either is or is not a superhost, and so we would not expect any NAs. But we can see that there are. It might be that the host removed a listing or similar. For now, we will remove them.\n\nairbnb_data_selected |>\n  filter(is.na(host_is_superhost))\n\n# A tibble: 11 × 21\n     host_id host_since host_response_time host_is_superhost host_listings_count\n       <dbl> <date>     <chr>              <lgl>                           <dbl>\n 1  23472830 NA         <NA>               NA                                 NA\n 2  31675651 NA         <NA>               NA                                 NA\n 3  75779190 NA         <NA>               NA                                 NA\n 4  47614482 NA         <NA>               NA                                 NA\n 5 201103629 NA         <NA>               NA                                 NA\n 6 111820893 NA         <NA>               NA                                 NA\n 7  23472830 NA         <NA>               NA                                 NA\n 8 196269219 NA         <NA>               NA                                 NA\n 9  23472830 NA         <NA>               NA                                 NA\n10 266594170 NA         <NA>               NA                                 NA\n11 118516038 NA         <NA>               NA                                 NA\n# … with 16 more variables: host_total_listings_count <dbl>,\n#   host_neighbourhood <chr>, neighbourhood_cleansed <chr>, room_type <chr>,\n#   bathrooms <lgl>, bedrooms <dbl>, price <int>, number_of_reviews <dbl>,\n#   has_availability <lgl>, review_scores_rating <dbl>,\n#   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>,\n#   review_scores_checkin <dbl>, review_scores_communication <dbl>,\n#   review_scores_location <dbl>, review_scores_value <dbl>\n\n\nWe will also want to create a binary variable from this. It is true/false at the moment, which is fine for the modelling, but there are a handful of situations where it will be easier if we have a 0/1. And for now we will just remove anyone with an NA for whether they are a super host.\n\nairbnb_data_selected <- \n  airbnb_data_selected |>\n  filter(!is.na(host_is_superhost)) |>\n  mutate(host_is_superhost_binary = if_else(\n    host_is_superhost == TRUE, 1, 0)\n    )\n\nOn Airbnb, guests can give 1-5 star ratings across a variety of different aspects, including cleanliness, accuracy, value, and others. But when we look at the reviews in our dataset, it is less clear how this is being constructed, because it appears to be a rating between 20 and 100 and there are also NA values (Figure 13.12).\n\nairbnb_data_selected |>\n  ggplot(aes(x = review_scores_rating)) +\n  geom_bar() +\n  theme_classic() +\n  labs(x = \"Reviews scores rating\",\n       y = \"Number of properties\")\n\n\n\n\nFigure 13.12: Distribution of review scores rating for Toronto Airbnb rentals in January 2021\n\n\n\n\nWe would like to deal with the NAs in ‘review_scores_rating’, but this is more complicated as there are a lot of them. It may be that this is just because they do not have any reviews.\n\nairbnb_data_selected |>\n  filter(is.na(review_scores_rating)) |>\n  nrow()\n\n[1] 4308\n\nairbnb_data_selected |>\n  filter(is.na(review_scores_rating)) |> \n  select(number_of_reviews) |> \n  table()\n\n\n   0    1    2    3    4 \n4046  226   23   10    3 \n\n\nSo these properties do not have a review rating yet because they do not have enough reviews. It is a large proportion of the total, at almost a fifth of them so we might like to look at this in more detail using vis_miss() from visdat (Tierney 2017). We are interested to see whether there is something systematic happening with these properties. For instance, if the NAs were being driven by, say, some requirement of a minimum number of reviews, then we would expect they would all be missing.\n\nlibrary(visdat)\nairbnb_data_selected |> \n  select(review_scores_rating, \n         review_scores_accuracy, \n         review_scores_cleanliness, \n         review_scores_checkin, \n         review_scores_communication, \n         review_scores_location, \n         review_scores_value) |> \n  vis_miss()\n\n\n\n\nGiven it looks convincing that in almost all cases, the different types of reviews are missing for the same observation. One approach would be to just focus on those that are not missing and the main review score. It is clear that almost all the reviews are more than 80. Let us just zoom in on that 60 to 80 range to check what the distribution looks like in that range (Figure 13.13).\n\nairbnb_data_selected |>\n  filter(!is.na(review_scores_rating)) |> \n  filter(review_scores_rating > 60) |>\n  filter(review_scores_rating < 80) |> \n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(x = \"Average review score\",\n       y = \"Number of properties\")\n\n\n\n\nFigure 13.13: Distribution of review scores, between 60 and 80, for Toronto Airbnb rentals in January 2021\n\n\n\n\nFor now, we will remove anyone with an NA in their main review score, even though this will remove roughly 20 per cent of observations. And we will also focus only on those hosts with a main review score of at least 70. If we ended up using this dataset for actual analysis, then we would want to justify this decision in an appendix or similar.\n\nairbnb_data_selected <- \n  airbnb_data_selected |>\n  filter(!is.na(review_scores_rating)) |>\n  filter(review_scores_rating >= 70)\n\nAnother important factor is how quickly a host responds to an enquiry. Airbnb allows hosts up to 24 hours to respond, but encourages responses within an hour.\n\nairbnb_data_selected |>\n  count(host_response_time)\n\n# A tibble: 5 × 2\n  host_response_time     n\n  <chr>              <int>\n1 a few days or more   494\n2 N/A                 5708\n3 within a day         952\n4 within a few hours  1649\n5 within an hour      4668\n\n\nIt is unclear a host could have a response time of NA. It may be that they are related to some other variable. Interestingly it seems like what looks like ‘NAs’ in ‘host_response_time’ variable are not coded as proper NAs, but are instead being treated as another category. We will recode them to be actual NAs and also change the variable to be a factor.\n\nairbnb_data_selected <- \n  airbnb_data_selected |>\n  mutate(host_response_time = if_else(host_response_time == \"N/A\", NA_character_, host_response_time),\n         host_response_time = factor(host_response_time)\n  )\n\nThere is clearly an issue with NAs as there are a lot of them. For instance, we might be interested to see if there is a relationship with the review score (Figure 13.14). There are a lot that have an overall review of 100.\n\nairbnb_data_selected |> \n  filter(is.na(host_response_time)) |> \n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(x = \"Average review score\",\n       y = \"Number of properties\")\n\n\n\n\nFigure 13.14: Distribution of review scores for properties with NA response time, for Toronto Airbnb rentals in January 2021\n\n\n\n\nFor now, we will remove anyone with a NA in their response time. This will again removes roughly another 20 per cent of the observations.\n\nairbnb_data_selected <- \n  airbnb_data_selected |> \n  filter(!is.na(host_response_time))\n\nThere are two versions of a variable that suggest how many properties a host has on Airbnb. We might be interested to know whether there is a difference between them.\n\nairbnb_data_selected |> \n  mutate(listings_count_is_same = if_else(host_listings_count == host_total_listings_count, 1, 0)) |> \n  filter(listings_count_is_same == 0)\n\n# A tibble: 0 × 23\n# … with 23 variables: host_id <dbl>, host_since <date>,\n#   host_response_time <fct>, host_is_superhost <lgl>,\n#   host_listings_count <dbl>, host_total_listings_count <dbl>,\n#   host_neighbourhood <chr>, neighbourhood_cleansed <chr>, room_type <chr>,\n#   bathrooms <lgl>, bedrooms <dbl>, price <int>, number_of_reviews <dbl>,\n#   has_availability <lgl>, review_scores_rating <dbl>,\n#   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>, …\n\n\nAs there are no differences in this dataset, we can just remove one variable for now and have a look at the other one (Figure 13.15).\n\nairbnb_data_selected <- \n  airbnb_data_selected |> \n  select(-host_listings_count)\n\nairbnb_data_selected |> \n  ggplot(aes(x = host_total_listings_count)) +\n  geom_bar() +\n  scale_x_log10()\n\n\n\n\nFigure 13.15: Distribution of the number of properties a host has on Airbnb, for Toronto Airbnb rentals in January 2021\n\n\n\n\nThere are a large number who have somewhere in the 2-10 properties range, but the usual long tail. The number with 0 listings is unexpected and worth following up on. And there are a bunch with NA that we will need to deal with.\n\nairbnb_data_selected |> \n  filter(host_total_listings_count == 0) |> \n  head()\n\n# A tibble: 6 × 21\n  host_id host_since host_response_time host_is_superhost host_total_listings_c…\n    <dbl> <date>     <fct>              <lgl>                              <dbl>\n1 3783106 2012-10-06 within an hour     FALSE                                  0\n2 3814089 2012-10-09 within an hour     FALSE                                  0\n3 3827668 2012-10-10 within a day       FALSE                                  0\n4 2499198 2012-05-30 within a day       FALSE                                  0\n5 3268493 2012-08-15 within a day       FALSE                                  0\n6 8675040 2013-09-06 within an hour     TRUE                                   0\n# … with 16 more variables: host_neighbourhood <chr>,\n#   neighbourhood_cleansed <chr>, room_type <chr>, bathrooms <lgl>,\n#   bedrooms <dbl>, price <int>, number_of_reviews <dbl>,\n#   has_availability <lgl>, review_scores_rating <dbl>,\n#   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>,\n#   review_scores_checkin <dbl>, review_scores_communication <dbl>,\n#   review_scores_location <dbl>, review_scores_value <dbl>, …\n\n\nThere is nothing that immediately jumps out as odd about the people with zero listings, but there must be something going on. For now, we will focus on only those with one property.\n\nairbnb_data_selected <- \n  airbnb_data_selected |> \n  add_count(host_id) |> \n  filter(n == 1) |> \n  select(-n)\n\n\n\n13.3.2 Explore relationships between variables\nWe might like to make some graphs to see if there are any relationships that become clear. Some aspects that come to mind is looking at prices and reviews and super hosts, and number of properties and neighborhood.\nLook at the relationship between price and reviews, and whether they are a super-host (Figure 13.16).\n\nairbnb_data_selected |>\n  ggplot(aes(x = price, y = review_scores_rating, color = host_is_superhost)) +\n  geom_point(size = 1, alpha = 0.1) + \n  theme_classic() +\n  labs(x = \"Price per night\",\n       y = \"Average review score\",\n       color = \"Super host\") +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 13.16: Relationship between price and review and whether a host is a super host, for Toronto Airbnb rentals in January 2021\n\n\n\n\nOne of the aspects that may make someone a super host is how quickly they respond to inquiries. One could imagine that being a superhost involves quickly saying yes or no to inquiries. Let us look at the data. First, we want to look at the possible values of superhost by their response times.\n\nairbnb_data_selected |> \n  count(host_is_superhost) |>\n  mutate(proportion = n / sum(n),\n         proportion = round(proportion, digits = 2))\n\n# A tibble: 2 × 3\n  host_is_superhost     n proportion\n  <lgl>             <int>      <dbl>\n1 FALSE              1677       0.58\n2 TRUE               1234       0.42\n\n\nFortunately, it looks like when we removed the reviews rows we removed any NAs from whether they were a super host, but if we go back and look into that we may need to check again. We could build a table that looks at a hosts response time by whether they are a superhost using tabyl() from janitor (Firke 2020). It is clear that is a host does not respond within an hour then it is unlikely that they are a super host.\n\nairbnb_data_selected |> \n  tabyl(host_response_time, host_is_superhost) |> \n  adorn_percentages(\"row\") |>\n  adorn_pct_formatting(digits = 0) |>\n  adorn_ns() |> \n  adorn_title()\n\n                    host_is_superhost          \n host_response_time             FALSE      TRUE\n a few days or more         90% (223) 10%  (26)\n       within a day         70% (348) 30% (149)\n within a few hours         55% (354) 45% (284)\n     within an hour         49% (752) 51% (775)\n\n\nFinally, we could look at neighborhood. The data provider has attempted to clean the neighborhood variable for us, so will just use that variable for now. Although if we ended up using this variable for our actual analysis we would want to check they had not made any errors.\n\nairbnb_data_selected |> \n  tabyl(neighbourhood_cleansed) |> \n  adorn_totals(\"row\") |> \n  adorn_pct_formatting() |> \n  nrow()\n\n[1] 140\n\n\n\nairbnb_data_selected |> \n  tabyl(neighbourhood_cleansed) |> \n  adorn_pct_formatting() |> \n  arrange(-n) |> \n  filter(n > 100) |> \n  adorn_totals(\"row\") |> \n  head()\n\n            neighbourhood_cleansed   n percent\n Waterfront Communities-The Island 488   16.8%\n                           Niagara 129    4.4%\n                             Annex 102    3.5%\n                             Total 719       -\n\n\n\n\n13.3.3 Explore multiple relationships with a model\nWe will now run some models on our dataset. We will cover modeling in more detail in Chapter @ref(ijalm), but we can use models during EDA to help get a better sense of relationships that may exist between multiple variables in a dataset. For instance, we may like to see whether we can forecast whether someone is a super host, and the factors that go into explaining that. As the dependent variable is binary, this is a good opportunity to use logistic regression. We expect that better reviews will be associated with faster responses and higher reviews. Specifically, the model that we estimate is:\n\\[\\mbox{Prob(Is super host} = 1) = \\beta_0 + \\beta_1 \\mbox{Response time} + \\beta_2 \\mbox{Reviews} + \\epsilon\\]\nWe estimate the model using glm in the R language (R Core Team 2021).\n\nlogistic_reg_superhost_response_review <- glm(host_is_superhost ~ \n                                                host_response_time + \n                                                review_scores_rating,\n                                              data = airbnb_data_selected,\n                                              family = binomial\n                                              )\n\nWe can have a quick look at the results using modelsummary() from modelsummary (Arel-Bundock 2021)\n\nlibrary(modelsummary)\nmodelsummary(logistic_reg_superhost_response_review)\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    −18.931 \n  \n  \n     \n    (1.273) \n  \n  \n    host_response_timewithin a day \n    1.334 \n  \n  \n     \n    (0.235) \n  \n  \n    host_response_timewithin a few hours \n    1.932 \n  \n  \n     \n    (0.227) \n  \n  \n    host_response_timewithin an hour \n    2.213 \n  \n  \n     \n    (0.219) \n  \n  \n    review_scores_rating \n    0.173 \n  \n  \n     \n    (0.013) \n  \n  \n    Num.Obs. \n    2911 \n  \n  \n    AIC \n    3521.4 \n  \n  \n    BIC \n    3551.3 \n  \n  \n    Log.Lik. \n    −1755.698 \n  \n  \n    F \n    75.649"
  },
  {
    "objectID": "13-eda.html#exercises-and-tutorial",
    "href": "13-eda.html#exercises-and-tutorial",
    "title": "13  Exploratory data analysis",
    "section": "13.4 Exercises and tutorial",
    "text": "13.4 Exercises and tutorial\n\n13.4.1 Exercises\n\nIn your own words what is exploratory data analysis (this will be difficult, but please write only one nuanced paragraph)?\nIn Tukey’s words, what is exploratory data analysis (please write one paragraph)?\nWho was Tukey (please write a paragraph or two)?\nIf you have a dataset called ‘my_data’, which has two columns: ‘first_col’ and ‘second_col’, then could you please write some rough R code that would generate a graph (the type of graph does not matter).\nConsider a dataset that has 500 rows and 3 columns, so there are 1,500 cells. If 100 of the cells are missing data for at least one of the columns, then would you remove the whole row your dataset or try to run your analysis on the data as is, or some other procedure? What if your dataset had 10,000 rows instead, but the same number of missing cells?\nPlease note three ways of identifying unusual values.\nWhat is the difference between a categorical and continuous variable?\nWhat is the difference between a factor and an integer variable?\nHow can we think about who is systematically excluded from a dataset?\nUsing the opendatatoronto package, download the data on mayoral campaign contributions for 2014. (note: the 2014 file you will get from get_resource, so just keep the sheet that relates to the Mayor election).\n\nClean up the data format (fixing the parsing issue and standardizing the column names using janitor)\nSummarize the variables in the dataset. Are there missing values, and if so, should we be worried about them? Is every variable in the format it should be? If not, create new variable(s) that are in the right format.\nVisually explore the distribution of values of the contributions. What contributions are notable outliers? Do they share a similar characteristic(s)? It may be useful to plot the distribution of contributions without these outliers to get a better sense of the majority of the data.\nList the top five candidates in each of these categories: 1) total contributions; 2) mean contribution; and 3) number of contributions.\nRepeat that process, but without contributions from the candidates themselves.\nHow many contributors gave money to more than one candidate?\n\nName three geoms that produce graphs that have bars on them ggplot().\nConsider a dataset 10,000 observations and 27 variables. For each observation, there is at least one missing variable. Please discuss, in a paragraph or two, the steps that you would take to understand what is going on.\nKnown missing data, are those that leave holes in your dataset. But what about data that were never collected? Please look at McClelland (2019) and Luscombe and McClelland (2020). Look into how they gathered their dataset and what it took to put this together. What is in the dataset and why? What is missing and why? How could this affect the results? How might similar biases enter into other datasets that you have used or read about?\n\n\n\n13.4.2 Tutorial\nRedo the Airbnb EDA but for Paris. Please submit a PDF.\n\n\n\n\n\nArel-Bundock, Vincent. 2021. Modelsummary: Summary Tables and Plots for Statistical Models and Data: Beautiful, Customizable, and Publication-Ready. https://CRAN.R-project.org/package=modelsummary.\n\n\nCox, Murray. 2021. “Inside Airbnb - Toronto Data.” http://insideairbnb.com/get-the-data.html.\n\n\nFirke, Sam. 2020. Janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGelfand, Sharla. 2020. Opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. http://www.jstatsoft.org/v40/i03/.\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nLuscombe, Alex, and Alexander McClelland. 2020. “Policing the Pandemic: Tracking the Policing of COVID-19 Across Canada.”\n\n\nMcClelland, Alexander. 2019. “\"Lock This Whore up\": Legal Violence and Flows of Information Precipitating Personal Violence Against People Criminalised for HIV-Related Crimes in Canada.” European Journal of Risk Regulation 10 (1): 132–47.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nStaniak, Mateusz, and Przemyslaw Biecek. 2019. “The Landscape of r Packages for Automated Exploratory Data Analysis.” arXiv Preprint arXiv:1904.02101.\n\n\nTierney, Nicholas. 2017. “Visdat: Visualising Whole Data Frames.” JOSS 2 (16): 355. https://doi.org/10.21105/joss.00355.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "14-ijalm.html",
    "href": "14-ijalm.html",
    "title": "14  It’s Just A Linear Model",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "14-ijalm.html#introduction",
    "href": "14-ijalm.html#introduction",
    "title": "14  It’s Just A Linear Model",
    "section": "14.1 Introduction",
    "text": "14.1 Introduction\nLinear models have been around for a long time. For instance, speaking about the development of least squares, which is one way to fit linear models, in the 1700s, Stigler (1986, 16) describes how it was associated with foundational problems in astronomy, such as determining the motion of the moon and reconciling the non-periodic motion of Jupiter and Saturn. The fundamental issue at the time with least squares was that of hesitancy to combine different observations. Astronomers were early to develop a comfort with doing this because they had typically gathered their observations themselves and knew that the conditions of the data gathering were similar, even though the value of the observation was different. It took longer for social scientists to become comfortable with linear models, possibly because they were hesitant to group together data they worried was not alike. In this sense, astronomers had an advantage because they could compare their predictions with what actually happened whereas this was more difficult for social scientists (Stigler 1986, 163).\nGalton and others of his generation, some of whom were eugenicists, used linear regression in earnest in the late 1800s and early 1900s. Binary outcomes quickly became of interest and needed special treatment, leading to the development and wide adaption of logistic regression and similar methods in the mid-1900s (Cramer 2002). The generalized linear model framework came into being, in a formal sense, in the 1970s with Nelder and Wedderburn (1972) who brought this all together. Generalized linear models (GLM) broaden the types of outcomes that are allowed. We still model outcomes as a linear function, but we are not constrained to an outcome that is normally distributed. The outcome can be anything in the exponential family, and popular choices include the logistic distribution, and the Poisson distribution. A further generalization of GLMs is generalized additive models where we broaden the structure of the explanatory side. We still explain the dependent variable as an additive function of various bits and pieces, but those bits and pieces can be functions. This framework, in this way, came about in the 1990s, with Hastie and Tibshirani (1990).\nIt is important to recognize that when we build models, we are not discovering ‘the truth’. We are using the model to help us explore and understand the data that we have. There is no one best model, there are just useful models that help us learn something about the data that we have and hence, hopefully, something about the world from which the data were generated. When we use models, we are trying to understand the world, but there are enormous constraints on the perspective we bring to this. It is silly to expect one model to be universal. Further, we should not just blindly throw data into a regression model and hope that it will sort it out. ‘Regression will not sort it out. Regression is indeed an oracle, but a cruel one. It speaks in riddles and delights in punishing us for asking bad questions’ (McElreath 2020, 162).\nWe use models to understand the world. We poke, push, and test them. We build them and rejoice in their beauty, and then seek to understand their limits and ultimately destroy them. It is this process that is important, it is this process that allows us to better understand the world; not the outcome. When we build models, we need to keep in mind both the world of the model and the broader world that we want to be able to speak about. To what extent does a model trained on the experiences of straight, cis, men, speak to the world as it is? It is not worthless, but it is also not unimpeachable. To what extent does the model teach us about the data that we have? To what extent do the data that we have reflect the world about which we would like to draw conclusions? We need to keep such questions front of mind.\nMuch of statistics was developed without concern for broader implications. And that was reasonable because it was developed for situations such as astronomy and agriculture. Folks were literally able to randomize the order of fields and planting because they literally worked at agricultural stations. But many of the subsequent applications in the twentieth and twenty-first centuries, do not have those properties. Statistical science is often taught as though it proceeds through some idealized process where a hypothesis appears, is tested, and is either confirmed or not. But that is not what happens. We react to incentives. We dabble, guess, and test, and then follow our intuition, backfilling as we need. All of this is fine. But it is not a world in which a traditional null hypothesis holds completely, which means concepts such as p-values and power lose some of their meaning. While we need to understand the ‘old world’, we also need to be sophisticated enough to know when we need to move away from it. We can appreciate the beauty and ingenuity of a Ford Model T, at the same time recognizing we could not use it to win Le Mans.\nIn this chapter we begin with simple linear regression, and then move to multiple linear regression, the difference being the number of explanatory variables that we allow. We then consider logistic and Poisson regression. We consider three approaches for each of these: base R, which is useful when we want to quickly use the models in EDA; tidymodels (Kuhn and Wickham 2020) which is useful when we are interested in forecasting; and rstanarm (Goodrich et al. 2020) when we are interested in understanding. Regardless of the approach we use, the important thing to remember is that modelling in this way is just fancy averaging. The chapter is named for a quote by Daniela Witten, Professor, University of Washington, who identifies how far we can get with linear models and the huge extent to which they underpin statistics."
  },
  {
    "objectID": "14-ijalm.html#simple-linear-regression",
    "href": "14-ijalm.html#simple-linear-regression",
    "title": "14  It’s Just A Linear Model",
    "section": "14.2 Simple linear regression",
    "text": "14.2 Simple linear regression\nWhen we are interested in the relationship between two continuous variables, say \\(y\\) and \\(x\\), we can use simple linear regression. This is based on the Normal, also ‘Gaussian’, distribution. The shape of the Normal distribution is determined by two parameters, the mean \\(\\mu\\) and the standard deviation, \\(\\sigma\\):\n\\[y = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{1}{2}z^2},\\] where \\(z = (x - \\mu)/\\sigma\\) is the difference between the mean, \\(\\mu\\), and \\(x\\) in terms of the standard deviation (Pitman 1993, 94).\nAs discussed in Chapter @ref(r-essentials), we use rnorm() to simulate data from the Normal distribution.\n\nlibrary(tidyverse)\n\nset.seed(853)\n\ntwenty_draws_from_normal_distribution <- \n  tibble(draws = rnorm(n = 20, mean = 0, sd = 1))\n  \ntwenty_draws_from_normal_distribution\n\n# A tibble: 20 × 1\n     draws\n     <dbl>\n 1 -0.360 \n 2 -0.0406\n 3 -1.78  \n 4 -1.12  \n 5 -1.00  \n 6  1.78  \n 7 -1.39  \n 8 -0.497 \n 9 -0.558 \n10 -0.824 \n11  1.67  \n12 -0.682 \n13  0.0652\n14 -0.260 \n15  0.329 \n16 -0.437 \n17 -0.323 \n18  0.115 \n19  0.842 \n20  0.342 \n\n\nHere we specified \\(n=20\\) draws from a Normal distribution with mean of 0 and standard deviation of 1. When we deal with real data, we will not know these parameters and we want to use our data to estimate them. We can estimate the mean, \\(\\bar{x}\\), and standard deviation, \\(\\hat{\\sigma}_x\\), with the following estimators:\n\\[\n\\begin{aligned}\n\\bar{x} &= \\frac{1}{n} \\times \\sum_{i = 1}^{n}x_i\\\\\n\\hat{\\sigma}_{x} &= \\sqrt{\\frac{1}{n} \\times \\sum_{i = 1}^{n}\\left(x_i - \\bar{x}\\right)^2}\n\\end{aligned}\n\\]\nIf \\(\\hat{\\sigma}_x\\) is the standard deviation, then a standard error of an estimate, say, \\(\\bar{x}\\) is: \\[\\mbox{SE}(\\bar{x})^2 = \\frac{\\sigma^2}{n}\\]\n\nestimated_mean <-\n  sum(twenty_draws_from_normal_distribution$draws) / nrow(twenty_draws_from_normal_distribution)\n\nestimated_mean\n\n[1] -0.2069253\n\nestimated_mean <-\n  twenty_draws_from_normal_distribution |> \n  mutate(draws_diff_square = (draws - estimated_mean)^2)\n\nestimated_standard_deviation <- \n  sqrt(\n    sum(estimated_mean$draws_diff_square) / nrow(twenty_draws_from_normal_distribution)\n  )\n\nestimated_standard_deviation\n\n[1] 0.8832841\n\n\nWe should not be worried that our estimates are slightly off. It will typically take a larger number of draws before we get the expected shape, and our estimated parameters get close to the actual parameters, but it will happen (Figure 14.1). Wasserman (2005, 76) describes our certainty of this, which is due to the Law of Large Numbers, as ‘a crowning achievement in probability’.\n\nlibrary(tidyverse)\n\nset.seed(853)\n\ntibble(\n  number_of_draws = c(\n    rep.int(x = \"2 draws\", times = 2),\n    rep.int(x = \"5 draws\", times = 5),\n    rep.int(x = \"10 draws\", times = 10),\n    rep.int(x = \"50 draws\", times = 50),\n    rep.int(x = \"100 draws\", times = 100),\n    rep.int(x = \"500 draws\", times = 500),\n    rep.int(x = \"1,000 draws\", times = 1000),\n    rep.int(x = \"10,000 draws\", times = 10000),\n    rep.int(x = \"100,000 draws\", times = 100000)\n  ),\n  draws = c(\n    rnorm(n = 2, mean = 0, sd = 1),\n    rnorm(n = 5, mean = 0, sd = 1),\n    rnorm(n = 10, mean = 0, sd = 1),\n    rnorm(n = 50, mean = 0, sd = 1),\n    rnorm(n = 100, mean = 0, sd = 1),\n    rnorm(n = 500, mean = 0, sd = 1),\n    rnorm(n = 1000, mean = 0, sd = 1),\n    rnorm(n = 10000, mean = 0, sd = 1),\n    rnorm(n = 100000, mean = 0, sd = 1)\n  )\n) |>\n  mutate(number_of_draws = as_factor(number_of_draws)) |>\n  ggplot(aes(x = draws)) +\n  geom_density() +\n  theme_minimal() +\n  facet_wrap(vars(number_of_draws),\n             scales = \"free_y\") +\n  labs(x = 'Draw',\n       y = 'Density')\n\n\n\n\nFigure 14.1: The Normal distribution takes its familiar shape as the number of draws increases\n\n\n\n\nWhen we use simple linear regression, we assume that our relationship is characterized by the variables and the parameters. If we have two variables, \\(Y\\) and \\(X\\), then we could characterize a linear relationship between these as: \\[Y \\approx \\beta_0 + \\beta_1 X.\\]\nThere are two coefficients, also ‘parameters’: the ‘intercept’, \\(\\beta_0\\), and the ‘slope’, \\(\\beta_1\\). We are saying that \\(Y\\) will have some value, \\(\\beta_0\\), even when \\(X\\) is 0, and that \\(Y\\) will change by \\(\\beta_1\\) units for every one unit change in \\(X\\). The language that we use is that ‘X is being regressed on Y’. We may then take this relationship to the data that we have, in order to estimate these coefficients, for the particular data that we have.\nTo make this example concrete, we will simulate some data and then discuss it in that context. For instance, we could consider the time it takes someone to run five kilometers, compared with the time it takes them to run a marathon (Figure 14.2). We impute a relationship of 8.4, as that is roughly the ratio between the distance of a marathon and a five-kilometer race.\n\nset.seed(853)\n\nnumber_of_observations <- 100\nexpected_relationship <- 8.4\n\nsimulated_running_data <- \n  tibble(five_km_time = \n           runif(n = number_of_observations, \n                 min = 15, \n                 max = 30),\n         noise = rnorm(number_of_observations, 0, 20),\n         marathon_time = five_km_time * expected_relationship + noise\n         ) |>\n  mutate(five_km_time = round(five_km_time, digits = 1),\n         marathon_time = round(marathon_time, digits = 1)) |>\n  select(-noise)\n\nsimulated_running_data\n\n# A tibble: 100 × 2\n   five_km_time marathon_time\n          <dbl>         <dbl>\n 1         20.4          152.\n 2         16.8          134.\n 3         22.3          198.\n 4         19.7          166.\n 5         15.6          163.\n 6         21.1          168.\n 7         17            131.\n 8         18.6          150.\n 9         17.4          158.\n10         17.8          147.\n# … with 90 more rows\n\n\n\nsimulated_running_data |> \n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point() +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\") +\n  theme_classic()\n\n\n\n\nFigure 14.2: Simulated data of the relationship between the time to run five kilometers and a marathon\n\n\n\n\nIn this simulated example, we know what \\(\\beta_0\\) and \\(\\beta_1\\) are. But our challenge is to see if we can use only the data, and simple linear regression, to recover them. That is, can we use \\(x\\), which is the five-kilometer time, to produce estimates of \\(y\\), which is the marathon time, and which we will put a hat on to denote our estimate:\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x.\\] The hats are used to indicate that these are estimated values.\nThis involves estimating values for \\(\\beta_0\\) and \\(\\beta_1\\), and again, our estimates will be denoted by a hat on them. But how should we estimate these coefficients? Even if we impose a linear relationship there are many options, because a large number of straight lines could be drawn. But some of those lines would fit the data better than others.\nOne way we may define a line as being ‘better’ than another, is if it is as close as possible to each of the \\(x\\) and \\(y\\) combinations that we know. There are a lot of candidates for how we define ‘as close as possible’, but one is to minimize the residual sum of squares. To do this we produce estimates for \\(\\hat{y}\\) based on some guesses of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), given the \\(x\\). We then work out how ‘wrong’, for every point \\(i\\), we were: \\[e_i = y_i - \\hat{y}_i.\\]\nTo compute the residual sum of squares (RSS), we sum across all the points, taking the square to account for negative differences: \\[\\mbox{RSS} = e^2_1+ e^2_2 +\\dots + e^2_n.\\] This results in one ‘linear best-fit’ line (Figure 14.3), but it is worth thinking about all the assumptions and decisions that it took to get us to this point.\n\nsimulated_running_data |> \n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", \n              se = FALSE, \n              color = \"black\", \n              linetype = \"dashed\",\n              formula = 'y ~ x') +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\") +\n  theme_classic()\n\n\n\n\nFigure 14.3: Simulated data of the relationship between the time to run five kilometers and a marathon\n\n\n\n\nUnderpinning our use of simple linear regression is a belief that there is some ‘true’ relationship between \\(X\\) and \\(Y\\), that is:\n\\[Y = f(X) + \\epsilon.\\]\nWe are going to say that function, \\(f()\\), is linear, and so for simple linear regression:\n\\[\\hat{Y} = \\beta_0 + \\beta_1 X + \\epsilon.\\]\nThere is some ‘true’ relationship between \\(X\\) and \\(Y\\), but we do not know what it is. All we can do is use our sample of data to estimate it. But because our understanding depends on that sample, for every possible sample, we would get a slightly different relationship, as measured by the coefficients.\nThat \\(\\epsilon\\) is a measure of our error—what does the model not know? There is going to be plenty that the model does not know, but we hope the error does not depend on \\(X\\), and that the error is normally distributed.\nWe can conduct simple linear regression with lm() from base R. We specify the dependent variable first, then ~, followed by the independent variables. Finally, we specify the dataset.\n\nsimulated_running_data_first_model <- \n  lm(marathon_time ~ five_km_time, \n     data = simulated_running_data)\n\nTo see the result of the regression, we can use summary() from base R.\n\nsummary(simulated_running_data_first_model)\n\n\nCall:\nlm(formula = marathon_time ~ five_km_time, data = simulated_running_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.654  -9.278   0.781  12.606  56.898 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    8.2393     8.9550    0.92     0.36    \nfive_km_time   7.9407     0.4072   19.50   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.96 on 98 degrees of freedom\nMultiple R-squared:  0.7951,    Adjusted R-squared:  0.793 \nF-statistic: 380.3 on 1 and 98 DF,  p-value: < 2.2e-16\n\n\nThe first part of the result tells us the regression that we specified, then information about the residuals, and our estimated coefficients. And then finally some useful diagnostics.\nThe intercept is the marathon time that we would expect with a five-kilometer time of 0 minutes. Hopefully this example illustrates the need to carefully interpret the intercept coefficient! The coefficient on five-kilometer run time shows how we expect the marathon time to change if the five-kilometer run time changed by one unit. In this case it is about 8.4, which makes sense seeing as a marathon is roughly that many times longer than a five-kilometer run.\nWe use augment() from broom (Robinson, Hayes, and Couch 2021) to add the fitted values and residuals to our original dataset. This allows us to plot the residuals (?fig-fivekmvsmarathonresids).\n\nlibrary(broom)\n\nsimulated_running_data <- \n  augment(simulated_running_data_first_model,\n          data = simulated_running_data)\n\nsimulated_running_data\n\n# A tibble: 100 × 8\n   five_km_time marathon_time .fitted .resid   .hat .sigma   .cooksd .std.resid\n          <dbl>         <dbl>   <dbl>  <dbl>  <dbl>  <dbl>     <dbl>      <dbl>\n 1         20.4          152.    170. -17.8  0.0108   17.0 0.00611       -1.06 \n 2         16.8          134.    142.  -7.84 0.0232   17.0 0.00260       -0.468\n 3         22.3          198.    185.  13.1  0.0103   17.0 0.00312        0.775\n 4         19.7          166.    165.   1.83 0.0121   17.1 0.0000718      0.108\n 5         15.6          163.    132.  31.3  0.0307   16.7 0.0556         1.87 \n 6         21.1          168.    176.  -8.09 0.0101   17.0 0.00118       -0.479\n 7         17            131.    143. -11.8  0.0222   17.0 0.00564       -0.705\n 8         18.6          150.    156.  -6.04 0.0152   17.0 0.000990      -0.359\n 9         17.4          158.    146.  11.1  0.0201   17.0 0.00448        0.661\n10         17.8          147.    150.  -2.68 0.0183   17.0 0.000238      -0.160\n# … with 90 more rows\n\n\n\nggplot(simulated_running_data, \n       aes(x = .resid)) + \n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(y = \"Number of occurrences\",\n       x = \"Residuals\")\n\nggplot(simulated_running_data, \n       aes(x = five_km_time, y = .resid)) + \n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"grey\") +\n  theme_classic() +\n  labs(y = \"Residuals\",\n       x = \"Five-kilometer time (minutes)\")\n\nggplot(simulated_running_data, \n       aes(x = marathon_time, .fitted)) + \n  geom_point() +\n  theme_classic() +\n  labs(y = \"Estimated marathon time\",\n       x = \"Actual marathon time\")\n\n\n\n\n\n\nFigure 14.4: Residuals from the simple linear regression with simulated data on the time someone takes to run five kilometers and a marathon\n\n\n\n\n\n\n\nFigure 14.5: Residuals from the simple linear regression with simulated data on the time someone takes to run five kilometers and a marathon\n\n\n\n\n\n\n\nFigure 14.6: Residuals from the simple linear regression with simulated data on the time someone takes to run five kilometers and a marathon\n\n\n\n\n\n\nWe want our estimate to be unbiased. When we say our estimate is unbiased, we are trying to say that even though with some sample our estimate might be too high, and with another sample our estimate might be too low, eventually if we have a lot of data then our estimate would be the same as the population. An estimator is unbiased if it does not systematically over- or under-estimate (James et al. 2017, 65).\nBut we want to try to speak to the ‘true’ relationship, so we need to try to capture how much we think our understanding depends on the particular sample that we have to analyze. And this is where standard error comes in. It tells us how off our estimate is compared with the actual (Figure 14.7).\n\nsimulated_running_data |> \n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", \n              se = TRUE, \n              color = \"black\", \n              linetype = \"dashed\",\n              formula = 'y ~ x') +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\") +\n  theme_classic()\n\n\n\n\nFigure 14.7: Simple linear regression with simulated data on the time someone takes to run five kilometers and a marathon, along with standard errors\n\n\n\n\nFrom standard errors, we can compute a confidence interval. A 95 per cent confidence interval is a range, such that there is roughly a 0.95 probability that the interval happens to contain the population parameter, which is typically unknown. The lower end of this range is: \\(\\hat{\\beta_1} - 2 \\times \\mbox{SE}\\left(\\hat{\\beta_1}\\right)\\) and the upper end of this range is: \\(\\hat{\\beta_1} + 2 \\times \\mbox{SE}\\left(\\hat{\\beta_1}\\right)\\).\nNow that we have a range, for which we can say there is a roughly 95 per cent probability that range contains the true population parameter, we could test claims. For instance, we could claim that there is no relationship between \\(X\\) and \\(Y\\), i.e. \\(\\beta_1 = 0\\), as an alternative to a claim that there is some relationship between \\(X\\) and \\(Y\\), i.e. \\(\\beta_1 \\neq 0\\).\nIn the same way that in Chapter 10 we needed to decide how much evidence it would take to convince us that our tea taster could distinguish whether milk or tea had been added first, we need to decide whether our estimate of \\(\\beta_1\\), which is \\(\\hat{\\beta}_1\\), is ‘far enough’ away from zero for us to be comfortable claiming that \\(\\beta_1 \\neq 0\\). How far is ‘far enough’? If we were very confident in our estimate of \\(\\beta_1\\) then it would not have to be far, but if we were not then it would have to be substantial. The standard error of \\(\\hat{\\beta}_1\\) does an awful lot of work here in accounting for a variety of factors, only some of which it can actually account for.\nWe compare this standard error with \\(\\hat{\\beta}_1\\) to get the t-statistic: \\[t = \\frac{\\hat{\\beta}_1 - 0}{\\mbox{SE}(\\hat{\\beta}_1)}.\\] And we then compare our t-statistic to the t-distribution to compute the probability of getting this absolute t-statistic or a larger one, if it was actually the case that \\(\\beta_1 = 0\\). This probability is the p-value. A smaller p-value means it is less likely that we would observe our data due to chance if there was not a relationship.\n\nWords! Mere words! How terrible they were! How clear, and vivid, and cruel! One could not escape from them. And yet what a subtle magic there was in them! They seemed to be able to give a plastic form to formless things, and to have a music of their own as sweet as that of viol or of lute. Mere words! Was there anything so real as words?\nThe Picture of Dorian Gray (Wilde 1891).\n\nWe will not make much use of p-values in this book because they are a specific and subtle concept. They are difficult to understand and easy to abuse. The main issue is that they embody, and assume correct, every assumption of the model, including everything that went into gathering and cleaning the data. While smaller p-values do imply the data are more unusual if all the assumptions were correct; when we consider the full data science workflow there are usually an awful lot of assumptions. And we do not get guidance from p-values about the reasonableness of specific assumptions (Greenland et al. 2016, 339). A p-value may reject a null hypothesis because the null hypothesis is actually false, but it may also be that some data were incorrectly gathered or prepared. We can only be sure that the p-value speaks to the hypothesis we are interested in testing, if all the other assumptions are correct. There is nothing inherently wrong about using p-values, but it is important to use them in sophisticated and thoughtful ways. Cox (2018) provides a lovely discussion of what this requires.\nOne application where it is easy to see abuse of p-values is in power analysis. Power, in a statistical sense, refers to probability of rejecting a null hypothesis that is actually false. As power relates to hypothesis testing, it also related to sample size. There is often a worry that a study is ‘under-powered’, meaning there was not a large enough sample, but rarely a worry that, say, the data were inappropriately cleaned, even though we cannot distinguish between these based only on a p-value."
  },
  {
    "objectID": "14-ijalm.html#multiple-linear-regression",
    "href": "14-ijalm.html#multiple-linear-regression",
    "title": "14  It’s Just A Linear Model",
    "section": "14.3 Multiple linear regression",
    "text": "14.3 Multiple linear regression\nTo this point we have just considered one explanatory variable. But we will usually have more than one. One approach would be to run separate regressions for each explanatory variable. But compared with separate linear regressions for each, adding more explanatory variables allows us to have a better understanding of the intercept and accounts for interaction. Often the results will be quite different.\nWe may also like to consider explanatory variables that do not have an inherent ordering. For instance: pregnant or not; day or night. When there are only two options then we can use a binary variable, which is considered either 0 or 1. If we have a column of character values that only has two values, such as: c(\"Myles\", \"Ruth\", \"Ruth\", \"Myles\", \"Myles\", \"Ruth\"), then using this as an explanatory variable in the usual regression set up, would mean that it is treated as a binary variable. If there are more than two levels then we can use a combination of binary variables, where the ‘missing’ outcome (baseline) gets pushed into the intercept.\n\nsimulated_running_data <-\n  simulated_running_data |>\n  mutate(was_raining = sample(\n    c(\"Yes\", \"No\"),\n    size = number_of_observations,\n    replace = TRUE,\n    prob = c(0.2, 0.8)\n  )) |> \n  select(five_km_time, marathon_time, was_raining)\n\nsimulated_running_data\n\n# A tibble: 100 × 3\n   five_km_time marathon_time was_raining\n          <dbl>         <dbl> <chr>      \n 1         20.4          152. No         \n 2         16.8          134. No         \n 3         22.3          198. No         \n 4         19.7          166. No         \n 5         15.6          163. No         \n 6         21.1          168. No         \n 7         17            131. No         \n 8         18.6          150. No         \n 9         17.4          158. No         \n10         17.8          147. No         \n# … with 90 more rows\n\n\nWe can add additional explanatory variables to lm() with +.\n\nsimulated_running_data_rain_model <-\n  lm(marathon_time ~ five_km_time + was_raining,\n     data = simulated_running_data)\n\nsummary(simulated_running_data_rain_model)\n\n\nCall:\nlm(formula = marathon_time ~ five_km_time + was_raining, data = simulated_running_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.150  -8.828   0.968  10.522  58.224 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      9.1030     9.0101   1.010    0.315    \nfive_km_time     7.8660     0.4154  18.934   <2e-16 ***\nwas_rainingYes   4.1673     4.5048   0.925    0.357    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.98 on 97 degrees of freedom\nMultiple R-squared:  0.7969,    Adjusted R-squared:  0.7927 \nF-statistic: 190.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nThe result probably is not too surprising if we look at a plot of the data (Figure 14.8).\n\nsimulated_running_data |>\n  ggplot(aes(x = five_km_time, y = marathon_time, color = was_raining)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              color = \"black\", \n              linetype = \"dashed\",\n              formula = 'y ~ x') +\n  labs(x = \"Five-kilometer time (minutes)\",\n       y = \"Marathon time (minutes)\",\n       color = \"Was raining\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 14.8: Simple linear regression with simulated data on the time someone takes to run five kilometers and a marathon, with a binary variable for whether it was raining\n\n\n\n\nIn addition to wanting to include additional explanatory variables, we may think that they are related with each another. For instance, if we were wanting to explain the amount of snowfall, then we may be interested in the humidity and the temperature, but those two variables may also interact. We can do this by using * instead of + when we specify the model. When we interact variables in this way, then we almost always need to include the individual variables as well and lm() will do this by default.\n\nsimulated_running_data <-\n  simulated_running_data |>\n  mutate(humidity = sample(\n    c(\"High\", \"Low\"),\n    size = number_of_observations,\n    replace = TRUE,\n    prob = c(0.2, 0.8)\n  ))\n\nsimulated_running_data\n\n# A tibble: 100 × 4\n   five_km_time marathon_time was_raining humidity\n          <dbl>         <dbl> <chr>       <chr>   \n 1         20.4          152. No          Low     \n 2         16.8          134. No          Low     \n 3         22.3          198. No          Low     \n 4         19.7          166. No          Low     \n 5         15.6          163. No          Low     \n 6         21.1          168. No          Low     \n 7         17            131. No          Low     \n 8         18.6          150. No          Low     \n 9         17.4          158. No          High    \n10         17.8          147. No          Low     \n# … with 90 more rows\n\n\n\nsimulated_running_data_rain_and_humidity_model <-\n  lm(marathon_time ~ five_km_time + was_raining*humidity,\n     data = simulated_running_data)\n\nsummary(simulated_running_data_rain_and_humidity_model)\n\n\nCall:\nlm(formula = marathon_time ~ five_km_time + was_raining * humidity, \n    data = simulated_running_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.904  -8.523   0.404  10.130  59.951 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                 15.0595    10.3144   1.460    0.148    \nfive_km_time                 7.7313     0.4167  18.552   <2e-16 ***\nwas_rainingYes              15.6008     9.6199   1.622    0.108    \nhumidityLow                 -3.7380     4.9569  -0.754    0.453    \nwas_rainingYes:humidityLow -14.5825    10.7410  -1.358    0.178    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.79 on 95 degrees of freedom\nMultiple R-squared:  0.8054,    Adjusted R-squared:  0.7972 \nF-statistic: 98.31 on 4 and 95 DF,  p-value: < 2.2e-16\n\n\nThere are a variety of threats to the validity of linear regression estimates, and aspects to think about. We need to address these when we use it, and usually four graphs and associated text are sufficient to assuage most of these. Aspects of concern include:\n\nLinearity of explanatory variables. We are concerned with whether the independent variables enter in a linear way. Sometimes if we are worried that there might be a multiplicative relationship between the explanatory variables, rather than an additive one, then we may consider a logarithmic transform. We can usually be convinced there is enough linearity in our explanatory variables for our purposes by using graphs of the variables.\nIndependence of errors. We are concerned that the errors are not correlated. For instance, if we are interested in weather-related measurement such as average daily temperature, then we may find a pattern because the temperature on one day is likely similar to the temperature on another. We can be convinced that we have satisfied this condition by making graphs of the errors, such as ?fig-fivekmvsmarathonresids).\nHomoscedasticity of errors. We are concerned that the errors are not becoming systematically larger or smaller throughout the sample. If that is happening, then we term it heteroscedasticity. Again, graphs of errors, such as ?fig-fivekmvsmarathonresids) are used to convince us of this.\nNormality of errors. We are concerned that our errors are normally distributed when we are interested in making individual-level predictions.\nOutliers and other high-impact observations. Finally, we might be worried that our results are being driven by a handful of observations. For instance, thinking back to Chapter @ref(static-communication) and Anscombe’s Quartet, we notice that linear regression estimates would be heavily influenced by the inclusion of one or two particular points. We can become comfortable with this by considering our analysis on various sub-sets\n\nThose aspects are statistical concerns and relate to whether the model is working. The most important threat to validity and hence the aspect that must be addressed at some length, is speaking to the fact that this model is appropriate to the circumstances and addresses the research question at hand.\nTo this point, we have just had a quick look at the regression results using summary(). A better approach is to use modelsummary() from modelsummary (Arel-Bundock 2021) (Table 14.1).\n\nlibrary(modelsummary)\n\nmodelsummary(list(simulated_running_data_first_model,\n                  simulated_running_data_rain_model, \n                  simulated_running_data_rain_and_humidity_model),\n             fmt = 2)\n\n\nTable 14.1:  Explaining marathon time based on five-kilometer run times and weather features \n \n  \n      \n    Model 1 \n    Model 2 \n    Model 3 \n  \n \n\n  \n    (Intercept) \n    8.24 \n    9.10 \n    15.06 \n  \n  \n     \n    (8.96) \n    (9.01) \n    (10.31) \n  \n  \n    five_km_time \n    7.94 \n    7.87 \n    7.73 \n  \n  \n     \n    (0.41) \n    (0.42) \n    (0.42) \n  \n  \n    was_rainingYes \n     \n    4.17 \n    15.60 \n  \n  \n     \n     \n    (4.50) \n    (9.62) \n  \n  \n    humidityLow \n     \n     \n    −3.74 \n  \n  \n     \n     \n     \n    (4.96) \n  \n  \n    was_rainingYes × humidityLow \n     \n     \n    −14.58 \n  \n  \n     \n     \n     \n    (10.74) \n  \n  \n    Num.Obs. \n    100 \n    100 \n    100 \n  \n  \n    R2 \n    0.795 \n    0.797 \n    0.805 \n  \n  \n    R2 Adj. \n    0.793 \n    0.793 \n    0.797 \n  \n  \n    AIC \n    854.0 \n    855.1 \n    854.8 \n  \n  \n    BIC \n    861.8 \n    865.5 \n    870.4 \n  \n  \n    Log.Lik. \n    −423.993 \n    −423.554 \n    −421.405 \n  \n  \n    F \n    380.262 \n    190.279 \n    98.314 \n  \n\n\n\n\n\n\nWhen we are focused on prediction, we will often want to fit many models. One way to do this is to copy and paste code many times. There is nothing wrong with that. And that is the way that most people get started. But we need an approach that:\n\nscales more easily;\nenables us to think carefully about over-fitting; and\nadds model evaluation.\n\nThe use of tidymodels (Kuhn and Wickham 2020) satisfies these criteria by providing a coherent grammar that allows us to easily fit a variety of models. Like tidyverse, it is a package of packages.\nAs we are focused on prediction, we are worried about over-fitting our data, which would limit our ability to make claims about other datasets. One way to partially address this is to split our dataset into training and test datasets using initial_split().\n\nlibrary(tidymodels)\n\nset.seed(853)\n\nsimulated_running_data_split <- \n  initial_split(data = simulated_running_data, \n                prop = 0.80)\n\nsimulated_running_data_split\n\n<Analysis/Assess/Total>\n<80/20/100>\n\n\nHaving split the data, we then create the training and test datasets.\n\nsimulated_running_data_train <- training(simulated_running_data_split)\n\nsimulated_running_data_train\n\n# A tibble: 80 × 4\n   five_km_time marathon_time was_raining humidity\n          <dbl>         <dbl> <chr>       <chr>   \n 1         17.4          158. No          High    \n 2         23.8          205. No          High    \n 3         23.4          198. Yes         Low     \n 4         22.3          175  No          Low     \n 5         19.3          158  No          Low     \n 6         24.4          204. No          High    \n 7         17            120  No          High    \n 8         19.1          178. No          Low     \n 9         22.3          198. No          Low     \n10         20.6          166. No          Low     \n# … with 70 more rows\n\nsimulated_running_data_test <- testing(simulated_running_data_split)\n\nsimulated_running_data_test\n\n# A tibble: 20 × 4\n   five_km_time marathon_time was_raining humidity\n          <dbl>         <dbl> <chr>       <chr>   \n 1         17            131. No          Low     \n 2         16.6          118. No          Low     \n 3         19.6          164. No          Low     \n 4         21.1          164. Yes         Low     \n 5         21            180. No          Low     \n 6         27.9          246. No          Low     \n 7         23.7          198. No          High    \n 8         16            143  No          Low     \n 9         24.9          202. No          Low     \n10         15.2          140. Yes         Low     \n11         28.9          238. No          Low     \n12         19.2          132  Yes         Low     \n13         22            200. No          Low     \n14         26.5          229  Yes         High    \n15         25.3          222  Yes         High    \n16         25.9          208. Yes         Low     \n17         15.5          120  No          Low     \n18         18            144. No          Low     \n19         27.2          227. No          High    \n20         20.8          201. No          Low     \n\n\nWhen we look at the training and test datasets, we can see that we have placed most of our dataset into the training dataset. We will use that to estimate the parameters of our model. We have kept a small amount of it back, and we will use that to evaluate our model.\n\nsimulated_running_data_first_model_tidymodels <- \n  linear_reg() |>\n  set_engine(engine = \"lm\") |> \n  fit(marathon_time ~ five_km_time + was_raining, \n      data = simulated_running_data_train\n      )\n\nsimulated_running_data_first_model_tidymodels\n\nparsnip model object\n\nFit time:  2ms \n\nCall:\nstats::lm(formula = marathon_time ~ five_km_time + was_raining, \n    data = data)\n\nCoefficients:\n   (Intercept)    five_km_time  was_rainingYes  \n        16.601           7.490           8.244  \n\n\nWe will use tidymodels for forecasting. But when we are focused on inference, instead, we will use Bayesian approaches. To do this we use the probabilistic programming language ‘Stan’, and interface with it using rstanarm (Goodrich et al. 2020). We keep these separate, rather than adapting Bayesian approaches within tidymodels, because to this point the ecosystems have developed separately, and so the best books to go onto next are also separate.\nIn order to use Bayesian approaches we will need to specify a starting point, or prior. This is another reason for the workflow advocated in this book; the simulate stage leads directly to priors. We will also more thoroughly specify the model that we are interested in:\n\\[\n\\begin{aligned}\ny_i &\\sim \\mbox{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 +\\beta_1x_i\\\\\n\\beta_0 &\\sim \\mbox{Normal}(0, 3) \\\\\n\\beta_1 &\\sim \\mbox{Normal}(0, 3) \\\\\n\\sigma &\\sim \\mbox{Normal}(0, 3) \\\\\n\\end{aligned}\n\\]\nOn a practical note, one aspect that different between Bayesian approaches and the way we have been doing modelling to this point, is that Bayesian models will usually take longer to run. Because of this, it can be useful to run the model, either within the R Markdown document or in a separate R script, and then save it with saveRDS(). With sensible R Markdown chunk options, the model can then be read into the R Markdown document with readRDS(). In this way, the model, and hence delay, is only imposed once for a given model.\n\nlibrary(rstanarm)\n\nsimulated_running_data_first_model_rstanarm <-\n  stan_lm(\n    marathon_time ~ five_km_time + was_raining, \n    data = simulated_running_data,\n    prior = NULL,\n    seed = 853\n  )\n\n# simulated_running_data_first_model_rstanarm <-\n#   stan_lm(\n#     formula = marathon_time ~ five_km_time,\n#     data = simulated_running_data,\n#     prior = normal(0, 3),\n#     prior_intercept = normal(0, 3),\n#     prior_aux = normal(0, 3),\n#     seed = 853\n#     )\n\nsaveRDS(simulated_running_data_first_model_rstanarm,\n        file = \"simulated_running_data_first_model_rstanarm.rds\")\n\n\nsimulated_running_data_first_model_rstanarm\n\nstan_lm\n family:       gaussian [identity]\n formula:      marathon_time ~ five_km_time + was_raining\n observations: 100\n predictors:   3\n------\n               Median MAD_SD\n(Intercept)    9.2    8.7   \nfive_km_time   7.9    0.4   \nwas_rainingYes 4.6    4.5   \n\nAuxiliary parameter(s):\n              Median MAD_SD\nR2             0.8    0.0  \nlog-fit_ratio  0.0    0.0  \nsigma         17.1    1.3  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "objectID": "14-ijalm.html#logistic-regression",
    "href": "14-ijalm.html#logistic-regression",
    "title": "14  It’s Just A Linear Model",
    "section": "14.4 Logistic regression",
    "text": "14.4 Logistic regression\nLinear regression is a nice way to come to understand better our data. But it assumes a continuous outcome variable which can take any number on the real line. We would like some way to use this same machinery when we cannot satisfy this condition. We turn to logistic and Poisson regression for binary and count outcome variables, respectively.\nLogistic regression and its close variants are useful in a variety of settings, from elections (Wang et al. 2015) through to horse racing (Chellel 2018; Bolton and Chapman 1986). We use logistic regression when the dependent variable is a binary outcome, such as 0 or 1. Although the presence of a binary outcome variable may sound limiting, there are a lot of circumstances in which the outcome either naturally falls into this situation, or can be adjusted into it.\nThe reason that we use logistic regression is that we will be modelling a probability and so it will be bounded between 0 and 1. Whereas with linear regression we may end up with values outside this. This all said, logistic regression, as Daniella Witten teaches us, is just a linear model. The foundation of logistic regression is the logit function:\n\\[\n\\mbox{logit}(x) = \\log\\left(\\frac{x}{1-x}\\right),\n\\] which will transpose values between 0 and 1, onto the real line. For instance, logit(0.1) = -2.2, logit(0.5) = 0, and logit(0.9) = 2.2.\nWe will simulate data on whether it is day or night, based on the number of cars that we can see.\n\nlibrary(tidyverse)\n\nset.seed(853)\n\nday_or_night <- \n  tibble(\n    number_of_cars = runif(n = 1000, min = 0, 100),\n    noise = rnorm(n = 1000, mean = 0, sd = 2),\n    is_night = if_else(number_of_cars + noise > 50, 1, 0)\n  ) |> \n  mutate(number_of_cars = round(number_of_cars)) |> \n  select(-noise)\n  \nday_or_night\n\n# A tibble: 1,000 × 2\n   number_of_cars is_night\n            <dbl>    <dbl>\n 1             36        0\n 2             12        0\n 3             48        0\n 4             32        0\n 5              4        0\n 6             40        0\n 7             13        0\n 8             24        0\n 9             16        0\n10             19        0\n# … with 990 more rows\n\n\nAs with linear regression, logistic regression with can use glm() from base to put together a quick model and summary() to look at it. In this case we will try to work out whether it is day or night, based on the number of cars we can see. We are interested in estimating Equation @ref(eq:logisticexample): \\[\n\\mbox{Pr}(y_i=1) = \\mbox{logit}^{-1}\\left(\\beta_0+\\beta_1 x_i\\right). (\\#eq:logisticexample)\n\\]\n\nday_or_night_model <- \n  glm(is_night ~ number_of_cars,\n      data = day_or_night,\n      family = 'binomial')\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(day_or_night_model)\n\n\nCall:\nglm(formula = is_night ~ number_of_cars, family = \"binomial\", \n    data = day_or_night)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.39419  -0.00002   0.00000   0.00002   2.33776  \n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -45.5353     7.3389  -6.205 5.48e-10 ***\nnumber_of_cars   0.9121     0.1470   6.205 5.47e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1386.194  on 999  degrees of freedom\nResidual deviance:   77.243  on 998  degrees of freedom\nAIC: 81.243\n\nNumber of Fisher Scoring iterations: 11\n\n\nOne reason that logistic regression can be a bit tricky initially, is because the coefficients take a bit of work to interpret. In particular, our estimate on likelihood of it being night is 0.91 This is the odds. So, the odds that it is night, increase by 0.91 as the number of cars that we saw increases. We can translate the result into probabilities using augment() from broom (Robinson, Hayes, and Couch 2021) and this allows us to graph the results (Figure 14.9)).\n\nlibrary(broom)\n\nday_or_night <-\n  augment(day_or_night_model,\n          data = day_or_night,\n          type.predict = \"response\")\n\nday_or_night\n\n# A tibble: 1,000 × 8\n   number_of_cars is_night  .fitted   .resid .std.resid     .hat .sigma  .cooksd\n            <dbl>    <dbl>    <dbl>    <dbl>      <dbl>    <dbl>  <dbl>    <dbl>\n 1             36        0 3.06e- 6 -2.47e-3   -2.47e-3 1.30e- 5  0.278 1.98e-11\n 2             12        0 2.22e-16 -2.11e-8   -2.11e-8 6.91e-15  0.278 7.67e-31\n 3             48        0 1.48e- 1 -5.65e-1   -5.71e-1 2.04e- 2  0.278 1.84e- 3\n 4             32        0 7.95e- 8 -3.99e-4   -3.99e-4 5.57e- 7  0.278 2.21e-14\n 5              4        0 2.22e-16 -2.11e-8   -2.11e-8 1.01e-14  0.278 1.12e-30\n 6             40        0 1.17e- 4 -1.53e-2   -1.53e-2 2.58e- 4  0.278 1.51e- 8\n 7             13        0 2.22e-16 -2.11e-8   -2.11e-8 6.55e-15  0.278 7.27e-31\n 8             24        0 5.39e-11 -1.04e-5   -1.04e-5 7.85e-10  0.278 2.11e-20\n 9             16        0 2.22e-16 -2.11e-8   -2.11e-8 5.53e-15  0.278 6.14e-31\n10             19        0 5.63e-13 -1.06e-6   -1.06e-6 1.17e-11  0.278 3.29e-24\n# … with 990 more rows\n\n\n\nday_or_night |>\n  mutate(is_night = factor(is_night)) |>\n  ggplot(aes(x = number_of_cars,\n             y = .fitted,\n             color = is_night)) +\n  geom_jitter(width = 0.01, height = 0.01) +\n  labs(x = \"Number of cars that were seen\",\n       y = \"Estimated probability it is night\",\n       color = \"Was actually night\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 14.9: Logistic regression probability results with simulated data of whether it is day or night based on the number of cars that are around\n\n\n\n\nWe can use tidymodels to run this if we wanted. In order to do that, we first need to change the class of our dependent variable into a factor.\n\nset.seed(853)\n\nday_or_night <-\n  day_or_night |>\n  mutate(is_night = as_factor(is_night))\n\nday_or_night_split <- initial_split(day_or_night, prop = 0.80)\nday_or_night_train <- training(day_or_night_split)\nday_or_night_test <- testing(day_or_night_split)\n\nday_or_night_tidymodels <-\n  logistic_reg(mode = \"classification\") |>\n  set_engine(\"glm\") |>\n  fit(is_night ~ number_of_cars,\n      data = day_or_night_train)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nday_or_night_tidymodels\n\nparsnip model object\n\nFit time:  4ms \n\nCall:  stats::glm(formula = is_night ~ number_of_cars, family = stats::binomial, \n    data = data)\n\nCoefficients:\n   (Intercept)  number_of_cars  \n      -44.4817          0.8937  \n\nDegrees of Freedom: 799 Total (i.e. Null);  798 Residual\nNull Deviance:      1109 \nResidual Deviance: 62.5     AIC: 66.5\n\n\nAs before, we can make a graph of the actual results compared with our estimates. But one nice aspect of this is that we could use our test dataset to more thoroughly evaluate our model’s forecasting ability, for instance through a confusion matrix. We find that the model does well on the held-out dataset.\n\nday_or_night_tidymodels |>\n  predict(new_data = day_or_night_test) |>\n  cbind(day_or_night_test) |>\n  conf_mat(truth = is_night, estimate = .pred_class)\n\n          Truth\nPrediction   0   1\n         0  95   0\n         1   3 102\n\n\nFinally, we might be interested in inference, and so want to build a Bayesian model using rstanarm. Again, we will more fully specify our model:\nFinally, we can build a Bayesian model and estimate it with rstanarm.\n\\[\n\\begin{aligned}\n\\mbox{Pr}(y_i=1) & = \\mbox{logit}^{-1}\\left(\\beta_0+\\beta_1 x_i\\right)\\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 3)\\\\\n\\beta_1 & \\sim \\mbox{Normal}(0, 3)\n\\end{aligned}\n\\]\n\nday_or_night_rstanarm <-\n  stan_glm(\n    is_night ~ number_of_cars,\n    data = day_or_night,\n    family = binomial(link = \"logit\"),\n    prior = normal(0, 3),\n    prior_intercept = normal(0, 3),\n    seed = 853\n  )\n\nsaveRDS(day_or_night_rstanarm,\n        file = \"day_or_night_rstanarm.rds\")\n\n\nday_or_night_rstanarm\n\nstan_glm\n family:       binomial [logit]\n formula:      is_night ~ number_of_cars\n observations: 1000\n predictors:   2\n------\n               Median MAD_SD\n(Intercept)    -47.3    8.0 \nnumber_of_cars   0.9    0.2 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "objectID": "14-ijalm.html#poisson-regression",
    "href": "14-ijalm.html#poisson-regression",
    "title": "14  It’s Just A Linear Model",
    "section": "14.5 Poisson regression",
    "text": "14.5 Poisson regression\nWhen we have count data, we should initially think to use Poisson distribution. The Poisson distribution has the interesting feature that the mean is also the variance, and so as the mean increases, so does the variance. As such, the Poisson distribution is governed by the parameter, \\(\\lambda\\) and it distributes probabilities over the non-negative integers. The Poisson distribution is (Pitman 1993, 121):\n\\[P_{\\lambda}(k) = e^{-\\lambda}\\mu^k/k!\\mbox{, for }k=0,1,2,...\\] We can simulate \\(n=20\\) draws from the Poisson distribution with rpois(), where \\(\\lambda\\) is both the mean and the variance. The \\(\\lambda\\) parameter governs the shape of the distribution (?fig-poissondistributiontakingshape).\n\nrpois(n = 20, lambda = 3)\n\n [1] 1 5 5 4 5 2 1 4 5 4 6 2 3 4 4 6 5 1 3 5\n\n\n\nset.seed(853)\n\nnumber_of_each <- 1000\n\ntibble(\n  lambda = c(\n    rep(0, number_of_each),\n    rep(1, number_of_each),\n    rep(2, number_of_each),\n    rep(4, number_of_each),\n    rep(7, number_of_each),\n    rep(10, number_of_each),\n    rep(15, number_of_each),\n    rep(50, number_of_each),\n    rep(100, number_of_each)\n  ),\n  draw = c(\n    rpois(n = number_of_each, lambda = 0),\n    rpois(n = number_of_each, lambda = 1),\n    rpois(n = number_of_each, lambda = 2),\n    rpois(n = number_of_each, lambda = 4),\n    rpois(n = number_of_each, lambda = 7),\n    rpois(n = number_of_each, lambda = 10),\n    rpois(n = number_of_each, lambda = 15),\n    rpois(n = number_of_each, lambda = 50),\n    rpois(n = number_of_each, lambda = 100)\n  )\n) |>\n  ggplot(aes(x = draw)) +\n  geom_density() +\n  facet_wrap(vars(lambda),\n             scales = \"free_y\") +\n  theme_minimal() +\n  labs(x = 'Integer',\n       y = 'Density')\n\n\n\n\nThe Poisson distribution is governed by the value of the mean, which is the same as its variance\n\n\n\n\nFor instance, if we look at the number of A+ grades that are awarded in each university course in a given term then for each course we would have a count.\n\nset.seed(853)\n\ncount_of_A_plus <-\n  tibble(\n    # Thanks to Chris DuBois: https://stackoverflow.com/a/1439843\n    department = c(rep.int(\"1\", 26), rep.int(\"2\", 26), rep.int(\"3\", 26)),\n    course = c(paste0(\"DEP_1_\", letters), paste0(\"DEP_2_\", letters), paste0(\"DEP_3_\", letters)),\n    number_of_A_plus = c(\n      sample(c(1:10),\n             size = 26,\n             replace = TRUE),\n      sample(c(1:50),\n             size = 26,\n             replace = TRUE),\n      sample(c(1:25),\n             size = 26,\n             replace = TRUE)\n    )\n  )\n\ncount_of_A_plus\n\n# A tibble: 78 × 3\n   department course  number_of_A_plus\n   <chr>      <chr>              <int>\n 1 1          DEP_1_a                9\n 2 1          DEP_1_b               10\n 3 1          DEP_1_c                1\n 4 1          DEP_1_d                5\n 5 1          DEP_1_e                2\n 6 1          DEP_1_f                4\n 7 1          DEP_1_g                3\n 8 1          DEP_1_h                3\n 9 1          DEP_1_i                1\n10 1          DEP_1_j                3\n# … with 68 more rows\n\n\nOur simulated dataset has the number of A+ grades awarded by courses, which are structured within departments. We can use glm() and summary() from base to quickly get a sense of the data.\n\ngrades_model_base <- \n  glm(number_of_A_plus ~ department, \n    data = count_of_A_plus, \n    family = 'poisson')\n\nsummary(grades_model_base)\n\n\nCall:\nglm(formula = number_of_A_plus ~ department, family = \"poisson\", \n    data = count_of_A_plus)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-6.739  -1.210  -0.171   1.424   3.952  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.44238    0.09535  15.128   <2e-16 ***\ndepartment2  1.85345    0.10254  18.075   <2e-16 ***\ndepartment3  1.00663    0.11141   9.035   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 952.12  on 77  degrees of freedom\nResidual deviance: 450.08  on 75  degrees of freedom\nAIC: 768.21\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe interpretation of the coefficient on ‘department2’ is that it is the log of the expected difference between departments. So we expect \\(\\exp(1.85345) \\approx 6.3\\) and \\(\\exp(1.00663) \\approx 2.7\\) additional A+ grades in departments 2 and 3, compared with department 1.\nWe can use tidymodels to estimate Poisson regression models with poissonreg (Kuhn 2021).\n\nlibrary(poissonreg)\n\nset.seed(853)\n\ncount_of_A_plus_split <-\n  rsample::initial_split(count_of_A_plus, prop = 0.80)\ncount_of_A_plus_train <- rsample::training(count_of_A_plus_split)\ncount_of_A_plus_test <- rsample::testing(count_of_A_plus_split)\n\na_plus_model_tidymodels <-\n  poisson_reg(mode = \"regression\") |>\n  set_engine(\"glm\") |>\n  fit(number_of_A_plus ~ department,\n      data = count_of_A_plus_train)\n\na_plus_model_tidymodels\n\nparsnip model object\n\nFit time:  2ms \n\nCall:  stats::glm(formula = number_of_A_plus ~ department, family = stats::poisson, \n    data = data)\n\nCoefficients:\n(Intercept)  department2  department3  \n      1.470        1.925        1.011  \n\nDegrees of Freedom: 61 Total (i.e. Null);  59 Residual\nNull Deviance:      758 \nResidual Deviance: 276.8    AIC: 534.8\n\n\nAnd finally, we can build a Bayesian model and estimate it with rstanarm. We put a tight prior on the coefficients because of the propensity for the Poisson distribution to expand them substantially.\n\\[\n\\begin{aligned}\ny_i &\\sim \\mbox{Poisson}(\\lambda_i)\\\\\n\\log(\\lambda_i) & = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 0.5)\\\\\n\\beta_1 & \\sim \\mbox{Normal}(0, 0.5)\\\\\n\\beta_2 & \\sim \\mbox{Normal}(0, 0.5)\n\\end{aligned}\n\\]\n\ncount_of_A_plus_rstanarm <-\n  stan_glm(\n    number_of_A_plus ~ department,\n    data = count_of_A_plus,\n    family = poisson(link = \"log\"),\n    prior = normal(0, 0.5),\n    prior_intercept = normal(0, 0.5),\n    seed = 853\n  )\n\nsaveRDS(count_of_A_plus_rstanarm,\n        file = \"count_of_A_plus_rstanarm.rds\")\n\n\ncount_of_A_plus_rstanarm\n\nstan_glm\n family:       poisson [log]\n formula:      number_of_A_plus ~ department\n observations: 78\n predictors:   3\n------\n            Median MAD_SD\n(Intercept) 1.5    0.1   \ndepartment2 1.8    0.1   \ndepartment3 0.9    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "objectID": "14-ijalm.html#exercises-and-tutorial",
    "href": "14-ijalm.html#exercises-and-tutorial",
    "title": "14  It’s Just A Linear Model",
    "section": "14.6 Exercises and tutorial",
    "text": "14.6 Exercises and tutorial\n\n14.6.1 Exercises\n\nPlease write a linear relationship between some response variable, Y, and some predictor, X. What is the intercept term? What is the slope term? What would adding a hat to these indicate?\nWhat is the least squares criterion? Similarly, what is RSS and what are we trying to do when we run least squares regression?\nWhat is statistical bias?\nIf there were three variables: Snow, Temperature, and Wind, please write R code that would fit a simple linear regression to explain Snow as a function of Temperature and Wind. What do you think about another explanatory variable - daily stock market returns - to your model?\nAccording to Greenland et al. (2016), p-values test (pick one)?\n\nAll the assumptions about how the data were generated (the entire model), not just the targeted hypothesis it is supposed to test (such as a null hypothesis).\nWhether the hypothesis targeted for testing is true or not.\nA dichotomy whereby results can be declared ‘statistically significant’.\n\nAccording to Greenland et al. (2016), a p-value may be small because (select all)?\n\nThe targeted hypothesis is false.\nThe study protocols were violated.\nIt was selected for presentation based on its small size.\n\nAccording to Obermeyer et al. (2019), why does racial bias occur in an algorithm used to guide health decisions in the US (pick one)?\n\nThe algorithm uses health costs as a proxy for health needs.\nThe algorithm was trained on Reddit data.\n\nWhen should we use logistic regression (pick one)?\n\nContinuous dependent variable.\nBinary dependent variable.\nCount dependent variable.\n\nWe are interested in studying how voting intentions in the recent US presidential election vary by an individual’s income. We set up a logistic regression model to study this relationship. In this study, one possible dependent variable would be (pick one)?\n\nWhether the respondent is a US citizen (yes/no)\nThe respondent’s personal income (high/low)\nWhether the respondent is going to vote for Trump (yes/no)\nWho the respondent voted for in 2016 (Trump/Clinton)\n\nWe are interested in studying how voting intentions in the recent US presidential election vary by an individual’s income. We set up a logistic regression model to study this relationship. In this study, one possible dependent variable would be (pick one)?\n\nThe race of the respondent (white/not white)\nThe respondent’s marital status (married/not)\nWhether the respondent is registered to vote (yes/no)\nWhether the respondent is going to vote for Biden (yes/no)\n\nPlease explain what a p-value is, using only the term itself (i.e. ‘p-value’) and words that are amongst the 1,000 most common in the English language according to the XKCD Simple Writer - https://xkcd.com/simplewriter/. (Please write one or two paragraphs.)\nThe mean of a Poisson distribution is equal to its?\n\nMedian.\nStandard deviation.\nVariance.\n\nWhat is power (in a statistical context)?\nAccording to McElreath (2020, 162) ‘Regression will not sort it out. Regression is indeed an oracle, but a cruel one. It speaks in riddles and delights in punishing us for…’ (please select one answer)?\n\novercomplicating models.\nasking bad questions.\nusing bad data.\n\nIs a model that fits the small or large world more important to you, and why?\n\n\n\n14.6.2 Tutorial\nSimulate some data that are similar to those discussed by Gould (2013). Then build a regression model. Discuss your results\n\n\n\n\n\n\n\nArel-Bundock, Vincent. 2021. Modelsummary: Summary Tables and Plots for Statistical Models and Data: Beautiful, Customizable, and Publication-Ready. https://CRAN.R-project.org/package=modelsummary.\n\n\nBolton, Ruth, and Randall Chapman. 1986. “Searching for Positive Returns at the Track.” Management Science 32 (August): 1040–60. https://doi.org/10.1287/mnsc.32.8.1040.\n\n\nChellel, Kit. 2018. “The Gambler Who Cracked the Horse-Racing Code.” Bloomberg Businessweek (May 2018). Featured in Bloomberg Businessweek, May 14.\n\n\nCox, David. 2018. “In Gentle Praise of Significance Tests.” YouTube, October. https://youtu.be/txLj_P9UlCQ.\n\n\nCramer, Jan Salomon. 2002. “The Origins of Logistic Regression.”\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2020. “Rstanarm: Bayesian Applied Regression Modeling via Stan.” https://mc-stan.org/rstanarm.\n\n\nGould, Stephen Jay. 2013. “The Median Isn’t the Message.” AMA Journal of Ethics 15 (1): 77–81.\n\n\nGreenland, Sander, Stephen J Senn, Kenneth J Rothman, John B Carlin, Charles Poole, Steven N Goodman, and Douglas G Altman. 2016. “Statistical Tests, p Values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31 (4): 337–50.\n\n\nHastie, Trevor J, and Robert J Tibshirani. 1990. Generalized Additive Models. Vol. 43. CRC press.\n\n\nIoannidis, John PA. 2005. “Why Most Published Research Findings Are False.” PLoS Medicine 2 (8): e124.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2017. An Introduction to Statistical Learning with Applications in r.\n\n\nKuhn, Max. 2021. Poissonreg: Model Wrappers for Poisson Regression. https://CRAN.R-project.org/package=poissonreg.\n\n\nKuhn, Max, and Hadley Wickham. 2020. Tidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles. https://www.tidymodels.org.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC Press.\n\n\nNelder, John Ashworth, and Robert WM Wedderburn. 1972. “Generalized Linear Models.” Journal of the Royal Statistical Society: Series A (General) 135 (3): 370–84.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53.\n\n\nPitman, Jim. 1993. Probability.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2021. Broom: Convert Statistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom.\n\n\nStigler, Stephen. 1986. The History of Statistics. Harvard University Press.\n\n\nWang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman. 2015. “Forecasting Elections with Non-Representative Polls.” International Journal of Forecasting 31 (3): 980–91.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWilde, Oscar. 1891. The Picture of Dorian Gray."
  },
  {
    "objectID": "15-causality_from_obs.html",
    "href": "15-causality_from_obs.html",
    "title": "15  Causality from observational data",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "15-causality_from_obs.html#introduction",
    "href": "15-causality_from_obs.html#introduction",
    "title": "15  Causality from observational data",
    "section": "15.1 Introduction",
    "text": "15.1 Introduction\nLife is grand when we can conduct experiments to be able to speak to causality. But there are circumstances in which we cannot run an experiment, but nonetheless want to be able to make causal claims. And data from outside experiments have value that experiments do not have. In this chapter we discuss the circumstances and methods that allow us to speak to causality using observational data. We use relatively simple methods, in sophisticated ways, drawing from statistics, but also a variety of social sciences, including economics, and political science, as well as epidemiology.\nFor instance, Dagan et al. (2021) use observational data to confirm the effectiveness of the Pfizer-BioNTech vaccine. They discuss how one concern with using observational data in this way is confounding, which is where we are concerned that there is some variable that affects both the explanatory and dependent variables and can lead to spurious relationships. Dagan et al. (2021) adjust for this by first making a list of potential confounders, such as age, sex, geographic location, healthcare usage and then adjusting for each of them, by matching, one-to-one between people that were vaccinated and those that were not. The experimental data guided the use of observational data, and the larger size of the later enabled a focus on specific age-groups and extent of disease.\nUsing observational data in sophisticated ways is what this chapter is about. How we can nonetheless be comfortable making causal statements, even when we cannot run A/B tests or RCTs. Indeed, in what circumstances may we prefer to not run those or to run observational-based approaches in addition to them. We cover three of the major methods: difference in differences; regression discontinuity; and instrumental variables."
  },
  {
    "objectID": "15-causality_from_obs.html#directed-acyclic-graphs",
    "href": "15-causality_from_obs.html#directed-acyclic-graphs",
    "title": "15  Causality from observational data",
    "section": "15.2 Directed acyclic graphs",
    "text": "15.2 Directed acyclic graphs\nWhen we are discussing causality, it can help to be very specific about what we mean. It is easy to get caught up in observational data and trick ourselves. It is important to think hard, and to use all the tools available to us. For instance, in that earlier example, Dagan et al. (2021) were able to use experimental data as a guide. Most of the time, we will not be so lucky as to have both experimental data and observational data available to us. But one framework that can help with thinking hard about our data is the use of directed acyclic graph (DAG). DAGs are a fancy name for a flow diagram and involves drawing arrows and lines between the variables to indicate the relationship between them. Following Igelström (2020) we use DiagrammeR (Iannone 2020) to build them here, because we can use the same skills outside of just DAGs and DiagrammeR provides quite a lot of control (Figure 15.1)). But ggdag is also useful (Barrett 2021).\n\nlibrary(DiagrammeR)\n\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext, fontsize = 10, fontname = Helvetica]\n    x\n    y\n  edge [minlen = 2, arrowhead = vee]\n    x->y\n  { rank = same; x; y }\n}\n\", height = 200)\n\n\n\n\nFigure 15.1: Using a DAG to illustrate perceived relationships\n\n\n\nIn Figure 15.1, we think that x causes y. We could build another DAG where the situation is less clear. To make the examples a little easier to follow, we will switch to fruits (Figure 15.2).\n\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext, fontsize = 10, fontname = Helvetica]\n    Apple\n    Banana\n    Carrot\n  edge [minlen = 2, arrowhead = vee]\n    Apple->Banana\n    Carrot->Apple\n    Carrot->Banana\n  { rank = same; Apple; Banana }\n}\n\", height = 300)\n\n\n\n\nFigure 15.2: A DAG showing Carrot as a confounder\n\n\n\nIn Figure 15.2, we think Apple causes Banana. But we also think that Carrot causes Banana, and that Carrot also causes Apple. That relationship is a ‘backdoor path’, and would create spurious correlation in our analysis. We may think that changes in Apple are causing changes in Banana, but it could be that Carrot is changing them both. That variable, in this case, Carrot, is called a ‘confounder’.\nHernan and Robins (2020, 83) discuss an interesting case where a researcher was interested in whether one person looking up at the sky makes others look up at the sky also. There was a clear relationship between the responses of both people. But it was also the case that there was noise in the sky. So, it was unclear whether the second person looked up because the first person looked up, or they both looked up because of the noise. When using experimental data, randomization allows us to avoid this concern, but with observational data we cannot rely on that. It is also not the case that bigger data necessarily get around this problem for us. Instead, it is important to think carefully about the situation.\nIf there are confounders, but we are still interested in causal effects, then we need to adjust for them. One way is to include them in the regression. But the validity of this requires several assumptions. In particular, Gelman and Hill (2007, 169) warn that our estimate will only correspond to the average causal effect in the sample if we include all of the confounders and have the right model. Putting the second requirement to one side, and focusing only on the first, if we do not think about and observe a confounder, then it can be difficult to adjust for it. And this is an area where both domain expertise and theory can bring a lot to an analysis.\nIn Figure 15.3 we have a similar situation where again, we may think that Apple causes Banana. But in Figure 15.3 Apple also causes Carrot, which itself causes Banana.\n\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext, fontsize = 10, fontname = Helvetica]\n    Apple\n    Banana\n    Carrot\n  edge [minlen = 2, arrowhead = vee]\n    Apple->Banana\n    Apple->Carrot\n    Carrot->Banana\n  { rank = same; Apple; Banana }\n}\n\", height = 300)\n\n\n\n\nFigure 15.3: A DAG showing Carrot as a mediator\n\n\n\nIn Figure 15.3, Carrot is called a ‘mediator’ and we would not adjust for it if we were interested in the effect of Apple on Banana. If we were to adjust for it, then some of what we are attributing to Apple, would be due to Carrot.\nFinally, in Figure 15.4 we have yet another similar situation, where we again, think that Apple causes Banana. But this time both Apple and Banana also cause Carrot.\n\nDiagrammeR::grViz(\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = plaintext, fontsize = 10, fontname = Helvetica]\n    Apple\n    Banana\n    Carrot\n  edge [minlen = 2, arrowhead = vee]\n    Apple->Banana\n    Apple->Carrot\n    Banana->Carrot\n  { rank = same; Apple; Banana }\n}\n\", height = 300)\n\n\n\n\nFigure 15.4: A DAG showing Carrot as a collider\n\n\n\nIn this case, Carrot is called a ‘collider’ and if we were to condition on it, then we would create a misleading relationship.\nIt is important to be clear about this: we must create the DAG ourselves, in the same way that we must put together the model ourselves. There is nothing that will create it for us. This means that we need to think carefully about the situation. Because it is one thing to see something in the DAG and then do something about it. But it is another to not even know that it is there. McElreath (2020, 180) describes these as haunted DAGs. DAGs are helpful, but they are just a tool to help us think deeply about our situation."
  },
  {
    "objectID": "15-causality_from_obs.html#two-common-paradoxes",
    "href": "15-causality_from_obs.html#two-common-paradoxes",
    "title": "15  Causality from observational data",
    "section": "15.3 Two common paradoxes",
    "text": "15.3 Two common paradoxes\n\n15.3.1 Simpson’s paradox\nThere are two situations where data can trick us that are so common that we will explicitly go through them. These are: 1) Simpson’s paradox, and 2) Berkson’s paradox. It is important to keep these situations in mind, and the use of DAGs can help identify them.\nSimpson’s paradox occurs when we estimate some relationship for subsets of our data, but a different relationship when we consider the entire dataset (Simpson 1951). It is a particular case of the ecological fallacy, which is when we try to make claims about individuals, based on their group. For instance, it may be that there is a positive relationship between undergraduate grades and performance in graduate school in two departments when considering each department individually. But if undergraduate grades tended to be higher in one department than another while graduate school performance tended to be opposite, we may find a negative relationship between undergraduate grades and performance in graduate school. We can simulate some data to show this more clearly (Figure 15.5).\n\nlibrary(tidyverse)\n\nset.seed(853)\n\nnumber_in_each <- 1000\n\ndepartment_one <-\n  tibble(\n    undergrad = runif(n = number_in_each, min = 0.7, max = 0.9),\n    noise = rnorm(n = number_in_each, 0, sd = 0.1),\n    grad = undergrad + noise,\n    type = \"Department 1\"\n  )\n\ndepartment_two <-\n  tibble(\n    undergrad = runif(n = number_in_each, min = 0.6, max = 0.8),\n    noise = rnorm(n = number_in_each, 0, sd = 0.1),\n    grad = undergrad + noise + 0.3,\n    type = \"Department 2\"\n  )\n\nboth_departments <- rbind(department_one, department_two)\n\nboth_departments\n\n# A tibble: 2,000 × 4\n   undergrad   noise  grad type        \n       <dbl>   <dbl> <dbl> <chr>       \n 1     0.772 -0.0566 0.715 Department 1\n 2     0.724 -0.0312 0.693 Department 1\n 3     0.797  0.0770 0.874 Department 1\n 4     0.763 -0.0664 0.697 Department 1\n 5     0.707  0.0717 0.779 Department 1\n 6     0.781 -0.0165 0.764 Department 1\n 7     0.726 -0.104  0.623 Department 1\n 8     0.749  0.0527 0.801 Department 1\n 9     0.732 -0.0471 0.684 Department 1\n10     0.738  0.0552 0.793 Department 1\n# … with 1,990 more rows\n\n\n\nboth_departments |>\n  ggplot(aes(x = undergrad, y = grad)) +\n  geom_point(aes(color = type), alpha = 0.1) +\n  geom_smooth(aes(color = type), method = 'lm', formula = 'y ~ x') +\n  geom_smooth(method = 'lm',\n              formula = 'y ~ x',\n              color = 'black') +\n  labs(x = \"Undergraduate results\",\n       y = \"Graduate results\",\n       color = \"Department\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 15.5: Illustration of simulated data that shows Simpson’s paradox\n\n\n\n\nSimpson’s paradox is often illustrated using real-world data from University of California, Berkeley, on graduate admissions (Bickel, Hammel, and O’Connell 1975). Bickel, Hammel, and O’Connell (1975) include what might be one of the greatest sub-titles ever published: ‘Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation’. More recently, as shown in its documentation, the ‘penguins’ dataset from parlmerpenguins (Horst, Hill, and Gorman 2020) provides an example of Simpson’s paradox, using real-world data (Figure 15.6).\n\nlibrary(palmerpenguins)\n\npenguins |> \n  ggplot(aes(x = body_mass_g, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = 0.1) +\n  geom_smooth(aes(color = species), method = 'lm', formula = 'y ~ x') +\n  geom_smooth(method = 'lm',\n              formula = 'y ~ x',\n              color = 'black') +\n  labs(x = \"Body mass (grams)\",\n       y = \"Bill depth (millimeters)\",\n       color = \"Species\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 15.6: Illustration of Simpson’s paradox in a dataset of penguin bill depth compared with their body mass\n\n\n\n\n\n\n15.3.2 Berkson’s paradox\nBerkson’s paradox occurs when we estimate some relationship based on the dataset that we have. But because the dataset is so selected, the relationship is different in a more general dataset (Berkson 1946). For instance, if we have a dataset of professional cyclists then we might find there is no relationship between their VO2 max and their chance of winning a bike race. But if we had a dataset of the general population then we might find a relationship between these two variables. The professional dataset has just been so selected that the relationship disappears; one cannot become a professional cyclist unless one has a good-enough VO2 max. Again, we can simulate some data to show this more clearly (Figure 15.7).\n\nset.seed(853)\n\nnumber_of_pros <- 100\n\nnumber_of_public <- 1000\n\nprofessionals <-\n  tibble(\n    VO2 = runif(n = number_of_pros, min = 0.7, max = 0.9),\n    chance_of_winning = runif(n = number_of_pros, min = 0.7, max = 0.9),\n    type = \"Professionals\"\n  )\n\ngeneral_public <-\n  tibble(\n    VO2 = runif(n = number_of_public, min = 0.6, max = 0.8),\n    noise = rnorm(n = number_of_public, 0, sd = 0.03),\n    chance_of_winning = VO2 + noise + 0.1,\n    type = \"Public\"\n  ) |>\n  select(-noise)\n\nprofessionals_and_public = rbind(professionals, general_public)\n\nprofessionals_and_public\n\n# A tibble: 1,100 × 3\n     VO2 chance_of_winning type         \n   <dbl>             <dbl> <chr>        \n 1 0.772             0.734 Professionals\n 2 0.724             0.773 Professionals\n 3 0.797             0.772 Professionals\n 4 0.763             0.754 Professionals\n 5 0.707             0.843 Professionals\n 6 0.781             0.740 Professionals\n 7 0.726             0.803 Professionals\n 8 0.749             0.750 Professionals\n 9 0.732             0.890 Professionals\n10 0.738             0.821 Professionals\n# … with 1,090 more rows\n\n\n\nprofessionals_and_public |> \n  ggplot(aes(x = VO2, y = chance_of_winning)) +\n  geom_point(aes(color = type), alpha = 0.1) +\n  geom_smooth(aes(color = type), method = 'lm', formula = 'y ~ x') +\n  geom_smooth(method = 'lm', formula = 'y ~ x', color = 'black') +\n  labs(x = \"VO2 max\",\n       y = \"Chance of winning a bike race\",\n       color = \"Type\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 15.7: Illustration of simulated data that shows Berkson’s paradox"
  },
  {
    "objectID": "15-causality_from_obs.html#difference-in-differences",
    "href": "15-causality_from_obs.html#difference-in-differences",
    "title": "15  Causality from observational data",
    "section": "15.4 Difference in differences",
    "text": "15.4 Difference in differences\n\n15.4.1 Overview\nThe ideal situation of being able to conduct an experiment is rarely possible. Can we reasonably expect that Netflix would allow us to change prices? And even if they did once, would they let us do it again, and again, and again? Further, rarely can we explicitly create treatment and control groups. Finally, experiments can be expensive or unethical. Instead, we need to make do with what we have. Rather than our counterfactual coming to us through randomization, and hence us knowing that the two are the same but for the treatment, we try to identify groups that were similar but for the treatment, and hence any differences can be attributed to the treatment.\nWith observational data, sometimes there are differences between our two groups before we treat. Provided those pre-treatment differences satisfy assumptions that essentially amount to the differences being both consistent, and that we expect that consistency to continue in the absence of the treatment—the ‘parallel trends’ assumption—then we can look to any difference in the differences as the effect of the treatment. One of the aspects of difference in differences analysis is that we can do it using relatively straight-forward methods. Linear regression with a binary variable is enough to get started and do a convincing job.\nConsider wanting to know the effect of a new tennis racket on serve speed. One way to test this would be to measure the difference between, say, Roger Federer’s serve speed without the tennis racket and the serve speed of an enthusiastic amateur, let us call them Ville, with the tennis racket. Yes, we would find a difference, but would we know how much to attribute to the tennis racket? Another way would be to consider the difference between Ville’s serve speed without the new tennis racket and Ville’s serve speed with the new tennis racket. But what if serves were just getting faster naturally over time? Instead, we combine the two approaches to look at the difference in the differences.\nWe begin by measuring Federer’s serve speed and compare it to Ville’s serve speed, both without the new racket. We then measure Federer’s serve speed again, and measure Ville’s serve speed with the new racket. That difference in the differences would then be the estimate of the effect of the new racket. There are a few key assumptions that we need to make for this analysis to be appropriate:\n\nIs there something else that may have affected only Ville, and not Federer that could affect Ville’s serve speed?\nIs it likely that Federer and Ville have the same trajectory of serve speed improvement? This is the ‘parallel trends’ assumption, and it dominates many discussions of difference in differences analysis.\nFinally, is it likely that the variance of our serve speeds of Federer and Ville are the same?\n\nDespite these requirements, difference in differences is a powerful approach because we do not need the treatment and control group to be the same before the treatment. We just need to have a good idea of how they differed.\n\n\n15.4.2 Simulated example\nTo be more specific about the situation, we simulate data. We will simulate a situation in which there is initially a difference of one between the serve speeds of the different people, and then after a new tennis racket, there is a difference of six. We can use a graph to illustrate the situation (Figure 15.8).\n\nlibrary(tidyverse)\n\nset.seed(853)\n\nsimulated_difference_in_differences <-\n  tibble(\n    person = rep(c(1:1000), times = 2),\n    time = c(rep(0, times = 1000), rep(1, times = 1000)),\n    treatment_group = rep(\n      sample(\n        x = 0:1,\n        size  = 1000,\n        replace = TRUE\n        ), \n      times = 2)\n    ) |>\n  mutate(treatment_group = as.factor(treatment_group),\n         time = as.factor(time)\n  )\n\n\nsimulated_difference_in_differences <-\n  simulated_difference_in_differences |>\n  rowwise() |>\n  mutate(\n    serve_speed = case_when(\n      time == 0 & treatment_group == 0 ~ rnorm(n = 1, mean = 5, sd = 1),\n      time == 1 & treatment_group == 0 ~ rnorm(n = 1, mean = 6, sd = 1),\n      time == 0 & treatment_group == 1 ~ rnorm(n = 1, mean = 8, sd = 1),\n      time == 1 & treatment_group == 1 ~ rnorm(n = 1, mean = 14, sd = 1)\n    )\n  )\n\nsimulated_difference_in_differences\n\n# A tibble: 2,000 × 4\n# Rowwise: \n   person time  treatment_group serve_speed\n    <int> <fct> <fct>                 <dbl>\n 1      1 0     0                      4.43\n 2      2 0     1                      6.96\n 3      3 0     1                      7.77\n 4      4 0     0                      5.31\n 5      5 0     0                      4.09\n 6      6 0     0                      4.85\n 7      7 0     0                      6.43\n 8      8 0     0                      5.77\n 9      9 0     1                      6.13\n10     10 0     1                      7.32\n# … with 1,990 more rows\n\n\n\nsimulated_difference_in_differences |>\n  ggplot(aes(x = time,\n             y = serve_speed,\n             color = treatment_group)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(group = person), alpha = 0.1) +\n  theme_minimal() +\n  labs(x = \"Time period\",\n       y = \"Serve speed\",\n       color = \"Person got a new racket\") +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 15.8: Illustration of simulated data that shows a difference before and after getting a new tennis racket\n\n\n\n\nAs it is a straight-forward example, we can obtain our estimate manually, by looking at the average difference of the differences. When we do that, we find that we estimate the effect of the new tennis racket to be 5.06, which is similar to what we simulated.\n\naverage_differences <-\n  simulated_difference_in_differences |>\n  pivot_wider(names_from = time,\n              values_from = serve_speed,\n              names_prefix = \"time_\") |>\n  mutate(difference = time_1 - time_0) |>\n  group_by(treatment_group) |>\n  summarize(average_difference = mean(difference))\n\naverage_differences$average_difference[2] - average_differences$average_difference[1]\n\n[1] 5.058414\n\n\nAnd we can use linear regression to get the same result. The equation that we are interested in is: \\[Y_{i,t} = \\beta_0 + \\beta_1\\mbox{Treatment binary}_i + \\beta_2\\mbox{Time binary}_t + \\beta_3(\\mbox{Treatment binary} \\times\\mbox{Time binary})_{i,t} + \\epsilon_{i,t}\\]\nWhile we should include the separate aspects as well, it is the estimate of the interaction that we are interested in. In this case it is \\(\\beta_3\\). And we find that our estimated effect is 5.06.\n\ndiff_in_diff_example_regression <- \n  lm(serve_speed ~ treatment_group*time, \n     data = simulated_difference_in_differences)\n\nsummary(diff_in_diff_example_regression)\n\n\nCall:\nlm(formula = serve_speed ~ treatment_group * time, data = simulated_difference_in_differences)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1415 -0.6638 -0.0039  0.6708  3.2664 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             4.97131    0.04281  116.12   <2e-16 ***\ntreatment_group1        3.03350    0.06225   48.73   <2e-16 ***\ntime1                   1.00680    0.06055   16.63   <2e-16 ***\ntreatment_group1:time1  5.05841    0.08803   57.46   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9828 on 1996 degrees of freedom\nMultiple R-squared:  0.9268,    Adjusted R-squared:  0.9266 \nF-statistic:  8418 on 3 and 1996 DF,  p-value: < 2.2e-16\n\n\n\n\n15.4.3 Assumptions\nIf we want to use difference in differences, then we need to satisfy the assumptions. There were three that were touched on earlier, but here we will focus on one: the ‘parallel trends’ assumption. The parallel trends assumption haunts everything to do with difference in differences analysis because we can never prove it, we can just be convinced of it, and try to convince others.\nTo see why we can never prove it, consider an example in which we want to know the effect of a new stadium on a professional sports team’s wins/loses. To do this we consider two teams: the Golden State Warriors and the Toronto Raptors. The Warriors changed stadiums at the start of the 2019-20 season, while the Raptors did not, so we will consider four time periods: the 2016-17 season, 2017-18 season, 2018-19 season, and finally we will compare the performance with the one after they moved, so the 2019-20 season. The Raptors here act as our counterfactual. This means that we assume the relationship between the Warriors and the Raptors, in the absence of a new stadium, would have continued to change in a consistent way. But the fundamental problem of causal inference means that we can never know that for certain. We must present sufficient evidence to assuage any concerns that a reader may have.\nThere are four main threats to validity when we use difference in differences, and we need to address all of them (Cunningham 2021, 272–77):\n\nNon-parallel trends. The treatment and control groups may be based on differences. As such it can be difficult to convincingly argue for parallel trends. In this case, maybe try to find another factor to consider in your model that may adjust for some of that. This may require difference in difference in differences (in the earlier example, we could perhaps add the San Francisco 49ers as they are in the same broad geographic area as the Warriors). Or maybe re-think the analysis to see if we can make a different control group. Adding additional earlier time periods may help but may introduce more issues, which we touch on in the third point.\nCompositional differences. This is a concern when working with repeated cross-sections. What if the composition of those cross-sections change? For instance, if we are working at an app that is rapidly growing, and we want to look at the effect of some change. In our initial cross-section, we may have mostly young people, but in a subsequent cross-section, we may have more older people as the demographics of the app usage change. Hence our results may just be an age-effect, not an effect of the change that we are interested in.\nLong-term effects compared with reliability. As we discussed in Chapter 10, there is a trade-off between the length of the analysis that we run. As we run the analysis for longer there is more opportunity for other factors to affect the results. There is also increased chance for someone who was not treated to be treated. But, on the other hand, it can be difficult to convincingly argue that short-term results will continue in the long-term.\nFunctional form dependence. This is less of an issue when the outcomes are similar, but if they are different then functional form may be responsible for some aspects of the results.\n\n\n\n15.4.4 Case study: French newspaper prices between 1960 and 1974\nIn this case study we introduce Angelucci and Cagé (2019), and replicate its main findings.\nThe business model of newspapers was challenged by the internet and many local newspapers have closed. And this issue is not new. When television was introduced, there were similar concerns. Angelucci and Cagé (2019) use the introduction of television advertising in France, announced in 1967, to examine the effect of decreased advertising revenue on newspapers. They create a dataset of French newspapers from 1960 to 1974 and then use difference in differences to examine the effect of the reduction in advertising revenues on newspapers’ content and prices. The change that they focus on is the introduction of television advertising, which they argue affected national newspapers more than local newspapers. They find that this change results in both less journalism-content in the newspapers and lower newspaper prices. Focusing on this change, and analyzing it using difference in differences, is important because it allows us to disentangle a few competing effects. For instance, did newspapers become redundant because they could no longer charge high prices for their advertisements, or because consumers preferred to get their news from the television?\nWe can get free access to the data that underpins Angelucci and Cagé (2019) after registration. The dataset is in the Stata data format, ‘dta’, which we can read with read_dta() from haven (Wickham and Miller 2020). The file that we are interested in is ‘Angelucci_Cage_AEJMicro_dataset.dta’, which is the ‘dta’ folder.\n\nlibrary(haven)\n\nnewspapers <- read_dta(\"Angelucci_Cage_AEJMicro_dataset.dta\")\n\nnewspapers\n\n\n\n# A tibble: 1,196 × 52\n    year id_news local national after_national   Had po_cst ps_cst etotal_cst\n   <dbl>   <dbl> <dbl>    <dbl>          <dbl> <dbl>  <dbl>  <dbl>      <dbl>\n 1  1960       1     1        0              0     0   2.60   2.29  109455728\n 2  1961       1     1        0              0     0   2.51   2.20  118256840\n 3  1962       1     1        0              0     0   2.39   2.13  108848768\n 4  1963       1     1        0              0     0   2.74   2.43  160962528\n 5  1964       1     1        0              0     0   2.65   2.35  173738208\n 6  1965       1     1        0              0     0   2.59   2.29  177229600\n 7  1966       1     1        0              0     0   2.52   2.31  211282496\n 8  1967       1     1        0              0     0   3.27   2.88  212639760\n 9  1968       1     1        0              0     0   3.91   3.45  209852864\n10  1969       1     1        0              0     0   3.67   3.28  239392080\n# … with 1,186 more rows, and 43 more variables: ra_cst <dbl>, ra_s <dbl>,\n#   rs_cst <dbl>, rtotal_cst <dbl>, profit_cst <dbl>, nb_journ <dbl>,\n#   qs_s <dbl>, qtotal <dbl>, pages <dbl>, ads_q <dbl>, ads_s <dbl>,\n#   news_hole <dbl>, share_Hard <dbl>, ads_p4_cst <dbl>,\n#   R_sh_edu_primaire_ipo <dbl>, R_sh_edu_secondaire_ipo <dbl>,\n#   R_sh_edu_no_ipo <dbl>, R_sh_pcs_agri_ipo <dbl>, R_sh_pcs_patron_ipo <dbl>,\n#   R_sh_pcs_cadre_ipo <dbl>, R_sh_pcs_employes_ipo <dbl>, …\n\n\nThere are 1,196 observations in the dataset and 52 variables. Angelucci and Cagé (2019) are interested in the 1960-1974 time-period which has around 100 newspapers. There are 14 national newspapers at the beginning of the period and 12 at the end. The key period is 1967, when the French government announced it would allow advertising on television. Angelucci and Cagé (2019) argue that national newspapers were affected by this chance, but local newspapers were not. So, the national newspapers are the treatment group and the local newspapers are the control group.\nWe focus just on the headline difference in differences result and construct summary statistics (Table 15.1).\n\nnewspapers <- \n  newspapers |> \n  select(year, id_news, after_national, local, national, # Diff in diff variables\n         ra_cst, ads_p4_cst, ads_s, # Advertising side dependents\n         ps_cst, po_cst, qtotal, qs_s, rs_cst) |> # Reader side dependents\n  mutate(ra_cst_div_qtotal = ra_cst / qtotal) |> # An advertising side dependent needs to be built\n  mutate(across(c(id_news, after_national, local, national), as.factor)) |> \n  mutate(year = as.integer(year))\n\nnewspapers\n\n# A tibble: 1,196 × 14\n    year id_news after_national local national    ra_cst ads_p4_cst ads_s ps_cst\n   <int> <fct>   <fct>          <fct> <fct>        <dbl>      <dbl> <dbl>  <dbl>\n 1  1960 1       0              1     0         52890272       NA    30.6   2.29\n 2  1961 1       0              1     0         56601060       NA    38.4   2.20\n 3  1962 1       0              1     0         64840752       24.0  31.6   2.13\n 4  1963 1       0              1     0         70582944       50.3  27.2   2.43\n 5  1964 1       0              1     0         74977888       48.6  31.1   2.35\n 6  1965 1       0              1     0         74438248       47.5  47.8   2.29\n 7  1966 1       0              1     0         81383000       46.2  29.7   2.31\n 8  1967 1       0              1     0         80263152       87.9  49.4   2.88\n 9  1968 1       0              1     0         87165704       84.1  26.9   3.45\n10  1969 1       0              1     0        102596384       79.0  31.7   3.28\n# … with 1,186 more rows, and 5 more variables: po_cst <dbl>, qtotal <dbl>,\n#   qs_s <dbl>, rs_cst <dbl>, ra_cst_div_qtotal <dbl>\n\n\n\nlibrary(modelsummary)\n\ndatasummary_skim(newspapers)\n\n\nTable 15.1:  Summary statistics for French newspapers dataset (1960-1974) \n \n  \n      \n    Unique (#) \n    Missing (%) \n    Mean \n    SD \n    Min \n    Median \n    Max \n       \n  \n \n\n  \n    year \n    15 \n    0 \n    1967.0 \n    4.3 \n    1960.0 \n    1967.0 \n    1974.0 \n     \n\n\n  \n  \n    ra_cst \n    1053 \n    12 \n    91531796.9 \n    137207312.4 \n    549717.2 \n    35994710.0 \n    864369088.0 \n     \n\n\n  \n  \n    ads_p4_cst \n    558 \n    32 \n    86.4 \n    75.3 \n    3.8 \n    69.0 \n    327.2 \n     \n\n\n  \n  \n    ads_s \n    988 \n    13 \n    18.7 \n    9.7 \n    1.6 \n    16.9 \n    59.6 \n     \n\n\n  \n  \n    ps_cst \n    665 \n    13 \n    2.8 \n    0.7 \n    0.7 \n    2.8 \n    5.6 \n     \n\n\n  \n  \n    po_cst \n    146 \n    11 \n    3.2 \n    0.9 \n    0.8 \n    3.3 \n    9.3 \n     \n\n\n  \n  \n    qtotal \n    1052 \n    11 \n    130817.5 \n    172954.3 \n    1480.0 \n    56775.2 \n    1143676.0 \n     \n\n\n  \n  \n    qs_s \n    914 \n    10 \n    27.2 \n    22.7 \n    0.7 \n    22.5 \n    100.1 \n     \n\n\n  \n  \n    rs_cst \n    1047 \n    13 \n    97666503.6 \n    125257120.3 \n    255760.1 \n    40736368.0 \n    750715008.0 \n     \n\n\n  \n  \n    ra_cst_div_qtotal \n    1049 \n    12 \n    661.2 \n    352.7 \n    61.3 \n    596.6 \n    3048.4 \n     \n\n\n  \n\n\n\n\n\n\nWe are interested in what happened from 1967 onward, especially in terms of advertising revenue, and whether that was different for national, compared with local newspapers (Figure 15.9). We use scales to adjust the y axis (Wickham and Seidel 2020).\n\nlibrary(scales)\nnewspapers |> \n  mutate(type = if_else(local == 1, \"Local\", \"National\")) |> \n  ggplot(aes(x = year, y = ra_cst)) +\n  geom_point(alpha = 0.5) +\n  scale_y_continuous(labels = dollar_format(prefix=\"$\", suffix = \"M\", scale = 0.000001)) +\n  labs(x = \"Year\",\n       y = \"Advertising revenue\") +\n  facet_wrap(vars(type),\n               nrow = 2) +\n  theme_minimal() +\n  geom_vline(xintercept = 1966.5, linetype = \"dashed\")\n\n\n\n\nFigure 15.9: Revenue of French newspapers (1960-1974), by whether they were local or national\n\n\n\n\nThe model that we are interested in estimating is: \\[\\mbox{ln}(y_{n,t}) = \\beta_0 + \\beta_1(\\mbox{National binary}\\times\\mbox{1967 onward binary}) + \\lambda_n + \\gamma_y + \\epsilon.\\] It is the \\(\\beta_1\\) coefficient that we are especially interested in. We use \\(\\lambda_n\\) as fixed effect for each newspaper, and the \\(\\gamma_y\\) as a fixed effect for each year. We estimate the models using lm().\n\n# Advertising side\nad_revenue <-\n  lm(log(ra_cst) ~ after_national + id_news + year, data = newspapers)\nad_revenue_div_circulation <-\n  lm(log(ra_cst_div_qtotal) ~ after_national + id_news + year, data = newspapers)\nad_price <-\n  lm(log(ads_p4_cst) ~ after_national + id_news + year, data = newspapers)\nad_space <-\n  lm(log(ads_s) ~ after_national + id_news + year, data = newspapers)\n\n# Consumer side\nsubscription_price <-\n  lm(log(ps_cst) ~ after_national + id_news + year, data = newspapers)\nunit_price <-\n  lm(log(po_cst) ~ after_national + id_news + year, data = newspapers)\ncirculation <-\n  lm(log(qtotal) ~ after_national + id_news + year, data = newspapers)\nshare_of_sub <-\n  lm(log(qs_s) ~ after_national + id_news + year, data = newspapers)\nrevenue_from_sales <-\n  lm(log(rs_cst) ~ after_national + id_news + year, data = newspapers)\n\nLooking at the advertising-side variables (Table 15.2) we find consistently negative coefficients for everything apart from advertising space.\n\nselected_variables <- \n  c(\"year\" = \"Year\",\n  \"after_national1\" = \"Is after advertising change\")\n\nadvertising_models <- list(\n  \"Ad revenue\" = ad_revenue,\n  \"Ad revenue over circulation\" = ad_revenue_div_circulation,\n  \"Ad prices\" = ad_price,\n  \"Ad space\" = ad_space\n)\n\nmodelsummary(\n  advertising_models,\n  fmt = 2,\n  coef_map = selected_variables\n)\n\n\nTable 15.2:  Effect of changed television advertising laws on revenue of French newspapers (1960-1974) \n \n  \n      \n    Ad revenue \n    Ad revenue over circulation \n    Ad prices \n    Ad space \n  \n \n\n  \n    Year \n    0.05 \n    0.04 \n    0.04 \n    0.02 \n  \n  \n     \n    (0.00) \n    (0.00) \n    (0.00) \n    (0.00) \n  \n  \n    Is after advertising change \n    −0.23 \n    −0.15 \n    −0.31 \n    0.01 \n  \n  \n     \n    (0.03) \n    (0.03) \n    (0.07) \n    (0.05) \n  \n  \n    Num.Obs. \n    1052 \n    1048 \n    809 \n    1046 \n  \n  \n    R2 \n    0.985 \n    0.903 \n    0.892 \n    0.720 \n  \n  \n    R2 Adj. \n    0.984 \n    0.895 \n    0.882 \n    0.699 \n  \n  \n    AIC \n    −526.7 \n    −735.0 \n    705.4 \n    478.0 \n  \n  \n    BIC \n    −120.1 \n    −328.8 \n    1057.6 \n    849.5 \n  \n  \n    Log.Lik. \n    345.341 \n    449.524 \n    −277.714 \n    −164.012 \n  \n  \n    F \n    814.664 \n    112.259 \n    83.464 \n    34.285 \n  \n\n\n\n\n\n\nAnd looking at the advertising-side variables (Table 15.3) we again, find consistently negative coefficients for everything apart from the share of subscriptions and unit price.\n\nconsumer_models <- list(\n  \"Subscription price\" = subscription_price,\n  \"Circulation\" = circulation,\n  \"Share of subscriptions\" = share_of_sub,\n  \"Revenue from sales\" = revenue_from_sales,\n  \"Unit price\" = unit_price\n)\n\nmodelsummary(\n  consumer_models,\n  fmt = 2,\n  coef_map = selected_variables\n)\n\n\nTable 15.3:  Effect of changed television advertising laws on consumers of French newspapers (1960-1974) \n \n  \n      \n    Subscription price \n    Circulation \n    Share of subscriptions \n    Revenue from sales \n    Unit price \n  \n \n\n  \n    Year \n    0.05 \n    0.01 \n    −0.01 \n    0.05 \n    0.05 \n  \n  \n     \n    (0.00) \n    (0.00) \n    (0.00) \n    (0.00) \n    (0.00) \n  \n  \n    Is after advertising change \n    −0.04 \n    −0.06 \n    0.19 \n    −0.06 \n    0.06 \n  \n  \n     \n    (0.02) \n    (0.02) \n    (0.03) \n    (0.03) \n    (0.02) \n  \n  \n    Num.Obs. \n    1044 \n    1070 \n    1072 \n    1046 \n    1063 \n  \n  \n    R2 \n    0.876 \n    0.991 \n    0.972 \n    0.988 \n    0.867 \n  \n  \n    R2 Adj. \n    0.865 \n    0.990 \n    0.970 \n    0.987 \n    0.856 \n  \n  \n    AIC \n    −1600.3 \n    −1355.1 \n    −477.8 \n    −738.2 \n    −1650.6 \n  \n  \n    BIC \n    −1194.3 \n    −947.2 \n    −64.7 \n    −332.1 \n    −1243.1 \n  \n  \n    Log.Lik. \n    882.140 \n    759.573 \n    321.907 \n    451.112 \n    907.285 \n  \n  \n    F \n    84.659 \n    1392.863 \n    421.297 \n    1030.303 \n    79.888 \n  \n\n\n\n\n\n\nIn general, we are able to replicate the main results of Angelucci and Cagé (2019) and find that in many cases there appears to be a difference from 1967 onward. Our results are similar to Angelucci and Cagé (2019)."
  },
  {
    "objectID": "15-causality_from_obs.html#propensity-score-matching",
    "href": "15-causality_from_obs.html#propensity-score-matching",
    "title": "15  Causality from observational data",
    "section": "15.5 Propensity score matching",
    "text": "15.5 Propensity score matching\nDifference in differences is a powerful analysis framework. But it can be tough to identify appropriate treatment and control groups. Alexander and Ward (2018) compare migrant brothers, where one brother had most of their education in a different country, and the other brother had most of their education in the US. Given the data that are available, this match provides a reasonable treatment and control group. But other matches could have given different results, for instance friends or cousins.\nWe can match based on observable variables. For instance, age-group or education. At two different times we compare smoking rates in 18-year-olds in one city with smoking rates in 18-year-olds in another city. This would be a coarse match because we know that there are many differences between 18-year-olds, even in terms of the variables that we commonly observe, say gender and education. One way to deal with this would be to create sub-groups: 18-year-old males with a high school education, etc. But then the sample sizes quickly become small. We also have the issue of how to deal with continuous variables. And, are an 18-year-old and a 19-year-old really so different? Why not also compare with them?\nOne way to proceed is to consider a nearest neighbor approach. But there can be limited concern for uncertainty with this approach. There can also be an issue with having many variables because we end up with a high-dimension graph. This leads to propensity score matching. Here we will explain the process of propensity score matching, but it is not something that should be widely used (King and Nielsen 2019), and we will then go through why that is the case.\nPropensity score matching involves assigning some probability to each observation. We construct that probability based on the observation’s values for the independent variables, at their values before the treatment. That probability is our best guess at the probability of the observation being treated, regardless of whether it was treated or not. For instance, if 18-year-old males were treated but 19-year-old males were not, then as there is not much difference between 18-year-old males and 19-year-old males our assigned probability would be similar. We can then compare the outcomes of observations with similar propensity scores.\nOne advantage of propensity score matching is that is allows us to easily consider many independent variables at once, and it can be constructed using logistic regression.\nTo be more specific we can simulate some data. We will pretend that we work for a large online retailer. We are going to treat some individuals with free shipping to see what happens to their average purchase.\n\nset.seed(853)\n\nsample_size <- 10000\n\npurchase_data <-\n  tibble(\n    unique_person_id = c(1:sample_size),\n    age = runif(n = sample_size,\n                min = 18,\n                max = 100),\n    city = sample(\n      x = c(\"Toronto\", \"Montreal\", \"Calgary\"),\n      size = sample_size,\n      replace = TRUE\n    ),\n    gender = sample(\n      x = c(\"Female\", \"Male\", \"Other/decline\"),\n      size = sample_size,\n      replace = TRUE,\n      prob = c(0.49, 0.47, 0.02)\n    ),\n    income = rlnorm(n = sample_size,\n                    meanlog = 0.5,\n                    sdlog = 1)\n  )\n\nThen we need to add some probability of being treated with free shipping. We will say that it depends on our variables and that younger, higher-income, male and Toronto-based individuals make this treatment slightly more likely.\n\npurchase_data <-\n  purchase_data |>\n  mutate(\n    age_num = case_when(age < 30 ~ 3,\n                        age < 50 ~ 2,\n                        age < 70 ~ 1,\n                        TRUE ~ 0),\n    city_num = case_when(\n      city == \"Toronto\" ~ 3,\n      city == \"Montreal\" ~ 2,\n      city == \"Calgary\" ~ 1,\n      TRUE ~ 0\n    ),\n    gender_num = case_when(\n      gender == \"Male\" ~ 3,\n      gender == \"Female\" ~ 2,\n      gender == \"Other/decline\" ~ 1,\n      TRUE ~ 0\n    ),\n    income_num = case_when(income > 3 ~ 3,\n                           income > 2 ~ 2,\n                           income > 1 ~ 1,\n                           TRUE ~ 0)\n  ) |>\n  rowwise() |>\n  mutate(\n    sum_num = sum(age_num, city_num, gender_num, income_num),\n    softmax_prob = exp(sum_num) / exp(12),\n    free_shipping = sample(\n      x = c(0:1),\n      size = 1,\n      replace = TRUE,\n      prob = c(1 - softmax_prob, softmax_prob)\n    )\n  ) |>\n  ungroup()\n\npurchase_data <-\n  purchase_data |>\n  select(-age_num,-city_num,-gender_num,-income_num,-sum_num,-softmax_prob)\n\nFinally, we need to have some measure of a person’s average spend. We want those with free shipping to be slightly higher than those without.\n\npurchase_data <-\n  purchase_data |>\n  mutate(mean_spend = if_else(free_shipping == 1, 60, 50)) |>\n  rowwise() |>\n  mutate(average_spend = rnorm(1, mean_spend, sd = 5)) |>\n  ungroup() |>\n  select(-mean_spend) |>\n  mutate(across(c(city, gender, free_shipping), as.factor))\n\npurchase_data\n\n# A tibble: 10,000 × 7\n   unique_person_id   age city     gender income free_shipping average_spend\n              <int> <dbl> <fct>    <fct>   <dbl> <fct>                 <dbl>\n 1                1  47.5 Calgary  Female  1.72  0                      41.1\n 2                2  27.8 Montreal Male    1.54  0                      55.7\n 3                3  57.7 Toronto  Female  3.16  0                      56.5\n 4                4  43.9 Toronto  Male    0.636 0                      50.5\n 5                5  21.1 Toronto  Female  1.43  0                      44.7\n 6                6  51.1 Calgary  Male    1.18  0                      48.8\n 7                7  28.7 Toronto  Female  1.49  0                      52.8\n 8                8  37.9 Toronto  Female  0.414 0                      52.4\n 9                9  31.0 Calgary  Male    0.384 0                      47.6\n10               10  33.5 Montreal Female  1.11  0                      49.2\n# … with 9,990 more rows\n\n\nWe use matchit() from MatchIt (Ho et al. 2011) to implement logistic regression and create matched groups. We then use match.data() to get the data of matches containing both all 371 people who were actually treated with free shipping and the untreated person who is considered as similar to them, based on propensity score, as possible. The result is a dataset of 742 observations.\n\nlibrary(MatchIt)\n\nmatched_groups <- matchit(free_shipping ~ age + city + gender + income, \n                  data = purchase_data,\n                  method = \"nearest\", distance = \"glm\")\n\nmatched_groups\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 10000 (original), 742 (matched)\n - target estimand: ATT\n - covariates: age, city, gender, income\n\nmatched_dataset <- match.data(matched_groups)\n\nmatched_dataset\n\n# A tibble: 742 × 10\n   unique_person_id   age city     gender income free_shipping average_spend\n              <int> <dbl> <fct>    <fct>   <dbl> <fct>                 <dbl>\n 1                5  21.1 Toronto  Female  1.43  0                      44.7\n 2               20  30.0 Montreal Male    8.65  0                      49.0\n 3               22  22.8 Toronto  Male    0.898 0                      50.1\n 4               38  41.3 Toronto  Female  6.01  1                      61.5\n 5               43  24.7 Toronto  Male    1.59  1                      59.6\n 6               76  56.4 Toronto  Male   15.0   0                      51.8\n 7              102  48.1 Toronto  Male    3.48  1                      59.8\n 8              105  76.7 Toronto  Male    2.84  0                      45.1\n 9              118  26.7 Toronto  Female  0.315 0                      56.4\n10              143  36.3 Toronto  Male   10.6   0                      49.4\n# … with 732 more rows, and 3 more variables: distance <dbl>, weights <dbl>,\n#   subclass <fct>\n\n\nFinally, we can estimate the effect of being treated on average spend using linear regression. We are particularly interested in the coefficient associated with the treatment variable, in this case free shipping.\n\npropensity_score_regression <- lm(average_spend ~ age + city + gender + income + free_shipping, \n                                  data = matched_dataset)\n\npropensity_score_regression\n\n\nCall:\nlm(formula = average_spend ~ age + city + gender + income + free_shipping, \n    data = matched_dataset)\n\nCoefficients:\n        (Intercept)                  age         cityMontreal  \n           49.56747              0.00735              0.12787  \n        cityToronto           genderMale  genderOther/decline  \n            0.58628             -1.09978             -1.99861  \n             income       free_shipping1  \n            0.01903             10.60550  \n\n\nWe cover propensity score matching because it is widely used. But there are many issues with propensity score matching that mean that propensity scores should not be used for matching (King and Nielsen 2019). These include:\n\nMatching. Propensity score matching cannot match on unobserved variables. This may be fine in a classroom setting, but in more realistic settings it will likely cause issues.\nModelling. The results tend to be specific to the model that is used.\nStatistically. We are using the data twice."
  },
  {
    "objectID": "15-causality_from_obs.html#regression-discontinuity-design",
    "href": "15-causality_from_obs.html#regression-discontinuity-design",
    "title": "15  Causality from observational data",
    "section": "15.6 Regression discontinuity design",
    "text": "15.6 Regression discontinuity design\n\n15.6.1 Overview\nRegression discontinuity design (RDD) was established by Thistlethwaite and Campbell (1960) and is a popular way to get causality when there is a continuous variable with cut-offs that determine treatment. Is there a difference between a student who gets 79 per cent and a student who gets 80 per cent? Probably not much, but one may get an A-, while the other may get a B+, and seeing that on a transcript could affect who gets a job which could affect income. In this case the percentage is a ‘forcing variable’ and the cut-off for an A- is a ‘threshold’. As the treatment is determined by the forcing variable we need to control for that variable. And, these seemingly arbitrary cut-offs can be seen all the time. Hence, there has been a great deal of work using RDD.\nThere is sometimes slightly different terminology used when it comes to RDD. For instance, Cunningham (2021) refers to the forcing function as a running variable. The exact terminology that is used does not matter provided we use it consistently.\n\n\n15.6.2 Simulated example\nTo be more specific about the situation, we simulate data. We will consider the relationship between income and grades, and simulate there to be a change if a student gets at least 80 (Figure 15.10).\n\nlibrary(tidyverse)\n\nset.seed(853)\n\nnumber_of_observation <- 1000\n\nrdd_example_data <- tibble(\n  person = c(1:number_of_observation),\n  mark = runif(number_of_observation, min = 78, max = 82),\n  income = rnorm(number_of_observation, 10, 1)\n)\n\n## Make income more likely to be higher if they have a mark at least 80\nrdd_example_data <-\n  rdd_example_data |>\n  mutate(\n    noise = rnorm(n = number_of_observation, mean = 2, sd = 1),\n    income = if_else(mark >= 80, income + noise, income)\n  )\n\nrdd_example_data\n\n# A tibble: 1,000 × 4\n   person  mark income noise\n    <int> <dbl>  <dbl> <dbl>\n 1      1  79.4   9.43 1.87 \n 2      2  78.5   9.69 2.26 \n 3      3  79.9  10.8  1.14 \n 4      4  79.3   9.34 2.50 \n 5      5  78.1  10.7  2.21 \n 6      6  79.6   9.83 2.47 \n 7      7  78.5   8.96 4.22 \n 8      8  79.0  10.5  3.11 \n 9      9  78.6   9.53 0.671\n10     10  78.8  10.6  2.46 \n# … with 990 more rows\n\n\n\nrdd_example_data |> \n  ggplot(aes(x = mark,\n             y = income)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(data = rdd_example_data |> filter(mark < 80), \n              method='lm',\n              color = \"black\",\n              formula = 'y ~ x') +\n  geom_smooth(data = rdd_example_data |> filter(mark >= 80), \n              method='lm',\n              color = \"black\",\n              formula = 'y ~ x') +\n  theme_minimal() +\n  labs(x = \"Mark\",\n       y = \"Income ($)\")\n\n\n\n\nFigure 15.10: Illustration of simulated data that shows an effect on income from getting a mark that is 80, compared with 79\n\n\n\n\nWe can use a binary variable with linear regression to estimate the effect. We expect the coefficient to be around two, which is what we simulated.\n\nrdd_example_data <- \n  rdd_example_data |> \n  mutate(mark_80_and_over = if_else(mark < 80, 0, 1)) \n\nlm(income ~ mark + mark_80_and_over, data = rdd_example_data) |> \n  summary()\n\n\nCall:\nlm(formula = income ~ mark + mark_80_and_over, data = rdd_example_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3418 -0.8218 -0.0043  0.7740  6.1209 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       5.30130    5.16331   1.027    0.305    \nmark              0.06025    0.06535   0.922    0.357    \nmark_80_and_over  1.89221    0.14921  12.682   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.189 on 997 degrees of freedom\nMultiple R-squared:  0.4178,    Adjusted R-squared:  0.4166 \nF-statistic: 357.7 on 2 and 997 DF,  p-value: < 2.2e-16\n\n\nThere are various caveats to this estimate that we will discuss, but the essentials of RDD are here. Given an appropriate set-up, and model, an RDD can compare favorably to randomized trials (Bloom, Bell, and Reiman 2020).\nWe could also implement RDD using rdrobust (Calonico et al. 2021). The advantage of this approach is that many extensions are easily available.\n\nlibrary(rdrobust)\nrdrobust(y = rdd_example_data$income, \n         x = rdd_example_data$mark, \n         c = 80, h = 2, all = TRUE) |> \n  summary()\n\nCall: rdrobust\n\nNumber of Obs.                 1000\nBW type                      Manual\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  497          503\nEff. Number of Obs.             497          503\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   2.000        2.000\nBW bias (b)                   2.000        2.000\nrho (h/b)                     1.000        1.000\nUnique Obs.                     497          503\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     1.913     0.161    11.876     0.000     [1.597 , 2.229]     \nBias-Corrected     1.966     0.161    12.207     0.000     [1.650 , 2.282]     \n        Robust     1.966     0.232     8.461     0.000     [1.511 , 2.422]     \n=============================================================================\n\n\n\n\n15.6.3 Assumptions\nThe key assumptions of RDD are (Cunningham 2021, 163):\n\nThe cut-off is specific, fixed, and known to all.\nThe forcing function is continuous.\n\nThe first assumption is largely about being unable to manipulate the cut-off, and ensures that the cut-off has meaning. The second assumption enables us to be confident that folks on either side of the threshold are similar, apart from just happening to just fall on either side of the threshold.\nWhen we discussed randomized control trials and A/B testing in Chapter @ref(hunt-data) the randomized assignment of the treatment meant that the control and treatment groups were the same, but for the treatment. Then we moved to difference in differences, and we assumed that there was a common trend between the treated and control groups. We allowed that the groups could be different, but that we could ‘difference out’ their differences. Finally, we considered matching, and we said that even if we the control and treatment groups seemed quite different, we were able to match, to some extent, those who were treated with a group that were similar to them in all ways, apart from the fact that they were not treated.\nIn regression discontinuity we consider a slightly different setting. The two groups are completely different in terms of the forcing variable. They are on either side of the threshold. So there is no overlap at all. But we know the threshold and believe that those on either side are essentially matched. Let us consider the 2019 NBA Eastern Conference Semifinals - Toronto and the Philadelphia. Game 1: Raptors win 108-95; Game 2: 76ers win 94-89; Game 3: 76ers win 116-95; Game 4: Raptors win 101-96; Game 5: Raptors win 125-89; Game 6: 76ers win 112-101; and finally, Game 7: Raptors win 92-90, because of a ball that win in after bouncing on the rim four times. Was there really that much difference between the teams?\nThe continuity assumption is important, but we cannot test this as it is based on a counterfactual. Instead, we need to convince people of it. Ways to do this include:\n\nUsing a test/train set-up.\nTrying different specifications. We are especially concerned if results do not broadly persist with just linear or quadratic functions.\nConsidering different subsets of the data.\nConsidering different windows.\nBe clear about uncertainty intervals, especially in graphs.\nDiscussing and assuage concerns about the possibility of omitted variables.\n\nThe threshold is also important. For instance, is there an actual shift or is there a non-linear relationship?\nThere are a variety of weaknesses of RDD including:\n\nExternal validity may be difficult. For instance, when we think about the A-/B+ example, it is hard to see those generalizing to also B-/C+ students.\nThe important responses are those that are close to the cut-off. This means that even if we have many A and B students, they do not help much. Hence, we need a lot of data or we may have concerns about our ability to support our claims (Green et al. 2009).\nAs the researcher, we have a lot of freedom to implement different options. This means that open science best practice becomes vital.\n\nTo this point we have considered ‘sharp’ RDD. That is, the threshold is strict. But, in reality, often the boundary is a little less strict. For instance, consider the drinking age. There is a legal drinking age, say 18. If we looked at the number of people who had drunk, then it is likely to increase in the few years leading up to that age.\nIn a sharp RDD setting, if we know the value of the forcing function then we know the outcome. For instance, if a student gets a mark of 80 then we know that they got an A-, but if they got a mark of 79 then we know that they got a B+. But with fuzzy RDD it is only known with some probability. We can say that a Canadian 19-year-old is more likely to have drunk alcohol than a Canadian 18-year-old, but the number of Canadian 18-year-olds who have drunk alcohol is not zero.\nIt may be possible to deal with fuzzy RDD settings with appropriate choice of model or data. It may also be possible to deal with them using instrumental variables.\nWe want as ‘sharp’ an effect as possible, but if the thresholds are known, then they will be gamed. For instance, there is a lot of evidence that people run for certain marathon times, and we know that people aim for certain grades. Similarly, from the other side, it is a lot easier for an instructor to just give out As than it is to have to justify Bs. One way to look at this is to consider how ‘balanced’ the sample is on either side of the threshold. We can do this using histograms with appropriate bins. For instance, think of the age-heaping that we found in the cleaned Kenyan census data in Chapter @ref(gather-data)\nAnother key factor for RDD is the possible effect of the decision around the choice of model. To see this, consider the difference between a linear and polynomial.\n\nsome_data <- \n  tibble(outcome = rnorm(n = 100, mean = 1, sd = 1),\n         running_variable = c(1:100),\n         location = \"before\")\n\nsome_more_data <- \n  tibble(outcome = rnorm(n = 100, mean = 2, sd = 1),\n         running_variable = c(101:200),\n         location = \"after\")\n\nboth <- \n  rbind(some_data, some_more_data)\n\nboth |> \n  ggplot(aes(x = running_variable, y = outcome, color = location)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(formula = y~x, method = 'lm')\n\n\n\nboth |> \n  ggplot(aes(x = running_variable, y = outcome, color = location)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(formula = y ~ poly(x, 3), method = 'lm')\n\n\n\n\nThe result is that our estimate is dependent on the choice of model. We see this issue occur often in RDD (Gelman 2019)."
  },
  {
    "objectID": "15-causality_from_obs.html#instrumental-variables",
    "href": "15-causality_from_obs.html#instrumental-variables",
    "title": "15  Causality from observational data",
    "section": "15.7 Instrumental variables",
    "text": "15.7 Instrumental variables\n\n15.7.1 Overview\nInstrumental variables (IV) is an approach that can be handy when we have some type of treatment and control going on, but we have a lot of correlation with other variables and we possibly do not have a variable that actually measures what we are interested in. So adjusting for observables will not be enough to create a good estimate. Instead we find some variable—the eponymous instrumental variable—that is:\n\ncorrelated with the treatment variable, but\nnot correlated with the outcome.\n\nThis solves our problem because the only way the instrumental variable can have an effect is through the treatment variable, and so we can adjust our understanding of the effect of the treatment variable appropriately. The trade-off is that instrumental variables must satisfy a bunch of different assumptions, and that, frankly, they are difficult to identify ex ante. Nonetheless, when we are able to use them, they are a powerful tool for speaking about causality.\nThe canonical instrumental variables example is smoking. These days we know that smoking causes cancer. But because smoking is correlated with a lot of other variables, for instance, education, it could be that it was actually education that causes cancer. RCTs may be possible, but they are likely to be troublesome in terms of speed and ethics, and so instead we look for some other variable that is correlated with smoking, but not, in and of itself, with lung cancer. In this case, we look to tax rates, and other policy responses, on cigarettes. As the tax rates on cigarettes are correlated with the number of cigarettes that are smoked, but not correlated with lung cancer, other than through their impact on cigarette smoking, through them we can assess the effect of cigarettes smoked on lung cancer.\nTo implement instrumental variables we first regress tax rates on cigarette smoking to get some coefficient on the instrumental variable, and then (in a separate regression) regress tax rates on lung cancer to again, get some coefficient on the instrumental variable. Our estimate is then the ratio of these coefficients, which is described as a ‘Wald estimate’ (Gelman and Hill 2007, 219).\nFollowing the language of (Gelman and Hill 2007, 216) when we use instrumental variables we make a variety of assumptions including:\n\nIgnorability of the instrument.\nCorrelation between the instrumental variable and the treatment variable.\nMonotonicity.\nExclusion restriction.\n\n\n\n\n\nThe history of instrumental variables is intriguing, and Stock and Trebbi (2003) provide a brief overview. The method was first published in Wright (1928). This is a book about the effect of tariffs on animal and vegetable oil. So why might instrumental variables be important in a book about tariffs on animal and vegetable oil? The fundamental problem is that the effect of tariffs depends on both supply and demand. But we only know prices and quantities, so we do not know what is driving the effect. We can use instrumental variables to pin down causality. The intriguing aspect is that the instrumental variables discussion is only in Appendix B. It would seem odd to relegate a major statistical break-through to an appendix. Further, Philip G. Wright, the book’s author, had a son Sewall Wright, who had considerable expertise in statistics and the specific method used in Appendix B. Hence the mystery of Appendix B: did Philip or Sewall write it? Both Cunningham (2021) and Stock and Trebbi (2003) go into more detail, but on balance feel that it is likely that Philip did actually author the work.\n\n\n15.7.2 Simulated example\nLet us generate some data. We will explore a simulation related to the canonical example of health status, smoking, and tax rates. So we are looking to explain how healthy someone is based on the amount they smoke, via the tax rate on smoking. We are going to generate different tax rates by provinces. My understanding is that the tax rate on cigarettes is now pretty much the same in each of the provinces, but that this is fairly recent. So we will pretend that Alberta had a low tax, and Nova Scotia had a high tax.\n\nAs a reminder, we are simulating data for illustrative purposes, so we need to impose the answer that we want. When you actually use instrumental variables you will be reversing the process.\n\nlibrary(broom)\nlibrary(tidyverse)\n\nset.seed(853)\n\nnumber_of_observation <- 10000\n\niv_example_data <- tibble(person = c(1:number_of_observation),\n                          smoker = sample(x = c(0:1),\n                                          size = number_of_observation, \n                                          replace = TRUE)\n                          )\n\nNow we need to relate the number of cigarettes that someone smoked to their health. We will model health status as a draw from the normal distribution, with either a high or low mean depending on whether the person smokes.\n\niv_example_data <- \n  iv_example_data |> \n  mutate(health = if_else(smoker == 0,\n                          rnorm(n = n(), mean = 1, sd = 1),\n                          rnorm(n = n(), mean = 0, sd = 1)\n                          )\n         )\n## So health will be one standard deviation higher for people who do not or barely smoke.\n\nNow we need a relationship between cigarettes and the province (because in this illustration, the provinces have different tax rates).\n\niv_example_data <- \n  iv_example_data |> \n  rowwise() |> \n  mutate(province = \n           case_when(smoker == 0 ~ sample(x = c(\"Nova Scotia\", \"Alberta\"),\n                                          size = 1, \n                                          replace = FALSE, \n                                          prob = c(1/2, 1/2)),\n                     smoker == 1 ~ sample(x = c(\"Nova Scotia\", \"Alberta\"),\n                                          size = 1, \n                                          replace = FALSE, \n                                          prob = c(1/4, 3/4)))) |> \n  ungroup()\n\niv_example_data <- \n  iv_example_data |> \n  mutate(tax = case_when(province == \"Alberta\" ~ 0.3,\n                         province == \"Nova Scotia\" ~ 0.5,\n                         TRUE ~ 9999999\n  )\n  )\n\niv_example_data$tax |> table()\n\n\n 0.3  0.5 \n6206 3794 \n\nhead(iv_example_data)\n\n# A tibble: 6 × 5\n  person smoker  health province      tax\n   <int>  <int>   <dbl> <chr>       <dbl>\n1      1      0  1.11   Alberta       0.3\n2      2      1 -0.0831 Alberta       0.3\n3      3      1 -0.0363 Alberta       0.3\n4      4      0  2.48   Alberta       0.3\n5      5      0  0.617  Alberta       0.3\n6      6      0  0.748  Nova Scotia   0.5\n\n\nNow we can look at our data.\n\niv_example_data |> \n  mutate(smoker = as_factor(smoker)) |> \n  ggplot(aes(x = health, fill = smoker)) +\n  geom_histogram(position = \"dodge\", binwidth = 0.2) +\n  theme_minimal() +\n  labs(x = \"Health rating\",\n       y = \"Number of people\",\n       fill = \"Smoker\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(vars(province))\n\n\n\n\nFinally, we can use the tax rate as an instrumental variable to estimate the effect of smoking on health.\n\nhealth_on_tax <- lm(health ~ tax, data = iv_example_data)\nsmoker_on_tax <- lm(smoker ~ tax, data = iv_example_data)\n\ncoef(health_on_tax)[\"tax\"] / coef(smoker_on_tax)[\"tax\"]\n\n       tax \n-0.8554502 \n\n\nSo we find, luckily, that if you smoke then your health is likely to be worse than if you do not smoke.\nEquivalently, we can think of instrumental variables in a two-stage regression context.\n\nfirst_stage <- lm(smoker ~ tax, data = iv_example_data)\nhealth_hat <- first_stage$fitted.values\nsecond_stage <- lm(health ~ health_hat, data = iv_example_data)\n\nsummary(second_stage)\n\n\nCall:\nlm(formula = health ~ health_hat, data = iv_example_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9867 -0.7600  0.0068  0.7709  4.3293 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.91632    0.04479   20.46   <2e-16 ***\nhealth_hat  -0.85545    0.08911   -9.60   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.112 on 9998 degrees of freedom\nMultiple R-squared:  0.009134,  Adjusted R-squared:  0.009034 \nF-statistic: 92.16 on 1 and 9998 DF,  p-value: < 2.2e-16\n\n\nWe can use iv_robust() from estimatr (Blair et al. 2021) to estimate IV. One nice reason for doing this is that it can help to keep everything organised and adjust the standard errors.\n\nlibrary(estimatr)\niv_robust(health ~ smoker | tax, data = iv_example_data) |> \n  summary()\n\n\nCall:\niv_robust(formula = health ~ smoker | tax, data = iv_example_data)\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value   Pr(>|t|) CI Lower CI Upper   DF\n(Intercept)   0.9163    0.04057   22.59 3.163e-110   0.8368   0.9958 9998\nsmoker       -0.8555    0.08047  -10.63  2.981e-26  -1.0132  -0.6977 9998\n\nMultiple R-squared:  0.1971 ,   Adjusted R-squared:  0.197 \nF-statistic:   113 on 1 and 9998 DF,  p-value: < 2.2e-16\n\n\n\n\n15.7.3 Assumptions\nAs discussed earlier, there are a variety of assumptions that are made when using instrumental variables. The two most important are:\n\nExclusion Restriction. This assumption is that the instrumental variable only affects the dependent variable through the independent variable of interest.\nRelevance. There must actually be a relationship between the instrumental variable and the independent variable.\n\nThere is typically a trade-off between these two. There are plenty of variables that satisfy one, precisely because they do not satisfy the other. Cunningham (2021, 211) describes how one test of a good instrument is if people are initially confused before you explain it to them, only to think it obvious in hindsight.\nRelevance can be tested using regression and other tests for correlation. The exclusion restriction cannot be tested. We need to present evidence and convincing arguments. The difficult aspect is that the instrument needs to seem irrelevant because that is the implication of the exclusion restriction (Cunningham 2021, 225).\nInstrumental variables is a useful approach because one can obtain causal estimates even without explicit randomization. Finding instrumental variables used to be a bit of a white whale, especially in academia. But there has been increased use of IV approaches downstream of A/B tests (Taddy 2019, 162)."
  },
  {
    "objectID": "15-causality_from_obs.html#exercises-and-tutorial",
    "href": "15-causality_from_obs.html#exercises-and-tutorial",
    "title": "15  Causality from observational data",
    "section": "15.8 Exercises and tutorial",
    "text": "15.8 Exercises and tutorial\n\n15.8.1 Exercises\n\nFor three months Sharla Gelfand shared two functions each day: one that was new to them and another that they already knew and love. Please go the ‘Two Functions Most Days’ GitHub repo, and find a package that they mention that you have never used. Find the relevant website for the package, and then in a paragraph or two, describe what the package does and a context in which it could be useful to you.\nPlease again, go to Sharla’s ‘Two Functions Most Days’ GitHub repo, and find a function that they mention that you have never used. Please look at the help file for that function, and then detail the arguments of the function, and a context in which it could be useful to you.\nWhat is propensity score matching? If you were matching people, then what are some of the features that you would like to match on? What sort of ethical questions does collecting and storing such information raise for you?\nPutting the ethical issues to one side, following King and Nielsen (2019), in at least two paragraphs, please describe some of the statistical concerns with propensity score matching.\nWhat is the key assumption when using difference in differences?\nPlease read the fascinating article in The Markup about car insurance algorithms: https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list. Please read the article and tell me what you think. You may wish to focus on ethical, legal, social, statistical, or other, aspects.\nPlease go to the GitHub page related to the fascinating article in The Markup about car insurance algorithms: https://github.com/the-markup/investigation-allstates-algorithm. What is great about their work? What could be improved?\nWhat are the fundamental features of regression discontinuity design?\nWhat are the conditions that are needed in order for regression discontinuity design to be able to be used?\nCan you think of a situation in your own life where regression discontinuity design may be useful?\nWhat are some threats to the validity of regression discontinuity design estimates?\nPlease read and reproduce the main findings from Eggers, Fowler, Hainmueller, Hall, Snyder, 2015.\nWhat is an instrumental variable?\nWhat are some circumstances in which instrumental variables might be useful?\nWhat conditions must instrumental variables satisfy?\nWho were some of the early instrumental variable authors?\nCan you please think of and explain an application of instrumental variables in your own life?\nWhat is the key assumption in difference in differences\n\nParallel trends.\nHeteroscedasticity.\n\nIf you are using regression discontinuity, whare are some aspects to be aware of and think hard about (select all that apply)?\n\nIs the cut-off free of manipulation?\nIs the forcing function continuous?\nTo what extent is the functional form driving the estimate?\nWould different fitted lines affect the results?\n\nWhat is the main reason that Oostrom (2021) finds that the outcome of an RCT can depend on who is funding it (pick one)?\n\nPublication bias\nExplicit manipulation\nSpecialisation\nLarger number of arms\n\nWhat is the key coefficient of interest in Angelucci and Cagé, 2019 (pick one)?\n\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\lambda\\)\n\\(\\gamma\\)\n\nThe instrumental variable is (please pick all that apply):\n\nCorrelated with the treatment variable.\nNot correlated with the outcome.\nHeteroskedastic.\n\nWho are the two candidates to have invented instrumental variables?\n\nSewall Wright\nPhilip G. Wright\nSewall Cunningham\nPhilip G. Cunningham\n\nWhat are the two main assumptions of instrumental variables?\n\nExclusion Restriction.\nRelevance.\nIgnorability.\nRandomization.\n\nAccording to Meng (2021) ‘Data science can persuade via…’ (pick all that apply):\n\nthe careful establishment of evidence from fair-minded and high-quality data collection\nprocessing and analysis\nthe honest interpretation and communication of findings\nlarge sample sizes\n\nAccording to Reiderer, 2021, if I have ‘disjoint treated and untreated groups partitioned by a sharp cut-off’ then which method should I use to measure the local treatment effect at the juncture between groups (pick one)?\n\nregression discontinuity\nmatching\ndifference in differences\nevent study methods\n\nAccording to Reiderer, 2021, ‘Causal inference requires investment in’ (pick all that apply):\n\ndata management\ndomain knowledge\nprobabilistic reasoning\ndata science\n\nI am an Australian 30-39 year old male living in Toronto with one child and a PhD. Which of the following do you think I would match most closely with and why (please explain in a paragraph or two)?\n\nAn Australian 30-39 year old male living in Toronto with one child and a bachelors degree\nA Canadian 30-39 year old male living in Toronto with one child and a PhD\nAn Australian 30-39 year old male living in Ottawa with one child and a PhD\nA Canadian 18-29 year old male living in Toronto with one child and a PhD\n\nIn your most disdainful tone (jokes, I love DAGs), what is a DAG (in your own words please)?\nWhat is a confounder (please select one answer)?\n\nA variable, z, that causes both x and y, where x also causes y.\nA variable, z, that is caused by both x and y, where x also causes y.\nA variable, z, that causes y and is caused by x, where x also causes y.\n\nWhat is a mediator (please select one answer)?\n\nA variable, z, that causes y and is caused by x, where x also causes y.\nA variable, z, that causes both x and y, where x also causes y.\nA variable, z, that is caused by both x and y, where x also causes y.\n\nWhat is a collider (please select one answer)?\n\nA variable, z, that causes both x and y, where x also causes y.\nA variable, z, that causes y and is caused by x, where x also causes y.\nA variable, z, that is caused by both x and y, where x also causes y.\n\nPlease talk through a brief example of when you may want to be very careful about checking for Simpson’s paradox.\nPlease talk through a brief example of when you may want to be very careful about checking for Berkson’s paradox.\nIn Kahneman, Sibony, and Sunstein (2021) the authors, including the Nobel Prize winner Daniel Kahneman, say ‘… while correlation does not imply causation, causation does imply correlation. Where there is a causal link, we should find a correlation’. With reference to Cunningham (2021, chap. 1), are they right or wrong, and why?\n\n\n\n15.8.2 Tutorial\n\n\n\n\n\n\n\nAlexander, Rohan, and Zachary Ward. 2018. “Age at Arrival and Assimilation During the Age of Mass Migration.” The Journal of Economic History 78 (3): 904–37.\n\n\nAngelucci, Charles, and Julia Cagé. 2019. “Newspapers in Times of Low Advertising Revenues.” American Economic Journal: Microeconomics 11 (3): 319–64.\n\n\nArel-Bundock, Vincent. 2021. Modelsummary: Summary Tables and Plots for Statistical Models and Data: Beautiful, Customizable, and Publication-Ready. https://CRAN.R-project.org/package=modelsummary.\n\n\nBarrett, Malcolm. 2021. Ggdag: Analyze and Create Elegant Directed Acyclic Graphs. https://CRAN.R-project.org/package=ggdag.\n\n\nBerkson, Joseph. 1946. “Limitations of the Application of Fourfold Table Analysis to Hospital Data.” Biometrics Bulletin 2 (3): 47–53. http://www.jstor.org/stable/3002000.\n\n\nBickel, Peter J, Eugene A Hammel, and J William O’Connell. 1975. “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring Bias Is Harder Than Is Usually Assumed, and the Evidence Is Sometimes Contrary to Expectation.” Science 187 (4175): 398–404.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and Luke Sonnet. 2021. Estimatr: Fast Estimators for Design-Based Inference. https://CRAN.R-project.org/package=estimatr.\n\n\nBloom, Howard, Andrew Bell, and Kayla Reiman. 2020. “Using Data from Randomized Trials to Assess the Likely Generalizability of Educational Treatment-Effect Estimates from Regression Discontinuity Designs.” Journal of Research on Educational Effectiveness, 1–30. https://doi.org/10.1080/19345747.2019.1634169.\n\n\nCalonico, Sebastian, Matias D. Cattaneo, Max H. Farrell, and Rocio Titiunik. 2021. Rdrobust: Robust Data-Driven Statistical Inference in Regression-Discontinuity Designs. https://CRAN.R-project.org/package=rdrobust.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. Yale Press.\n\n\nDagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark A Katz, Miguel A Hernán, Marc Lipsitch, Ben Reis, and Ran D Balicer. 2021. “BNT162b2 mRNA Covid-19 Vaccine in a Nationwide Mass Vaccination Setting.” New England Journal of Medicine.\n\n\nGelman, Andrew. 2019. Another Regression Discontinuity Disaster and What Can We Learn from It. https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models.\n\n\nGreen, Donald P, Terence Y Leong, Holger L Kern, Alan S Gerber, and Christopher W Larimer. 2009. “Testing the Accuracy of Regression Discontinuity Analysis Using Experimental Benchmarks.” Political Analysis 17 (4): 400–417.\n\n\nHernan, Miguel A, and James M Robins. 2020. What If. CRC Press.\n\n\nHo, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2011. “MatchIt: Nonparametric Preprocessing for Parametric Causal Inference.” Journal of Statistical Software 42 (8): 1–28. https://doi.org/10.18637/jss.v042.i08.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://allisonhorst.github.io/palmerpenguins/.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. Chapman & Hall.\n\n\nIannone, Richard. 2020. DiagrammeR: Graph/Network Visualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nIgelström, Erik. 2020. “Causal Graphs in r with DiagrammeR.” https://www.erikigelstrom.com/articles/causal-graphs-in-r-with-diagrammer/.\n\n\nKahneman, Daniel, Olivier Sibony, and Cass Sunstein. 2021. Noise: A Flaw in Human Judgment. William Collins.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores Should Not Be Used for Matching.” Political Analysis 27 (4): 435–54.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC Press.\n\n\nMeng, Xiao-Li. 2021. “What Are the Values of Data, Data Science, or Data Scientists?” Harvard Data Science Review, January. https://doi.org/10.1162/99608f92.ee717cf7.\n\n\nOostrom, Tamar. 2021. “Funding of Clinical Trials and Reported Drug Efficacy.” https://drive.google.com/file/d/1EQLCH0ns99IxYBkxPNbagcZtGgE9a8MQ/view.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2021. Broom: Convert Statistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom.\n\n\nSekhon, Jasjeet S, and Rocio Titiunik. 2017. “Understanding Regression Discontinuity Designs as Observational Studies.” Observational Studies 3 (2): 174–82.\n\n\nSimpson, Edward H. 1951. “The Interpretation of Interaction in Contingency Tables.” Journal of the Royal Statistical Society: Series B (Methodological) 13 (2): 238–41.\n\n\nStock, James H, and Francesco Trebbi. 2003. “Retrospectives: Who Invented Instrumental Variable Regression?” Journal of Economic Perspectives 17 (3): 177–94.\n\n\nTaddy, Matt. 2019. Business Data Science. McGraw Hill.\n\n\nThistlethwaite, Donald L, and Donald T Campbell. 1960. “Regression-Discontinuity Analysis: An Alternative to the Ex Post Facto Experiment.” Journal of Educational Psychology 51 (6): 309.\n\n\nWickham, Hadley. 2017. Tidyverse: Easily Install and Load the ’Tidyverse’. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, and Evan Miller. 2020. Haven: Import and Export ’SPSS’, ’Stata’ and ’SAS’ Files. https://CRAN.R-project.org/package=haven.\n\n\nWickham, Hadley, and Dana Seidel. 2020. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nWright, Philip G. 1928. The Tariff on Animal and Vegetable Oils. Macmillan Company."
  },
  {
    "objectID": "16-mrp.html",
    "href": "16-mrp.html",
    "title": "16  Multilevel regression with post-stratification",
    "section": "",
    "text": "Required material\nKey libraries"
  },
  {
    "objectID": "16-mrp.html#introduction",
    "href": "16-mrp.html#introduction",
    "title": "16  Multilevel regression with post-stratification",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\n\n[The Presidential election of] 2016 was the largest analytics failure in US political history.\nDavid Shor, 13 August 2020\n\nMultilevel regression with post-stratification (MRP) is a popular way to adjust non-representative samples to better analyse opinion and other survey responses. It uses a regression model to relate individual-level survey responses to various characteristics and then rebuilds the sample to better match the population. In this way MRP can not only allow a better understanding of responses, but also allow us to analyse data that may otherwise be unusable. However, it can be a challenge to get started with MRP as the terminology may be unfamiliar, and the data requirements can be onerous.\nLet us say that we have a biased survey. Maybe we conducted a survey about computers at an academic seminar, so folks with post-graduate degrees are likely over-represented. We are nonetheless interested in making claims about the population. Let us say that we found 37.5 per cent of our respondents prefer Macs. One way forward is to just ignore the bias and say that ‘37.5 per cent of people prefer Macs’. Another way is to say, well 50 per cent of our respondents with a post-graduate degree prefer Macs, and of those without a post-graduate degree, 25 per cent prefer Macs. If we knew what proportion of the broader population has post-graduate degree, let us assume 10 per cent, then we could conduct re-weighting, or post-stratification, as follows: \\(0.5 \\times 0.1 + 0.25 \\times 0.9 = 0.275\\), and so our estimate is that 27.5 per cent of people prefer Macs. MRP is a third approach, and uses a model to help do that re-weighting. So we use logistic regression to estimate the relationship between preferring Macs and highest educational attainment in our survey. We then apply that relationship to population dataset.\nMRP is a handy approach when dealing with survey data. Hanretty (2020) describes how we use MRP because the alternatives are either very poor or very expensive. Essentially, it trains a model based on the survey, and then applies that trained model to another dataset. There are two main, related, advantages:\n\nIt can allow us to ‘re-weight’ in a way that includes uncertainty front-of-mind and isn’t as hamstrung by small samples. The alternative way to deal with having a small sample is to either go and gather more data or throw it away.\nIt can allow us to use broad surveys to speak to subsets. As Hanretty (2020) says ‘A poor alternative [to using MRP] is simply splitting a large sample into (much) smaller geographic subsamples. This is a poor alternative because there is no guarantee that a sample which is representative at the national level will be representative when it is broken down into smaller groups.’.\n\nFrom a practical perspective, it tends to be less expensive to collect non-probability samples and so there are benefits of being able to use these types of data. That said, it is not a magic-bullet and the laws of statistics still apply. We will have larger uncertainty around our estimates and they will still be subject to all the usual biases. Dr Lauren Kennedy, Lecturer, Monash University, describes how we have used MRP for some time with probability surveys, and shows great potential when it comes to non-probability surveys, but the limitations of MRP are not known at the moment. It is an exciting area of research in both academia and industry.\nThe workflow that we need for MRP is straight-forward, but the details and tiny decisions that have to be made at each step can become overwhelming. The point to keep in mind is that we are trying to create a relationship between two datasets using a statistical model, and so we need to establish similarity between the two datasets in terms of their variables and levels. The steps are:\n\ngather and prepare the survey dataset, thinking about what is needed for coherence with the post-stratification dataset;\ngather and prepare the post-stratification dataset thinking about what is needed for coherence with the survey dataset;\nmodel the variable of interest from the survey using independent variables and levels that are available in both the survey and the post-stratification datasets;\napply the model to the post-stratification data.\n\nIn general, MRP is a good way to accomplish specific aims, but it is not without trade-offs. If we have a good quality survey, then it may be a way to speak to disaggregated aspects of it. Or if we are concerned about uncertainty then it is a good way to think about that. If we have a biased survey, then it is a great place to start, but it is not a panacea. There is plenty of scope for exciting work from a variety of approaches:\n\nFrom a more statistical perspective, there is a lot of work to do in terms of thinking through how survey design and modelling approaches interact and the extent to which we are underestimating uncertainty. It is also also very interested in thinking through the implications of small samples and uncertainty in the post-stratification dataset. There is an awful lot to do in terms of thinking through what the appropriate model is to use, and how do we even evaluate what ‘appropriate’ means here? Si (2020) would be an appropriate starting point.\nThere is a lot to be done from a sociology perspective in terms of survey responses and how we can better design our surveys, knowing they are going to be used for MRP and putting respect for our respondents first.\nFrom a political science perspective, we just have very little idea of the conditions under which we will have the stable preferences and relationships that are required for MRP to be accurate, and further understanding how this relates to uncertainty in survey design. For those with political science interests, a natural next step would be to go through Lauderdale et al. (2020) or Ghitza and Gelman (2020).\nEconomists might be interested to think about how we could use MRP to better understand the inflation and unemployment rates at local levels.\nFrom a statistical software side of things, we really need to develop better packages around this.\nAnd finally, from an information perspective it is exciting to think about how we store and protect our datasets, yet retain the ability to have them correspond with each other for the purposes of MRP. How do we put the levels together in a way that is meaningful? To what extent do people appreciate uncertainty estimates and how can we better communicate these estimates?\n\nMore generally, we could pretty much use MRP anywhere we have samples. Determining the conditions under which we actually should, is the work of whole generations.\nIn this chapter, we begin with simulating a situation in which we pretend that we know the features of the population. We then move to a famous example of MRP that used survey data from the Xbox platform and exit poll data to forecast the 2012 US election. We will then move to examples from the Australian political situation. We will then discuss some features to be aware of when conducting MRP."
  },
  {
    "objectID": "16-mrp.html#simulation---toddler-bedtimes",
    "href": "16-mrp.html#simulation---toddler-bedtimes",
    "title": "16  Multilevel regression with post-stratification",
    "section": "16.2 Simulation - Toddler bedtimes",
    "text": "16.2 Simulation - Toddler bedtimes\n\n16.2.1 Construct a population\nTo get started we will simulate some data from a population that has various properties, take a biased sample, and then conduct MRP to demonstrate how we can get those properties back. We are going to have two ‘explanatory variables’ - age-group and toilet-trained - and one dependent variable - bedtime. Bed-time will increase as age-group increases, and will be later for children that are toilet-trained, compared with those that are not. To be clear, in this example we will ‘know’ the ‘true’ features of the population, but this isn’t something that occurs when we use real data - it is just to help to explain what is happening in MRP. We are going to rely heavily on tidyverse (Wickham et al. 2019).\n\nlibrary(tidyverse)\n\nset.seed(853)\n\nsize_of_population <- 1000000\n\npopulation_for_mrp_example <- \n  tibble(age_group = sample(x = c(1:3),\n                            size = size_of_population,\n                            replace = TRUE\n                            ),\n         toilet_trained = sample(x = c(0, 1),\n                                 size = size_of_population,\n                                 replace = TRUE\n                                 ),\n         noise = rnorm(size_of_population, mean = 0, sd = 1), \n         # No special reason for this intercept to be five; it could be anything.\n         bed_time = 5 + 0.5 * age_group + 1 * toilet_trained + noise, \n         ) |> \n  select(-noise) |> \n  mutate(age_group = as_factor(age_group),\n         toilet_trained = as_factor(toilet_trained)\n         )\n\npopulation_for_mrp_example |> \n  head()\n\n# A tibble: 6 × 3\n  age_group toilet_trained bed_time\n  <fct>     <fct>             <dbl>\n1 1         0                  5.74\n2 2         1                  6.48\n3 1         0                  6.53\n4 1         1                  5.39\n5 1         1                  8.40\n6 3         0                  6.54\n\n\nThat is, as always, when we have a dataset, we first try to graph it to better understand what is going on. As there are a million points, we can just grab the first 1,000 so that it plots nicely.\n\npopulation_for_mrp_example |> \n  slice(1:1000) |> \n  ggplot(aes(x = age_group, y = bed_time)) +\n  geom_jitter(aes(color = toilet_trained), \n              alpha = 0.4, \n              width = 0.1, \n              height = 0) +\n  labs(x = \"Age-group\",\n       y = \"Bed time\",\n       color = \"Toilet trained\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nAnd we can also work out what the ‘truth’ is for the information that we are interested in (remembering that we’d never actually know this when we move away from simulated examples).\n\npopulation_for_mrp_example_summarised <- \n  population_for_mrp_example |> \n  group_by(age_group, toilet_trained) |> \n  summarise(median_bed_time = median(bed_time)) \n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\npopulation_for_mrp_example_summarised |> \n  knitr::kable(digits = 2,\n               col.names = c(\"Age-group\", \"Is toilet trained\", \"Average bed time\"))\n\n\n\n\nAge-group\nIs toilet trained\nAverage bed time\n\n\n\n\n1\n0\n5.50\n\n\n1\n1\n6.50\n\n\n2\n0\n6.00\n\n\n2\n1\n7.00\n\n\n3\n0\n6.50\n\n\n3\n1\n7.51\n\n\n\n\n\n\n\n16.2.2 Get a biased sample from it\nNow we want to pretend that we have some survey that has a biased sample. We will allow that it over-samples children that are younger and those that are not toilet-trained. For instance, perhaps we gathered our sample based on the records of a paediatrician, so it is more likely that they will see this biased sample of children. We are interested in knowing what proportion of children are toilet-trained at various age-groups.\n\n# This code based on that of Monica Alexander\nset.seed(853)\n\n# Add a weight for each 'type' (has to sum to one)\npopulation_for_mrp_example <- \n  population_for_mrp_example |> \n  mutate(weight = \n           case_when(toilet_trained == 0 & age_group == 1 ~ 0.7,\n                     toilet_trained == 0 ~ 0.1,\n                     age_group %in% c(1, 2, 3) ~ 0.2\n                     ),\n         id = 1:n()\n         )\n\nget_these <- \n  sample(\n    x = population_for_mrp_example$id,\n    size = 1000,\n    prob = population_for_mrp_example$weight\n    )\n\nsample_for_mrp_example <- \n  population_for_mrp_example |> \n  filter(id %in% get_these) |> \n  select(-weight, -id)\n\n# Clean up\npoststratification_dataset <- \n  population_for_mrp_example |> \n  select(-weight, -id)\n\nAnd we can plot those also.\n\nsample_for_mrp_example |> \n  mutate(toilet_trained = as_factor(toilet_trained)) |> \n  ggplot(aes(x = age_group, y = bed_time)) +\n  geom_jitter(aes(color = toilet_trained), alpha = 0.4, width = 0.1, height = 0) +\n  labs(x = \"Age-group\",\n       y = \"Bed time\",\n       color = \"Toilet trained\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nIt is pretty clear that our sample has a different bedtime than the overall population, but let us just do the same exercise as before to look at the median, by age and toilet-trained status.\n\nsample_for_mrp_example_summarized <- \n  sample_for_mrp_example |> \n  group_by(age_group, toilet_trained) |> \n  summarise(median_bed_time = median(bed_time))\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\nsample_for_mrp_example_summarized |> \n  knitr::kable(digits = 2,\n               col.names = c(\"Age-group\", \"Is toilet trained\", \"Average bed time\"))\n\n\n\n\nAge-group\nIs toilet trained\nAverage bed time\n\n\n\n\n1\n0\n5.41\n\n\n1\n1\n6.35\n\n\n2\n0\n5.89\n\n\n2\n1\n6.85\n\n\n3\n0\n6.49\n\n\n3\n1\n7.62\n\n\n\n\n\n\n\n16.2.3 Model the sample\nWe will quickly train a model based on the (biased) survey. We will use modelsummary (Arel-Bundock 2021) to format our estimates.\n\nlibrary(modelsummary)\n\nmrp_example_model <- \n  lm(bed_time ~ age_group + toilet_trained, data = sample_for_mrp_example)\n\nmrp_example_model |> \n  modelsummary::modelsummary(fmt = 2)\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    5.47 \n  \n  \n     \n    (0.04) \n  \n  \n    age_group2 \n    0.54 \n  \n  \n     \n    (0.09) \n  \n  \n    age_group3 \n    1.15 \n  \n  \n     \n    (0.09) \n  \n  \n    toilet_trained1 \n    0.91 \n  \n  \n     \n    (0.07) \n  \n  \n    Num.Obs. \n    1000 \n  \n  \n    R2 \n    0.373 \n  \n  \n    R2 Adj. \n    0.371 \n  \n  \n    AIC \n    2839.3 \n  \n  \n    BIC \n    2863.8 \n  \n  \n    Log.Lik. \n    −1414.630 \n  \n  \n    F \n    197.594 \n  \n\n\n\n\n\nThis is the ‘multilevel regression’ part of the MRP (although this isn’t really a multilevel model just to keep things simple for now).\n\n\n16.2.4 Get a post-stratification dataset\nNow we will use a post-stratification dataset to get some estimates of the number in each cell. We typically use a larger dataset that may more closely reflection the population. In the US a popular choice is the ACS, while in other countries we typically have to use the census.\nIn this simulation example, we will just take a 10 per cent sample from the population and use that as our post-stratification dataset.\n\nset.seed(853)\n\npoststratification_dataset <- \n  population_for_mrp_example |> \n  slice(1:100000) |> \n  select(-bed_time)\n\npoststratification_dataset |> \n  head()\n\n# A tibble: 6 × 4\n  age_group toilet_trained weight    id\n  <fct>     <fct>           <dbl> <int>\n1 1         0                 0.7     1\n2 2         1                 0.2     2\n3 1         0                 0.7     3\n4 1         1                 0.2     4\n5 1         1                 0.2     5\n6 3         0                 0.1     6\n\n\nIn an ideal world we have individual-level data in our post-stratification dataset (that is the case above). In that world we can apply our model to each individual. The more likely situation, in reality, is that we just have counts by groups, so we are going to try to construct an estimate for each group.\n\npoststratification_dataset_grouped <- \n  poststratification_dataset |> \n  group_by(age_group, toilet_trained) |> \n  count()\n\npoststratification_dataset_grouped |> \n  head()\n\n# A tibble: 6 × 3\n# Groups:   age_group, toilet_trained [6]\n  age_group toilet_trained     n\n  <fct>     <fct>          <int>\n1 1         0              16766\n2 1         1              16649\n3 2         0              16801\n4 2         1              16617\n5 3         0              16625\n6 3         1              16542\n\n\n\n\n16.2.5 Post-stratify our model estimates\nNow we create an estimate for each group, and add some confidence intervals.\n\npoststratification_dataset_grouped <- \n  mrp_example_model |> \n  predict(newdata = poststratification_dataset_grouped, interval = \"confidence\") |> \n  as_tibble() |> \n  cbind(poststratification_dataset_grouped) \n\nAt this point we can have a look at our MRP estimates (circles) along with their confidence intervals, and compare them the raw estimates from the data (squares). In this case, because we know the truth, we can also compare them to the known truth (triangles) (but that is not something we can do normally).\n\npoststratification_dataset_grouped |> \n  ggplot(aes(x = age_group, y = fit)) +\n  geom_point(data = population_for_mrp_example_summarised,\n             aes(x = age_group, y = median_bed_time, color = toilet_trained), \n             shape = 17) +\n  geom_point(data = sample_for_mrp_example_summarized,\n             aes(x = age_group, y = median_bed_time, color = toilet_trained), \n             shape = 15) +\n  geom_pointrange(aes(ymin=lwr, ymax=upr, color = toilet_trained)) +\n  labs(x = \"Age-group\",\n       y = \"Bed time\",\n       color = \"Toilet trained\") +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\")"
  },
  {
    "objectID": "16-mrp.html#case-study---xbox-paper",
    "href": "16-mrp.html#case-study---xbox-paper",
    "title": "16  Multilevel regression with post-stratification",
    "section": "16.3 Case study - Xbox paper",
    "text": "16.3 Case study - Xbox paper\n\n16.3.1 Overview\nOne famous MRP example is Wang et al. (2015). They used data from the Xbox gaming platform to forecast the 2012 US Presidential Election.\nKey facts about the set-up:\n\nData are from an opt-in poll which was available on the Xbox gaming platform during the 45 days leading up to the 2012 US presidential election (Obama and Romney).\nEach day there were three to five questions, including voter intention: ‘If the election were held today, who would you vote for?’.\nRespondents were allowed to answer at most once per day.\nFirst-time respondents were asked to provide information about themselves, including their sex, race, age, education, state, party ID, political ideology, and who they voted for in the 2008 presidential election.\nIn total, 750,148 interviews were conducted, with 345,858 unique respondents - over 30,000 of whom completed five or more polls.\nYoung men dominate the Xbox population: 18-to-29-year-olds comprise 65 per cent of the Xbox dataset, compared to 19 per cent in the exit poll; and men make up 93 per cent of the Xbox sample but only 47 per cent of the electorate.\n\n\n\n16.3.2 Model\nGiven the structure of the US electorate, they use a two-stage modelling approach. The details don’t really matter too much, but essentially they model how likely a respondent is to vote for Obama, given various information such as state, education, sex, etc:\n\\[\nPr\\left(Y_i = \\mbox{Obama} | Y_i\\in\\{\\mbox{Obama, Romney}\\}\\right) = \\mbox{logit}^{-1}(\\alpha_0 + \\alpha_1(\\mbox{state last vote share}) + \\alpha_{j[i]}^{\\mbox{state}} + \\alpha_{j[i]}^{\\mbox{edu}} + \\alpha_{j[i]}^{\\mbox{sex}} + ...)\n\\]\nThey run this in R using glmer() from ‘lme4’ (Bates et al. 2015).\n\n\n16.3.3 Post-stratify\nHaving a trained model that considers the effect of these various independent variables on support for the candidates, they now post-stratify, where each of these ‘cell-level estimates are weighted by the proportion of the electorate in each cell and aggregated to the appropriate level (i.e., state or national).’\nThis means that they need cross-tabulated population data. In general, the census would have worked, or one of the other large surveys available in the US, but the difficulty is that the variables need to be available on a cross-tab basis. As such, they use exit polls (not a viable option for most other countries).\nThey make state-specific estimates by post-stratifying to the features of each state (Figure 16.1).\n\n\n\nFigure 16.1: Post-stratified estimates for each state based on the Xbox survey and MRP\n\n\nSimilarly, they can examine demographic-differences (Figure 16.2).\n\n\n\nFigure 16.2: Post-stratified estimates on a demographic basis based on the Xbox survey and MRP\n\n\nFinally, they convert their estimates into electoral college estimates (Figure 16.3).\n\n\n\nFigure 16.3: Post-stratified estimates of electoral college outcomes based on the Xbox survey and MRP"
  },
  {
    "objectID": "16-mrp.html#simulation---australian-voting",
    "href": "16-mrp.html#simulation---australian-voting",
    "title": "16  Multilevel regression with post-stratification",
    "section": "16.4 Simulation - Australian voting",
    "text": "16.4 Simulation - Australian voting\n\n16.4.1 Overview\nAs a reminder, the workflow that we use is:\n\nread in the poll;\nmodel the poll;\nread in the post-stratification data; and\napply the model to the post-stratification data.\n\nIn the earlier example, we did not really do too much in the modelling step, and despite the name ‘multilevel modelling with post-stratification’, we did not actually use a multilevel model. There is nothing that says you have to use a multilevel model, but a lot of situations will have circumstances such that it is not likely to do any worse. To be clear, this means that although we have individual-level data, there is some grouping of the individuals that we’ll take advantage of. For instance, in the case of trying to model elections, usually districts/divisions/electorates/ridings/etc exist within provinces/states so it would likely make sense to, at least, include a coefficient that adjusts the intercept for each province.\nIn this section we are simulate another dataset and then fit a few different models to it. We are going to draw on the Australian elections set-up. In Australia we have a parliamentary system, with 151 seats in the parliament, one for each electorate. These electorates are grouped within six states and two territories. There are two major parties - the Australian Labor Party (ALP) and the Liberal Party (LP). Somewhat confusingly, the Liberal party are actually the conservative, right-wing party, while the Labor party are the progressive, left-wing, party.\n\n\n16.4.2 Construct a survey\nTo move us slightly closer to reality, we are going to simulate a survey (rather than sample from a population as we did earlier) and then post-stratify it using real data. The dependent variable is ‘supports_ALP’, which is a binary variable - either 0 or 1. We will just start with three independent variables here:\n\n‘gender’, which is either ‘female’ or ‘male’ (as that is what is available from the Australian Bureau of Statistics);\n‘age_group’, which is one of four groups: ‘ages 18 to 29’, ‘ages 30 to 44’, ‘ages 45 to 59’, ‘ages 60 plus’;\n‘state’, which is one of eight integers: 1 - 8 (inclusive).\n\n\nlibrary(tidyverse)\nset.seed(853)\n\nsize_of_sample_for_australian_polling <- 2000\n\nsample_for_australian_polling <- \n  tibble(age_group = \n           sample(x = c(0:3), \n                  size = size_of_sample_for_australian_polling,\n                  replace = TRUE\n                  ),\n         gender = \n           sample(x = c(0:1),\n                  size = size_of_sample_for_australian_polling,\n                  replace = TRUE\n                  ),\n         state = \n           sample(x = c(1:8),\n                  size = size_of_sample_for_australian_polling,\n                  replace = TRUE\n                  ),\n         noise = rnorm(size_of_sample_for_australian_polling, mean = 0, sd = 1), \n         support_alp = 1 + 0.5 * age_group + 0.5 * gender + 0.01 * state + noise\n         ) \n\n# Normalize the outcome variable\nsample_for_australian_polling <- \n  sample_for_australian_polling |> \n  mutate(support_alp = \n           if_else(support_alp > median(support_alp, na.rm = TRUE), \n                   'Supports ALP', \n                   'Does not')\n         )\n\n# Clean up the simulated data\nsample_for_australian_polling <- \n  sample_for_australian_polling |> \n  mutate(\n    age_group = case_when(\n      age_group == 0 ~ 'Ages 18 to 29',\n      age_group == 1 ~ 'Ages 30 to 44',\n      age_group == 2 ~ 'Ages 45 to 59',\n      age_group == 3 ~ 'Ages 60 plus',\n      TRUE ~ 'Problem'\n      ),\n    gender = case_when(\n      gender == 0 ~ 'Male',\n      gender == 1 ~ 'Female',\n      TRUE ~ 'Problem'\n      ),\n    state = case_when(\n      state == 1 ~ 'Queensland',\n      state == 2 ~ 'New South Wales',\n      state == 3 ~ 'Australian Capital Territory',\n      state == 4 ~ 'Victoria',\n      state == 5 ~ 'Tasmania',\n      state == 6 ~ 'Northern Territory',\n      state == 7 ~ 'South Australia',\n      state == 8 ~ 'Western Australia',\n      TRUE ~ 'Problem'\n      ),\n    \n    ) |> \n  select(-noise)\n\n# Tidy the class\nsample_for_australian_polling <- \n  sample_for_australian_polling |> \n  mutate(across(c(age_group, gender, state, support_alp), as_factor))\n\nsample_for_australian_polling |>   \n  head()\n\n# A tibble: 6 × 4\n  age_group     gender state           support_alp \n  <fct>         <fct>  <fct>           <fct>       \n1 Ages 18 to 29 Female South Australia Supports ALP\n2 Ages 60 plus  Male   South Australia Supports ALP\n3 Ages 30 to 44 Male   Victoria        Does not    \n4 Ages 18 to 29 Male   Tasmania        Does not    \n5 Ages 18 to 29 Female Victoria        Does not    \n6 Ages 18 to 29 Male   Queensland      Supports ALP\n\n\nFinally, we want our survey to over-sample females, so we will just get rid of 300 males.\n\nsample_for_australian_polling <- \n  sample_for_australian_polling |> \n  arrange(gender) |> \n  slice(1:1700)\n\n\n\n16.4.3 Model the survey\nThis polling data was generated to make both males and older people less likely to vote for the ALP; and females and younger people more likely to vote for the Labor Party. Females are over-sampled. As such, we should have an ALP skew on the dataset. We are going to use the gtsummary package to quickly make a summary table (Arel-Bundock 2021).\n\nlibrary(modelsummary)\n\nsample_for_australian_polling |> \n    datasummary_skim(type = \"categorical\")\n\n\n \n  \n      \n       \n    N \n    % \n  \n \n\n  \n    age_group \n    Ages 18 to 29 \n    458 \n    26.9 \n  \n  \n     \n    Ages 60 plus \n    421 \n    24.8 \n  \n  \n     \n    Ages 30 to 44 \n    401 \n    23.6 \n  \n  \n     \n    Ages 45 to 59 \n    420 \n    24.7 \n  \n  \n    gender \n    Female \n    1023 \n    60.2 \n  \n  \n     \n    Male \n    677 \n    39.8 \n  \n  \n    state \n    South Australia \n    233 \n    13.7 \n  \n  \n     \n    Victoria \n    189 \n    11.1 \n  \n  \n     \n    Tasmania \n    229 \n    13.5 \n  \n  \n     \n    Queensland \n    214 \n    12.6 \n  \n  \n     \n    Western Australia \n    198 \n    11.6 \n  \n  \n     \n    New South Wales \n    219 \n    12.9 \n  \n  \n     \n    Australian Capital Territory \n    237 \n    13.9 \n  \n  \n     \n    Northern Territory \n    181 \n    10.6 \n  \n  \n    support_alp \n    Supports ALP \n    896 \n    52.7 \n  \n  \n     \n    Does not \n    804 \n    47.3 \n  \n\n\n\n\n\nNow we’d like to see if we can get our results back (we should find females less likely than males to vote for Australian Labor Party and that people are less likely to vote Australian Labor Party as they get older). Our model is:\nADD THE MODEL.\nThis model says that the probability that some person, \\(j\\), will vote for the Australian Labor Party depends on their gender and their age-group. Based on our simulated data, we would like older age-groups to be less likely to vote for the Australian Labor Party and for males to be less likely to vote for the Australian Labor Party.\n\nalp_support <- \n  glm(support_alp ~ gender + age_group + state, \n      data = sample_for_australian_polling,\n      family = \"binomial\"\n      )\n\nalp_support |> \n  modelsummary(fmt = 2, exponentiate = TRUE)\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    1.44 \n  \n  \n     \n    (0.18) \n  \n  \n    genderMale \n    3.22 \n  \n  \n     \n    (0.12) \n  \n  \n    age_groupAges 60 plus \n    0.07 \n  \n  \n     \n    (0.17) \n  \n  \n    age_groupAges 30 to 44 \n    0.42 \n  \n  \n     \n    (0.15) \n  \n  \n    age_groupAges 45 to 59 \n    0.17 \n  \n  \n     \n    (0.15) \n  \n  \n    stateVictoria \n    1.65 \n  \n  \n     \n    (0.22) \n  \n  \n    stateTasmania \n    1.24 \n  \n  \n     \n    (0.21) \n  \n  \n    stateQueensland \n    1.46 \n  \n  \n     \n    (0.22) \n  \n  \n    stateWestern Australia \n    1.18 \n  \n  \n     \n    (0.22) \n  \n  \n    stateNew South Wales \n    1.42 \n  \n  \n     \n    (0.21) \n  \n  \n    stateAustralian Capital Territory \n    1.73 \n  \n  \n     \n    (0.21) \n  \n  \n    stateNorthern Territory \n    1.49 \n  \n  \n     \n    (0.23) \n  \n  \n    Num.Obs. \n    1700 \n  \n  \n    AIC \n    1959.7 \n  \n  \n    BIC \n    2024.9 \n  \n  \n    Log.Lik. \n    −967.836 \n  \n  \n    F \n    28.555 \n  \n\n\n\n\n\nEssentially we have got our inputs back. Our dependent variable is a binary, and so we used logistic regression so the results are a little more difficult to interpret.\n\n\n16.4.4 Post-stratify\nNow we’d like to see if we can use what we found in the poll to get an estimate for each state based on their demographic features.\nFirst read in some real demographic data, on a state basis, from the ABS.\n\npost_strat_census_data <- \n  read_csv(\"outputs/data/census_data.csv\")\n\nhead(post_strat_census_data)\n\n# A tibble: 6 × 5\n  state gender age_group  number cell_prop_of_division_total\n  <chr> <chr>  <chr>       <dbl>                       <dbl>\n1 ACT   Female ages18to29  34683                       0.125\n2 ACT   Female ages30to44  42980                       0.155\n3 ACT   Female ages45to59  33769                       0.122\n4 ACT   Female ages60plus  30322                       0.109\n5 ACT   Male   ages18to29  34163                       0.123\n6 ACT   Male   ages30to44  41288                       0.149\n\n\nAt this point, we have got a decision to make because we need the variables to be the same in the survey and the post-stratification dataset, but here the state abbreviations have been used, while in the survey, the full names were used. We will change the post-stratification dataset because the survey data has already modelled.\n\npost_strat_census_data <- \n  post_strat_census_data |> \n  mutate(\n    state = \n      case_when(\n        state == 'ACT' ~ 'Australian Capital Territory',\n        state == 'NSW' ~ 'New South Wales',\n        state == 'NT' ~ 'Northern Territory',\n        state == 'QLD' ~ 'Queensland',\n        state == 'SA' ~ 'South Australia',\n        state == 'TAS' ~ 'Tasmania',\n        state == 'VIC' ~ 'Victoria',\n        state == 'WA' ~ 'Western Australia',\n        TRUE ~ \"Problem\"\n      ),\n    age_group = \n      case_when(\n        age_group == 'ages18to29' ~ 'Ages 18 to 29',\n        age_group == 'ages30to44' ~ 'Ages 30 to 44',\n        age_group == 'ages45to59' ~ 'Ages 45 to 59',\n        age_group == 'ages60plus' ~ 'Ages 60 plus',\n        TRUE ~ \"Problem\"\n      )\n  )\n\nWe are just going to do some rough forecasts. For each gender and age-group we want the relevant coefficient in the example data and we can construct the estimates.\n\npost_strat_census_data <- \n  alp_support |> \n  predict(newdata = post_strat_census_data, type = 'response', se.fit = TRUE) |> \n  as_tibble() |> \n  cbind(post_strat_census_data)\n\npost_strat_census_data |> \n  mutate(alp_predict_prop = fit*cell_prop_of_division_total) |> \n  group_by(state) |> \n  summarise(alp_predict = sum(alp_predict_prop))\n\n# A tibble: 8 × 2\n  state                        alp_predict\n  <chr>                              <dbl>\n1 Australian Capital Territory       0.551\n2 New South Wales                    0.487\n3 Northern Territory                 0.546\n4 Queensland                         0.491\n5 South Australia                    0.403\n6 Tasmania                           0.429\n7 Victoria                           0.521\n8 Western Australia                  0.460\n\n\nWe now have post-stratified estimates for each state Our model has a fair few weaknesses. For instance, small cell counts are going to be problematic. And our approach ignores uncertainty, but now that we have something working we can complicate it.\n\n\n16.4.5 Improving the model\nWe’d like to address some of the major issues with our approach, specifically being able to deal with small cell counts, and also taking better account of uncertainty. As we are dealing with survey data, prediction intervals or something similar are critical, and it is not appropriate to only report central estimates. To do this we’ll use the same broad approach as before, but just improve the model. We are going to change to a Bayesian model and use the rstanarm package (Goodrich et al. 2020).\nNow, using the same basic model as before, but in a Bayesian setting.\n\nlibrary(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.21.1\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nimproved_alp_support <- \n  stan_glm(support_alp ~ gender + age_group + state,\n                     data = sample_for_australian_polling,\n                     family = binomial(link = \"logit\"),\n                     prior = normal(0, 1), \n                     prior_intercept = normal(0, 1),\n                     cores = 2, \n                     seed = 12345)\n\nAs before, we’d like an estimate for each state based on their demographic features. We are just going to do some rough forecasts. For each gender and age-group we want the relevant coefficient in the example data and we can construct the estimates (this code is from Monica Alexander). We are going to use tidybayes for this (Kay 2020).\n\nlibrary(tidybayes)\n\npost_stratified_estimates <- \n  improved_alp_support |> \n  add_fitted_draws(newdata = post_strat_census_data) |> \n  rename(alp_predict = .value) |> \n  mutate(alp_predict_prop = alp_predict*cell_prop_of_division_total) |> \n  group_by(state, .draw) |> \n  summarise(alp_predict = sum(alp_predict_prop)) |> \n  group_by(state) |> \n  summarise(mean = mean(alp_predict), \n            lower = quantile(alp_predict, 0.025), \n            upper = quantile(alp_predict, 0.975))\n\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\nUse [add_]epred_draws() to get the expectation of the posterior predictive.\nUse [add_]linpred_draws() to get the distribution of the linear predictor.\nFor example, you used [add_]fitted_draws(..., scale = \"response\"), which\nmeans you most likely want [add_]epred_draws(...).\n\n\n`summarise()` has grouped output by 'state'. You can override using the\n`.groups` argument.\n\npost_stratified_estimates\n\n# A tibble: 8 × 4\n  state                         mean lower upper\n  <chr>                        <dbl> <dbl> <dbl>\n1 Australian Capital Territory 0.550 0.494 0.604\n2 New South Wales              0.486 0.429 0.544\n3 Northern Territory           0.544 0.483 0.607\n4 Queensland                   0.491 0.432 0.548\n5 South Australia              0.412 0.361 0.464\n6 Tasmania                     0.429 0.372 0.487\n7 Victoria                     0.519 0.453 0.583\n8 Western Australia            0.460 0.401 0.520\n\n\nWe now have post-stratified estimates for each division. Our new Bayesian approach will enable us to think more deeply about uncertainty. We could complicate this in a variety of ways including adding more coefficients (but remember that we’d need to get new cell counts), or adding some layers.\nOne interesting aspect is that our multilevel approach will allow us to deal with small cell counts by borrowing information from other cells. Even if we were to remove most of the, say, 18-to-29-year-old, male respondents from Tasmania our model would still provide estimates. It does this by pooling, in which the effect of these young, male, Tasmanians is partially determined by other cells that do have respondents.\nThere are many interesting aspects that we may like to communicate to others. For instance, we may like to show how the model is affecting the results. We can make a graph that compares the raw estimate with the model estimate.\n\npost_stratified_estimates |> \n  ggplot(aes(y = mean, x = forcats::fct_inorder(state), color = \"MRP estimate\")) + \n  geom_point() +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + \n  labs(y = \"Proportion ALP support\",\n       x = \"State\") + \n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\") +\n  theme(legend.title = element_blank()) +\n  coord_flip()\n\n\n\n\nSimilarly, we may like to plot the distribution of the coefficients. We can work out which coefficients to be pass to gather_draws with tidybayes::get_variables(model; in this example we passed ‘b_.’, but that will not always be the case.\n\n# tidybayes::get_variables(improved_alp_support)\n# improved_alp_support |>\n#   gather_draws(genderMale) |>\n#   ungroup() |>\n#   # mutate(coefficient = stringr::str_replace_all(.variable, c(\"b_\" = \"\"))) |>\n#   mutate(coefficient = forcats::fct_recode(coefficient,\n#                                            Intercept = \"Intercept\",\n#                                            `Is male` = \"genderMale\",\n#                                            `Age 30-44` = \"age_groupages30to44\",\n#                                            `Age 45-59` = \"age_groupages45to59\",\n#                                            `Age 60+` = \"age_groupages60plus\"\n#                                            )) |> \n# \n# # both |> \n#   ggplot(aes(y=fct_rev(coefficient), x = .value)) + \n#   ggridges::geom_density_ridges2(aes(height = ..density..),\n#                                  rel_min_height = 0.01, \n#                                  stat = \"density\",\n#                                  scale=1.5) +\n#   xlab(\"Distribution of estimate\") +\n#   ylab(\"Coefficient\") +\n#   scale_fill_brewer(name = \"Dataset: \", palette = \"Set1\") +\n#   theme_minimal() +\n#   theme(panel.grid.major = element_blank(),\n#         panel.grid.minor = element_blank()) +\n#   theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "16-mrp.html#forecasting-the-2020-us-election",
    "href": "16-mrp.html#forecasting-the-2020-us-election",
    "title": "16  Multilevel regression with post-stratification",
    "section": "16.5 Forecasting the 2020 US election",
    "text": "16.5 Forecasting the 2020 US election\nThe US election has a lot of features that are unique to the US, but the model that we are going to build here is going to be fairly generic and, largely a generalization of the earlier model for the Australian election. One good thing about forecasting the US election is that there is a lot of data around. In this case we can use survey data from the Democracy Fund Voter Study Group.1 They conducted polling in the lead-up to the US election and make this publicly available after registration. We will use the Integrated Public Use Microdata Series (IPUMS), to access the 2018 American Community Survey (ACS) as a post-stratification dataset. We will use state, age-group, gender, and education as explanatory variables.\n\n16.5.1 Survey data\nThe first step is that we need to actually get the survey data. Go to their website: https://www.voterstudygroup.org and then you are looking for ‘Nationscape’ and then a button along the lines of ‘Get the latest Nationscape data’. To get the dataset, you need to fill out a form, which they will process and then email you. There is a real person on the other side of this form, and so your request could take a few days.\nOnce you get access you will want to download the .dta files. Nationscape conducted many surveys and so there are many files. The filename is the reference date, and so ‘ns20200625’ refers to 25 June 2020, which is the one that we will use here.\nWe can read in ‘.dta’ files using the haven package (Wickham and Miller 2020). This code is based on that of Alen Mitrovski, Xiaoyan Yang, Matthew Wankiewicz: https://github.com/matthewwankiewicz/US_election_forecast.\n\nlibrary(haven)\nlibrary(tidyverse)\n\nraw_nationscape_data <- \n  read_dta(here::here(\"dont_push/ns20200625.dta\"))\n\n# The Stata format separates labels so reunite those\nraw_nationscape_data <- \n  labelled::to_factor(raw_nationscape_data)\n\n# Just keep relevant variables\nnationscape_data <- \n  raw_nationscape_data |> \n  select(vote_2020,\n         gender,\n         education,\n         state,\n         age)\n\n# For simplicity, remove anyone undecided or planning to vote for someone other than Biden/Trump and make vote a binary variable: 1 for Biden, 0 for Trump.\nnationscape_data <- \n  nationscape_data |> \n  filter(vote_2020 == \"Joe Biden\" | vote_2020 == \"Donald Trump\") |> \n  mutate(vote_biden = if_else(vote_2020 == \"Joe Biden\", 1, 0)) |> \n  select(-vote_2020)\n\n# Create the dependent variables by grouping the existing variables\nnationscape_data <- \n  nationscape_data |> \n  mutate(\n    age_group = case_when( # case_when works in order and exits when there's a match\n      age <= 29 ~ 'age_18-29',\n      age <= 44 ~ 'age_30-44',\n      age <= 59 ~ 'age_45-59',\n      age >= 60 ~ 'age_60_or_more',\n      TRUE ~ 'Trouble'\n      ),\n    gender = case_when(\n      gender == \"Female\" ~ 'female',\n      gender == \"Male\" ~ 'male',\n      TRUE ~ 'Trouble'\n      ),\n    education_level = case_when(\n      education == \"3rd Grade or less\" ~ \"High school or less\",\n      education == \"Middle School - Grades 4 - 8\" ~ \"High school or less\",\n      education == \"Completed some high school\" ~ \"High school or less\",\n      education == \"High school graduate\" ~ \"High school or less\",\n      education == \"Other post high school vocational training\" ~ \"Some post secondary\",\n      education == \"Completed some college, but no degree\" ~ \"Some post secondary\",\n      education == \"Associate Degree\" ~ \"Post secondary or higher\",\n      education == \"College Degree (such as B.A., B.S.)\" ~ \"Post secondary or higher\",\n      education == \"Completed some graduate, but no degree\" ~ \"Post secondary or higher\",\n      education == \"Masters degree\" ~ \"Graduate degree\",\n      education == \"Doctorate degree\" ~ \"Graduate degree\",\n      TRUE ~ 'Trouble'\n      )\n    ) |> \n  select(-education, -age)\n\ntests <- \n  nationscape_data |> \n  mutate(test = stringr::str_detect(age_group, 'Trouble'),\n         test = if_else(test == TRUE, TRUE, \n                        stringr::str_detect(education_level, 'Trouble')),\n         test = if_else(test == TRUE, TRUE, \n                        stringr::str_detect(gender, 'Trouble'))\n         ) |> \n  filter(test == TRUE)\n\nif(nrow(tests) != 0) {\n  print(\"Check nationscape_data\")\n  } else {\n    rm(tests)\n    }\n\nnationscape_data |> \n  head()\n\n# A tibble: 6 × 5\n  gender state vote_biden age_group      education_level         \n  <chr>  <chr>      <dbl> <chr>          <chr>                   \n1 female WI             0 age_45-59      Post secondary or higher\n2 female VA             0 age_45-59      Post secondary or higher\n3 female TX             0 age_60_or_more High school or less     \n4 female WA             0 age_45-59      High school or less     \n5 female MA             1 age_18-29      Some post secondary     \n6 female TX             1 age_30-44      Some post secondary     \n\n\nAs we have seen, one of the most difficult aspects with MRP is ensuring consistency between the datasets. In this case, we need to do some work to make the variables consistent.\n\n# This code is very directly from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz.\n# Format state names so the whole state name is written out, to match IPUMS data\nstates_names_and_abbrevs <- \n  tibble(stateicp = state.name, state = state.abb)\n\nnationscape_data <-\n  nationscape_data |>\n  left_join(states_names_and_abbrevs)\n\nJoining, by = \"state\"\n\nrm(states_names_and_abbrevs)\n\n# Make lowercase to match IPUMS data\nnationscape_data <- \n  nationscape_data |> \n  mutate(stateicp = tolower(stateicp))\n\n# Replace NAs with DC\nnationscape_data$stateicp <- \n  replace_na(nationscape_data$stateicp, \"district of columbia\")\n\n# Tidy the class\nnationscape_data <- \n  nationscape_data |> \n  mutate(across(c(gender, stateicp, education_level, age_group), as_factor))\n\n# Save data\nwrite_csv(nationscape_data, \"outputs/data/polling_data.csv\")\n\nnationscape_data |> \n  head()\n\n# A tibble: 6 × 6\n  gender state vote_biden age_group      education_level          stateicp     \n  <fct>  <chr>      <dbl> <fct>          <fct>                    <fct>        \n1 female WI             0 age_45-59      Post secondary or higher wisconsin    \n2 female VA             0 age_45-59      Post secondary or higher virginia     \n3 female TX             0 age_60_or_more High school or less      texas        \n4 female WA             0 age_45-59      High school or less      washington   \n5 female MA             1 age_18-29      Some post secondary      massachusetts\n6 female TX             1 age_30-44      Some post secondary      texas        \n\n\n\n\n16.5.2 Post-stratification data\nWe have a lot of options for a dataset to post-stratify by and there are various considerations. We are after a dataset that is better quality (however that is to be defined), and likely larger. From a strictly data perspective, the best choice would probably be something like the Cooperative Congressional Election Study (CCES), however for whatever reason that is only released after the election and so it is not a reasonable choice. Wang et al. (2015) use exit poll data, but again that is only available after the election.\nIn most countries we’d be stuck using the census, which is of course quite large, but likely out-of-date. Luckily in the US we have the opportunity to use the American Community Survey (ACS) which asks analogous questions to a census, is conducted every month, and over the course of a year, we end up with a few million responses. In this case we are going to access the ACS through IPUMS.\nTo do this go to the IPUMS website - https://ipums.org - and we are looking for something like IPUMS USA and then ‘get data’. Create an account, if you need to. That’ll take a while to process. But once you have an account, go to ‘Select Samples’ and de-select everything apart from the 2019 ACS. Then we need to get the variables that we are interested in. From the household we want ‘STATEICP’, then in person we want ‘SEX’, ‘AGE’, ‘EDUC’. Once everything is selected, ‘view cart’, and we want to be careful to change the ‘data format’ to ‘.dta’ (there’s nothing wrong with the other formats, but we have just already got code earlier to deal with that type). Briefly just check how many rows and columns you are requesting. It should be around a million rows, and around ten to twenty columns. If it is much more than 300MB then maybe just see if you have accidentally selected something that you don’t need. Submit the request and within a day, you should get an email saying that your data can be downloaded. It should only take 30 minutes or so, but if you don’t get an email within a day then check again the size of the dataset, and customize the sample size to reduce the size initially.\nIn any case let us tidy up the data.\n\n# Again, closely following code from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz.\nlibrary(haven)\nlibrary(tidyverse)\n\nraw_poststrat_data <- \n  read_dta(here::here(\"dont_push/usa_00004.dta\"))\n\n# The Stata format separates labels so reunite those\nraw_poststrat_data <- \n  labelled::to_factor(raw_poststrat_data)\nhead(raw_poststrat_data)\n\n# A tibble: 6 × 28\n  year  sample serial cbserial  hhwt cluster region stateicp strata gq    pernum\n  <fct> <fct>   <dbl>    <dbl> <dbl>   <dbl> <fct>  <fct>     <dbl> <fct>  <dbl>\n1 2018  2018 …      2  2.02e12 392.  2.02e12 east … alabama  190001 othe…      1\n2 2018  2018 …      7  2.02e12  94.1 2.02e12 east … alabama   40001 grou…      1\n3 2018  2018 …     13  2.02e12  83.7 2.02e12 east … alabama  130301 othe…      1\n4 2018  2018 …     18  2.02e12  57.5 2.02e12 east … alabama  100001 grou…      1\n5 2018  2018 …     23  2.02e12 157.  2.02e12 east … alabama  190001 grou…      1\n6 2018  2018 …     28  2.02e12 157.  2.02e12 east … alabama  220001 othe…      1\n# … with 17 more variables: perwt <dbl>, sex <fct>, age <fct>, marst <fct>,\n#   race <fct>, raced <fct>, hispan <fct>, hispand <fct>, bpl <fct>,\n#   bpld <fct>, citizen <fct>, educ <fct>, educd <fct>, empstat <fct>,\n#   empstatd <fct>, labforce <fct>, inctot <dbl>\n\nraw_poststrat_data$age <- as.numeric(raw_poststrat_data$age)\n\npoststrat_data <- \n  raw_poststrat_data |> \n  filter(inctot < 9999999) |> \n  filter(age >= 18) |> \n  mutate(gender = sex) |> \n  mutate(\n    age_group = case_when( # case_when works in order and exits when there's a match\n      age <= 29 ~ 'age_18-29',\n      age <= 44 ~ 'age_30-44',\n      age <= 59 ~ 'age_45-59',\n      age >= 60 ~ 'age_60_or_more',\n      TRUE ~ 'Trouble'\n      ),\n    education_level = case_when(\n      educd == \"nursery school, preschool\" ~ \"High school or less\",\n      educd == \"kindergarten\" ~ \"High school or less\",\n      educd == \"grade 1\" ~ \"High school or less\",\n      educd == \"grade 2\" ~ \"High school or less\",\n      educd == \"grade 3\" ~ \"High school or less\",\n      educd == \"grade 4\" ~ \"High school or less\",\n      educd == \"grade 5\" ~ \"High school or less\",\n      educd == \"grade 6\" ~ \"High school or less\",\n      educd == \"grade 7\" ~ \"High school or less\",\n      educd == \"grade 8\" ~ \"High school or less\",\n      educd == \"grade 9\" ~ \"High school or less\",\n      educd == \"grade 10\" ~ \"High school or less\",\n      educd == \"grade 11\" ~ \"High school or less\",\n      educd == \"12th grade, no diploma\" ~ \"High school or less\",\n      educd == \"regular high school diploma\" ~ \"High school or less\",\n      educd == \"ged or alternative credential\" ~ \"High school or less\",\n      educd == \"some college, but less than 1 year\" ~ \"Some post secondary\",\n      educd == \"1 or more years of college credit, no degree\" ~ \"Some post secondary\",\n      educd == \"associate's degree, type not specified\" ~ \"Post secondary or higher\",\n      educd == \"bachelor's degree\" ~ \"Post secondary or higher\",\n      educd == \"master's degree\" ~ \"Graduate degree\",\n      educd == \"professional degree beyond a bachelor's degree\" ~ \"Graduate degree\",\n      educd == \"doctoral degree\" ~ \"Graduate degree\",\n      educd == \"no schooling completed\" ~ \"High school or less\",\n      TRUE ~ 'Trouble'\n      )\n    )\n\n# Just keep relevant variables\npoststrat_data <- \n  poststrat_data |> \n  select(gender,\n         age_group,\n         education_level,\n         stateicp)\n\n# Tidy the class\npoststrat_data <- \n  poststrat_data |> \n  mutate(across(c(gender, stateicp, education_level, age_group), as_factor))\n\n# Save data\nwrite_csv(poststrat_data, \"outputs/data/us_poststrat.csv\")\n\npoststrat_data |> \n  head()\n\n# A tibble: 6 × 4\n  gender age_group      education_level     stateicp\n  <fct>  <fct>          <fct>               <fct>   \n1 female age_18-29      Some post secondary alabama \n2 female age_60_or_more Some post secondary alabama \n3 male   age_45-59      Some post secondary alabama \n4 male   age_30-44      High school or less alabama \n5 female age_60_or_more High school or less alabama \n6 male   age_30-44      High school or less alabama \n\n\nThis dataset is on an individual level. So we’ll create counts of each sub-cell, and then proportions by state.\n\npoststrat_data_cells <- \n  poststrat_data |> \n  group_by(stateicp, gender, age_group, education_level) |> \n  count()\n\nNow we would like to add proportions by state.\n\npoststrat_data_cells <- \n  poststrat_data_cells |> \n  group_by(stateicp) |> \n  mutate(prop = n/sum(n)) |> \n  ungroup()\n\npoststrat_data_cells |> head()\n\n# A tibble: 6 × 6\n  stateicp    gender age_group      education_level              n    prop\n  <fct>       <fct>  <fct>          <fct>                    <int>   <dbl>\n1 connecticut male   age_18-29      Some post secondary        149 0.0260 \n2 connecticut male   age_18-29      High school or less        232 0.0404 \n3 connecticut male   age_18-29      Post secondary or higher    96 0.0167 \n4 connecticut male   age_18-29      Graduate degree             25 0.00436\n5 connecticut male   age_60_or_more Some post secondary        142 0.0248 \n6 connecticut male   age_60_or_more High school or less        371 0.0647 \n\n\n\n\n16.5.3 Model\nWe are going to use logistic regression to estimate a model where the binary of support for Biden is explained by gender, age-group, education-level, and state. We are going to do this in a Bayesian framework using rstanarm (Goodrich et al. 2020). There are a variety of reasons for using rstanarm here, but the main one is that Stan is pre-compiled which eases some of the computer set-up issues that we may otherwise have. A great further resource about implementing MRP with rstanarm is Kennedy and Gabry (2020).\n\nlibrary(rstanarm)\n\nus_election_model <- \n  rstanarm::stan_glmer(vote_biden ~ gender + age_group + (1 | stateicp) + education_level,\n                       data = nationscape_data,\n                       family = binomial(link = \"logit\"),\n                       prior = normal(0, 1), \n                       prior_intercept = normal(0, 1),\n                       cores = 2, \n                       seed = 853)\n\nThere are a variety of options here that we have largely unthinkingly set, and exploring the effect of these would be a good idea, but for now we can just have a quick look at the model.\n\nmodelsummary::get_estimates(us_election_model)\n\n                                     term effect    estimate conf.level\n1                             (Intercept)  fixed  0.27591651       0.95\n2                              gendermale  fixed -0.54094841       0.95\n3                 age_groupage_60_or_more  fixed  0.04886803       0.95\n4                      age_groupage_18-29  fixed  0.87817445       0.95\n5                      age_groupage_30-44  fixed  0.12455081       0.95\n6      education_levelHigh school or less  fixed -0.35080948       0.95\n7      education_levelSome post secondary  fixed -0.14970941       0.95\n8          education_levelGraduate degree  fixed -0.21988079       0.95\n9 Sigma[stateicp:(Intercept),(Intercept)] random  0.08002654       0.95\n     conf.low    conf.high      pd rope.percentage      rhat      ess\n1  0.10322566  0.447878764 0.99850       0.1281242 1.0002399 2262.704\n2 -0.65227570 -0.430121742 1.00000       0.0000000 0.9996776 5264.121\n3 -0.10847716  0.200831286 0.72600       0.9765851 0.9998468 3726.472\n4  0.69830955  1.061899702 1.00000       0.0000000 0.9997188 3931.709\n5 -0.02530517  0.284994549 0.94125       0.7729545 0.9995061 3567.152\n6 -0.51262507 -0.204222132 1.00000       0.0000000 0.9997706 3700.321\n7 -0.29927232  0.006135831 0.96625       0.6771902 1.0005471 4090.628\n8 -0.38172115 -0.036764485 0.99100       0.3249145 0.9999286 4589.040\n9  0.02912381  0.160211514 1.00000       1.0000000 1.0031942 1284.436\n  prior.distribution prior.location prior.scale\n1             normal              0           1\n2             normal              0           1\n3             normal              0           1\n4             normal              0           1\n5             normal              0           1\n6             normal              0           1\n7             normal              0           1\n8             normal              0           1\n9               <NA>             NA          NA\n\n# The default usage of modelsummary requires statistics that we don't have.\n# Uncomment the following line if you want to look at what is available and specify your own:\n# modelsummary::get_estimates(us_election_model)\nmodelsummary::modelsummary(us_election_model,\n                            statistic = c('conf.low', 'conf.high')\n                            )\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    0.276 \n  \n  \n     \n    (0.103) \n  \n  \n     \n    (0.448) \n  \n  \n    gendermale \n    −0.541 \n  \n  \n     \n    (−0.652) \n  \n  \n     \n    (−0.430) \n  \n  \n    age_groupage_60_or_more \n    0.049 \n  \n  \n     \n    (−0.108) \n  \n  \n     \n    (0.201) \n  \n  \n    age_groupage_18-29 \n    0.878 \n  \n  \n     \n    (0.698) \n  \n  \n     \n    (1.062) \n  \n  \n    age_groupage_30-44 \n    0.125 \n  \n  \n     \n    (−0.025) \n  \n  \n     \n    (0.285) \n  \n  \n    education_levelHigh school or less \n    −0.351 \n  \n  \n     \n    (−0.513) \n  \n  \n     \n    (−0.204) \n  \n  \n    education_levelSome post secondary \n    −0.150 \n  \n  \n     \n    (−0.299) \n  \n  \n     \n    (0.006) \n  \n  \n    education_levelGraduate degree \n    −0.220 \n  \n  \n     \n    (−0.382) \n  \n  \n     \n    (−0.037) \n  \n  \n    Sigma[stateicp × (Intercept),(Intercept)] \n    0.080 \n  \n  \n     \n    (0.029) \n  \n  \n     \n    (0.160) \n  \n  \n    Num.Obs. \n    5200 \n  \n  \n    R2 \n    0.057 \n  \n  \n    R2 Marg. \n    0.045 \n  \n  \n    ELPD \n    −3468.0 \n  \n  \n    ELPD s.e. \n    16.4 \n  \n  \n    LOOIC \n    6936.1 \n  \n  \n    LOOIC s.e. \n    32.8 \n  \n  \n    WAIC \n    6936.0 \n  \n  \n    RMSE \n    0.48 \n  \n\n\n\n\n\n\n\n16.5.4 Post-stratify\n\nbiden_support_by_state <- \n  us_election_model |>\n  tidybayes::add_fitted_draws(newdata=poststrat_data_cells) |>\n  rename(support_biden_predict = .value) |> \n  mutate(support_biden_predict_prop = support_biden_predict*prop) |> \n  group_by(stateicp, .draw) |> \n  summarise(support_biden_predict = sum(support_biden_predict_prop)) |> \n  group_by(stateicp) |> \n  summarise(mean = mean(support_biden_predict), \n            lower = quantile(support_biden_predict, 0.025), \n            upper = quantile(support_biden_predict, 0.975))\n\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\nUse [add_]epred_draws() to get the expectation of the posterior predictive.\nUse [add_]linpred_draws() to get the distribution of the linear predictor.\nFor example, you used [add_]fitted_draws(..., scale = \"response\"), which\nmeans you most likely want [add_]epred_draws(...).\n\n\n`summarise()` has grouped output by 'stateicp'. You can override using the\n`.groups` argument.\n\n\nAnd we can have a look at our estimates, if we like.\n\nbiden_support_by_state |> \n  ggplot(aes(y = mean, x = stateicp, color = \"MRP estimate\")) + \n  geom_point() +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) + \n  geom_point(data = \n               nationscape_data |> \n               group_by(stateicp, vote_biden) |>\n               summarise(n = n()) |> \n               group_by(stateicp) |> \n               mutate(prop = n/sum(n)) |> \n               filter(vote_biden==1), \n             aes(y = prop, x = stateicp, color = 'Nationscape raw data')) +\n  geom_hline(yintercept = 0.5, linetype = 'dashed') +\n  labs(x = 'State',\n       y = 'Estimated proportion support for Biden',\n       color = 'Source') +\n  theme_classic() +\n  scale_color_brewer(palette = 'Set1') +\n  coord_flip()\n\n`summarise()` has grouped output by 'stateicp'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "16-mrp.html#exercises-and-tutorial",
    "href": "16-mrp.html#exercises-and-tutorial",
    "title": "16  Multilevel regression with post-stratification",
    "section": "16.6 Exercises and tutorial",
    "text": "16.6 Exercises and tutorial\n\n16.6.1 Exercises\n\nPlease explain what MRP is to someone who has university-education, but who has not necessarily taken any statistics, so you will need to explain any technical terms that you use, and be clear about strengths and weaknesses (write at least three paragraphs).\nWith respect to Wang et al. (2015): Why is this paper interesting? What do you like about this paper? What do you wish it did better? To what extent can you reproduce this paper? (Write at least four paragraphs).\nWith respect to Wang et al. (2015), what is not a feature they mention election forecasts need?\n\nExplainable.\nAccurate.\nCost-effective.\nRelevant.\nTimely.\n\nWith respect to Wang et al. (2015), what is a weakness of MRP?\n\nDetailed data requirement.\nAllows use of biased data.\nExpensive to conduct.\n\nWith respect to Wang et al. (2015), what is concerning about the Xbox sample?\n\nNon-representative.\nSmall sample size.\nMultiple responses from the same respondent.\n\nWe are interested in studying how voting intentions in the 2020 US presidential election vary by an individual’s income. We set up a logistic regression model to study this relationship. In this study, what are some possible independent variables (select all)?\n\nWhether the respondent is registered to vote (yes/no).\nWhether the respondent is going to vote for Biden (yes/no).\nThe race of the respondent (white/not white).\nThe respondent’s marital status (married/not).\n\nPlease think about Cohn (2016) Why is this type of exercise not carried out more? Why do you think that different groups, even with the same background and level of quantitative sophistication, could have such different estimates even when they use the same data? (Write at least four paragraphs).\nWhen we think about multilevel regression with post-stratification, what are the key assumptions that we are making? (Write at least four paragraphs).\nI train a model based on a survey, and then post-stratify it using the 2020 ACS dataset. What are some of the practical considerations that I may have to contend when I am doing this? (Write at least four paragraphs).\nIf I have a model output from lm() called ‘my_model_output’ how can I use modelsummary to display the output (assume the package has been loaded) (select all)?\n\nmodelsummary::modelsummary(my_model_output)\nmodelsummary(my_model_output)\nmy_model_output |> modelsummary()\nmy_model_output |> modelsummary(statistic = NULL)\n\nWhich of the following are examples of linear models (select all)?\n\nlm(y ~ x_1 + x_2 + x_3, data = my_data)\nlm(y ~ x_1 + x_2^2 + x_3, data = my_data)\nlm(y ~ x_1 * x_2 + x_3, data = my_data)\nlm(y ~ x_1 + x_1^2 + x_2 + x_3, data = my_data)\n\nConsider a situation in which you have a survey dataset with these age-groups: 18-29; 30-44; 45- 60; and 60+. And a post-stratification dataset with these age-groups: 18-24; 25-29; 30-34; 35-39; 40-44; 45-49; 50-54; 55-59; and 60+. What approach would you take to bringing these together? [Please write a paragraph.]\nConsider a situation in which you again have a survey dataset with these age-groups: 18-29; 30-44; 45- 60; and 60+. But this time the post-stratification dataset has these age-groups: 18-34; 35-49; 50-64; and 65+. What approach would you take to bringing these together? (Write at least one paragraph).\nPlease consider Kennedy et al. (2020). What are some statistical facets when considering a survey focused on gender, with a post-stratification survey that is not (select all)?\n\nImpute all non-male as female\nEstimate gender using auxiliary information\nImpute population\nImpute sample values\nModel population distribution using auxiliary data\nRemove all non-binary respondents\nRemove respondents\nAssume population distribution\n\nPlease consider Kennedy et al. (2020). What are some ethical facets when considering a survey focused on gender, with a post-stratification survey that is not (select all)?\n\nImpute all non-male as female\nEstimate gender using auxiliary information\nImpute population\nImpute sample values\nModel population distribution using auxiliary data\nRemove all non-binary respondents\nRemove respondents\nAssume population distribution\n\nPlease consider Kennedy et al. (2020). How do they define ethics?\n\nRespecting the perspectives and dignity of individual survey respondents.\nGenerating estimates of the general population and for subpopulations of interest.\nUsing more complicated procedures only when they serve some useful function.\n\n\n\n\n16.6.2 Tutorial\nIn a similar manner to Ghitza and Gelman (2020) pretend you have got access to a US voter file record from a private company. You train a model on the 2020 US CCES, and post-stratify it, on an individual-basis, based on that voter file. a. Could you please put-together a datasheet for the voter file dataset following Gebru et al. (2021)? As a reminder, datasheets accompany datasets and document ‘motivation, composition, collection process, recommended uses,’ among other aspects. b. Could you also please put together a model card for your model, following Mitchell et al. (2019)? As a reminder, model cards are deliberately straight-forward one- or two-page documents that report aspects such as: model details; intended use; metrics; training data; ethical considerations; as well as caveats and recommendations (Mitchell et al. 2019). c. Could you please discuss three ethical aspects around the features that you are using in your model? [Please write a paragraph or two for each point.] d. Could you please detail the protections that you would put in place in terms of the dataset, the model, and the predictions?\n\n\n16.6.3 Paper\nAt about this point, the Paper Five (Appendix @ref(paper-five)) would be appropriate.\n\n\n\n\n\nAlexander, Monica. 2019. “Analyzing Name Changes After Marriage Using a Non-Representative Survey,” August. https://www.monicaalexander.com/posts/2019-08-07-mrp/.\n\n\nArel-Bundock, Vincent. 2021. Modelsummary: Summary Tables and Plots for Statistical Models and Data: Beautiful, Customizable, and Publication-Ready. https://CRAN.R-project.org/package=modelsummary.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nCohn, Nate. 2016. We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021. “Datasheets for Datasets.” Communications of the ACM 64 (12): 86–92.\n\n\nGelman, Andrew. 2020. “Statistical Models of Election Outcomes.” YouTube, August. https://youtu.be/7gjDnrbLQ4k.\n\n\nGhitza, Yair, and Andrew Gelman. 2020. “Voter Registration Databases and MRP: Toward the Use of Large-Scale Databases in Public Opinion Research.” Political Analysis 28 (4): 507–31.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2020. “Rstanarm: Bayesian Applied Regression Modeling via Stan.” https://mc-stan.org/rstanarm.\n\n\nGreen, Eric. 2020. “Nivi Research: Mister p Helps Us Understand Vaccine Hesitancy.” https://research.nivi.io/posts/2020-12-08-mister-p-helps-us-understand-vaccine-hesitancy/.\n\n\nHanretty, Chris. 2020. “An Introduction to Multilevel Regression and Post-Stratification for Estimating Constituency Opinion.” Political Studies Review 18 (4): 630–45.\n\n\nKay, Matthew. 2020. tidybayes: Tidy Data and Geoms for Bayesian Models. https://doi.org/10.5281/zenodo.1308151.\n\n\nKennedy, Lauren, and Jonah Gabry. 2020. “MRP with Rstanarm.” https://mc-stan.org/rstanarm/articles/mrp.html.\n\n\nKennedy, Lauren, Katharine Khanna, Daniel Simpson, and Andrew Gelman. 2020. “Using Sex and Gender in Survey Adjustment.” https://arxiv.org/abs/2009.14401.\n\n\nLarmarange, Joseph. 2021. Labelled: Manipulating Labelled Data. https://CRAN.R-project.org/package=labelled.\n\n\nLauderdale, Benjamin E, Delia Bailey, Jack Blumenau, and Douglas Rivers. 2020. “Model-Based Pre-Election Polling for National and Sub-National Outcomes in the US and UK.” International Journal of Forecasting 36 (2): 399–413.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. “Model Cards for Model Reporting.” Proceedings of the Conference on Fairness, Accountability, and Transparency, January. https://doi.org/10.1145/3287560.3287596.\n\n\nSi, Yajuan. 2020. “On the Use of Auxiliary Variables in Multilevel Regression and Poststratification.” https://arxiv.org/abs/2011.00360.\n\n\nWang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman. 2015. “Forecasting Elections with Non-Representative Polls.” International Journal of Forecasting 31 (3): 980–91.\n\n\nWickham, Hadley. 2017. Tidyverse: Easily Install and Load the ’Tidyverse’. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Evan Miller. 2020. Haven: Import and Export ’SPSS’, ’Stata’ and ’SAS’ Files. https://CRAN.R-project.org/package=haven."
  },
  {
    "objectID": "17-text.html",
    "href": "17-text.html",
    "title": "17  Text as data",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "17-text.html#introduction",
    "href": "17-text.html#introduction",
    "title": "17  Text as data",
    "section": "17.1 Introduction",
    "text": "17.1 Introduction\nText can be considered an unwieldy, but in general similar, version of the datasets that we have used throughout this book. The main difference is that we will typically begin with very wide data, insofar as often each column is a word, or token more generally. Each entry is then often a count. We would then typically transform this into rather long data, with a column of the word and another column of the count.\nThe larger size of text datasets means that it is especially important to simulate, and start small, when it comes to their analysis. Using text as data is exciting because of the quantity and variety of text that is available to us. In general, dealing with text datasets is messy. There is a lot of cleaning and preparation that is typically required. Often text datasets are large. As such, having a workflow in place, in which you work in a reproducible way, simulating data first, and then clearly communicating your findings becomes critical, if only to keep everything organized in your own mind. Nonetheless, it is an exciting area.\nIn terms of next steps there are two, related, concerns: data and analysis.\nIn terms of data there are many places to get large amounts of text data relatively easily, from sources that we have already used, including:\n\nAccessing the Twitter API using rtweet (Kearney 2019).\nUsing the Inside Airbnb, which provides text from reviews.\nGetting the text of out-of-copyright books using gutenbergr (Robinson 2021).\nAnd finally, scraping Wikipedia.\n\nIn this chapter we first consider preparing text datasets. We then consider logistic and lasso regression. We finally consider topic models."
  },
  {
    "objectID": "17-text.html#tf-idf",
    "href": "17-text.html#tf-idf",
    "title": "17  Text as data",
    "section": "17.2 TF-IDF",
    "text": "17.2 TF-IDF\nInspired by Gelfand (2019) and following Amaka and Thomas (2021), we will draw on the dataset they put together of makeup names and descriptions from Sephora and Ulta. We are interested in the counts of each word. We can read in the data using read_csv().\n\nlibrary(tidyverse)\n\nmakeup <-\n  read_csv(file = \n             \"https://raw.githubusercontent.com/the-pudding/data/master/foundation-names/allNumbers.csv\")\n\nmakeup\n\n# A tibble: 3,117 × 9\n   brand        product name  specific lightness hex   lightToDark numbers    id\n   <chr>        <chr>   <chr> <chr>        <dbl> <chr> <lgl>         <dbl> <dbl>\n 1 Makeup Revo… Concea… <NA>  F0           0.949 #F2F… TRUE            0       1\n 2 HOURGLASS    Veil F… Porc… No. 0        0.818 #F6D… TRUE            0       2\n 3 TOM FORD     Tracel… Pearl 0.0          0.851 #F0D… TRUE            0       3\n 4 Armani Beau… Neo Nu… <NA>  0            0.912 #F0E… TRUE            0       4\n 5 TOM FORD     Tracel… Pearl 0.0          0.912 #FDE… TRUE            0       5\n 6 Charlotte T… Magic … <NA>  0            0.731 #D9A… TRUE            0       6\n 7 Bobbi Brown  Skin W… Porc… 0            0.822 #F3C… TRUE            0       7\n 8 Givenchy     Matiss… <NA>  N00          0.831 #F5D… TRUE            0       8\n 9 Smashbox     Studio… <NA>  0.1          0.814 #F8C… TRUE            0.1     9\n10 Smashbox     Studio… <NA>  0.1          0.910 #F9E… TRUE            0.1    10\n# … with 3,107 more rows\n\n\nWe will focus on ‘product’, which provides the name of the item, and ‘lightness’ which is a value between 0 and 1. We are interested in whether products with lightness values that are less than 0.5, typically use different words to those with lightness values that are at least 0.5.\n\nmakeup <-\n  makeup |>\n  select(product, lightness) |>\n  mutate(lightness_above_half = if_else(lightness >= 0.5, \"Yes\", \"No\")\n         )\n\ntable(makeup$lightness_above_half)\n\n\n  No  Yes \n 702 2415 \n\n\nIn this example we are going to split everything into separate words. When we do this, it is just searching for a space. And so, it will be the case that more than just words will be considered ‘words’, for instance, numbers. We use unnest_tokens() from tidytext (Silge and Robinson 2016) to do this.\n\nlibrary(tidytext)\n\nmakeup_by_words <-\n  makeup |>\n  unnest_tokens(output = word, \n                input = product, \n                token = \"words\")\n\nhead(makeup_by_words)\n\n# A tibble: 6 × 3\n  lightness lightness_above_half word      \n      <dbl> <chr>                <chr>     \n1     0.949 Yes                  conceal   \n2     0.949 Yes                  define    \n3     0.949 Yes                  full      \n4     0.949 Yes                  coverage  \n5     0.949 Yes                  foundation\n6     0.818 Yes                  veil      \n\n\nWe now want to count the number of times each word is used by each of the lightness classifications.\n\nmakeup_by_words <-\n  makeup_by_words |>\n  count(lightness_above_half, word, sort = TRUE)\n\nmakeup_by_words |>\n  filter(lightness_above_half == \"Yes\") |>\n  slice(1:5)\n\n# A tibble: 5 × 3\n  lightness_above_half word           n\n  <chr>                <chr>      <int>\n1 Yes                  foundation  2214\n2 Yes                  skin         452\n3 Yes                  spf          443\n4 Yes                  matte        422\n5 Yes                  powder       327\n\nmakeup_by_words |>\n  filter(lightness_above_half == \"No\") |>\n  slice(1:5)\n\n# A tibble: 5 × 3\n  lightness_above_half word           n\n  <chr>                <chr>      <int>\n1 No                   foundation   674\n2 No                   matte        106\n3 No                   spf          103\n4 No                   skin          98\n5 No                   liquid        89\n\n\nWe can see that the most popular words appear to be similar between the two categories. At this point, we could use the data in a variety of ways. We might be interested to know which words characterize each group—that is to say, which words are commonly used only in each group. We can do that by first looking at a word’s term frequency (tf), which is how many times a word is used in the product name. The issue is that there are a lot of words that are commonly used regardless of context. As such, we may also like to look at the inverse document frequency (idf) in which we ‘penalize’ words that occur in both groups. For instance, we have seen that ‘foundation’ occurs in both products with high and low lightness values. And so, its idf would be lower than another word which only occurred in products that did not have a lightness value above half. The term frequency–inverse document frequency (tf-idf) is then the product of these.\nWe can create this value using bind_tf_idf() from tidytext. It will create a bunch of new columns, one for each word and star combination.\n\nmakeup_by_words_tf_idf <-\n  makeup_by_words |>\n  bind_tf_idf(term = word, \n              document = lightness_above_half, \n              n = n) |>\n  arrange(-tf_idf)\n\nmakeup_by_words_tf_idf\n\n# A tibble: 505 × 6\n   lightness_above_half word        n       tf   idf   tf_idf\n   <chr>                <chr>   <int>    <dbl> <dbl>    <dbl>\n 1 Yes                  cushion    25 0.00187  0.693 0.00130 \n 2 Yes                  combo      16 0.00120  0.693 0.000831\n 3 Yes                  custom     16 0.00120  0.693 0.000831\n 4 Yes                  oily       16 0.00120  0.693 0.000831\n 5 Yes                  perfect    16 0.00120  0.693 0.000831\n 6 Yes                  refill     16 0.00120  0.693 0.000831\n 7 Yes                  50         14 0.00105  0.693 0.000727\n 8 Yes                  compact    13 0.000974 0.693 0.000675\n 9 Yes                  lifting    12 0.000899 0.693 0.000623\n10 Yes                  satte      12 0.000899 0.693 0.000623\n# … with 495 more rows\n\n\n\nmakeup_by_words_tf_idf |>\n  group_by(lightness_above_half) |>\n  slice(1:5)\n\n# A tibble: 10 × 6\n# Groups:   lightness_above_half [2]\n   lightness_above_half word            n       tf   idf   tf_idf\n   <chr>                <chr>       <int>    <dbl> <dbl>    <dbl>\n 1 No                   able            2 0.000525 0.693 0.000364\n 2 No                   concentrate     2 0.000525 0.693 0.000364\n 3 No                   marc            2 0.000525 0.693 0.000364\n 4 No                   re              2 0.000525 0.693 0.000364\n 5 No                   look            1 0.000263 0.693 0.000182\n 6 Yes                  cushion        25 0.00187  0.693 0.00130 \n 7 Yes                  combo          16 0.00120  0.693 0.000831\n 8 Yes                  custom         16 0.00120  0.693 0.000831\n 9 Yes                  oily           16 0.00120  0.693 0.000831\n10 Yes                  perfect        16 0.00120  0.693 0.000831"
  },
  {
    "objectID": "17-text.html#lasso-regression",
    "href": "17-text.html#lasso-regression",
    "title": "17  Text as data",
    "section": "17.3 Lasso regression",
    "text": "17.3 Lasso regression\nOne of the nice aspects of text is that we can adapt our existing methods to use it as an input. Here we are going to use logistic regression, along with text inputs, to forecast. Inspired by Silge (2018) we are going to have two different text inputs, train a model on a sample of text from each of them, and then try to use that model to forecast the text in a training set. Although this is a arbitrary example, we could imagine many real-world applications. For instance, we may be interested in whether some text was likely written by a bot or a human\nFirst we need to get some data. We use books from Project Gutenberg using gutenberg_download() from gutenbergr (Robinson 2021). We will consider Jane Eyre (Bronte 1847) and Alice’s Adventures in Wonderland (Carroll 1865).\n\nlibrary(gutenbergr)\nlibrary(tidyverse)\n\n# The books that we are interested in have the keys of 1260 and 11, respectively.\nalice_and_jane <- \n  gutenberg_download(\n    gutenberg_id = c(1260, 11), \n    meta_fields = \"title\")\n\nwrite_csv(alice_and_jane, \"alice_and_jane.csv\")\n\nhead(alice_and_jane)\n\nOne of the great things about this is that the dataset is a tibble. So we can work with all our familiar skills. Each line of the book is read in as a different row in the dataset. Notice that we have downloaded two books here at once, and so we added the title. The two books are one after each other.\nBy looking at the number of lines in each, it looks like Jane Eyre is much longer than Alice in Wonderland. We will start by getting rid of blank lines using remove_empty() from janitor (Firke 2020).\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nalice_and_jane <- \n  alice_and_jane |> \n  mutate(blank_line = if_else(text == \"\", 1, 0)) |> \n  filter(blank_line == 0) |> \n  select(-blank_line)\n\ntable(alice_and_jane$title)\n\n\nAlice's Adventures in Wonderland      Jane Eyre: An Autobiography \n                            2481                            16395 \n\n\nThere is still an overwhelming amount of Jane Eyre, compared with Alice in Wonderland, so we will sample from Jane Eyre to make it more equal.\n\nset.seed(853)\n\nalice_and_jane$rows <- c(1:nrow(alice_and_jane))\nsample_from_me <- alice_and_jane |> filter(title == \"Jane Eyre: An Autobiography\")\nkeep_me <- sample(x = sample_from_me$rows, size = 2481, replace = FALSE)\n\nalice_and_jane <- \n  alice_and_jane |> \n  filter(title == \"Alice's Adventures in Wonderland\" | rows %in% keep_me) |> \n  select(-rows)\n\ntable(alice_and_jane$title)\n\n\nAlice's Adventures in Wonderland      Jane Eyre: An Autobiography \n                            2481                             2481 \n\n\nThere are a variety of issues here, for instance, we have the whole of Alice, but we only have random bits of Jane, but nonetheless we will continue and add a counter with the line number for each book.\n\nalice_and_jane <- \n  alice_and_jane |> \n  group_by(title) |> \n  mutate(line_number = paste(gutenberg_id, row_number(), sep = \"_\")) |> \n  ungroup()\n\nWe now want to unnest the tokes. We will use unnest_tokens() from tidytext (Silge and Robinson 2016).\n\nlibrary(tidytext)\n\nalice_and_jane_by_word <- \n  alice_and_jane |> \n  unnest_tokens(word, text) |>\n  group_by(word) |>\n  filter(n() > 10) |>\n  ungroup()\n\nWe remove any word that was not used more than 10 times. Nonetheless we still have more than 500 unique words. (If we did not require that the word be used by the author at least 10 times then we end up with more than 6,000 words.)\nThe reason this is relevant is because these are our independent variables. So where we may be used to having something less than 10 explanatory variables, in this case we are going to have 585 As such, we need a model that can handle this.\nHowever, as mentioned before, we are going to have some rows that essentially just had one word. And so we filter for that also, which ensures that the model will have at least some words to work with.\n\nalice_and_jane_by_word <- \n  alice_and_jane_by_word |> \n  group_by(title, line_number) |> \n  mutate(number_of_words_in_line = n()) |> \n  ungroup() |> \n  filter(number_of_words_in_line > 2) |> \n  select(-number_of_words_in_line)\n\nWe’ll create a test/training split, and load in tidymodels.\n\nlibrary(tidymodels)\n\nset.seed(853)\n\nalice_and_jane_by_word_split <- \n  alice_and_jane_by_word |>\n  select(title, line_number) |> \n  distinct() |> \n  initial_split(prop = 3/4, strata = title)\n\nThen we can use cast_dtm() to create a document-term matrix. This provides a count of how many times each word is used.\n\nalice_and_jane_dtm_training <- \n  alice_and_jane_by_word |> \n  count(line_number, word) |> \n  inner_join(training(alice_and_jane_by_word_split) |> select(line_number)) |> \n  cast_dtm(term = word, document = line_number, value = n)\n\nJoining, by = \"line_number\"\n\ndim(alice_and_jane_dtm_training)\n\n[1] 3413  585\n\n\nSo we have our independent variables sorted, now we need our binary dependent variable, which is whether the book is Alice in Wonderland or Jane Eyre.\n\nresponse <- \n  data.frame(id = dimnames(alice_and_jane_dtm_training)[[1]]) |> \n  separate(id, into = c(\"book\", \"line\", sep = \"_\")) |> \n  mutate(is_alice = if_else(book == 11, 1, 0)) \n\nWarning: Expected 3 pieces. Missing pieces filled with `NA` in 3413 rows [1, 2,\n3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].\n\npredictor <- alice_and_jane_dtm_training[] |> as.matrix()\n\nNow we can run our model.\n\nlibrary(glmnet)\n\nmodel <- cv.glmnet(x = predictor,\n                   y = response$is_alice,\n                   family = \"binomial\",\n                   keep = TRUE\n                   )\n\nsave(model, file = \"alice_vs_jane.rda\")\n\n\n\n\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-3\n\nlibrary(broom)\n\ncoefs <- model$glmnet.fit |>\n  tidy() |>\n  filter(lambda == model$lambda.1se)\n\ncoefs |> head()\n\n# A tibble: 6 × 5\n  term         step estimate  lambda dev.ratio\n  <chr>       <dbl>    <dbl>   <dbl>     <dbl>\n1 (Intercept)    36 -0.335   0.00597     0.562\n2 in             36 -0.144   0.00597     0.562\n3 she            36  0.390   0.00597     0.562\n4 so             36  0.00249 0.00597     0.562\n5 a              36 -0.117   0.00597     0.562\n6 about          36  0.279   0.00597     0.562\n\n\n\ncoefs |>\n  group_by(estimate > 0) |>\n  top_n(10, abs(estimate)) |>\n  ungroup() |>\n  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = \"Coefficient\",\n       y = \"Word\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\nPerhaps unsurprisingly, if a line mentions Alice then it is likely to be a Alice in Wonderland and if it mention Jane then it is likely to be Jane Eyre."
  },
  {
    "objectID": "17-text.html#topic-models",
    "href": "17-text.html#topic-models",
    "title": "17  Text as data",
    "section": "17.4 Topic models",
    "text": "17.4 Topic models\nSometimes we have a statement and we want to know what it is about. Sometimes this will be easy, but we do not always have titles for statements, and even when we do, sometimes we do not have titles that define topics in a well-defined and consistent way. One way to get consistent estimates of the topics of each statement is to use topic models. While there are many variants, one way is to use the latent Dirichlet allocation (LDA) method of Blei, Ng, and Jordan (2003), as implemented by the R package ‘topicmodels’ by Grün and Hornik (2011).\nThe key assumption behind the LDA method is that each statement, ‘a document’, is made by a person who decides the topics they would like to talk about in that document, and then chooses words, ‘terms’, that are appropriate to those topics. A topic could be thought of as a collection of terms, and a document as a collection of topics. The topics are not specified ex ante; they are an outcome of the method. Terms are not necessarily unique to a particular topic, and a document could be about more than one topic. This provides more flexibility than other approaches such as a strict word count method. The goal is to have the words found in documents group themselves to define topics.\nLDA considers each statement to be a result of a process where a person first chooses the topics they want to speak about. After choosing the topics, the person then chooses appropriate words to use for each of those topics. More generally, the LDA topic model works by considering each document as having been generated by some probability distribution over topics. For instance, if there were five topics and two documents, then the first document may be comprised mostly of the first few topics; the other document may be mostly about the final few topics (Figure ?fig-topicsoverdocuments).\n\n\n\n\n\nFigure 17.1: Probability distributions over topics\n\n\n\n\n\n\n\nFigure 17.2: Probability distributions over topics\n\n\n\n\nSimilarly, each topic could be considered a probability distribution over terms. To choose the terms used in each document the speaker picks terms from each topic in the appropriate proportion. For instance, if there were ten terms, then one topic could be defined by giving more weight to terms related to immigration; and some other topic may give more weight to terms related to the economy (Figure ?fig-topicsoverterms).\n\n\n\n\n\nFigure 17.3: Probability distributions over terms\n\n\n\n\n\n\n\nFigure 17.4: Probability distributions over terms\n\n\n\n\nFollowing Blei and Lafferty (2009), Blei (2012) and Griffiths and Steyvers (2004), the process by which a document is generated is more formally considered to be:\n\nThere are \\(1, 2, \\dots, k, \\dots, K\\) topics and the vocabulary consists of \\(1, 2, \\dots, V\\) terms. For each topic, decide the terms that the topic uses by randomly drawing distributions over the terms. The distribution over the terms for the \\(k\\)th topic is \\(\\beta_k\\). Typically a topic would be a small number of terms and so the Dirichlet distribution with hyperparameter \\(0<\\eta<1\\) is used: \\(\\beta_k \\sim \\mbox{Dirichlet}(\\eta)\\).[^Dirichletfootnote] Strictly, \\(\\eta\\) is actually a vector of hyperparameters, one for each \\(K\\), but in practice they all tend to be the same value.\nDecide the topics that each document will cover by randomly drawing distributions over the \\(K\\) topics for each of the \\(1, 2, \\dots, d, \\dots, D\\) documents. The topic distributions for the \\(d\\)th document are \\(\\theta_d\\), and \\(\\theta_{d,k}\\) is the topic distribution for topic \\(k\\) in document \\(d\\). Again, the Dirichlet distribution with the hyperparameter \\(0<\\alpha<1\\) is used here because usually a document would only cover a handful of topics: \\(\\theta_d \\sim \\mbox{Dirichlet}(\\alpha)\\). Again, strictly \\(\\alpha\\) is vector of length \\(K\\) of hyperparameters, but in practice each is usually the same value.\nIf there are \\(1, 2, \\dots, n, \\dots, N\\) terms in the \\(d\\)th document, then to choose the \\(n\\)th term, \\(w_{d, n}\\):\n\nRandomly choose a topic for that term \\(n\\), in that document \\(d\\), \\(z_{d,n}\\), from the multinomial distribution over topics in that document, \\(z_{d,n} \\sim \\mbox{Multinomial}(\\theta_d)\\).\nRandomly choose a term from the relevant multinomial distribution over the terms for that topic, \\(w_{d,n} \\sim \\mbox{Multinomial}(\\beta_{z_{d,n}})\\).\n\n\nThe Dirichlet distribution is a variation of the beta distribution that is commonly used as a prior for categorical and multinomial variables. If there are just two categories, then the Dirichlet and the beta distributions are the same. In the special case of a symmetric Dirichlet distribution, \\(\\eta=1\\), it is equivalent to a uniform distribution. If \\(\\eta<1\\), then the distribution is sparse and concentrated on a smaller number of the values, and this number decreases as \\(\\eta\\) decreases. A hyperparameter is a parameter of a prior distribution.\nGiven this set-up, the joint distribution for the variables is (Blei (2012), p.6): \\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \\prod^{K}_{i=1}p(\\beta_i) \\prod^{D}_{d=1}p(\\theta_d) \\left(\\prod^N_{n=1}p(z_{d,n}|\\theta_d)p\\left(w_{d,n}|\\beta_{1:K},z_{d,n}\\right) \\right).\\]\nBased on this document generation process the analysis problem, discussed in the next section, is to compute a posterior over \\(\\beta_{1:K}\\) and \\(\\theta_{1:D}\\), given \\(w_{1:D, 1:N}\\). This is intractable directly, but can be approximated (Griffiths and Steyvers (2004) and Blei (2012)).\nAfter the documents are created, they are all that we have to analyze. The term usage in each document, \\(w_{1:D, 1:N}\\), is observed, but the topics are hidden, or ‘latent’. We do not know the topics of each document, nor how terms defined the topics. That is, we do not know the probability distributions of Figures ?fig-topicsoverdocuments or ?fig-topicsoverterms. In a sense we are trying to reverse the document generation process – we have the terms and we would like to discover the topics.\nIf the earlier process around how the documents were generated is assumed and we observe the terms in each document, then we can obtain estimates of the topics (Steyvers and Griffiths (2006)). The outcomes of the LDA process are probability distributions and these define the topics. Each term will be given a probability of being a member of a particular topic, and each document will be given a probability of being about a particular topic. That is, we are trying to calculate the posterior distribution of the topics given the terms observed in each document (Blei (2012), p.7): \\[p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \\frac{p\\left(\\beta_{1:K}, \\theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\\right)}{p(w_{1:D, 1:N})}.\\]\nThe initial practical step when implementing LDA given a corpus of documents is to remove ‘stop words’. These are words that are common, but that do not typically help to define topics. There is a general list of stop words such as: “a”; “a’s”; “able”; “about”; “above”… We also remove punctuation and capitalization. The documents need to then be transformed into a document-term-matrix. This is essentially a table with a column of the number of times each term appears in each document.\nAfter the dataset is ready, the R package ‘topicmodels’ by Grün and Hornik (2011) can be used to implement LDA and approximate the posterior. It does this using Gibbs sampling or the variational expectation-maximization algorithm. Following Steyvers and Griffiths (2006) and Darling (2011), the Gibbs sampling process attempts to find a topic for a particular term in a particular document, given the topics of all other terms for all other documents. Broadly, it does this by first assigning every term in every document to a random topic, specified by Dirichlet priors with \\(\\alpha = \\frac{50}{K}\\) and \\(\\eta = 0.1\\) (Steyvers and Griffiths (2006) recommends \\(\\eta = 0.01\\)), where \\(\\alpha\\) refers to the distribution over topics and \\(\\eta\\) refers to the distribution over terms (Grün and Hornik (2011), p.7). It then selects a particular term in a particular document and assigns it to a new topic based on the conditional distribution where the topics for all other terms in all documents are taken as given (Grün and Hornik (2011), p.6): \\[p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \\propto \\frac{\\lambda'_{n\\rightarrow k}+\\eta}{\\lambda'_{.\\rightarrow k}+V\\eta} \\frac{\\lambda'^{(d)}_{n\\rightarrow k}+\\alpha}{\\lambda'^{(d)}_{-i}+K\\alpha} \\] where \\(z'_{d, n}\\) refers to all other topic assignments; \\(\\lambda'_{n\\rightarrow k}\\) is a count of how many other times that term has been assigned to topic \\(k\\); \\(\\lambda'_{.\\rightarrow k}\\) is a count of how many other times that any term has been assigned to topic \\(k\\); \\(\\lambda'^{(d)}_{n\\rightarrow k}\\) is a count of how many other times that term has been assigned to topic \\(k\\) in that particular document; and \\(\\lambda'^{(d)}_{-i}\\) is a count of how many other times that term has been assigned in that document. Once \\(z_{d,n}\\) has been estimated, then estimates for the distribution of words into topics and topics into documents can be backed out.\nThis conditional distribution assigns topics depending on how often a term has been assigned to that topic previously, and how common the topic is in that document (Steyvers and Griffiths (2006)). The initial random allocation of topics means that the results of early passes through the corpus of document are poor, but given enough time the algorithm converges to an appropriate estimate.\nThe choice of the number of topics, k, affects the results, and must be specified a priori. If there is a strong reason for a particular number, then this can be used. Otherwise, one way to choose an appropriate number is to use a test and training set process. Essentially, this means running the process on a variety of possible values for k and then picking an appropriate value that performs well.\nOne weakness of the LDA method is that it considers a ‘bag of words’ where the order of those words does not matter (Blei (2012)). It is possible to extend the model to reduce the impact of the bag-of-words assumption and add conditionality to word order. Additionally, alternatives to the Dirichlet distribution can be used to extend the model to allow for correlation. For instance, in Hansard topics related the army may be expected to be more commonly found with topics related to the navy, but less commonly with topics related to banking."
  },
  {
    "objectID": "17-text.html#exercises-and-tutorial",
    "href": "17-text.html#exercises-and-tutorial",
    "title": "17  Text as data",
    "section": "17.5 Exercises and tutorial",
    "text": "17.5 Exercises and tutorial\n\n17.5.1 Exercises\n\n\n17.5.2 Tutorial\nConsider a situation where we run a news website and are trying to understand whether to allow anonymous comments. We decide to do an A/B test, where we will keep everything the same, but only allow anonymous comments on one version of the site. Please simulate some text data that we may obtain from our test. And then build a model that could test whether there is a difference between the situations.\n\n\n\n\n\nAmaka, Ofunne, and Amber Thomas. 2021. The Naked Truth: How the Names of 6,816 Complexion Products Can Reveal Bias in Beauty. https://pudding.cool/2021/03/foundation-names/.\n\n\nBlei, David M. 2012. “Probabilistic Topic Models.” Communications of the ACM 55 (4): 77–84.\n\n\nBlei, David M, and John D Lafferty. 2009. “Topic Models.” In Text Mining, 101–24. Chapman; Hall/CRC.\n\n\nBlei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3 (Jan): 993–1022.\n\n\nBronte, Charlotte. 1847. Jane Eyre. https://www.gutenberg.org/files/1260/1260-h/1260-h.htm.\n\n\nCarroll, Lewis. 1865. Alice’s Adventures in Wonderland. Macmillan.\n\n\nDarling, William M. 2011. “A Theoretical and Practical Implementation Tutorial on Topic Modeling and Gibbs Sampling.” In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 642–47.\n\n\nFirke, Sam. 2020. Janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGelfand, Sharla. 2019. Crying @ Sephora. https://sharla.party/post/crying-sephora/.\n\n\nGriffiths, Thomas, and Mark Steyvers. 2004. “Finding Scientific Topics.” PNAS 101: 5228–35.\n\n\nGrün, Bettina, and Kurt Hornik. 2011. “topicmodels: An R Package for Fitting Topic Models.” Journal of Statistical Software 40 (13): 1–30. https://doi.org/10.18637/jss.v040.i13.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in r. Chapman; Hall/CRC.\n\n\nKearney, Michael W. 2019. “Rtweet: Collecting and Analyzing Twitter Data.” Journal of Open Source Software 4 (42): 1829. https://doi.org/10.21105/joss.01829.\n\n\nRobinson, David. 2021. Gutenbergr: Download and Process Public Domain Works from Project Gutenberg. https://CRAN.R-project.org/package=gutenbergr.\n\n\nSilge, Julia. 2018. Text Classification with Tidy Data Principles. https://juliasilge.com/blog/tidy-text-classification/.\n\n\nSilge, Julia, and David Robinson. 2016. “Tidytext: Text Mining and Analysis Using Tidy Data Principles in r.” JOSS 1 (3). https://doi.org/10.21105/joss.00037.\n\n\nSteyvers, Mark, and Tom Griffiths. 2006. “Probabilistic Topic Models.” In Latent Semantic Analysis: A Road to Meaning, edited by T. Landauer, D McNamara, S. Dennis, and W. Kintsch."
  },
  {
    "objectID": "18-deploy.html",
    "href": "18-deploy.html",
    "title": "18  Deploying models",
    "section": "",
    "text": "Required reading\nKey concepts and skills\nKey libraries"
  },
  {
    "objectID": "18-deploy.html#introduction",
    "href": "18-deploy.html#introduction",
    "title": "18  Deploying models",
    "section": "18.1 Introduction",
    "text": "18.1 Introduction\nHaving done the work to develop a dataset and explore it with a model that we are confident can be used, we may wish to enable this to be used more widely thank just our own computer. There are a variety of ways of doing this, including:\n\nusing the cloud,\ncreating R packages,\nmaking shiny applications, and\nusing plumber to create an API.\n\nThe general idea here is that we need to know, and allow others to come to trust, the whole workflow. That is what our approach to this point brings. After this, then we may like to use our model more broadly. Say we been able to scrape some data from a website, bring some order to that chaos, make some charts, appropriately model it, and write this all up. In most academic settings that is more than enough. But in many industry settings we would like to use the model to do something. For instance, setting up a website that allows a model to be used to generate an insurance quote given several inputs.\nIn this chapter, we begin by moving our compute from our local computer to the cloud. We then describe the use of R packages and Shiny for sharing models. That works well, but in some settings other users may like to interact with our model in ways that we are not focused on. One way to allow this is to make our results available to other computers, and for that we will want to make an APIs. Hence, we introduce plumber (Schloerke and Allen 2021), which is a way of creating APIs."
  },
  {
    "objectID": "18-deploy.html#amazon-web-services",
    "href": "18-deploy.html#amazon-web-services",
    "title": "18  Deploying models",
    "section": "18.2 Amazon Web Services",
    "text": "18.2 Amazon Web Services\nThe apocryphal quote is that the cloud is another name for someone else’s computer. And while that is true to various degrees, for our purposes that is enough. Learning to use someone else’s computer can be great for a number of reasons including:\n\nScalability: It can be quite expensive to buy a new computer, especially if we only need it to run something every now and then, but by using someone else’s computer, we can just rent for a few hours or days. This allows use to amortize this cost and work out what we actually need before committing to a purchase. It also allows us to easily increase or decrease the compute scale if we suddenly have a substantial increase in demand.\nPortability: If we can shift our analysis workflow from a local machine to the cloud, then that suggests that there is we are likely doing good things in terms of reproducibility and portability. At the very least, code can run both locally and on the cloud, which is a big step in terms of reproducibility.\nSet-and-forget: If we are doing something that will take a while, then it can be great to not have to worry about our own computer needing to run overnight, or, say, not being able to watch Netflix on that same computer. Additionally, on many cloud options, open-source statistical software, such as R and Python, is either already available, or relatively easy to set-up.\n\nThat said, there are downsides, including:\n\nCost: While most cloud options are cheap, they are rarely free. To provide an idea of cost, using a well-featured AWS instance for a few days, may end up being a few dollars. It is also easy to accidentally forget about something, and generate unexpectedly large bills, especially initially.\nPublic: It can be easy to make mistakes and accidentally make everything public.\nTime: It takes time to get set-up and comfortable on the cloud.\n\nWhen we use the cloud, we are typically running code on a ‘virtual machine’ (VM). This is an allocation that is part of a larger collection of computers that has been designed to act like a computer with specific features. For instance, we may specify that our virtual machine has, say, 8 GB RAM, 128 storage, and 4 CPUs. The VM would then act like a computer with those specifications. The cost to use cloud options increases based on the specifications of the virtual machine.\nIn a sense, we started with a cloud option, through our initial recommendation, in Chapter @ref(drinking-from-a-fire-hose) of using R Studio Cloud, before we moved to our local machine in Chapter @ref(r-essentials). That cloud option was specifically designed for beginners. We will now introduce  a more general cloud option: Amazon Web Services (AWS). Often a particular business will use a particular cloud option, such as Google, AWS, or Azure, but developing familiarity with one will make the use of the others easier.\n\n\n\n\n\n\nAmazon Web Services is a cloud service from Amazon. To get started we need to create an AWS Developer account here (Figure Figure 18.1).\n\n\n\nFigure 18.1: AWS Developer website\n\n\nAfter we have created an account, we need to select a region where the computer that we will access is located. After this, we want to “Launch a virtual machine” with EC2 (Figure Figure 18.2).\n\n\n\nFigure 18.2: AWS Developer console\n\n\nThe first step is to choose an Amazon Machine Image (AMI). This provides the details of the computer that you will be using. For instance, a local computer may be a MacBook running Monterey. Louis Aslett provides AMIs that are already set-up with R Studio and much else here. We can either search for the AMI of the region that we registered for, or click on the relevant link on Aslett’s website. For instance, to use the AMI set-up for the Canadian central region we search for ‘ami-0bdd24fd36f07b638’. The benefit of using these AMIs is that they are set-up specifically for R Studio, but the trade-off is that they are a little outdated, as they were compiled in August 2020.\nIn the next step we can choose how powerful the computer will be. The free tier is basic computer, but we can choose better ones when we need them. At this point we can pretty much just launch the instance (Figure Figure 18.3). If we start using AWS more seriously we could go back and select different options, especially around the security of the account. AWS relies on key pairs. And so we will need to create a PEM and save it locally (Figure Figure 18.4). We can then launch the instance.\n\n\n\nFigure 18.3: AWS Developer launch instance\n\n\n\n\n\nFigure 18.4: AWS Developer establishing a key-pair\n\n\nAfter a few minutes, the instance will be running. We can use it by pasting the ‘public DNS’ into a browser. The username is ‘rstudio’ and the password is the instance ID.\nWe should have R Studio running, which is exciting. The first thing to do is probably to change the default password using the instructions in the instance.\nWe do not need to install, say, tidyverse, instead we can just call the library and keep going. This is because this AMI comes with many packages already installed. We can see the list of packages that are installed with installed.packages(). For instance, rstan is already installed, and we could set-up an instance with GPUs if we needed.\nPerhaps as important as being able to start an AWS instance is being able to stop it (so that we do not get billed). The free tier is pretty great, but we do need to turn it off. To stop an instance, in the AWS instances page, select it, then ‘Actions -> Instance State -> Terminate’."
  },
  {
    "objectID": "18-deploy.html#plumber-and-model-apis",
    "href": "18-deploy.html#plumber-and-model-apis",
    "title": "18  Deploying models",
    "section": "18.3 Plumber and model APIs",
    "text": "18.3 Plumber and model APIs\nThe general idea behind the plumber package (Schloerke and Allen 2021) is that we can train a model and make it available via an API that we can call when we want a forecast. It is pretty great.\nJust to get something working, let us make a function that returns ‘Hello Toronto’ regardless of the output. Open a new R file, add the following, and then save it as ‘plumber.R’ (you may need to install the plumber package if you’ve not done that yet).\n\nlibrary(plumber)\n\n#* @get /print_toronto\nprint_toronto <- function() {\n  result <- \"Hello Toronto\"\n  return(result)\n}\n\nAfter that is saved, in the top right of the editor you should get a button to ‘Run API’. Click that, and your API should load. It will be a ‘Swagger’ application, which provides a GUI around our API. Expand the GET method, and then click ‘Try it out’ and ‘Execute’. In the response body, you should get ‘Toronto’.\nTo more closely reflect the fact that this is an API designed for computers, you can copy/paste the ‘request HTML’ into a browser and it should return ‘Hello Toronto’.\n\n18.3.1 Local model\nNow, we are going to update the API so that it serves a model output, given some input. We are going to follow Buhr (2017) fairly closely.\nAt this point, we should start a new R Project. To get started, let us simulate some data and then train a model on it. In this case we are interested in forecasting how long a baby may sleep overnight, given we know how long they slept during their afternoon nap.\n\nlibrary(tidyverse)\nset.seed(853)\n\nnumber_of_observations <- 1000\n\nbaby_sleep <- \n  tibble(afternoon_nap_length = rnorm(number_of_observations, 120, 5) |> abs(),\n         noise = rnorm(number_of_observations, 0, 120),\n         night_sleep_length = afternoon_nap_length * 4 + noise,\n         )\n\nbaby_sleep |> \n  ggplot(aes(x = afternoon_nap_length, y = night_sleep_length)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Baby's afternoon nap length (minutes)\",\n       y = \"Baby's overnight sleep length (minutes)\") +\n  theme_classic()\n\nLet us now use tidymodels to quickly make a dodgy model.\n\nset.seed(853)\nlibrary(tidymodels)\n\nbaby_sleep_split <- rsample::initial_split(baby_sleep, prop = 0.80)\nbaby_sleep_train <- rsample::training(baby_sleep_split)\nbaby_sleep_test <- rsample::testing(baby_sleep_split)\n\nmodel <- \n  parsnip::linear_reg() |>\n  parsnip::set_engine(engine = \"lm\") |> \n  parsnip::fit(night_sleep_length ~ afternoon_nap_length, \n               data = baby_sleep_train\n               )\n\nwrite_rds(x = model, file = \"baby_sleep.rds\")\n\nAt this point, we have a model. One difference from what you might be used to is that we have saved the model as an ‘.rds’ file. We are going to read that in.\nNow that we have our model we want to put that into a file that we will use the API to access, again called ‘plumber.R’. And we also want a file that sets up the API, called ‘server.R’. So make an R script called ‘server.R’ and add the following content:\n\nlibrary(plumber)\n\nserve_model <- plumb(\"plumber.R\")\nserve_model$run(port = 8000)\n\nThen in ‘plumber.R’ add the following content:\n\nlibrary(plumber)\nlibrary(tidyverse)\n\nmodel <- readRDS(\"baby_sleep.rds\")\n\nversion_number <- \"0.0.1\"\n\nvariables <- \n  list(\n    afternoon_nap_length = \"A value in minutes, likely between 0 and 240.\",\n    night_sleep_length = \"A forecast, in minutes, likely between 0 and 1000.\"\n  )\n\n#* @param afternoon_nap_length\n#* @get /survival\npredict_sleep <- function(afternoon_nap_length=0) {\n  afternoon_nap_length = as.integer(afternoon_nap_length)\n  \n  payload <- data.frame(afternoon_nap_length=afternoon_nap_length)\n  \n  prediction <- predict(model, payload)\n\n  result <- list(\n    input = list(payload),\n    response = list(\"estimated_night_sleep\" = prediction),\n    status = 200,\n    model_version = version_number)\n\n  return(result)\n}\n\nAgain, after we save the ‘plumber.R’ file we should have an option to ‘Run API’. Click that and you can try out the API locally in the same way as before.\n\n\n18.3.2 Cloud model\nTo this point, we have got an API working on our own machine, but what we really want to do is to get it working on a computer such that the API can be accessed by anyone. To do this we are going to use DigitalOcean. It is a charged service, but when you create an account, it will come with $100 in credit, which will be enough to get started.\nThis set-up process will take some time, but we only need to do it once. Install two additional packages that will assist here are plumberDeploy (Allen 2021) and analogsea (Chamberlain, Wickham, and Chang 2021).\n\ninstall.packages(\"plumberDeploy\")\nremotes::install_github(\"sckott/analogsea\")\n\nNow we need to connect the local computer with the DigitalOcean account.\n\nanalogsea::account()\n\nNow we need to authenticate the connection, and this is done using a SSH public key.\n\nanalogsea::key_create()\n\nWhat you want is to have a ‘.pub’ file on our computer. Then copy the public key aspect in that file, and add it to the SSH keys section in the account security settings. When we have the key on our local computer, then we can check this.\n\nssh::ssh_key_info()\n\nAgain, this will all take a while to validate. DigitalOcean calls every computer that we start a ‘droplet’. So if we start three computers, then we will have started three droplets. We can check the droplets that are running.\n\nanalogsea::droplets()\n\nIf everything is set-up properly, then this will print the information about all droplets that you have associated with the account (which at this point, is probably none). We first create a droplet.\n\nid <- plumberDeploy::do_provision(example = FALSE)\n\nThen we get asked for the SSH passphrase and then it will just set-up a bunch of things. After this we are going to need to install a whole bunch of things onto our droplet.\n\nanalogsea::install_r_package(droplet = id, c(\"plumber\", \n                                             \"remotes\", \n                                             \"here\"))\nanalogsea::debian_apt_get_install(id, \"libssl-dev\", \n                                  \"libsodium-dev\", \n                                  \"libcurl4-openssl-dev\")\nanalogsea::debian_apt_get_install(id, \n                                  \"libxml2-dev\")\n\nanalogsea::install_r_package(id, c(\"config\",\n                                   \"httr\",\n                                   \"urltools\",\n                                   \"plumber\"))\n\nanalogsea::install_r_package(id, c(\"xml2\"))\nanalogsea::install_r_package(id, c(\"tidyverse\"))\n\nanalogsea::install_r_package(id, c(\"tidymodels\"))\n\nAnd then when that is finally set-up (it will take 30 minutes or so) we can deploy our API.\n\nplumberDeploy::do_deploy_api(droplet = id, \n                             path = \"example\", \n                             localPath = getwd(), \n                             port = 8000, \n                             docs = TRUE, \n                             overwrite=TRUE)"
  },
  {
    "objectID": "18-deploy.html#exercises-and-tutorial",
    "href": "18-deploy.html#exercises-and-tutorial",
    "title": "18  Deploying models",
    "section": "18.4 Exercises and tutorial",
    "text": "18.4 Exercises and tutorial\n\n18.4.1 Exercises\n\n\n18.4.2 Tutorial\n\n\n\n\n\nAllen, Jeff. 2021. plumberDeploy: Plumber Deployment. https://CRAN.R-project.org/package=plumberDeploy.\n\n\nBlair, James. 2019. Democratizing r with Plumber APIs. https://www.rstudio.com/resources/rstudioconf-2019/democratizing-r-with-plumber-apis/.\n\n\nBuhr, Ray. 2017. Using r as a Production Machine Learning Language (Part i). https://raybuhr.github.io/blog/posts/making-predictions-over-http/.\n\n\nChamberlain, Scott, Hadley Wickham, and Winston Chang. 2021. Analogsea: Interface to ’Digital Ocean’.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2021. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny.\n\n\nHuyen, Chip. 2020. Machine Learning Is Going Real-Time. https://huyenchip.com/2020/12/27/real-time-machine-learning.html.\n\n\n———. 2022. Real-Time Machine Learning: Challenges and Solutions. https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html.\n\n\nSchloerke, Barret, and Jeff Allen. 2021. Plumber: An API Generator for r. https://CRAN.R-project.org/package=plumber."
  },
  {
    "objectID": "19-efficiency.html",
    "href": "19-efficiency.html",
    "title": "19  Efficiency",
    "section": "",
    "text": "Required material\nKey concepts and skills\nKey libraries\nKey functions"
  },
  {
    "objectID": "19-efficiency.html#introduction",
    "href": "19-efficiency.html#introduction",
    "title": "19  Efficiency",
    "section": "19.1 Introduction",
    "text": "19.1 Introduction\nFor much of this book we have been largely concerned with just getting something done. Not necessarily getting it done in the best or most efficient way. To a large extent, being worried about getting something done in the best or most efficient way is almost always a waste of time. Until it is not. Eventually inefficient ways of storing data, ugly or slow code, and an insistence on using R do have an effect. And it is at that point that we need to be open to new approaches to ensure efficiency.\nIn this chapter we briefly cover ways to be more efficient with data, by using SQL, feather and parquet. We then discuss code efficiency, particularly, the need to measure and refactor code. Then we discuss experimental efficiency, in particular, the multi-armed bandit which enables us to more quickly test different effects. Finally, we briefly introduce other languages, such as Python and Julia, that have an important role to play in data science."
  },
  {
    "objectID": "19-efficiency.html#code-efficiency",
    "href": "19-efficiency.html#code-efficiency",
    "title": "19  Efficiency",
    "section": "19.2 Code efficiency",
    "text": "19.2 Code efficiency\nBy and large, worrying about performance is a waste of time. For the most part we are better off just pushing things into the cloud, letting them run for a reasonable time, and using that time to worry about other aspects of the pipeline. But, eventually this becomes unfeasible. For instance, if something takes more than a day, then it becomes a pain because of the need to completely switch tasks and then return to it. There is rarely a most common area for obvious performance gains. Instead it is important to develop the ability to measure and then refactor code.\nBeing fast is valuable but it is mostly about being able to iterate fast not necessarily that the code runs fast. The first thing we should do if we find that the speed at which code is running is becoming a bottle neck is to shard. Then we should throw more machines at it. But eventually we should go back and refactor the code.\nTo refactor code means to re-write it so that the new code achieves the same outcome as the old code, it is just that the new code does it better. We can use tic() and toc() from tictoc (Izrailev 2014) to time various aspects of our code and find where the largest delays are.\n\nlibrary(tictoc)\ntic(\"First bit of code\")\nprint(\"Fast code\")\n\n[1] \"Fast code\"\n\ntoc()\n\nFirst bit of code: 0.002 sec elapsed\n\ntic(\"Second bit of code\")\nSys.sleep(3)\nprint(\"Slow code\")\n\n[1] \"Slow code\"\n\ntoc()\n\nSecond bit of code: 3.003 sec elapsed\n\n\nAnd so we know that there is something slowing down the code; which in this artificial case is Sys.sleep() causing a delay of 3 seconds.\nWhen we start to refactor our code, we want to make sure that the re-written code achieves the same outcomes as the original code. This means that it is important to have tests written. We generally want to reduce the size of functions, by breaking them into smaller ones."
  },
  {
    "objectID": "19-efficiency.html#data-efficiency",
    "href": "19-efficiency.html#data-efficiency",
    "title": "19  Efficiency",
    "section": "19.3 Data efficiency",
    "text": "19.3 Data efficiency\n\n19.3.1 SQL\nWhile it may be true that the SQL is never as good as the original, SQL is a popular way of working with data. Advanced users probably do a lot with it alone, but even just having a working knowledge of SQL increases the number of datasets that we can access. We can use SQL within RStudio\nSQL is a straightforward variant of the dplyr verbs that we have used throughout this book. Having used mutate(), filter() and left_join() in the tidyverse means that much of the core commands will be familiar. That means that the main difficulty will be getting on top of the order of operations because SQL can be pedantic.\nSQL (“see-quell” or “S.Q.L.”) is used with relational databases. A relational database is just a collection of at least one table, and a table is just some data organized into rows and columns. If there is more than one table in the database, then there should be some column that links them. Using it feels a bit like HTML/CSS in terms of being halfway between markup and programming. One fun aspect is that line spaces mean nothing: include them or do not, but always end a SQL command in a semicolon;\nWe can create an empty table of three columns of type: int, text, int:\n\nCREATE TABLE table_name (\n  column1 INTEGER,\n  column2 TEXT,\n  column3 INTEGER\n);\n\nAdd a row of data:\n\nINSERT INTO table_name (column1, column2, column3)\n  VALUES (1234, 'Gough Menzies', 32);\n\nAdd a column:\n\nALTER TABLE table_name\n  ADD COLUMN column4 TEXT;\n\nWe can view particular aspects of the data, using SELECT in a similar way to select().\n\nSELECT column2\n  FROM table_name;\n\nSee two columns:\n\nSELECT column1, column2\n  FROM table_name;\n\nSee all columns:\n\nSELECT *\n  FROM table_name;\n\nSee unique rows in a column (similar to R’s distinct):\n\nSELECT DISTINCT column2\n  FROM table_name;\n\nSee the rows that match a criteria (similar idea to R’s which or filter):\n\nSELECT *\n  FROM table_name\n    WHERE column3 > 30;\n\nAll the usual operators are fine with WHERE: =, !=, >, <, >=, <=. Just make sure the condition evaluates to true/false.\nSee the rows that are pretty close to a criteria:\n\nSELECT *\n  FROM table_name\n    WHERE column2 LIKE  '_ough Menzies';\n\nThe _ above is a wildcard that matches to any character e.g. ‘Cough Menzies’ would be matched here, as would ‘Gough Menzies’. LIKE is not case-sensitive: ‘Gough Menzies’ and ‘gough menzies’ would both match here.\nUse % as an anchor to matches pieces:\n\nSELECT *\n  FROM table_name\n    WHERE column2 LIKE  '%Menzies';\n\nThat matches anything ending with ‘Menzies’, so ‘Cough Menzies’, ‘Gough Menzies’, ‘Sir Menzies’ etc, would all be matched here. Use surrounding percentages to match within, e.g. %Menzies% would also match ‘Sir Menzies Jr’ whereas %Menzies would not.\nThis is wild: NULL values (!) (True/False/NULL) are possible, not just True/False, but they need to be explicitly matched for:\n\nSELECT *\n  FROM table_name\n    WHERE column2 IS NOT NULL;\n\nThis too is wild: There’s an underlying ordering build into number, date and text fields that allows you to use BETWEEN on all those, not just numeric! The following looks for text that starts with a letter between A and M (not including M) so would match ‘Gough Menzies’, but not ‘Sir Gough Menzies’!\n\nSELECT *\n  FROM table_name\n    WHERE column2 BETWEEN 'A' AND 'M';\n\nIf you look for a numeric (as opposed to text) then BETWEEN is inclusive.\nCombine conditions with AND (both must be true to be returned) or OR (at least one must be true):\n\nSELECT *\n  FROM table_name\n    WHERE column2 BETWEEN 'A' AND 'M'\n    AND column3 = 32;\n\nYou can order the result:\n\nSELECT *\n  FROM table_name\n    ORDER BY column3;\n\nAscending is the default, add DESC for alternative:\n\nSELECT *\n  FROM table_name\n    ORDER BY column3 DESC;\n\nRestrict the return to a certain number of values by adding LIMIT at the end:\n\nSELECT *\n  FROM table_name\n    ORDER BY column3 DESC\n    LIMIT 1;\n\n(This doesn’t work all the time - only certain SQL databases.)\nWe can modify data and use logic. For instance we can edit a value.\n\nUPDATE table_name\n  SET column3 = 33\n    WHERE column1 = 1234;\n\nImplement if/else logic:\n\nSELECT *,\n  CASE\n    WHEN column2 = 'Gough Whitlam' THEN 'Labor'\n    WHEN column2 = 'Robert Menzies' THEN 'Liberal'\n    ELSE 'Who knows'\n  END AS 'Party'\n  FROM table_name;\n\nThis returns a column called ‘Party’ that looks at the name of the person to return a party.\nDelete some rows:\n\nDELETE FROM table_name\n  WHERE column3 IS NULL;\n\nAdd an alias to a column name (this just shows in the output):\n\nSELECT column2 AS 'Names'\n  FROM table_name;\n\nWe can use COUNT, SUM, MAX, MIN, AVG and ROUND in the place of summarize(). COUNT counts the number of rows that are not empty for some column by passing the column name, or for all using *.\n\nSELECT COUNT(*)\n  FROM table_name;\n\nSimilarly, we can pass a column to SUM, MAX, MIN, and AVG.\n\nSELECT SUM(column1)\n  FROM table_name;\n\nROUND takes a column and an integer to specify how many decimal places.\n\nSELECT ROUND(column1, 0)\n  FROM table_name;\n\nSELECT and GROUP BY is similar to group_by in R.\n\nSELECT column3, COUNT(*)\n  FROM table_name\n    GROUP BY column3;\n\nWe can GROUP BY column number instead of name e.g. 1 instead of column3 in the GROUP BY line or 2 instead of COUNT(*) if that was of interest.\nHAVING for aggregates, is similar to filter in R or the WHERE for rows from earlier. Use it after GROUP BY and before ORDER BY and LIMIT.\nWe can combine two tables using JOIN or LEFT JOIN.\n\nSELECT *\n  FROM table1_name\n  JOIN table2_name\n    ON table1_name.colum1 = table2_name.column1;\n\nBe careful to specify the matching columns using dot notation. Primary key columns uniquely identify rows and are: 1) never NULL; 2) unique; 3) only one column per table. A primary key can be primary in one table and foreign in another. Unique columns have a different value for every row and there can be many in one table.\nUNION is the equivalent of cbind if the tables are already fairly similar.\n\n\n\n19.3.2 Parquet\nWhile the use of CSVs is great because they are so widely used and have very little overhead, they are also very minimal. This can lead to issues, especially in terms of class. There are various modern alternatives, including arrow (Richardson et al. 2022). Where we use write_csv() and read_csv() we can use write_parquet() and read_parquet(). One advantage is that it should retain the class between R and Python. It should also be faster than CSV.\n\nlibrary(arrow)\nlibrary(tictoc)\nlibrary(tidyverse)\n\nnumber_of_draws <- 1000000\n\nsome_data <- \n  tibble(\n    first = runif(n = number_of_draws),\n    second = sample(x = LETTERS, size = number_of_draws, replace = TRUE)\n  )\n\ntic(\"CSV\")\nwrite_csv(x = some_data,\n          file = \"some_data.csv\")\nread_csv(file = \"some_data.csv\")\n\n# A tibble: 1,000,000 × 2\n    first second\n    <dbl> <chr> \n 1 0.239  I     \n 2 0.923  W     \n 3 0.300  H     \n 4 0.361  P     \n 5 0.187  V     \n 6 0.0567 Q     \n 7 0.718  B     \n 8 0.361  G     \n 9 0.211  M     \n10 0.688  Q     \n# … with 999,990 more rows\n\ntoc()\n\nCSV: 0.395 sec elapsed\n\ntic(\"parquet\")\nwrite_parquet(x = some_data,\n              sink = \"some_data.parquet\")\nread_parquet(file = \"some_data.parquet\")\n\n# A tibble: 1,000,000 × 2\n    first second\n    <dbl> <chr> \n 1 0.239  I     \n 2 0.923  W     \n 3 0.300  H     \n 4 0.361  P     \n 5 0.187  V     \n 6 0.0567 Q     \n 7 0.718  B     \n 8 0.361  G     \n 9 0.211  M     \n10 0.688  Q     \n# … with 999,990 more rows\n\ntoc()\n\nparquet: 0.32 sec elapsed"
  },
  {
    "objectID": "19-efficiency.html#exercises-and-tutorial",
    "href": "19-efficiency.html#exercises-and-tutorial",
    "title": "19  Efficiency",
    "section": "19.4 Exercises and tutorial",
    "text": "19.4 Exercises and tutorial\n\n19.4.1 Exercises\n\n\n19.4.2 Tutorial\n\n\n19.4.3 Paper\nAt about this point, the Final Paper Appendix A.6 would be appropriate.\n\n\n\n\n\nBryan, Jenny. 2018. “Code Smells and Feels.” YouTube, July. https://youtu.be/7oyiPBjLAWY.\n\n\nIzrailev, Sergei. 2014. Tictoc: Functions for Timing r Scripts. https://CRAN.R-project.org/package=tictoc.\n\n\nNavarro, Danielle. 2021. “Notes from a Data Witch: Getting Started with Apache Arrow.” https://blog.djnavarro.net/starting-apache-arrow-in-r.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Jonathan Keane, Romain François, Jeroen Ooms, and Apache Arrow. 2022. Arrow: Integration to ’Apache’ ’Arrow’. https://CRAN.R-project.org/package=arrow."
  },
  {
    "objectID": "20-concluding.html",
    "href": "20-concluding.html",
    "title": "20  Concluding remarks",
    "section": "",
    "text": "Required material"
  },
  {
    "objectID": "20-concluding.html#concluding-remarks",
    "href": "20-concluding.html#concluding-remarks",
    "title": "20  Concluding remarks",
    "section": "20.1 Concluding remarks",
    "text": "20.1 Concluding remarks\nThere is an old saying, something along the lines of ‘may you live in interesting times’. Possibly every generation feels this way, but we sure live in interesting times. In this book, we have covered a broad range of essential skills that would allow you to tell stories with data. But we are just getting started.\nIn a little over a decade data science has gone from something that barely existed, to a defining part of academia and industry. The extent and pace of this change has many implications for those learning data science. For instance, it may imply that one should not just make decisions that optimize for what data science looks like right now, but also what could happen. While that is a little difficult, that is also one of the things that makes data science so exciting. That might mean choices like:\n\ntaking courses on fundamentals, not just fashionable applications;\nreading books, not just whatever is trending; and\ntrying to be at the intersection of at least a few different areas, rather than hyper-specialized.\n\nOne of the most exciting times when you learn data science is realizing that you just love playing with data. A decade ago, this did not fit into any particular department, these days it fits into almost any of them. Data science needs diversity, both in terms of approaches and applications. It is increasingly the most important work in the world and hegemonic approaches have no place. It is just such an exciting time to be enthusiastic about data and able to build."
  },
  {
    "objectID": "20-concluding.html#some-issues",
    "href": "20-concluding.html#some-issues",
    "title": "20  Concluding remarks",
    "section": "20.2 Some issues",
    "text": "20.2 Some issues\nData science draws from a variety of disciplines. But there are a variety of concerns that are common across them. Here we detail a few.\n1. How do we write unit tests for data science?\n\nOne thing that computer scientists know is the importance of unit tests. Basically this just means writing down the small checks that we do in our heads all the time. Like if we have a column that purports to the year, then it’s unlikely that it’s a character, and it’s unlikely that it’s an integer larger than 2500, and it’s unlikely that it’s a negative integer. We know all this, but writing unit tests has us write this all down.\nIn this case it’s obvious what the unit test looks like. But more generally, we often have little idea what our results should look like if they’re running well. The approach that I have taken is to add simulation—so we simulate reasonable results, write unit tests based on that, and then bring the real data to bear and adjust as necessary. But I really think that we need extensive work in this area because the current state-of-the-art is lacking.\n2. What happened to the machine learning revolution?\nI don’t understand what happened to the promised machine learning revolution in social sciences. Specifically, I am yet to see any convincing application of machine learning methods that are designed for prediction to a social sciences problem where what we care about is understanding. I would like to either see evidence of them or a definitive thesis about why this can’t happen. The current situation is untenable where folks, especially those in fields that have been historically female, are made to feel inferior even though their results are no worse.\n3. What do we do about p-values and power?\nAs someone who learnt statistics from economists, but now is partly in a statistics department, I do think that everyone should learn statistics from statisticians. This isn’t anything against economists, but the conversations that I have in the statistics department about what statistical methods are and how they should be used are very different to those that I have had in other departments.\nI think the problem is that people outside statistics, treat statistics as a recipe in which they follow various steps and then out comes a cake. With regard to ‘power’—it turns out that there were a bunch of instructions that no one bothered to check—they turned the oven on to some temperature without checking that it was 180C, and that’s fine because whatever mess came out was accepted because the people evaluating the cake didn’t know that they needed to check the temperature had been appropriately set. (I am ditching this analogy right now).\nAs you know, the issue with power is related to the broader discussion about p-values, which basically no one is taught properly, because it would require changing an awful lot about how we teach statistics i.e. moving away from the recipe approach.\nAnd so, my specific issue is that people think that statistics is a recipe to be followed. They think that because that’s how they are trained especially in social sciences like political science and economics, and that’s what is rewarded. But that’s not what these methods are. Instead, statistics is a collection of different instruments that let us look at our data in a certain way. I think that we need a revolution here, not a metaphorical tucking in of one’s shirt.\n4. How do we teach data science?\nWe are beginning to start to have agreement on what the foundations of data science are. It involves computational thinking, concern for sampling, statistics, graphics, Git/GitHub, SQL, command line, comfort with messy data, comfort across a few languages including R and Python. But we have very little agreement on how best to teach it. Partly this is because data science instructors often come different fields, but also it is also partly a difference in resources and priorities.\n5. What is happening at the data cleaning and preparation stage?\nWe basically do not have a good understanding how much any of this matters. Huntington-Klein et al. (2021) showed that hidden research decisions have a big effect on subsequent estimates, sometimes greater than the standard errors. Such findings invalidate claims. We need much more investigation of how these early stages of the data science workflow affect the conclusions."
  },
  {
    "objectID": "20-concluding.html#next-steps",
    "href": "20-concluding.html#next-steps",
    "title": "20  Concluding remarks",
    "section": "20.3 Next steps",
    "text": "20.3 Next steps\nThis book has covered much ground, and while we are toward the end of it, as the butler Stevens is told in the novel The Remains of the Day (Ishiguro 1989):\n\nThe evening’s the best part of the day. You’ve done your day’s work. Now you can put your feet up and enjoy it.\n\nChances are there are aspects that you want to explore further, building on the foundation that you have established. If so, then I have accomplished what I set out to do.\nIf you were new to data science at the start of this book, then the next step would be to backfill that which I skipped over, and I would recommend Timbers, Campbell, and Lee (2022). After that, you should learn more about R in terms of data science by going through Wickham and Grolemund (2017). To deepen your understanding of R itself, go next to Wickham (2019).\nIf you are interested in learning more about causality then start with Cunningham (2021) and Huntington-Klein (2021).\nIf you are interested to learn more about statistics then begin with McElreath (2020), and then backfill with Johnson, Ott, and Dogucu (2022) and solidify the foundation with Gelman et al. (2014). You should probably also backfill some of the fundamentals around probability, starting with Wasserman (2005).\nThere is only one next natural step if you are interested in learning more about statistical (what has come to be called machine) learning and that’s James et al. (2017) followed by Friedman, Tibshirani, and Hastie (2009).\nIf you are interested in sampling then the next book to turn to is Lohr (2019). To deepen your understanding of surveys and experiments, go next to Gerber and Green (2012) in combination with Kohavi, Tang, and Xu (2020).\nFor developing better data visualization skills, begin by turning to Healy (2018), but then after that, develop strong foundations, such as Wilkinson (2005). For writing, it would be best to turn inward. Force yourself to write and publish everyday for a month. Then do it again and again. You will get better. That said, there are some useful books, including Caro (2019) and King (2000).\nWe often hear the phrase let the data speak. Hopefully by this point you understand that never happens. All that we can do is to acknowledge that we are the ones using data to tell stories, and strive and seek to make them worthy.\n\nIt was her voice that made\nThe sky acutest at its vanishing.\nShe measured to the hour its solitude.\nShe was the single artificer of the world\nIn which she sang. And when she sang, the sea,\nWhatever self it had, became the self\nThat was her song, for she was the maker.\n‘The Idea of Order at Key West’, (Stevens 1934)\n\n\n\n\n\n\nCaro, Robert. 2019. Working. 1st ed. Knopf.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. Yale Press.\n\n\nFriedman, Jerome H., Robert Tibshirani, and Trevor Hastie. 2009. The Elements of Statistical Learning. Springer.\n\n\nGelman, Andrew. 2021. “Wrong Again! 30+ Years of Statistical Mistakes.” YouTube, October. https://youtu.be/mB9Q26uptao.\n\n\nGelman, Andrew, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin. 2014. Bayesian Data Analysis. 3rd ed. CRC Press.\n\n\nGerber, Alan, and Donald Green. 2012. Field Experiments: Design, Analysis, and Interpretation. W W Norton.\n\n\nHealy, Kieran. 2018. Data Visualization. Princeton University Press.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. Chapman & Hall.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey R Bloem, Pralhad Burli, Naibin Chen, et al. 2021. “The Influence of Hidden Researcher Decisions in Applied Microeconomics.” Economic Inquiry.\n\n\nIshiguro, Kazuo. 1989. The Remains of the Day. Faber; Faber.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2017. An Introduction to Statistical Learning with Applications in r.\n\n\nJohnson, Alicia A., Miles Ott, and Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling with r. CRC Press.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft. Scribner.\n\n\nKohavi, Ron, Diane Tang, and Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical Guide to a/b Testing. Cambridge University Press.\n\n\nLohr, Sharon L. 2019. Sampling: Design and Analysis. CRC Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC Press.\n\n\nStevens, Wallace. 1934. The Idea of Order at Key West. https://www.poetryfoundation.org/poems/43431/the-idea-of-order-at-key-west.\n\n\nTimbers, Tiffany-Anne, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWickham, Hadley. 2019. Advanced r. CRC Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. https://r4ds.had.co.nz/.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed. Springer."
  },
  {
    "objectID": "22-assessment.html",
    "href": "22-assessment.html",
    "title": "Appendix A — Papers",
    "section": "",
    "text": "Working individually and in an entirely reproducible way, please find a dataset of interest on Open Data Toronto and write a short paper telling a story about the data.\n\nCreate a well-organized folder with appropriate sub-folders, and add it to GitHub. You are welcome to use this starter folder.\nFind a dataset of interest on Open Data Toronto.\n\nPut together an R script, ‘scripts/00-simulation.R’, that simulates the dataset of interest. Push to GitHub and include an informative commit message\nWrite an R script, ‘scripts/00-download_data.R’ to download the actual data in a reproducible way using opendatatoronto (Gelfand 2020). Save the data: ‘inputs/data/raw_data.csv’. Push to GitHub and include an informative commit message\n\nPrepare a PDF using Quarto ‘outputs/paper/paper.qmd’ with these sections: title, author, date, abstract, introduction, data, and references.\n\nThe title should be descriptive, informative, and specific.\nThe date should be in an unambiguous format. Add a link to the GitHub repo in the acknowledgements.\nThe abstract should be three or four sentences. The abstract must tell the reader the top-level finding. What is the one thing that we learn about the world because of this paper?\nThe introduction should be two or three paragraphs of content. And there should be an additional final paragraph that sets out the remainder of the paper.\nThe data section should thoroughly and precisely discuss the source of the data and the bias this brings (ethical, statistical, and otherwise). Comprehensively describe and summarize the data using text, graphs, and tables. Graphs must be made with ggplot2 (Wickham 2016) and tables must be made with knitr (Xie 2021) or gt (Iannone, Cheng, and Schloerke 2020). Graphs must show the actual data, or as close to it as possible, not summary statistics. Graphs and tables should be cross-referenced in the text e.g. ‘Table 1 shows…’).\nReferences should be added using BibTeX. Be sure to reference R and any R packages you use, as well as the dataset. Strong submissions will draw on related literature and reference those.\nThe paper should be well-written, draw on relevant literature, and explain all technical concepts. Pitch it at an educated, but non-specialist, audience.\nUse appendices for supporting, but not critical, material.\nPush to GitHub and include an informative commit message\n\n\nSubmit a PDF of your paper.\nThere should be no evidence that this is a class assignment.\n\n\n\n\n\nThere should be no R code or raw R output in the final PDF.\nCode should be entirely reproducible, well-documented, commented, and readable.\nThe paper should knit directly to PDF i.e. use ‘Knit to PDF’.\n\nDo not use ‘Knit to html’ and then save as a PDF.\nDo not use ‘Knit to Word’ and then save as a PDF\n\nGraphs, tables, and text should be clear, and of comparable quality to those of FiveThirtyEight.\nThe date should be up-to-date and unambiguous (e.g. 2/3/2022 is ambiguous, 2 March 2022 is not).\nThe entire workflow should be entirely reproducible.\nThere should not be any typos.\nThere should be no sign this is a school paper.\nThere must be a link to the paper’s GitHub repo using a footnote.\nThe GitHub repo should be well-organized, and contain an informative README.\nThe paper should be well-written and able to be understood by the average reader of, say, FiveThirtyEight. This means that you are allowed to use mathematical notation, but you must explain all of it in plain language. All statistical concepts and terminology must be explained. Your reader is someone with a university education, but not necessarily someone who understands what a p-value is.\n\n\n\n\n\nCan I use a dataset from Kaggle instead? No, because they have done the hard work for you.\nI cannot use code to download my dataset, can I just manually download it? No, because your entire workflow needs to be reproducible. Please fix the download problem or pick a different dataset.\nHow much should I write? Most students submit something in the two-to-six-page range, but it is up to you. Be precise and thorough.\nMy data is about apartment blocks/NBA/League of Legends so there’s no ethical or bias aspect, what do I do? Please re-read the readings to better understand bias and ethics. If you really cannot think of something, then it might be worth picking a different dataset.\nCan I use Python? No. If you already know Python then it doesn’t hurt to learn another language.\nWhy do I need to cite R, when I don’t need to cite Word? R is a free statistical programming language with academic origins, so it is appropriate to acknowledge the work of others. it is also important for reproducibility.\nWhat reference style should I use? Any major reference style is fine (APA, Harvard, Chicago, etc); just pick one that you are used to.\nThe paper in the starter folder has a model section, so do I need to put together a model? No. The starter folder is designed to be applicable to all of the papers; just delete the aspects that you do not need.\nWhat does ‘graph the actual data’ mean? If you have, say 5,000 observations in the dataset and three variables, then for every variable there should be a graph that has 5,000 points in the case of dots, or adds up to 5,000 in the case of bar charts and histograms.\n\n\n\n\n\n\n\n\n\n  \n  \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\n\nAdam Labas, Alicia Yang, Alyssa Schleifer, Amy Farrow, Ethan Sansom, Hudson Yuen, Jack McKay, Morgaine Westin, Rachael Lam, Roy Chan, Thomas D’Onofrio, and William Gerecke."
  },
  {
    "objectID": "22-assessment.html#sec-paper-two",
    "href": "22-assessment.html#sec-paper-two",
    "title": "Appendix A — Papers",
    "section": "A.2 Paper Two",
    "text": "A.2 Paper Two\n\nA.2.1 Task\n\nWorking as part of a team of one to three people, please pick a paper of interest to you, with code and data that are available, published anytime since 2019, in an American Economic Association journal. These journals are: ‘American Economic Review’, ‘AER: Insights’, ‘AEJ: Applied Economics’, ‘AEJ: Economic Policy’, ‘AEJ: Macroeconomics’, ‘AEJ: Microeconomics’, ‘Journal of Economic Literature’, ‘Journal of Economic Perspectives’, ‘AEA Papers & Proceedings’.\nFollowing the Guide for Accelerating Computational Reproducibility in the Social Sciences, please complete a replication1 of at least three graphs, tables, or a combination, from that paper, using the Social Science Reproduction Platform. Note the DOI of your replication.\nWorking in an entirely reproducible way then conduct a reproduction based on two or three aspects of the paper, and write a short paper about that.\n\nCreate a well-organized folder with appropriate sub-folders, add it to GitHub, and then prepare a PDF using Quarto with these sections (you are welcome to use this starter folder): title, author, date, abstract, introduction, data, results, discussion, and references.\nThe aspects that you focus on in your paper could be the same aspects that you replicated, but they do not need to be. Follow the direction of the paper, but make it your own. That means you should ask a slightly different question, or answer the same question in a slightly different way, but still use the same dataset.\nInclude the DOI of your replication in your paper and a link to the GitHub repo that underpins your paper.\nThe results section should convey findings.\nThe discussion should include three or four sub-sections that each focus on an interesting point, and there should be another sub-section on the weaknesses of your paper, and another on potential next steps for it.\nIn the discussion section, and any other relevant section, please be sure to discuss ethics and bias, with reference to relevant literature.\nThe paper should be well-written, draw on relevant literature, and explain all technical concepts. Pitch it at an educated, but non-specialist, audience.\nUse appendices for supporting, but not critical, material.\nCode should be entirely reproducible, well-documented, and readable.\n\nSubmit a PDF of your paper.\nThere should be no evidence that this is a class assignment.\n\n\n\nA.2.2 Checks\n\nThe paper should not just copy/paste the code from the original paper, but have instead used that as a foundation to work from.\nYour paper should have a link to the associated GitHub repository and the DOI of the Social Science Reproduction Platform replication that you conducted.\nMake sure you have referenced everything, including R. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, provided it is consistent.\n\n\n\nA.2.3 FAQ\n\nHow much should I write? Most students submit something in the 10-to-15-page range, but it is up to you. Be precise and thorough.\nDo I have to focus on a model result? No, it is likely best to stay away from that at this point, and instead focus on tables or graphs of summary or explanatory statistics.\nWhat if the paper I choose is in a language other than R? Both your replication and reproduction code should be in R. So you will need to translate the code into R for the replication. And the reproduction should be your own work, so that also should be in R. One common language is Stata, and Huntington-Klein (2022) might be useful as a ‘Rosetta Stone’ of sorts, for R, Python, and Stata.\nCan I work by myself? Yes.\nDo the graphs/tables have to look identical to the original? No, you are welcome to, and should, make them look better as part of the reproduction. And even as part of the replication, they do not have to be identical, just similar enough.\n\n\n\nA.2.4 Rubric\n\n\n\n\n\n  \n  \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Replication\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSSRP submission needs to be filled out completely for three elements.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Results\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nResults will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars.\n    Discussion\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSome questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\nA.2.5 Previous examples\nAlyssa Schleifer, Hudson Yuen, Tamsen Yau; Olaedo Okpareke, Arsh Lakhanpal, Swarnadeep Chattopadhyay; and Kimlin Chin."
  },
  {
    "objectID": "22-assessment.html#sec-paper-three",
    "href": "22-assessment.html#sec-paper-three",
    "title": "Appendix A — Papers",
    "section": "A.3 Paper Three",
    "text": "A.3 Paper Three\n\nA.3.1 Task\n\nWorking as part of a team of one to three people, and in an entirely reproducible way, please obtain data from the US General Social Survey2. (You are welcome to use a different government-run survey, but please obtain permission before starting.)\nObtain the data, focus on one aspect of the survey, and then use it to tell a story.\n\nCreate a well-organized folder with appropriate sub-folders, add it to GitHub, and then use Quarto to prepare a PDF with these sections (you are welcome to use this starter folder): title, author, date, abstract, introduction, data, results, discussion, an appendix that will, at least, contain a survey, and references.\nIn addition to conveying a sense of the dataset of interest, the data section should include, but not be limited to:\n\nA discussion of the survey’s methodology, and its key features, strengths, and weaknesses. For instance: what is the population, frame, and sample; how is the sample recruited; what sampling approach is taken, and what are some of the trade-offs of this; how is non-response handled.\nA discussion of the questionnaire: what is good and bad about it?\nIf this becomes too detailed, then use appendices for supporting but not essential aspects.\n\nIn an appendix, please put together a supplementary survey that could be used to augment the general social survey the paper focuses on. The purpose of the supplementary survey is to gain additional information on the topic that is the focus of the paper, beyond that gathered by the general social survey. The survey would be distributed in the same manner as the general social survey but needs to stand independently. The supplementary survey should be put together using a survey platform. A link to this should be included in the appendix. Additionally, a copy of the survey should be included in the appendix.\nPlease be sure to discuss ethics and bias, with reference to relevant literature.\nCode should be entirely reproducible, well-documented, and readable.\n\nSubmit a PDF of the paper.\nThe paper should be well-written, draw on relevant literature, and explain all technical concepts. Pitch it at a university-educated, but non-specialist, audience. Use survey, sampling, and statistical terminology, but be sure to explain it. The paper should flow, and be easy to follow and understand.\nThere should be no evidence that this is a class paper.\n\n\n\nA.3.2 Checks\n\nAn appendix should contain both a link to the supplementary survey and the details of it, including questions (in case the link fails, and to make the paper self-contained).\n\n\n\nA.3.3 FAQ\n\nWhat should I focus on? You may focus on any year, aspect, or geography that is reasonable given the focus and constraints of the general social survey that you are interested in. Please consider the year and topics that you are interested in together, as some surveys focus on particular topics in some years.\nDo I need to include the raw GSS data in the repo? For most of the general social surveys you will not have permission to share the GSS data. If that is the case, then you should add clear details in the README explaining how the data could be obtained.\nHow many graphs do I need? In general, you need at least as many graphs as you have variables, because you need to show all the observations for all variables. But you may be able to combine a few; or, vice versa, you may be interested in looking at different aspects or relationships.\n\n\n\nA.3.4 Rubric\n\n\n\n\n\n  \n  \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Results\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nResults will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars.\n    Discussion\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSome questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Survey\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nThe survey should have an introductory section and include the details of a contact person. The survey questions should be well constructed and appropriate to the task. The questions should have an appropriate ordering. A final section should thank the respondent.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\nA.3.5 Previous examples\n\nAnna Li and Mohammad Sardar Sheikh; Chyna Hui and Marco Chau; Ethan Sansom; Luckyna Laurent, Samita Prabhasavat, and Zoie So; Pascal Lee Slew, and Yunkyung_Park; and Ray Wen, Isfandyar Virani, and Rayhan Walia."
  },
  {
    "objectID": "22-assessment.html#sec-paper-four",
    "href": "22-assessment.html#sec-paper-four",
    "title": "Appendix A — Papers",
    "section": "A.4 Paper Four",
    "text": "A.4 Paper Four\n\nA.4.1 Task\n\nWorking as part of a team of one to three people, and in an entirely reproducible way, please convert one full-page table from one DHS Program ‘Final Report’, from the 1980s or 1990s, as available here, into a usable dataset, then write a short paper telling a story with the data.\nCreate a well-organized folder with appropriate sub-folders, and add it to GitHub. You are welcome to use this starter folder.\nCreate and document a dataset:\n\nSave the PDF to ‘inputs’.\nPut together a simulation of your plan for the usable dataset and save the script to ‘scripts/00-simulation.R’.\nWrite R code, saved as ‘scripts/01-gather_data.R’, to either OCR or parse the PDF, as appropriate, and save the output to ‘outputs/data/raw_data.csv’.\nWrite R code, saved as ‘scripts/02-clean_and_prepare_data.R’, that draws on ‘raw_data.csv’ to clean and prepare the dataset. Use pointblank to put together tests that the dataset passes (at a minimum, every variable should have a test for class and another for content). Save the dataset to ‘outputs/data/cleaned_data.csv’.\nFollowing Gebru et al. (2021), put together a data sheet for the dataset you put together (put this in the appendix of your paper). You are welcome to start from the template ‘inputs/data/datasheet_template.qmd’ in the starter folder, although, again, you should add it to the appendix of your paper, rather than a stand-alone document.\n\nUse the dataset to tell a story by using Quarto to prepare a PDF with these sections: title, author, date, abstract, introduction, data, results, discussion, an appendix that will, at least, contain a datasheet for the dataset, and references.\n\nIn addition to conveying a sense of the dataset of interest, the data section should include details of the methodology used by the DHS you used, and its key features, strengths, and weaknesses.\n\nSubmit a PDF of the paper.\nThere should be no evidence that this is a class paper.\n\n\n\nA.4.2 Checks\n\nUse GitHub in a well-developed way by making at least a few commits and using descriptive commit messages.\n\n\n\n\nA.4.3 FAQ\n\n\n\n\n\n\nA.4.4 Rubric\n\n\n\n\n\n  \n  \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Results\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nResults will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars.\n    Discussion\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSome questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    Datasheet\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA thorough datasheet for the dataset that was constructed is included.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\nA.4.5 Previous examples\nBilal Haq and Ritvik Puri; Charles Lu, Mahak Jain, and Yujun Jiao; and Pascal Lee Slew and Yunkyung Park."
  },
  {
    "objectID": "22-assessment.html#sec-paper-five",
    "href": "22-assessment.html#sec-paper-five",
    "title": "Appendix A — Papers",
    "section": "A.5 Paper Five",
    "text": "A.5 Paper Five\n\nA.5.1 Task\n\nWorking as part of a team of one to three people, please forecast the popular vote of the 2020 US election using multilevel regression with post-stratification and then write a short paper telling a story. This requires individual-level survey data, post-stratification data, and a model that brings them together. Given the expense of collecting these data, and the privilege of having access to them, please be sure to properly cite all datasets that you use.\nIndividual-level survey data:\n\nRequest access to the Democracy Fund + UCLA Nationscape ‘Full Data Set’. This could take a day or two. Please start early.\nSimulate the survey dataset that you will use, and save the script to ‘scripts/00-simulation-survey.R’.\nOnce you have access then pick one survey of interest (they were conducted at different times).\nThis will be a large file and is not yours to share. Do not push it to GitHub. Use a .gitignore file to accomplish this. Instead document how to get the raw data in the README.\nClean and prepare the dataset based on what you need.\n\nPost-stratification data:\n\nCreate an account with IPUMS and then use this to access the American Community Surveys (ACS).\nSimulate the post-stratification dataset that you will use, and save the script to ‘scripts/00-simulation-poststratification.R’.\nPick an appropriate 1-year ACS (there is one every year). Then select some variables. This will depend on what you want to model and the survey data, but some options include: REGION, STATEICP, AGE, SEX, MARST, RACE, HISPAN, BPL, CITIZEN, EDUC, LABFORCE, or INCTOT. Have a look around and see what you are interested in, remembering that you will need to establish a correspondence to the survey.\nDownload the relevant post-stratification data (it is probably easiest to change the data format to .dta).\nAgain, this will be a large file and is not yours to share. Do not push it to GitHub. Use a .gitignore file to accomplish this. Instead document how to get the raw data in the README.\nClean and prepare the post-stratification dataset. Remember that you need cell counts for the sub-populations in the model.\n\nModelling:\n\nYou will want to explain vote intention based on a variety of explanatory variables. The decision is yours, but you should probably use logistic regression. In that case, construct the vote intention variable so that it is binary (either ‘supports Trump’ or ‘supports Biden’). In increasing level of complexity, you would then build a model using: glm(), lme4::glmer(), rstanarm:: stan_glm, or brms::brm().\nThink about model fit, diagnostics, and other similar aspects that you need to convince someone that the model is appropriate.\nYou have flexibility of the model that you use, (and hence the cells that you will need to create). In general, the more cells the better, but you may want fewer cells for simplicity in the writing process and to ensure a decent sample in each cell. It would be best to start with a simple model and then complicate it, rather than vice versa.\nApply the trained model to the post-stratification dataset to forecast the election result. The specifics will depend on your modelling approach but will likely involve predict(), add_predicted_draws(), or similar. The primary aspect of interest is the forecast distribution of the popular vote, and how the explanatory variables affect this. Strong submissions would go beyond that.\n\nWrite-up:\n\nCreate a well-organized folder with appropriate sub-folders, add it to GitHub, and then prepare a PDF using Quarto with these sections (you are welcome to use this starter folder): title, author, date, abstract, introduction, data, model, results, discussion, and references. Use appendices for supporting, but not critical, material.\n\nIn the model section, you should carefully spell out the statistical model that you are using, being sure to define and explain each aspect and why it is important. The model should be appropriately complex; that is, not inappropriately simple, but not unnecessarily complicated. The model should have well-defined variables and these should correspond to what is discussed in the data section. You should explain how the aspects discussed in the data section assert themselves in the modelling decisions that you made. The model should be written out in appropriate mathematical notation but also in plain English. Every aspect of that notation should be defined. The model should make sense based on the substantive area, and the form of the model. If the model is Bayesian, then priors should be defined and sensible. There should be explanation of how features enter the model and why. For instance, why use age rather than age-groups, why does province have a levels effect, why is gender categorical, etc? In general, there should be a clear justification that this is the model for the situation. The assumptions underpinning the model should be clearly discussed. Alternative models, or variants, should be discussed, and strengths and weaknesses made clear. Why was this model chosen? You should mention the software that you used to run the model. There should be evidence of thought about the circumstances in which the model may not be appropriate. There should be evidence of model validation and checking, whether that is out-of-sample, RMSE, a test/training split, or appropriate sensitivity checks. You should be clear about model convergence, model checks, and diagnostic issues.\n\n\nSubmit a PDF of your paper.\nThere should be no evidence that this is a class assignment.\n\n\n\nA.5.2 Checks\n\nUse GitHub in a well-developed way by making at least a few commits and using descriptive commit messages.\nDo not include p-values, stars, or similar, in tables. If you invoke statistical significance, then you should draw on and integrate Fisher (1926) and others.\n\n\n\nA.5.3 FAQ\n\nHow much should I write? Most students submit something in the 10-to-15-page range, but it is up to you. Be precise and thorough.\n\n\n\nA.5.4 Rubric\n\n\n\n\n\n  \n  \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Model\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nThe model should be nicely written out, well-explained, justified, and appropriate.\n    Results\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nResults will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars.\n    Discussion\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSome questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\nA.5.5 Previous examples\nAlen Mitrovski, Xiaoyan Yang, Matthew Wankiewicz (this paper received an ‘Honorable Mention’ in the ASA December 2020 Undergraduate Statistics Research Project competition.)"
  },
  {
    "objectID": "22-assessment.html#sec-final-paper",
    "href": "22-assessment.html#sec-final-paper",
    "title": "Appendix A — Papers",
    "section": "A.6 Final paper",
    "text": "A.6 Final paper\n\nA.6.1 Task\n\nWorking individually and in an entirely reproducible way please write a paper that involves original work to tell a story with data.\nOptions include (pick one):\n\nDevelop a research question that is of interest to you based on your own interests, background, and expertise, then obtain or create a relevant dataset.\nA reproduction, being sure to use the paper as a foundation rather than as an end-in-itself.\n\nCreate a well-organized folder with appropriate sub-folders, add it to GitHub, and then prepare a PDF using Quarto with these sections (you are welcome to use this starter folder):\n\nTitle, date, author, abstract, introduction, data, model, results, discussion, appendix (optional, for supporting, but not critical, material), and a reference list.\nIt must also include an enhancement, and this would either be contained, or linked to, in the appendix.\n\n\n\n\nA.6.2 Peer review submission\n\nThis is an initial ‘submission’ where you get comments and feedback on a draft.\nSubmit a PDF of your draft.\nThe paper does not have to be finished at this point, but the following sections must be filled out: title, author, date, abstract, and introduction.\nAll other sections must be present in the paper, but do not have to be filled out (e.g. you must have a ‘Data’ heading, but you do not need to have content in that section).\nTo be clear, it is fine to later change any aspect of what you submit at this checkpoint.\nYou will be awarded one percentage point just for submitting a draft that meets this minimum.\nThere are no extensions possible for this submission because the following submission is dependent on this date.\n\n\n\nA.6.3 Conduct peer-review\n\nAs an individual, you will randomly be assigned a handful of rough drafts to provide feedback. You have three days to provide feedback to your peers.\nIf you provide feedback to one peer you will receive one percentage point, if you provide feedback to two peers you will receive two percentage points, etc.\nYour feedback must include at least five comments (meaningful and useful bullet points). These must be well-written and thoughtful.\nThere are no extensions granted for this submission since the following submission is dependent on this date.\nPlease remember that you are providing feedback here to help your colleagues. All comments should be professional and kind. It is challenging to receive criticism. Please remember that your goal here is to help your peers advance their writing/analysis.\n\n\n\nA.6.4 FAQ\n\nCan I work as part of a team? No. it is important that you have some work that is entirely your own. You really need your own work to show off for job applications etc.\nHow much should I write? Most students submit something that has 10-to-16-pages of main content, with additional pages devoted to appendices, but it is up to you. Be precise and thorough.\nDo I have to submit an initial paper in order to do the peer-review? Yes.\nCan I use the same paper for the reproduction as in Paper 3? No.\nCan I use any model? You are welcome to use any model, but you need to thoroughly explain it and this can be difficult for more complicated models.\n\n\n\nA.6.5 Rubric\n\n\n\n\n\n  \n  \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Model\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nThe model should be nicely written out, well-explained, justified, and appropriate.\n    Results\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nResults will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars.\n    Discussion\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSome questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    Enhancements\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nYou should pick at least one of the following and include it to enhance your submission: 1) A datasheet for the dataset; 2) A model card for the model; 3) A Shiny application; 4) An R package; or 5) API for the model.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\nA.6.6 Previous examples\nAmy Farrow, Annie Collins, Hong Shi, Jia Jia Ji, Laura Cline, Lorena Almaraz De La Garza, and Rachael Lam.\n\n\n\n\n\nBarba, Lorena A. 2018. “Terminologies for Reproducible Research.” https://arxiv.org/abs/1802.03311.\n\n\nFisher, Ronald Aylmer. 1926. “The Arrangement of Field Experiments,” 503–15. https://doi.org/10.23637/ROTHAMSTED.8V61Q.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021. “Datasheets for Datasets.” Communications of the ACM 64 (12): 86–92.\n\n\nGelfand, Sharla. 2020. Opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nHuntington-Klein, Nick. 2022. “Library of Statistical Techniques.” https://lost-stats.github.io.\n\n\nIannone, Richard, Joe Cheng, and Barret Schloerke. 2020. Gt: Easily Create Presentation-Ready Display Tables. https://CRAN.R-project.org/package=gt.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nXie, Yihui. 2021. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/."
  },
  {
    "objectID": "23-datasets.html",
    "href": "23-datasets.html",
    "title": "Appendix B — Datasets",
    "section": "",
    "text": "This is just a holding place for this content while it is being developed."
  },
  {
    "objectID": "23-datasets.html#oh-you-think-we-have-good-data-on-that",
    "href": "23-datasets.html#oh-you-think-we-have-good-data-on-that",
    "title": "Appendix B — Datasets",
    "section": "B.1 Oh, you think we have good data on that!",
    "text": "B.1 Oh, you think we have good data on that!\nChapter 8:\n\nOh, you think we have good data on that! City boundaries. What constitues ‘Atlanta’? Different definitions - metro, X, Y. (also an issue in countries with boundaries changing over time)\n\nChapter 10:\n\nOh, you think we have good data on that! One representation of reality that is commonplace, is in chess. A chess board (see Figure X - add photo of a chess board) is a 8 x 8 board of alternating black and white squares. The squares are denonated by a unique combination of a letter (A-G) and a number (1-8). And each piece has a unique abbreviation, for instance pawns are X, and knights are Y. A game is recorded by each player noting the move. In this way the entire game can be recreated. The 2021 World Championship was contested by Magnus Carlsen and Ian Nepomniachtchi. Figure X shows a score sheet from Game 6. There were a variety of reasons this game was particularly noteworthy, but one the uncharactertic mistakes that both Carlsen and Nepomniachtchi made. For instance, at Move 32 Carlsen did not exploit an opportunity; and Move 36 a different move would have provided Nepomniachtchi with a promising endgame (CITATION). One reason for this may have been that both players at that point in the game had very little time remaining—they had to decide on their moves very quickly. But there is no sense of that in the representation provided by the game sheet. It is a ‘correct’ representation of what happened in the game, but not necessarily why it happened.\n\n\nOh, you think we have good data on that! Migration.\n\n\nOh, you think we have good data on that! Weather stations\n\n\nOh, you think we have good data on that! Olympics events. Who decides on the scoring?. Who does the timing?\n\n\nOh, you think we have good data on that! Personality scores. Myers Briggs and Big 5 more generally.\n\n\nOh, you think we have good data on that! Cause of death\n\n\nOh, you think we have good data on that! Timing"
  },
  {
    "objectID": "23-datasets.html#shoulders-of-giants",
    "href": "23-datasets.html#shoulders-of-giants",
    "title": "Appendix B — Datasets",
    "section": "B.2 Shoulders of giants",
    "text": "B.2 Shoulders of giants\nChapter 1:\n\nShoulders of giants Dr Michael Jordan is Pehong Chen Distinguished Professor at the University of California, Berkeley. After taking a PhD in Cognitive Science from University of California, San Diego, in 1985, he was appointed as an assistant professor at MIT, being promoted to full professor in 1997, and in 1998 he moved to Berkeley. One area of his research is statistical machine learning. One particularly important paper is Blei, Ng, and Jordan (2003), which enables text to be grouped together to define topics, and we cover this in Chapter @ref(text-as-data).\n\nChapter 2:\n\nShoulders of giants Dr Robert Gentleman and Dr Ross Ihaka are the originators of R. After taking a PhD in Statistics from the University of Washington in 1988, Robert moved to the University of Auckland, then went onto various roles including 23andMe and is now the Executive Director of the Center for Computational Biomedicine at Harvard Medical School. After taking a PhD in ??? from the University of California, Berkeley, he moved to ???.\n\nChapter 3:\n\nShoulders of giants Hadley Wickham\n\nChapter 5:\n\nShoulders of giants Xiao-Li Meng\n\nChapter 8:\n\nShoulders of giants Barbara Bailar\n\nChapter 8:\n\nShoulders of giants Leo Goodman\n\nChapter 9:\n\nShoulders of giants Donald B. Rubin\n\n\nShoulders of giants Marcella Alsan\n\nChapter 10:\n\nShoulders of giants Susan Athey\n\nChapter 12:\n\nShoulders of giants Timnit Gebru\n\nChapter 12:\n\nShoulders of giants Katherine Wallman\n\nChapter 13\n\nShoulders of giants John Tukey\n\nChapter 14:\n\nShoulders of giants Dr Daniela Witten\n\nChapter 14:\n\nShoulders of giants Dr Nancy Reid is University Professor of Statistical Sciences, University of Toronto. After taking a PhD in Statistics from Stanford University in 1979, she was appointed as an assistant professor at UBC, and moved to the University of Toronto in 1986, where she was promoted to full professor in 1988 and served as chair between 1997 and 2002. One area of her research is higher-order approximate inference, which…. One particularly important paper is: X. She was awarded the 2022 Guy Medal in Gold from the Royal Statistical Society.\n\n\n\nChapter 14:\n\nShoulders of giants Rob Tibshirani\n\nChapter 15:\n\nShoulders of giants Evelyn Kitagawa\n\nChapter 16:\n\nShoulders of giants Andrew Gelman\n\n\nElizabeth Scott\nGertrude Mary Cox\nSimon Kuznets\nStella Cunliffe"
  },
  {
    "objectID": "23-datasets.html#possible-datasets",
    "href": "23-datasets.html#possible-datasets",
    "title": "Appendix B — Datasets",
    "section": "B.3 Possible datasets",
    "text": "B.3 Possible datasets\n\nhttps://som.yale.edu/faculty-research/our-centers/international-center-finance/data\nAlex cookson\nDavid Andrew’s book\nTidycensus\nhttps://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.html\nhttps://www.historicalstatistics.org/\nhttps://data.cityofberkeley.info/browse?limitTo=datasets&utf8\nhttps://data.gov.hk/en-datasets/category/education\nhttps://data.rijksmuseum.nl/object-metadata/download/\nWorld bank https://data.worldbank.org/ eg development indicators\nSouth sea bubble\nOECD\nAer R package and that paper?\nCESr\nPaspaley\nCanlang\nFred - does that have an api?\n538\nThe Economist\nhttps://pds.nasa.gov/datasearch/subscription-service/SS-Release.shtml\nhttps://github.com/BuzzFeedNews/nics-firearm-background-checks\nThe markup\nTom Cardoso\nList of APIs: https://bookdown.org/paul/apis_for_social_scientists/\n\n\n\n\n\n\nBlei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3 (Jan): 993–1022."
  },
  {
    "objectID": "98-cocktails.html",
    "href": "98-cocktails.html",
    "title": "Appendix C — Cocktails",
    "section": "",
    "text": "Each of the chapters inspired a cocktail.\n\nChapter 1\n\nChapter 2\n\\(2\\) oz Hennessy\n\\(1\\) oz lemon juice\n\\(1\\) oz strawberry syrup\nTop with lemonade\nChapter 3\n\\(1\\frac{1}{2}\\) oz bourbon\n\\(\\frac{1}{2}\\) oz Benedictine\n\\(1\\) oz cherry syrup\n\\(\\frac{1}{2}\\) oz raspberry syrup\n\\(1\\) oz lemon juice\nCherry garnish\nChapter 4\n\\(1\\) oz cognac\n\\(\\frac{1}{2}\\) oz Grand Marnier\n\\(\\frac{1}{2}\\) oz Amaro Nonino\n\\(\\frac{1}{4}\\) oz lemon juice\n\\(\\frac{1}{4}\\) oz honey syrup\nCherry garnish\nChapter 5\n\\(1\\frac{1}{2}\\) oz vodka\n\\(\\frac{3}{4}\\) oz lime juice\n\\(\\frac{1}{2}\\) oz ginger syrup\nOne dash Angostura bitters\nTop with lemonade\nChapter 6\n\\(1\\frac{1}{2}\\) oz cherry-infused vodka\n\\(1\\) oz sweet vermouth\n\\(1\\) oz raspberry syrup\n\\(1\\) oz lime juice\nOne dash Angostura bitters\nChapter 7\n\\(1\\) oz cherry-infused vodka\n\\(1\\) oz lemon juice\n\\(\\frac{1}{2}\\) oz St Germain\n\\(\\frac{1}{2}\\) oz limoncello\n\\(\\frac{1}{2}\\) oz strawberry syrup\nOne dash elderflower bitters\nChapter 8\n\\(1\\frac{1}{4}\\) oz rye\n\\(\\frac{1}{2}\\) oz Grand Marnier\n\\(\\frac{3}{4}\\) oz lemon juice\n\\(\\frac{3}{4}\\) oz orange juice\n\\(\\frac{1}{2}\\) oz grenadine\n\\(\\frac{1}{4}\\) oz raspberry syrup\n\nChapter 9\n\\(1\\frac{1}{4}\\) oz Four Roses bourbon\n\\(1\\) oz apple cider\n\\(\\frac{1}{2}\\) oz Grand Marnier\n\\(\\frac{1}{4}\\) oz ameretto\n\\(\\frac{1}{2}\\) oz lemon juice\n\\(\\frac{1}{2}\\) oz ginger syrup\nOne dash Angostura bitters\n\nChapter 10\nJanuary has April showers and 2+2 always makes up 5 (measurement error)\n\\(2\\) oz bourbon\n\\(1\\) oz sweet vermouth\n\\(1\\frac{1}{2}\\) oz cherry syrup\n\\(1\\frac{1}{2}\\) oz lemon juice\nOne dash Old Fashioned bitters\n\nChapter 11\nYou can try the best you can, the best you can is good enough (some models are useful)\n\\(2\\) oz bourbon\n\\(1\\) oz Galliano Vanilla\n\\(1\\) oz raspberry syrup\n\\(1\\) oz lemon juice\nOne dash Angostura bitters\n\nChapter 12\nJust cause you feel it doesn’t mean it’s there (p hacking)\n\\(1\\frac{1}{2}\\) oz dark rum\n\\(1\\) oz honey syrup\n\\(1\\) oz lemon juice\n\\(\\frac{1}{2}\\) oz ginger syrup\nOne dash Old Fashioned bitters\nCherry garnish\n\nChapter 13\nWe are accidents waiting to happen (model overfit)\n\\(1\\frac{1}{2}\\) oz bourbon\n\\(\\frac{1}{2}\\) oz Amaro Averna\n\\(\\frac{1}{2}\\) oz lemon juice\n\\(\\frac{1}{2}\\) oz ginger syrup\n\nChapter 14\n\\(1\\frac{1}{2}\\) oz Empress gin\n\\(\\frac{1}{2}\\) oz St Germain\n\\(\\frac{1}{2}\\) oz lime juice\n\\(\\frac{1}{4}\\) oz raspberry syrup\nOne dash Angostura bitters\nChapter 15\n\\(1\\frac{1}{2}\\) oz Amaro Averna\n\\(\\frac{1}{2}\\) oz bourbon\n\\(\\frac{3}{4}\\) oz lemon juice\nCherry garnish\nChapter 16\n\\(1\\frac{1}{2}\\) oz Canadian whiskey\n\\(\\frac{3}{4}\\) oz Sweet vermouth\n\\(\\frac{3}{4}\\) oz Benedictine\nOne dash absinthe\nTwo dashes Peychaud’s bitters\nTwo dashes Angostura bitters\nChapter 17\n\\(1\\frac{1}{2}\\) oz dark rum\n\\(\\frac{3}{4}\\) oz cane sugar syrup\n\\(\\frac{1}{2}\\) oz lemon juice\n\\(\\frac{1}{2}\\) oz orange juice\n\\(\\frac{1}{2}\\) oz ginger syrup\nOne dash Old Fashioned bitters\nChapter 18\n\\(1\\frac{1}{2}\\) oz Empress gin\n\\(1\\) oz raspberry syrup\n\\(\\frac{3}{4}\\) oz lemon juice\nOne dash Angostura bitters\nChapter 19\n\\(2\\) oz Reid’s gin\n\\(\\frac{3}{4}\\) oz lemon juice\n\\(\\frac{3}{4}\\) oz ginger syrup\nOne dash Angostura bitters\nChapter 20\n\\(2\\) oz rye whiskey\n\\(1\\) oz sweet vermouth\n\\(\\frac{1}{2}\\) oz blackberry syrup\nOne dash Angostura bitters"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu, and Mark Ziemann.\n2021. “Gene Name Errors: Lessons Not Learned.” PLOS\nComputational Biology 17 (7): 1–13. https://doi.org/10.1371/journal.pcbi.1008984.\n\n\nAlexander, Monica. 2019a. “Reproducibility in Demographic\nResearch.” https://www.monicaalexander.com/posts/2019-10-20-reproducibility/.\n\n\n———. 2019b. “The Concentration and Uniqueness of Baby Names in\nAustralia and the US,” January. https://www.monicaalexander.com/posts/2019-20-01-babynames/.\n\n\n———. 2019c. “Analyzing Name Changes After Marriage Using a\nNon-Representative Survey,” August. https://www.monicaalexander.com/posts/2019-08-07-mrp/.\n\n\n———. 2021. “Overcoming Barriers to Sharing Code.”\nYouTube, February. https://youtu.be/yvM2C6aZ94k.\n\n\nAlexander, Monica J, Mathew V Kiang, and Magali Barbieri. 2018.\n“Trends in Black and White Opioid Mortality in the United States,\n1979–2015.” Epidemiology (Cambridge, Mass.) 29 (5): 707.\n\n\nAlexander, Monica, and Leontine Alkema. 2018. “Global Estimation\nof Neonatal Mortality Using a Bayesian Hierarchical Splines Regression\nModel.” Demographic Research 38: 335–72.\n\n\n———. 2021. “A Bayesian Cohort Component Projection Model to\nEstimate Adult Populations at the Subnational Level in Data-Sparse\nSettings.” https://arxiv.org/abs/2102.06121.\n\n\nAlexander, Rohan, and Monica Alexander. 2021. “The Increased\nEffect of Elections and Changing Prime Ministers on Topics Discussed in\nthe Australian Federal Parliament Between 1901 and 2018.” https://arxiv.org/abs/2111.09299.\n\n\nAlexander, Rohan, and Paul A. Hodgetts. 2021. AustralianPoliticians:\nProvides Datasets about Australian Politicians. https://CRAN.R-project.org/package=AustralianPoliticians.\n\n\nAlexander, Rohan, and Zachary Ward. 2018. “Age at Arrival and\nAssimilation During the Age of Mass Migration.” The Journal\nof Economic History 78 (3): 904–37.\n\n\nAllaire, JJ, Rich Iannone, Alison Presmanes Hill, and Yihui Xie. 2021.\nDistill: ’R Markdown’ Format for Scientific and Technical\nWriting.\n\n\nAllen, Jeff. 2021. plumberDeploy: Plumber Deployment. https://CRAN.R-project.org/package=plumberDeploy.\n\n\nAlsan, Marcella, and Marianne Wanamaker. 2018. “Tuskegee and the\nHealth of Black Men.” The Quarterly Journal of Economics\n133 (1): 407–55.\n\n\nAmaka, Ofunne, and Amber Thomas. 2021. The Naked Truth: How the\nNames of 6,816 Complexion Products Can Reveal Bias in Beauty. https://pudding.cool/2021/03/foundation-names/.\n\n\nAngelucci, Charles, and Julia Cagé. 2019. “Newspapers in Times of\nLow Advertising Revenues.” American Economic Journal:\nMicroeconomics 11 (3): 319–64.\n\n\nAnnas, George J. 2003. “HIPAA Regulations: A New Era of\nMedical-Record Privacy?” New England Journal of Medicine\n348: 1486.\n\n\nArel-Bundock, Vincent. 2021a. Modelsummary: Summary Tables and Plots\nfor Statistical Models and Data: Beautiful, Customizable, and\nPublication-Ready. https://CRAN.R-project.org/package=modelsummary.\n\n\n———. 2021b. WDI: World Development Indicators and Other World Bank\nData. https://CRAN.R-project.org/package=WDI.\n\n\nArnold, Jeffrey B. 2021. Ggthemes: Extra Themes, Scales and Geoms\nfor ’Ggplot2’. https://CRAN.R-project.org/package=ggthemes.\n\n\nAssociation, American Medical, and New York Academy of Medicine. 1848.\nCode of Medical Ethics. Academy of Medicine.\n\n\nAthey, Susan, and Guido W Imbens. 2017. “The State of Applied\nEconometrics: Causality and Policy Evaluation.” Journal of\nEconomic Perspectives 31 (2): 3–32.\n\n\nAthey, Susan, Guido W Imbens, Jonas Metzger, and Evan Munro. 2021.\n“Using Wasserstein Generative Adversarial Networks for the Design\nof Monte Carlo Simulations.” Journal of Econometrics.\n\n\nAu, Randy. 2020. “Data Cleaning IS Analysis, Not Grunt\nWork.” Counting Stuff. https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt.\n\n\nBaker, Reg, J. Michael Brick, Nancy A. Bates, Mike Battaglia, Mick P.\nCouper, Jill A. Dever, Krista J. Gile, and Roger Tourangeau. 2013.\n“Summary Report of the AAPOR Task Force on\nNon-probability Sampling.” Journal of Survey\nStatistics and Methodology 1 (2): 90–143. https://doi.org/10.1093/jssam/smt008.\n\n\nBandy, Jack, and Nicholas Vincent. 2021. “Addressing\n\"Documentation Debt\" in Machine Learning Research: A Retrospective\nDatasheet for BookCorpus.” https://arxiv.org/abs/2105.05241.\n\n\nBarba, Lorena A. 2018. “Terminologies for Reproducible\nResearch.” https://arxiv.org/abs/1802.03311.\n\n\nBarrett, Malcolm. 2021a. Data Science as an Atomic Habit. https://malco.io/2021/01/04/data-science-as-an-atomic-habit/.\n\n\n———. 2021b. Ggdag: Analyze and Create Elegant Directed Acyclic\nGraphs. https://CRAN.R-project.org/package=ggdag.\n\n\nBarron, Alexander TJ, Jenny Huang, Rebecca L Spang, and Simon DeDeo.\n2018. “Individuals, Institutions, and Innovation in the Debates of\nthe French Revolution.” Proceedings of the National Academy\nof Sciences 115 (18): 4607–12.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015.\n“Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical\nSoftware 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nBaumer, Benjamin, Daniel Kaplan, and Nicholas Horton. 2021. Modern\nData Science with r. 2nd ed. CRC Press.\n\n\nBeauregard, Katrine, and Jill Sheppard. 2021. “Antiwomen but\nProquota: Disaggregating Sexism and Support for Gender Quota\nPolicies.” Political Psychology 42 (2): 219–37.\n\n\nBensinger, Greg. 2020. Google Redraws the Borders on Maps Depending\non Who’s Looking. Washington Post.\n\n\nBerkson, Joseph. 1946. “Limitations of the Application of Fourfold\nTable Analysis to Hospital Data.” Biometrics Bulletin 2\n(3): 47–53. http://www.jstor.org/stable/3002000.\n\n\nBerners-Lee, Timothy J. 1989. “Information Management: A\nProposal.”\n\n\nBerry, Donald A. 1989. “[Investigating Therapies of Potentially\nGreat Benefit: ECMO]: Comment: Ethics and ECMO.” Statistical\nScience 4 (4): 306–10.\n\n\nBertrand, Marianne, and Sendhil Mullainathan. 2004. “Are Emily and\nGreg More Employable Than Lakisha and Jamal? A Field Experiment on Labor\nMarket Discrimination.” American Economic Review 94 (4):\n991–1013.\n\n\nBickel, Peter J, Eugene A Hammel, and J William O’Connell. 1975.\n“Sex Bias in Graduate Admissions: Data from Berkeley: Measuring\nBias Is Harder Than Is Usually Assumed, and the Evidence Is Sometimes\nContrary to Expectation.” Science 187 (4175): 398–404.\n\n\nBiderman, Stella, Kieran Bicheno, and Leo Gao. 2022. “Datasheet\nfor the Pile.” https://arxiv.org/abs/2201.07311.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and\nLuke Sonnet. 2021. Estimatr: Fast Estimators for Design-Based\nInference. https://CRAN.R-project.org/package=estimatr.\n\n\nBlair, James. 2019. Democratizing r with Plumber APIs. https://www.rstudio.com/resources/rstudioconf-2019/democratizing-r-with-plumber-apis/.\n\n\nBland, J Martin, and DouglasG Altman. 1986. “Statistical Methods\nfor Assessing Agreement Between Two Methods of Clinical\nMeasurement.” The Lancet 327 (8476): 307–10.\n\n\nBlei, David M. 2012. “Probabilistic Topic Models.”\nCommunications of the ACM 55 (4): 77–84.\n\n\nBlei, David M, and John D Lafferty. 2009. “Topic Models.”\nIn Text Mining, 101–24. Chapman; Hall/CRC.\n\n\nBlei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent\nDirichlet Allocation.” Journal of Machine Learning\nResearch 3 (Jan): 993–1022.\n\n\nBloom, Howard, Andrew Bell, and Kayla Reiman. 2020. “Using Data\nfrom Randomized Trials to Assess the Likely Generalizability of\nEducational Treatment-Effect Estimates from Regression Discontinuity\nDesigns.” Journal of Research on Educational\nEffectiveness, 1–30. https://doi.org/10.1080/19345747.2019.1634169.\n\n\nBoland, Philip J. 1984. “A Biographical Glimpse of William Sealy\nGosset.” The American Statistician 38 (3): 179–83.\n\n\nBolton, Ruth, and Randall Chapman. 1986. “Searching for Positive\nReturns at the Track.” Management Science 32 (August):\n1040–60. https://doi.org/10.1287/mnsc.32.8.1040.\n\n\nBouie, Jamelle. 2022. We Still Can’t See American Slavery for What\nIt Was.\n\n\nBowers, Jake. 2011. “Six Steps to a Better Relationship with Your\nFuture Self.” The Political Methodologist 18 (2): 2–8.\n\n\nBowley, Arthur Lyon. 1901. Elements of Statistics. P. S. King.\n\n\n———. 1913. “Working-Class Households in Reading.”\nJournal of the Royal Statistical Society 76 (7): 672–701.\n\n\nBrandt, Allan M. 1978. “Racism and Research: The Case of the\nTuskegee Syphilis Study.” Hastings Center Report, 21–29.\n\n\nBriggs, Ryan C. 2021. “Why Does Aid Not Target the\nPoorest?” International Studies Quarterly 65 (3):\n739–52.\n\n\nBrokowski, Carolyn, and Mazhar Adli. 2019. “CRISPR Ethics: Moral\nConsiderations for Applications of a Powerful Tool.” Journal\nof Molecular Biology 431 (1): 88–101.\n\n\nBronte, Charlotte. 1847. Jane Eyre. https://www.gutenberg.org/files/1260/1260-h/1260-h.htm.\n\n\nBrontë, Charlotte. 1857. The Professor.\n\n\nBrook, Robert H, John E Ware, William H Rogers, Emmett B Keeler, Allyson\nRoss Davies, Cathy D Sherbourne, George A Goldberg, Kathleen N Lohr,\nPatricia Camp, and Joseph P Newhouse. 1984. “The Effect of\nCoinsurance on the Health of Adults: Results from the RAND Health\nInsurance Experiment.”\n\n\nBryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk\nabout Version Control?” The American Statistician 72\n(1): 20–27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\nBryan, Jennifer, and Jim Hester. 2020. What They Forgot to Teach You\nabout r. https://rstats.wtf/index.html.\n\n\nBryan, Jennifer, Jim Hester, David Robinson, and Hadley Wickham. 2019.\nReprex: Prepare Reproducible Example Code via the Clipboard. https://CRAN.R-project.org/package=reprex.\n\n\nBryan, Jenny. 2018. “Code Smells and Feels.”\nYouTube, July. https://youtu.be/7oyiPBjLAWY.\n\n\n———. 2020. Happy Git and GitHub for the\nuseR. https://happygitwithr.com.\n\n\nBuhr, Ray. 2017. Using r as a Production Machine Learning Language\n(Part i). https://raybuhr.github.io/blog/posts/making-predictions-over-http/.\n\n\nBuja, Andreas, Dianne Cook, and Deborah F Swayne. 1996.\n“Interactive High-Dimensional Data Visualization.”\nJournal of Computational and Graphical Statistics 5 (1): 78–99.\n\n\nBush, Vannevar. 1945. “As We May Think.” The Atlantic\nMonthly 176 (1): 101–8.\n\n\nCahill, Niamh, Michelle Weinberger, and Leontine Alkema. 2020.\n“What Increase in Modern Contraceptive Use Is Needed in Fp2020\nCountries to Reach 75% Demand Satisfied by 2030? An Assessment Using the\nAccelerated Transition Method and Family Planning Estimation\nModel.” Gates Open Research 4.\n\n\nCalonico, Sebastian, Matias D. Cattaneo, Max H. Farrell, and Rocio\nTitiunik. 2021. Rdrobust: Robust Data-Driven Statistical Inference\nin Regression-Discontinuity Designs. https://CRAN.R-project.org/package=rdrobust.\n\n\nCambon, Jesse, and Christopher Belanger. 2021. “Tidygeocoder:\nGeocoding Made Easy.” Zenodo. https://doi.org/10.5281/zenodo.3981510.\n\n\nCarle, Eric. 1969. The Very Hungry Caterpillar. World\nPublishing Company.\n\n\nCarleton, Chris. 2021. Wccarleton/Conflict-Europe: Acce\n(version v1.0.0). Zenodo. https://doi.org/10.5281/zenodo.4550688.\n\n\nCarleton, W Christopher, Dave Campbell, and Mark Collard. 2021. “A\nReassessment of the Impact of Temperature Change on European Conflict\nDuring the Second Millennium CE Using a Bespoke Bayesian Time-Series\nModel.” Climatic Change 165 (1): 1–16.\n\n\nCaro, Robert. 2019. Working. 1st ed. Knopf.\n\n\nCarroll, Lewis. 1865. Alice’s Adventures in Wonderland.\nMacmillan.\n\n\n———. 1871. Through the Looking-Glass. Macmillan.\n\n\nChamberlain, Scott, Hadley Wickham, and Winston Chang. 2021.\nAnalogsea: Interface to ’Digital Ocean’.\n\n\nChambliss, Daniel F. 1989. “The Mundanity of Excellence: An\nEthnographic Report on Stratification and Olympic Swimmers.”\nSociological Theory 7 (1): 70–86.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke,\nYihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara\nBorges. 2021. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny.\n\n\nChellel, Kit. 2018. “The Gambler Who Cracked the Horse-Racing\nCode.” Bloomberg Businessweek (May 2018). Featured in\nBloomberg Businessweek, May 14.\n\n\nChen, Wei, Xilu Chen, Chang-Tai Hsieh, and Zheng Song. 2019. “A\nForensic Examination of China’s National Accounts.” National\nBureau of Economic Research.\n\n\nCheng, Joe, Bhaskar Karambelkar, and Yihui Xie. 2021. Leaflet:\nCreate Interactive Web Maps with the JavaScript ’Leaflet’ Library.\nhttps://CRAN.R-project.org/package=leaflet.\n\n\nChouldechova, Alexandra, Diana Benavides-Prado, Oleksandr Fialko, and\nRhema Vaithianathan. 2018. “A Case Study of Algorithm-Assisted\nDecision Making in Child Maltreatment Hotline Screening\nDecisions.” In Proceedings of the 1st Conference on Fairness,\nAccountability and Transparency, edited by Sorelle A. Friedler and\nChristo Wilson, 81:134–48. Proceedings of Machine Learning Research.\nPMLR. https://proceedings.mlr.press/v81/chouldechova18a.html.\n\n\nChrétien, Jean. 2007. My Years as Prime Minister. Knopf Canada.\n\n\nChristensen, Garret, Allan Dafoe, Edward Miguel, Don A Moore, and Andrew\nK Rose. 2019. “A Study of the Impact of Data Sharing on Article\nCitations Using Journal Policies as a Natural Experiment.”\nPLoS One 14 (12): e0225883.\n\n\nChristensen, Garret, Jeremy Freese, and Edward Miguel. 2019.\nTransparent and Reproducible Social Science Research.\nUniversity of California Press.\n\n\nCirone, Alexandra, and Arthur Spirling. 2021. “Turning History\ninto Data: Data Collection, Measurement, and Inference in HPE.”\nJournal of Historical Political Economy 1 (1): 127–54. https://doi.org/10.1561/115.00000005.\n\n\nCity of Toronto. 2021. 2021 Street Needs Assessment. https://www.toronto.ca/city-government/data-research-maps/research-reports/housing-and-homelessness-research-and-reports/.\n\n\nCleveland, William. 1994. The Elements of Graphing Data. 2nd\ned. Hobart Press.\n\n\nCohen, I. Glenn, and Michelle M. Mello. 2018. “HIPAA\nand Protecting Health Information in the 21st Century.”\nJAMA 320 (3): 231. https://doi.org/10.1001/jama.2018.5630.\n\n\nCohn, Nate. 2016. We Gave Four Good Pollsters the Same Raw Data.\nThey Had Four Different Results.\n\n\nCook, Dianne, Nancy Reid, and Emi Tanaka. 2021. “The Foundation Is\nAvailable for Thinking about Data Visualization Inferentially.”\nHarvard Data Science Review, July. https://doi.org/10.1162/99608f92.8453435d.\n\n\nCooley, David. 2020. Mapdeck: Interactive Maps Using ’Mapbox GL JS’\nand ’Deck.gl’. https://CRAN.R-project.org/package=mapdeck.\n\n\nCouncil of European Union. 2016. “General Data Protection\nRegulation 2016/679.”\n\n\nCox, David. 2018. “In Gentle Praise of Significance Tests.”\nYouTube, October. https://youtu.be/txLj_P9UlCQ.\n\n\nCox, Murray. 2021. “Inside Airbnb - Toronto\nData.” http://insideairbnb.com/get-the-data.html.\n\n\nCraiu, Radu V. 2019. “The Hiring Gambit: In Search of the Twofer\nData Scientist.” Harvard Data Science Review 1 (1).\n\n\nCramer, Jan Salomon. 2002. “The Origins of Logistic\nRegression.”\n\n\nCrawford, Kate. 2021. Atlas of AI.\nYale University Press.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. Yale\nPress.\n\n\nD’Ignazio, Catherine, and Lauren F Klein. 2020. Data Feminism.\nMit Press.\n\n\nDagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark A\nKatz, Miguel A Hernán, Marc Lipsitch, Ben Reis, and Ran D Balicer. 2021.\n“BNT162b2 mRNA Covid-19 Vaccine in a Nationwide Mass Vaccination\nSetting.” New England Journal of Medicine.\n\n\nDarling, William M. 2011. “A Theoretical and Practical\nImplementation Tutorial on Topic Modeling and Gibbs Sampling.” In\nProceedings of the 49th Annual Meeting of the Association for\nComputational Linguistics: Human Language Technologies, 642–47.\n\n\nDeWitt, Helen. 2000. The Last Samurai. Talk Mirimax Books.\n\n\nDoll, Richard, and A Bradford Hill. 1950. “Smoking and Carcinoma\nof the Lung.” British Medical Journal 2 (4682): 739.\n\n\nEdgeworth, Francis Ysidro. 1885. “Methods of Statistics.”\nJournal of the Statistical Society of London, 181–217.\n\n\nEghbal, Nadia. 2020. Working in Public: The Making and Maintenance\nof Open Source Software. Stripe Press.\n\n\nFarrugia, Patricia, Bradley A Petrisor, Forough Farrokhyar, and Mohit\nBhandari. 2010. “Research Questions, Hypotheses and\nObjectives.” Canadian Journal of Surgery 53 (4): 278.\n\n\nFinkelstein, Amy, Sarah Taubman, Bill Wright, Mira Bernstein, Jonathan\nGruber, Joseph P Newhouse, Heidi Allen, Katherine Baicker, and Oregon\nHealth Study Group. 2012. “The Oregon Health Insurance Experiment:\nEvidence from the First Year.” The Quarterly Journal of\nEconomics 127 (3): 1057–1106.\n\n\nFirke, Sam. 2020. Janitor: Simple Tools for Examining and Cleaning\nDirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nFisher, Ronald. 1935. The Design of Experiments. Oliver; Boyd.\n\n\nFisher, Ronald Aylmer. 1926. “The Arrangement\nof Field Experiments,” 503–15. https://doi.org/10.23637/ROTHAMSTED.8V61Q.\n\n\nFiske, Susan T, and Shiro Kuriwaki. 2021. “Words to the Wise on\nWriting Scientific Papers.”\n\n\nFitts, Alexis Sobel. 2014. “The King of Content: How Upworthy Aims\nto Alter the Web, and Could End up Altering the World.”\nColumbia Journalism Review. https://archives.cjr.org/feature/the_king_of_content.php.\n\n\nFlynn, Michael. 2021. Troopdata: Tools for Analyzing Cross-National\nMilitary Deployment and Basing Data. https://CRAN.R-project.org/package=troopdata.\n\n\nForster, E M. 1927. Aspects of the Novel. Edward Arnold.\n\n\nFoster, Gordon. 1968. “Computers, Statistics and Planning: Systems\nor Chaos?” Geary Lecture. https://www.esri.ie/system/files/media/file-uploads/2016-03/GLS2.pdf.\n\n\nFourcade, Marion, and Kieran Healy. 2017. “Seeing Like a\nMarket.” Socio-Economic Review 15 (1): 9–29.\n\n\nFox, John, and Robert Andersen. 2006. “Effect Displays for\nMultinomial and Proportional-Odds Logit Models.” Sociological\nMethodology 36 (1): 225–55.\n\n\nFranconeri, Steven L, Lace M Padilla, Priti Shah, Jeffrey M Zacks, and\nJessica Hullman. 2021. “The Science of Visual Data Communication:\nWhat Works.” Psychological Science in the Public\nInterest 22 (3): 110–61.\n\n\nFranklin, Laura R. 2005. “Exploratory Experiments.”\nPhilosophy of Science 72 (5): 888–99.\n\n\nFriedman, Jerome H., Robert Tibshirani, and Trevor Hastie. 2009. The\nElements of Statistical Learning. Springer.\n\n\nFriendly, Michael, and Howard Wainer. 2021. A History of Data\nVisaulization and Graphic Communication. 1st ed. Harvard University\nPress.\n\n\nFry, Hannah. 2020. “Big Tech Is Testing You.” The New\nYorker, 61–65.\n\n\nGagolewski, Marek. 2020. R Package Stringi: Character String\nProcessing Facilities. http://www.gagolewski.com/software/stringi/.\n\n\nGarnier, Simon, Ross, Noam, Rudis, Robert, Camargo, et al. 2021.\nviridis - Colorblind-Friendly Color Maps\nfor r. https://doi.org/10.5281/zenodo.4679424.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021.\n“Datasheets for Datasets.” Communications of the\nACM 64 (12): 86–92.\n\n\nGelfand, Sharla. 2019. Crying @ Sephora. https://sharla.party/post/crying-sephora/.\n\n\n———. 2020. Opendatatoronto: Access the City of Toronto Open Data\nPortal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\n———. 2021. “Make a ReprEx... Please.” YouTube,\nFebruary. https://youtu.be/G5Nm-GpmrLw.\n\n\nGelman, Andrew. 2016. “What Has Happened down Here Is the Winds\nHave Changed.” https://statmodeling.stat.columbia.edu/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/.\n\n\n———. 2019. Another Regression Discontinuity Disaster and What Can We\nLearn from It. https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/.\n\n\n———. 2020. “Statistical Models of Election Outcomes.”\nYouTube, August. https://youtu.be/7gjDnrbLQ4k.\n\n\n———. 2021. “Wrong Again! 30+ Years of Statistical\nMistakes.” YouTube, October. https://youtu.be/mB9Q26uptao.\n\n\nGelman, Andrew, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and\nDonald Rubin. 2014. Bayesian Data Analysis. 3rd ed. CRC Press.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using\nRegression and Multilevel/Hierarchical Models.\n\n\nGelman, Andrew, and Eric Loken. 2013. “The Garden of Forking\nPaths: Why Multiple Comparisons Can Be a Problem, Even When There Is No\n‘Fishing Expedition’ or ‘p-Hacking’ and the\nResearch Hypothesis Was Posited Ahead of Time.” Department of\nStatistics, Columbia University 348.\n\n\nGelman, Andrew, Greggor Mattson, and Daniel Simpson. 2018. “Gaydar\nand the Fallacy of Decontextualized Measurement.”\nSociological Science 5 (12): 270–80. https://doi.org/10.15195/v5.a12.\n\n\nGelman, Andrew, Cristian Pasarica, and Rahul Dodhia. 2002. “Let’s\nPractice What We Preach: Turning Tables into Graphs.” The\nAmerican Statistician 56 (2): 121–30.\n\n\nGelman, Andrew, and Aki Vehtari. 2020. “What Are the Most\nImportant Statistical Ideas of the Past 50 Years?” arXiv\nPreprint arXiv:2012.00174.\n\n\nGerber, Alan, and Donald Green. 2012. Field Experiments: Design,\nAnalysis, and Interpretation. W W Norton.\n\n\nGertler, Paul J, Sebastian Martinez, Patrick Premand, Laura B Rawlings,\nand Christel MJ Vermeersch. 2016. Impact Evaluation in\nPractice. The World Bank.\n\n\nGeuenich, Michael J, Jinyu Hou, Sunyun Lee, Shanza Ayub, Hartland W\nJackson, and Kieran R Campbell. 2021. “Automated Assignment of\nCell Identity from Single-Cell Multiplexed Imaging and Proteomic\nData.” Cell Systems 12 (12): 1173–86.\n\n\nGhitza, Yair, and Andrew Gelman. 2020. “Voter Registration\nDatabases and MRP: Toward the Use of Large-Scale Databases in Public\nOpinion Research.” Political Analysis 28 (4): 507–31.\n\n\nGoodman, Leo A. 1961. “Snowball Sampling.” The Annals\nof Mathematical Statistics, 148–70.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2020.\n“Rstanarm: Bayesian Applied Regression Modeling via\nStan.” https://mc-stan.org/rstanarm.\n\n\nGould, Stephen Jay. 2013. “The Median Isn’t the Message.”\nAMA Journal of Ethics 15 (1): 77–81.\n\n\nGraham, Paul. 2020. How to Write Usefully. http://paulgraham.com/useful.html.\n\n\nGreen, Donald P, Terence Y Leong, Holger L Kern, Alan S Gerber, and\nChristopher W Larimer. 2009. “Testing the Accuracy of Regression\nDiscontinuity Analysis Using Experimental Benchmarks.”\nPolitical Analysis 17 (4): 400–417.\n\n\nGreen, Eric. 2020. “Nivi Research: Mister p Helps Us Understand\nVaccine Hesitancy.” https://research.nivi.io/posts/2020-12-08-mister-p-helps-us-understand-vaccine-hesitancy/.\n\n\nGreenland, Sander, Stephen J Senn, Kenneth J Rothman, John B Carlin,\nCharles Poole, Steven N Goodman, and Douglas G Altman. 2016.\n“Statistical Tests, p Values, Confidence Intervals, and Power: A\nGuide to Misinterpretations.” European Journal of\nEpidemiology 31 (4): 337–50.\n\n\nGriffiths, Thomas, and Mark Steyvers. 2004. “Finding Scientific\nTopics.” PNAS 101: 5228–35.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times\nMade Easy with lubridate.”\nJournal of Statistical Software 40 (3): 1–25. http://www.jstatsoft.org/v40/i03/.\n\n\nGrün, Bettina, and Kurt Hornik. 2011. “topicmodels: An R Package for Fitting\nTopic Models.” Journal of Statistical Software 40 (13):\n1–30. https://doi.org/10.18637/jss.v040.i13.\n\n\nHalberstam, David. 1972. The Best and the\nBrightest. Random House.\n\n\nHamming, Richard W. 1996. The Art of Doing\nScience and Engineering. Stripe Press.\n\n\nHandcock, Mark S, and Krista J Gile. 2011. “Comment: On the\nConcept of Snowball Sampling.” Sociological Methodology\n41 (1): 367–71.\n\n\nHangartner, Dominik, Daniel Kopp, and Michael Siegenthaler. 2021.\n“Monitoring Hiring Discrimination Through Online Recruitment\nPlatforms.” Nature 589 (7843): 572–76.\n\n\nHanretty, Chris. 2020. “An Introduction to Multilevel Regression\nand Post-Stratification for Estimating Constituency Opinion.”\nPolitical Studies Review 18 (4): 630–45.\n\n\nHao, Karen. 2019. “This is how AI bias really\nhappens—and why it’s so hard to fix.” MIT Technology\nReview.\n\n\nHart, Edmund M, Pauline Barmby, David LeBauer, François Michonneau,\nSarah Mount, Patrick Mulrooney, Timothée Poisot, Kara H Woo, Naupaka B\nZimmerman, and Jeffrey W Hollister. 2016. “Ten Simple Rules for\nDigital Data Storage.” Public Library of Science San Francisco,\nCA USA.\n\n\nHastie, Trevor J, and Robert J Tibshirani. 1990. Generalized\nAdditive Models. Vol. 43. CRC press.\n\n\nHayot, Eric. 2014. The Elements of Academic Style. Columbia\nUniversity Press.\n\n\nHealy, Kieran. 2018. Data Visualization. Princeton University\nPress.\n\n\n———. 2020. The Kitchen Counter Observatory. https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/.\n\n\nHeckathorn, Douglas D. 1997. “Respondent-Driven Sampling: A New\nApproach to the Study of Hidden Populations.” Social\nProblems 44 (2): 174–99.\n\n\nHeckman, James J. 1979. “Sample Selection Bias as a Specification\nError.” Econometrica: Journal of the Econometric\nSociety, 153–61.\n\n\nHeil, Benjamin J, Michael M Hoffman, Florian Markowetz, Su-In Lee, Casey\nS Greene, and Stephanie C Hicks. 2021. “Reproducibility Standards\nfor Machine Learning in the Life Sciences.” Nature\nMethods 18 (10): 1132–35.\n\n\nHenry, Lionel, and Hadley Wickham. 2020. Purrr: Functional\nProgramming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nHernan, Miguel A, and James M Robins. 2020. What If. CRC Press.\n\n\nHerndon, Thomas, Michael Ash, and Robert Pollin. 2014. “Does High\nPublic Debt Consistently Stifle Economic Growth? A Critique of Reinhart\nand Rogoff.” Cambridge Journal of Economics 38 (2):\n257–79.\n\n\nHillel, Wayne. 2017. How Do We Trust Our Science Code? https://www.hillelwayne.com/how-do-we-trust-science-code/.\n\n\nHo, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2011.\n“MatchIt: Nonparametric Preprocessing for Parametric\nCausal Inference.” Journal of Statistical Software 42\n(8): 1–28. https://doi.org/10.18637/jss.v042.i08.\n\n\nHofmeister, Johannes, Janet Siegmund, and Daniel V. Holt. 2017.\n“Shorter Identifier Names Take Longer to Comprehend.” In\n2017 IEEE 24th International Conference on Software Analysis,\nEvolution and Reengineering (SANER), 217–27. https://doi.org/10.1109/SANER.2017.7884623.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.”\nJournal of the American Statistical Association 81 (396):\n945–60.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020.\nPalmerpenguins: Palmer Archipelago (Antarctica) Penguin Data.\nhttps://allisonhorst.github.io/palmerpenguins/.\n\n\nHug, Lucia, Monica Alexander, Danzhen You, Leontine Alkema, and UN\nInter-agency Group for Child. 2019. “National, Regional, and\nGlobal Levels and Trends in Neonatal Mortality Between 1990 and 2017,\nwith Scenario-Based Projections to 2030: A Systematic Analysis.”\nThe Lancet Global Health 7 (6): e710–20.\n\n\nHughes, Nicola, and Jill Rutter. 2016. Oliver Letwin. https://www.instituteforgovernment.org.uk/ministers-reflect/person/oliver-letwin/.\n\n\nHulley, Stephen B. 2007. Designing Clinical Research.\nLippincott Williams & Wilkins.\n\n\nHullman, Jessica, and Andrew Gelman. 2021. “Designing for\nInteractive Exploratory Data Analysis Requires Theories of Graphical\nInference.” Harvard Data Science Review, July. https://doi.org/10.1162/99608f92.3ab8a587.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to\nResearch Design and Causality. Chapman & Hall.\n\n\n———. 2022. “Library of Statistical Techniques.” https://lost-stats.github.io.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni,\nJeffrey R Bloem, Pralhad Burli, Naibin Chen, et al. 2021. “The\nInfluence of Hidden Researcher Decisions in Applied\nMicroeconomics.” Economic Inquiry.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni,\nJeffrey Bloem, Pralhad H Burli, Naibin Chen, et al. 2020. “The\nInfluence of Hidden Researcher Decisions in Applied\nMicroeconomics.”\n\n\nHuyen, Chip. 2020. Machine Learning Is Going Real-Time. https://huyenchip.com/2020/12/27/real-time-machine-learning.html.\n\n\n———. 2022. Real-Time Machine Learning: Challenges and\nSolutions. https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning\nfor Text Analysis in r. Chapman; Hall/CRC.\n\n\nIannone, Richard. 2020. DiagrammeR: Graph/Network\nVisualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nIannone, Richard, Joe Cheng, and Barret Schloerke. 2020. Gt: Easily\nCreate Presentation-Ready Display Tables. https://CRAN.R-project.org/package=gt.\n\n\nIannone, Richard, and Mauricio Vargas. 2022. Pointblank: Data\nValidation and Organization of Metadata for Local and Remote\nTables. https://CRAN.R-project.org/package=pointblank.\n\n\nIgelström, Erik. 2020. “Causal Graphs in r with\nDiagrammeR.” https://www.erikigelstrom.com/articles/causal-graphs-in-r-with-diagrammer/.\n\n\nIoannidis, John PA. 2005. “Why Most Published Research Findings\nAre False.” PLoS Medicine 2 (8): e124.\n\n\nIsaacson, Walter. 2011. Steve Jobs. Simon & Schuster.\n\n\nIshiguro, Kazuo. 1989. The Remains of the Day. Faber; Faber.\n\n\nIzrailev, Sergei. 2014. Tictoc: Functions for Timing r Scripts.\nhttps://CRAN.R-project.org/package=tictoc.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2017. An Introduction to Statistical Learning with Applications in\nr.\n\n\nJohnson, Alicia A., Miles Ott, and Mine Dogucu. 2022. Bayes Rules!\nAn Introduction to Bayesian Modeling with r. CRC Press.\n\n\nJohnson, Kaneesha R. 2021. “Two Regimes of Prison Data\nCollection.” Harvard Data Science Review, July. https://doi.org/10.1162/99608f92.72825001.\n\n\nJones, Arnold HM. 1953. “Census Records of the Later Roman\nEmpire.” The Journal of Roman Studies 43 (1-2): 49–64.\n\n\nJordan, Michael I. 2019. “Artificial Intelligence—the Revolution\nHasn’t Happened Yet.” Harvard Data Science Review 1 (1).\nhttps://doi.org/10.1162/99608f92.f06c6e61.\n\n\nJoyner, MICHAEL J. 1991. “Modeling: Optimal Marathon Performance\non the Basis of Physiological Factors.” Journal of Applied\nPhysiology 70 (2): 683–87.\n\n\nKahle, David, and Hadley Wickham. 2013. “Ggmap: Spatial\nVisualization with Ggplot2.” The R Journal 5 (1):\n144–61. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf.\n\n\nKahneman, Daniel, Olivier Sibony, and Cass Sunstein. 2021. Noise: A\nFlaw in Human Judgment. William Collins.\n\n\nKay, Matthew. 2020. tidybayes: Tidy Data\nand Geoms for Bayesian Models. https://doi.org/10.5281/zenodo.1308151.\n\n\nKearney, Michael W. 2019. “Rtweet: Collecting and Analyzing\nTwitter Data.” Journal of Open Source Software 4 (42):\n1829. https://doi.org/10.21105/joss.01829.\n\n\nKennedy, Lauren, and Jonah Gabry. 2020. “MRP with\nRstanarm.” https://mc-stan.org/rstanarm/articles/mrp.html.\n\n\nKennedy, Lauren, and Andrew Gelman. 2020. “Know Your Population\nand Know Your Model: Using Model-Based Regression and Poststratification\nto Generalize Findings Beyond the Observed Sample.” https://arxiv.org/abs/1906.11323.\n\n\nKennedy, Lauren, Katharine Khanna, Daniel Simpson, and Andrew Gelman.\n2020. “Using Sex and Gender in Survey Adjustment.” https://arxiv.org/abs/2009.14401.\n\n\nKenny, Christopher T., Shiro Kuriwaki, Cory McCartan, Evan Rosenman,\nTyler Simko, and Kosuke Imai. 2021. “The Impact of the u.s. Census\nDisclosure Avoidance System on Redistricting and Voting Rights\nAnalysis.” https://arxiv.org/abs/2105.14197.\n\n\nKeyes, Os. 2019. “Counting the Countless.” Real\nLife. https://reallifemag.com/counting-the-countless/.\n\n\nKharecha, Pushker A, and James E Hansen. 2013. “Prevented\nMortality and Greenhouse Gas Emissions from Historical and Projected\nNuclear Power.” Environmental Science & Technology\n47 (9): 4889–95.\n\n\nKiang, Mathew V, Alexander C Tsai, Monica J Alexander, David H Rehkopf,\nand Sanjay Basu. 2021. “Racial/Ethnic Disparities in\nOpioid-Related Mortality in the USA, 1999–2019: The Extreme Case of\nWashington DC.” Journal of Urban Health 98 (5): 589–95.\n\n\nKimmerer, Robin Wall. 2012. Braiding Sweetgrass. Milkweed\nEditions.\n\n\nKing, Gary. 2006. “Publication, Publication.” PS:\nPolitical Science & Politics 39 (1): 119–25.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores\nShould Not Be Used for Matching.” Political Analysis 27\n(4): 435–54.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft.\nScribner.\n\n\nKleiber, Christian, and Achim Zeileis. 2008. Applied Econometrics\nwith R. New York: Springer-Verlag. https://CRAN.R-project.org/package=AER.\n\n\nKnuth, D. E. 1984. “Literate Programming.” The Computer\nJournal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nKnuth, Donald E. 1998. Art of Computer Programming, Volume 2:\nSeminumerical Algorithms. 2nd ed.\n\n\nKoenecke, Allison, and Hal Varian. 2020. “Synthetic Data\nGeneration for Economists.” https://arxiv.org/abs/2011.01374.\n\n\nKoenker, Roger, and Achim Zeileis. 2009. “On Reproducible\nEconometric Research.” Journal of Applied Econometrics\n24 (5): 833–47.\n\n\nKohavi, Ron, Diane Tang, and Ya Xu. 2020. Trustworthy Online\nControlled Experiments: A Practical Guide to a/b Testing. Cambridge\nUniversity Press.\n\n\nKröger, Jacob Leon, Milagros Miceli, and Florian Müller. 2021.\n“How Data Can Be Used Against People: A Classification of Personal\nData Misuses.” Available at SSRN 3887097.\n\n\nKross, Sean. 2021. Postcards: Create Beautiful, Simple Personal\nWebsites. https://CRAN.R-project.org/package=postcards.\n\n\nKuhn, Max. 2021. Poissonreg: Model Wrappers for Poisson\nRegression. https://CRAN.R-project.org/package=poissonreg.\n\n\nKuhn, Max, and Hadley Wickham. 2020. Tidymodels: A Collection of\nPackages for Modeling and Machine Learning Using Tidyverse\nPrinciples. https://www.tidymodels.org.\n\n\nKuriwaki, Shiro, Will Beasley, and Thomas J. Leeper. 2022.\nDataverse: R Client for Dataverse 4+ Repositories.\n\n\nKuznets, Simon. 1941. National Income and Its\nComposition, 1919-1938. National Bureau of Economic\nResearch.\n\n\nLamott, Anne. 1994. Bird by Bird: Some Instructions on Writing and\nLife. Anchor Books.\n\n\nLarmarange, Joseph. 2021. Labelled: Manipulating Labelled Data.\nhttps://CRAN.R-project.org/package=labelled.\n\n\nLauderdale, Benjamin E, Delia Bailey, Jack Blumenau, and Douglas Rivers.\n2020. “Model-Based Pre-Election Polling for National and\nSub-National Outcomes in the US and UK.” International\nJournal of Forecasting 36 (2): 399–413.\n\n\nLazear, Edward P. 2000. “Economic Imperialism.” The\nQuarterly Journal of Economics 115 (1): 99–146.\n\n\nLee, Benjamin D. 2018. “Ten Simple Rules for Documenting\nScientific Software.” Public Library of Science San Francisco, CA\nUSA.\n\n\nLeek, Jeff, and Roger D. Peng. 2020. “Advanced Data Science\n2020.” http://jtleek.com/ads2020/index.html.\n\n\nLin, Sarah, Ibraheem Ali, and Greg Wilson. 2020. “Ten Quick Tips\nfor Making Things Findable.” PLoS Computational Biology\n16 (12): e1008469.\n\n\nLocke, Steph, and Lucy D’Agostino McGowan. 2018. datasauRus:\nDatasets from the Datasaurus Dozen. https://CRAN.R-project.org/package=datasauRus.\n\n\nLohr, Sharon L. 2019. Sampling: Design and Analysis. CRC Press.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019.\nGeocomputation with r. CRC Press.\n\n\nLucas Jr, Robert E. 1978. “Asset Prices in an Exchange\nEconomy.” Econometrica: Journal of the Econometric\nSociety, 1429–45.\n\n\nLuebke, David Martin, and Sybil Milton. 1994. “Locating the\nVictim: An Overview of Census-Taking, Tabulation Technology, and\nPersecution in Nazi Germany.” IEEE Annals of the History of\nComputing 16 (3): 25.\n\n\nLuscombe, Alex, Kevin Dick, and Kevin Walby. 2021. “Algorithmic\nThinking in the Public Interest: Navigating Technical, Legal, and\nEthical Hurdles to Web Scraping in the Social Sciences.”\nQuality & Quantity, 1–22.\n\n\nLuscombe, Alex, and Alexander McClelland. 2020. “Policing the\nPandemic: Tracking the Policing of COVID-19 Across Canada.”\n\n\nMacDorman, Marian F, and Eugene Declercq. 2018. “The Failure of\nUnited States Maternal Mortality Reporting and Its Impact on Women’s\nLives.” Birth (Berkeley, Calif.) 45 (2): 105.\n\n\nMartinez, Luis R. 2019. “How Much Should We Trust the Dictator’s\nGDP Growth Estimates?” Available at SSRN 3093296.\n\n\nMatias, J. Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles\nEbersole. 2019. “The Upworthy Research Archive.” https://upworthy.natematias.com.\n\n\nMattson, Greggor. 2017. “Artificial Intelligence Discovers\nGayface. Sigh.” https://greggormattson.com/2017/09/09/artificial-intelligence-discovers-gayface/amp/.\n\n\nMcClelland, Alexander. 2019. “\"Lock This Whore up\": Legal Violence\nand Flows of Information Precipitating Personal Violence Against People\nCriminalised for HIV-Related Crimes in Canada.” European\nJournal of Risk Regulation 10 (1): 132–47.\n\n\nMcElreath, Richard. 2020. Statistical\nRethinking: A Bayesian Course with Examples in R and Stan.\nCRC Press.\n\n\nMcPhee, John. 2017. Draft No. 4. Farrar, Straus; Giroux.\n\n\nMcQuire, Scott. 2019. “One Map to Rule Them All? Google Maps as\nDigital Technical Object.” Communication and the Public\n4 (2): 150–65.\n\n\nMeng, Xiao-Li. 2018. “Statistical Paradises and Paradoxes in Big\nData (i): Law of Large Populations, Big Data Paradox, and the 2016 US\nPresidential Election.” The Annals of Applied Statistics\n12 (2): 685–726.\n\n\n———. 2021. “What Are the Values of Data, Data Science, or Data\nScientists?” Harvard Data Science Review, January. https://doi.org/10.1162/99608f92.ee717cf7.\n\n\nMerali, Zeeya. 2010. “Computational Science:... Error.”\nNature 467 (7317): 775–77.\n\n\nMichael, Geuenich, Hou Jinyu, Lee Sunyun, Ayub Shanza, Jackson Hartland,\nand Campbell Kieran. 2021. “Automated\nassignment of cell identity from single- cell multiplexed imaging and\nproteomic data.” Zenodo. https://doi.org/10.5281/zenodo.5156049.\n\n\nMichener, William K. 2015. “Ten Simple Rules for Creating a Good\nData Management Plan.” PLoS Computational Biology 11\n(10): e1004525. https://doi.org/https://doi.org/10.1371/journal.pcbi.1004525.\n\n\nMineault, Patrick, and The Good Research Code Handbook Community. 2021.\n“The Good Research Code Handbook.” https://doi.org/10.5281/ZENODO.5796873.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy\nVasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and\nTimnit Gebru. 2019. “Model Cards for Model Reporting.”\nProceedings of the Conference on Fairness, Accountability, and\nTransparency, January. https://doi.org/10.1145/3287560.3287596.\n\n\nMiyakawa, Tsuyoshi. 2020. “No Raw Data, No Science: Another\nPossible Source of the Reproducibility Crisis.” Molecular\nBrain. Springer.\n\n\nMurphy, Heather. 2017. Why Stanford Researchers Tried to Create a\n’Gaydar’ Machine.\n\n\nNavarro, Danielle. 2021. “Notes from a Data Witch: Getting Started\nwith Apache Arrow.” https://blog.djnavarro.net/starting-apache-arrow-in-r.\n\n\nNelder, John Ashworth, and Robert WM Wedderburn. 1972.\n“Generalized Linear Models.” Journal of the Royal\nStatistical Society: Series A (General) 135 (3): 370–84.\n\n\nNeufeld, Michael J. 2002. “Wernher von Braun, the SS, and\nConcentration Camp Labor: Questions of Moral, Political, and Criminal\nResponsibility.” German Studies Review 25 (1): 57–78.\n\n\nNeuwirth, Erich. 2014. RColorBrewer: ColorBrewer Palettes. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nNeyman, Jerzy. 1934. “On the Two Different Aspects of the\nRepresentative Method: The Method of Stratified Sampling and the Method\nof Purposive Selection.” Journal of the Royal Statistical\nSociety 97 (4): 558–625.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil\nMullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used\nto Manage the Health of Populations.” Science 366\n(6464): 447–53.\n\n\nOECD. 2014. “The Essential Macroeconomic Aggregates.” In\nUnderstanding National Accounts, 13–46. OECD. https://doi.org/10.1787/9789264214637-2-en.\n\n\n———. 2022. Quarterly GDP. https://data.oecd.org/gdp/quarterly-gdp.htm.\n\n\nOoms, Jeroen. 2014. “The Jsonlite Package: A Practical and\nConsistent Mapping Between JSON Data and r Objects.”\narXiv:1403.2805 [Stat.CO]. https://arxiv.org/abs/1403.2805.\n\n\n———. 2018a. Pdftools: Text Extraction, Rendering and Converting of\nPDF Documents. https://CRAN.R-project.org/package=pdftools.\n\n\n———. 2018b. Tesseract: Open Source OCR Engine. https://CRAN.R-project.org/package=tesseract.\n\n\n———. 2019a. Pdftools: Text Extraction, Rendering and Converting of\nPDF Documents. https://CRAN.R-project.org/package=pdftools.\n\n\n———. 2019b. Pdftools: Text Extraction, Rendering and Converting of\nPDF Documents. https://CRAN.R-project.org/package=pdftools.\n\n\n———. 2019c. Tesseract: Open Source OCR Engine. https://CRAN.R-project.org/package=tesseract.\n\n\n———. 2021. Openssl: Toolkit for Encryption, Signatures and\nCertificates Based on OpenSSL. https://CRAN.R-project.org/package=openssl.\n\n\nOostrom, Tamar. 2021. “Funding of Clinical Trials and Reported\nDrug Efficacy.” https://drive.google.com/file/d/1EQLCH0ns99IxYBkxPNbagcZtGgE9a8MQ/view.\n\n\nOrwell, George. 1946. Politics and the English Language. https://www.orwellfoundation.com/the-orwell-foundation/orwell/essays-and-other-works/politics-and-the-english-language/.\n\n\nPatki, Neha, Roy Wedge, and Kalyan Veeramachaneni. 2016. “The\nSynthetic Data Vault.” In 2016 IEEE International Conference\non Data Science and Advanced Analytics (DSAA), 399–410. IEEE.\n\n\nPaullada, Amandalynne, Inioluwa Deborah Raji, Emily M. Bender, Emily\nDenton, and Alex Hanna. 2021. “Data and Its (Dis)contents: A\nSurvey of Dataset Development and Use in Machine Learning\nResearch.” Patterns 2 (11): 100336. https://doi.org/10.1016/j.patter.2021.100336.\n\n\nPavlik, Kaylin. 2019. “Understanding + Classifying Genres Using\nSpotify Audio Features.” https://www.kaylinpavlik.com/classifying-songs-genres/.\n\n\nPedersen, Thomas Lin. 2020. Patchwork: The Composer of Plots.\nhttps://CRAN.R-project.org/package=patchwork.\n\n\nPhillips, Alban W. 1958. “The Relation Between Unemployment and\nthe Rate of Change of Money Wage Rates in the United Kingdom,\n1861-1957.” Economica 25 (100): 283–99.\n\n\nPineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent\nLarivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo\nLarochelle. 2021. “Improving Reproducibility in Machine Learning\nResearch: A Report from the NeurIPS 2019 Reproducibility\nProgram.” Journal of Machine Learning Research 22.\n\n\nPitman, Jim. 1993. Probability.\n\n\nPresmanes Hill, Alison. 2021a. M-F-E-O:\npostcards + distill. https://alison.rbind.io/post/2020-12-22-postcards-distill/.\n\n\n———. 2021b. Up & Running with Blogdown in 2021.\n\n\nR Core Team. 2021. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nRegister, Yim. 2020. “Data Science Ethics in 6 Minutes.” https://youtu.be/mA4gypAiRYU.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Jonathan Keane, Romain François,\nJeroen Ooms, and Apache Arrow. 2022. Arrow: Integration to ’Apache’\n’Arrow’. https://CRAN.R-project.org/package=arrow.\n\n\nRiederer, Emily. 2020. “Column Names as Contracts.” https://emilyriederer.netlify.app/post/column-name-contracts/.\n\n\n———. 2022. Convo: Enables Conversations and Contracts Through\nControlled Vocabulary Naming Conventions. https://github.com/emilyriederer/convo.\n\n\nRilke, Rainer Maria. 1929. Letters to a Young Poet.\n\n\nRobinson, David. 2021. Gutenbergr: Download and Process Public\nDomain Works from Project Gutenberg. https://CRAN.R-project.org/package=gutenbergr.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2021. Broom: Convert\nStatistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom.\n\n\nRobinson, Emily, and Jacqueline Nolis. 2020. Build a Career in Data\nScience. https://livebook.manning.com/book/build-a-career-in-data-science?origin=product-look-inside.\n\n\nRockoff, Hugh. 2019. “On the Controversies Behind the Origins of\nthe Federal Economic Statistics.” Journal of Economic\nPerspectives 33 (1): 147–64.\n\n\nRoss, Casey. 2022. “How a Decades-Old Database Became a Hugely\nProfitable Dossier on the Health of 270 Million Americans.”\nStat, February. https://www.statnews.com/2022/02/01/ibm-watson-health-marketscan-data/.\n\n\nRudis, Bob. 2020. Hrbrthemes: Additional Themes, Theme Components\nand Utilities for ’Ggplot2’. https://CRAN.R-project.org/package=hrbrthemes.\n\n\nRuggles, Steven, Catherine Fitch, Diana Magnuson, and Jonathan\nSchroeder. 2019. “Differential Privacy and Census Data:\nImplications for Social and Economic Research.” In AEA Papers\nand Proceedings, 109:403–8.\n\n\nRuggles, Steven, Sarah Flood, Sophia Foster, Ronald Goeken, Jose Pacas,\nMegan Schouweiler, and Matthew Sobek. 2021. “IPUMS USA: Version\n11.0.” Minneapolis, MN: IPUMS. https://doi.org/10.18128/D010.V11.0.\n\n\nSalganik, Matthew. 2018. Bit by Bit: Social Research in the Digital\nAge. Princeton University Press.\n\n\nSalganik, Matthew J, Peter Sheridan Dodds, and Duncan J Watts. 2006.\n“Experimental Study of Inequality and Unpredictability in an\nArtificial Cultural Market.” Science 311 (5762): 854–56.\n\n\nSalganik, Matthew J, and Douglas D Heckathorn. 2004. “Sampling and\nEstimation in Hidden Populations Using Respondent-Driven\nSampling.” Sociological Methodology 34 (1): 193–240.\n\n\nSamuel, Arthur L. 1959. “Some Studies in Machine Learning Using\nthe Game of Checkers.” IBM Journal of Research and\nDevelopment 3 (3): 210–29.\n\n\nSchloerke, Barret, and Jeff Allen. 2021. Plumber: An API Generator\nfor r. https://CRAN.R-project.org/package=plumber.\n\n\nSekhon, Jasjeet S, and Rocio Titiunik. 2017. “Understanding\nRegression Discontinuity Designs as Observational Studies.”\nObservational Studies 3 (2): 174–82.\n\n\nSi, Yajuan. 2020. “On the Use of Auxiliary Variables in Multilevel\nRegression and Poststratification.” https://arxiv.org/abs/2011.00360.\n\n\nSilberzahn, Raphael, Eric L Uhlmann, Daniel P Martin, Pasquale Anselmi,\nFrederik Aust, Eli Awtrey, Štěpán Bahnı́k, et al. 2018. “Many\nAnalysts, One Data Set: Making Transparent How Variations in Analytic\nChoices Affect Results.” Advances in Methods and Practices in\nPsychological Science 1 (3): 337–56.\n\n\nSilge, Julia. 2018. Text Classification with Tidy Data\nPrinciples. https://juliasilge.com/blog/tidy-text-classification/.\n\n\nSilge, Julia, and David Robinson. 2016. “Tidytext: Text Mining and\nAnalysis Using Tidy Data Principles in r.” JOSS 1 (3).\nhttps://doi.org/10.21105/joss.00037.\n\n\nSilver, Nate. 2020. We Fixed an Issue with How Our Primary Forecast\nWas Calculating Candidates’ Demographic Strengths. https://fivethirtyeight.com/features/we-fixed-a-mistake-in-how-our-primary-forecast-was-calculating-candidates-demographic-strengths/.\n\n\nSimonsohn, Uri. 2013. “Just Post It: The Lesson from Two Cases of\nFabricated Data Detected by Statistics Alone.” Psychological\nScience 24 (10): 1875–88.\n\n\nSimpson, Edward H. 1951. “The Interpretation of Interaction in\nContingency Tables.” Journal of the Royal Statistical\nSociety: Series B (Methodological) 13 (2): 238–41.\n\n\nSomers, James. 2017. “Torching the Modern-Day Library of\nAlexandria.” The Atlantic 20.\n\n\nSprint, Gina, and Jason Conci. 2019. “Mining Github Classroom\nCommit Behavior in Elective and Introductory Computer Science\nCourses.” The Journal of Computing Sciences in Colleges\n35 (1).\n\n\nStaniak, Mateusz, and Przemyslaw Biecek. 2019. “The Landscape of r\nPackages for Automated Exploratory Data Analysis.” arXiv\nPreprint arXiv:1904.02101.\n\n\nStatistics Canada. 2017. “Guide to the Census of Population,\n2016.” Statistics Canada. https://www12.statcan.gc.ca/census-recensement/2016/ref/98-304/98-304-x2016001-eng.pdf.\n\n\n———. 2020. “Sex at Birth and Gender: Technical Report on Changes\nfor the 2021 Census.” Statistics Canada. https://www12.statcan.gc.ca/census-recensement/2021/ref/98-20-0002/982000022020002-eng.pdf.\n\n\nStevens, Wallace. 1934. The Idea of Order at Key West. https://www.poetryfoundation.org/poems/43431/the-idea-of-order-at-key-west.\n\n\nSteyvers, Mark, and Tom Griffiths. 2006. “Probabilistic Topic\nModels.” In Latent Semantic Analysis: A Road to Meaning,\nedited by T. Landauer, D McNamara, S. Dennis, and W. Kintsch.\n\n\nStigler, Stephen. 1986. The History of Statistics. Harvard\nUniversity Press.\n\n\nStock, James H, and Francesco Trebbi. 2003. “Retrospectives: Who\nInvented Instrumental Variable Regression?” Journal of\nEconomic Perspectives 17 (3): 177–94.\n\n\nStolberg, Michael. 2006. “Inventing the Randomized Double-Blind\nTrial: The Nuremberg Salt Test of 1835.” Journal of the Royal\nSociety of Medicine 99 (12): 642–43.\n\n\nStudent. 1908. “The Probable Error of a Mean.”\nBiometrika, 1–25.\n\n\nSunstein, Cass R, and Lucia A Reisch. 2017. The Economics of\nNudge. Routledge.\n\n\nSuriyakumar, Vinith M., Nicolas Papernot, Anna Goldenberg, and Marzyeh\nGhassemi. 2021. “Chasing Your Long Tails.” In\nProceedings of the 2021 ACM Conference on Fairness,\nAccountability, and Transparency. ACM. https://doi.org/10.1145/3442188.3445934.\n\n\nTaddy, Matt. 2019. Business Data Science. McGraw Hill.\n\n\nThe Economist. 2013. Johnson: Those Six Little Rules: George Orwell\non Writing. https://www.economist.com/prospero/2013/07/29/johnson-those-six-little-rules.\n\n\n———. 2022. What Spotify Data Show about the Decline of English.\nhttps://www.economist.com/interactives/graphic-detail/2022/01/29/what-spotify-data-show-about-the-decline-of-english.\n\n\nThieme, Nick. 2018. “R Generation.” Significance\n15 (4): 14–19. https://doi.org/10.1111/j.1740-9713.2018.01169.x.\n\n\nThistlethwaite, Donald L, and Donald T Campbell. 1960.\n“Regression-Discontinuity Analysis: An Alternative to the Ex Post\nFacto Experiment.” Journal of Educational Psychology 51\n(6): 309.\n\n\nThompson, Charlie, Josiah Parry, Donal Phipps, and Tom Wolff. 2020.\nSpotifyr: R Wrapper for the ’Spotify’ Web API. http://github.com/charlie86/spotifyr.\n\n\nThornhill, John. 2021. “Lunch with the FT: Mathematician Hannah\nFry.” Financial Times.\n\n\nTierney, Nicholas. 2017. “Visdat: Visualising Whole Data\nFrames.” JOSS 2 (16): 355. https://doi.org/10.21105/joss.00355.\n\n\n———. 2022. Quarto for Scientists. https://qmd4sci.njtierney.com.\n\n\nTierney, Nicholas J, and Karthik Ram. 2020. “A Realistic Guide to\nMaking Data Available Alongside Code to Improve Reproducibility.”\nhttps://arxiv.org/abs/2002.11626.\n\n\nTimbers, Tiffany. 2020. Canlang: Canadian Census Language Data.\nhttps://ttimbers.github.io/canlang/.\n\n\nTimbers, Tiffany-Anne, Trevor Campbell, and Melissa Lee. 2022. Data\nScience: A First Introduction. CRC Press.\n\n\nTolley, Erin, and Mireille Paquet. 2021. “Gender, Municipal Party\nPolitics, and Montreal’s First Woman Mayor.” Canadian Journal\nof Urban Research 30 (1): 40–52.\n\n\nTrisovic, Ana, Matthew K. Lau, Thomas Pasquier, and Mercè Crosas. 2022.\n“A Large-Scale Study on Research Code Quality and\nExecution.” Scientific Data 9 (1). https://doi.org/10.1038/s41597-022-01143-6.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The\nAnnals of Mathematical Statistics 33 (1): 1–67.\n\n\nUN IGME. 2021. “Levels and Trends in Child Mortality,\n2021.” https://childmortality.org/wp-content/uploads/2021/12/UNICEF-2021-Child-Mortality-Report.pdf.\n\n\nVan den Broeck, Jan, Solveig Argeseanu Cunningham, Roger Eeckels, and\nKobus Herbst. 2005. “Data Cleaning: Detecting, Diagnosing, and\nEditing Data Abnormalities.” PLoS Medicine 2 (10): e267.\n\n\nVanderplas, Susan, Dianne Cook, and Heike Hofmann. 2020. “Testing\nStatistical Charts: What Makes a Good Graph?” Annual Review\nof Statistics and Its Application 7: 61–88.\n\n\nvon Bergmann, Jens, Dmitry Shkolnik, and Aaron Jacobs. 2021.\nCancensus: R Package to Access, Retrieve, and Work with Canadian\nCensus Data and Geography. https://mountainmath.github.io/cancensus/.\n\n\nWang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman. 2015.\n“Forecasting Elections with Non-Representative Polls.”\nInternational Journal of Forecasting 31 (3): 980–91.\n\n\nWang, Yilun, and Michal Kosinski. 2018. “Deep Neural Networks Are\nMore Accurate Than Humans at Detecting Sexual Orientation from Facial\nImages.” Journal of Personality and Social Psychology\n114 (2): 246.\n\n\nWardrop, Robert L. 1995. “Simpson’s Paradox and the Hot Hand in\nBasketball.” The American Statistician 49 (1): 24–28.\n\n\nWare, James. 1989. “Investigating Therapies of Potentially Great\nBenefit: ECMO.” Statistical Science, no. 4: 298–306.\n\n\nWare, James H. 1989. “Investigating Therapies of Potentially Great\nBenefit: ECMO.” Statistical Science 4 (4): 298–306.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWei, LJ, and S Durham. 1978. “The Randomized Play-the-Winner Rule\nin Medical Trials.” Journal of the American Statistical\nAssociation 73 (364): 840–43.\n\n\nWeissgerber, Tracey L, Natasa M Milic, Stacey J Winham, and Vesna D\nGarovic. 2015. “Beyond Bar and Line Graphs: Time for a New Data\nPresentation Paradigm.” PLoS Biology 13 (4): e1002128.\n\n\nWhitby, Andrew. 2020. The Sum of the\nPeople. Basic Books.\n\n\nWhitelaw, James. 1905. An Essay on the Population of Dublin. Being\nthe Result of an Actual Survey Taken in 1798, with Great Care and\nPrecision, and Arranged in a Manner Entirely New. Graisberry;\nCampbell.\n\n\nWHO. 2019. “Trends in Maternal Mortality 2000 to 2017: Estimates\nby WHO, UNICEF, UNFPA, World Bank Group and the United Nations\nPopulation Division.” https://www.who.int/reproductivehealth/publications/maternal-mortality-2000-2017/en/.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.”\nJournal of Computational and Graphical Statistics 19 (1): 3–28.\n\n\n———. 2011. “Testthat: Get Started with Testing.” The R\nJournal 3: 5–10. https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.\n\n\n———. 2014. “Tidy Data.” Journal of Statistical\nSoftware 59 (1): 1–23.\n\n\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis.\nSpringer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2017. Tidyverse: Easily Install and Load the ’Tidyverse’.\nhttps://CRAN.R-project.org/package=tidyverse.\n\n\n———. 2019a. Advanced r. CRC Press.\n\n\n———. 2019b. Babynames: US Baby Names 1880-2017. https://CRAN.R-project.org/package=babynames.\n\n\n———. 2019c. Httr: Tools for Working with URLs and HTTP. https://CRAN.R-project.org/package=httr.\n\n\n———. 2019d. Rvest: Easily Harvest (Scrape) Web Pages. https://CRAN.R-project.org/package=rvest.\n\n\n———. 2019e. Stringr: Simple, Consistent Wrappers for Common String\nOperations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2020a. Forcats: Tools for Working with Categorical Variables\n(Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2020b. Tidyverse. https://www.tidyverse.org/.\n\n\n———. 2021a. Mastering Shiny.\n\n\n———. 2021b. The Tidyverse Style Guide. https://style.tidyverse.org/index.html.\n\n\n———. 2021c. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019a.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\n———, et al. 2019b. “Welcome to the tidyverse.” Journal of Open Source\nSoftware 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2020. Usethis: Automate Package\nand Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2020.\nDplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data\nScience. https://r4ds.had.co.nz/.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2021. Readr: Read\nRectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Jim Hester, and Winston Chang. 2020. Devtools:\nTools to Make Developing r Packages Easier. https://CRAN.R-project.org/package=devtools.\n\n\nWickham, Hadley, Jim Hester, and Jeroen Ooms. 2021. Xml2: Parse\nXML. https://CRAN.R-project.org/package=xml2.\n\n\nWickham, Hadley, and Evan Miller. 2020. Haven: Import and Export\n’SPSS’, ’Stata’ and ’SAS’ Files. https://CRAN.R-project.org/package=haven.\n\n\nWickham, Hadley, and Dana Seidel. 2020. Scales: Scale Functions for\nVisualization. https://CRAN.R-project.org/package=scales.\n\n\nWiessner, Polly W. 2014. “Embers of Society: Firelight Talk Among\nthe Ju/’Hoansi Bushmen.” Proceedings of the National Academy\nof Sciences 111 (39): 14027–35.\n\n\nWilde, Oscar. 1891. The Picture of Dorian Gray.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed.\nSpringer.\n\n\nWilkinson, Mark D, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle\nAppleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016.\n“The FAIR Guiding Principles for Scientific Data Management and\nStewardship.” Scientific Data 3 (1): 1–9.\n\n\nWilson, Greg. 2021. Building Software Together. CRC Books.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex\nNederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in\nScientific Computing.�� PLOS Computational Biology 13\n(6): 1–20. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\nWong, Julia Carrie. 2020. One Year Inside Trump’s Monumental\nFacebook Campaign.\n\n\nWright, Philip G. 1928. The Tariff on Animal and Vegetable\nOils. Macmillan Company.\n\n\nWu, Changbao, and Mary E Thompson. 2020. Sampling Theory and\nPractice. Springer.\n\n\nXie, Yihui. 2019. “TinyTeX: A Lightweight, Cross-Platform, and\nEasy-to-Maintain LaTeX Distribution Based on TeX Live.”\nTUGboat, no. 1: 30–32. https://tug.org/TUGboat/Contents/contents40-1.html.\n\n\n———. 2021. Knitr: A General-Purpose Package for Dynamic Report\nGeneration in r. https://yihui.org/knitr/.\n\n\nXie, Yihui, Christophe Dervieux, and Alison Presmanes Hill. 2021.\nBlogdown: Create Blogs and Websites with r Markdown. https://github.com/rstudio/blogdown.\n\n\nXie, Yihui, Amber Thomas, and Alison Presmanes Hill. 2021. Blogdown:\nCreating Websites with r Markdown.\n\n\nZhu, Hao. 2020. kableExtra: Construct Complex Table with ’Kable’ and\nPipe Syntax. https://CRAN.R-project.org/package=kableExtra.\n\n\nZinsser, William. 1976. On Writing Well.\n\n\nZook, Matthew, Solon Barocas, Danah Boyd, Kate Crawford, Emily Keller,\nSeeta Peña Gangadharan, Alyssa Goodman, et al. 2017. “Ten Simple\nRules for Responsible Big Data Research.” PLoS Computational\nBiology. Public Library of Science San Francisco, CA USA."
  }
]