% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.46,0.14}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{siunitx}
\newcolumntype{d}{S[input-symbols = ()]}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\renewcommand*\contentsname{Table of contents}
\renewcommand*\listfigurename{List of Figures}
\renewcommand*\listtablename{List of Tables}
\renewcommand*\figurename{Figure}
\renewcommand*\tablename{Table}
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Telling Stories with Data},
  pdfauthor={Rohan Alexander},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Telling Stories with Data}
\author{Rohan Alexander}
\date{19 April 2022}

\begin{document}
\maketitle

\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[borderline west={3pt}{0pt}{shadecolor}, sharp corners, frame hidden, interior hidden, enhanced, boxrule=0pt]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\begin{figure}

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{./figures/tellingstorieswithdatapainting.png}

}

\caption{\label{fig-elephant}Telling stories with data}

\end{figure}

This book will help you tell stories with data. It establishes a
foundation on which you can build and share knowledge about an aspect of
the world of interest to you based on data that you observe. Telling
stories in small groups around a fire played a critical role in the
development of humans and society (Wiessner 2014). Today our stories,
based on data, can influence millions.

In this book we will explore, prod, push, manipulate, knead, and
ultimately, try to understand the implications of data. A variety of
features drive the choices in this book.

The motto of the university from which I took my PhD is \emph{naturam
primum cognoscere rerum} or roughly `learn the first nature of things'.
But the original quote continues \emph{temporis aeterni quoniam}, or
roughly `for eternal time'. We will do both of these things. I focus on
tools, approaches, and workflows that enable you to establish lasting
and reproducible knowledge.

When I talk of data in this book, it will typically be related to
humans. Humans will be at the center of most of our stories, and we will
tell social, cultural, and economic stories. In particular, throughout
this book I will draw attention to inequity both in social phenomena and
in data. Most data analysis reflects the world as it is. Many of the
least well-off face a double burden in this regard: not only are they
disadvantaged, but the extent is more difficult to measure. Respecting
those whose data are in our dataset is a primary concern, and so is
thinking of those who are systematically not in our dataset.

While data are often specific to various contexts and disciplines, the
approaches used to understand them tend to be similar. Data are also
increasingly global with resources and opportunities available from a
variety of sources. Hence, I draw on examples from many disciplines and
geographies.

To become knowledge, our findings must be communicated to, understood,
and trusted by other people. Scientific and economic progress can only
be made by building on the work of others. And this is only possible if
we can understand what they did. Similarly, if we are to create
knowledge about the world, then we must enable others to understand
precisely what we did, what we found, and how we went about our tasks.
As such, in this book I will be particularly prescriptive about
communication and reproducibility.

Improving the quality of quantitative work is an enormous challenge, yet
it is the challenge of our time. Data are all around us, but there is
little enduring knowledge being created. This book hopes to contribute,
in some small way, to changing that.

\hypertarget{audience-and-assumed-background}{%
\section*{Audience and assumed
background}\label{audience-and-assumed-background}}
\addcontentsline{toc}{section}{Audience and assumed background}

The typical person reading this book has some familiarity with
first-year statistics, for instance they have run a regression. But it
is not targeted at a particular level, instead providing aspects
relevant to almost any quantitative course. I have taught from this book
at high school, undergraduate, graduate, and professional, levels.
Everyone has unique needs, but hopefully some aspect of this book speaks
to you.

Enthusiasm and interest have taken folks far. If you have those, then do
not worry about too much else. Some of the most successful students have
been those with no quantitative or coding background.

This book covers a lot of ground, but does not go into depth about any
particular aspect. As such this book especially complements books such
as: \emph{Data Science: A First Introduction} (T.-A. Timbers, Campbell,
and Lee 2022), \emph{R for Data Science} (Wickham and Grolemund 2017),
\emph{An Introduction to Statistical Learning} (James et al. 2017),
\emph{Statistical Rethinking} (McElreath 2020), \emph{Causal Inference:
The Mixtape} (Cunningham 2021), \emph{The Effect: An Introduction to
Research Design and Causality} (Huntington-Klein 2021), and
\emph{Building Software Together} (Wilson 2021). If you are interested
in those books, then this might be a good one to start with.

\hypertarget{structure-and-content}{%
\section*{Structure and content}\label{structure-and-content}}
\addcontentsline{toc}{section}{Structure and content}

This book is structured around six parts: I) Foundations, II)
Communication, III) Acquisition, IV) Preparation, V) Modelling, and VI)
Enrichment.

Part I -- Foundations -- begins with Chapter~\ref{sec-introduction},
which provides an overview of what I am trying to achieve with this book
and why you should read it. Chapter~\ref{sec-fire-hose} provides some
worked examples. The intention of these is that you can experience the
full workflow recommended in this book without worrying too much about
the specifics of what is happening. That workflow is: plan, simulate,
acquire, model, and communicate. It is normal to not follow everything
in this chapter, but you should go through it, typing out and executing
the code yourself. If you only have time to read one chapter of this
book, then I recommend that one. Chapter~\ref{sec-r-essentials} goes
through some essential tasks in R, which is the statistical programming
language used in this book. It is more of a reference chapter, and you
may find yourself returning to it from time to time. And
Chapter~\ref{sec-reproducible-workflows} introduces some key tools for
reproducibility used in the workflow that I advocate. These are things
like using the command line, Quarto, R Projects, Git and GitHub, and
using R in practice.

Part II -- Communication -- considers three types of communication:
written, static, and interactive. Chapter~\ref{sec-on-writing} details
the features that quantitative writing should have and how to go about
writing a crisp, technical, paper. Static communication in
Chapter~\ref{sec-static-communication} introduces features like graphs,
tables, and maps. Interactive communication in
\textbf{?@sec-interactive} covers aspects such as websites, web
applications, and maps that can be manipulated.

Part III -- Acquisition -- focuses on three aspects: farming data,
gathering data, and hunting data. Farming data in
Chapter~\ref{sec-farm-data} begins with essential concepts from sampling
that govern our approach to data. It then focuses on datasets that are
explicitly provided for us to use as data, for instance censuses and
other government statistics. These are typically clean, pre-packaged
datasets, and sometimes complete. Gathering data in
Chapter~\ref{sec-gather-data} covers things like using Application
Programming Interface (APIs), scraping data, getting data from PDFs, and
Optical Character Recognition (OCR). The idea is that data are
available, but not necessarily designed to be datasets, and that we must
go and get them. Finally, hunting data in Chapter~\ref{sec-hunt-data}
covers aspects where more is expected of us. For instance, we may need
to conduct an experiment, run an A/B test, or do some surveys.

Part IV -- Preparation -- covers how to respectfully transform raw data
into something that can be explored and shared.
Chapter~\ref{sec-clean-and-prepare} begins by detailing some principles
to follow when approaching the task of cleaning and preparing data, and
then goes through specific steps to take and checks to implement.
Chapter~\ref{sec-store-and-share} focuses on methods of storing and
retrieving those datasets, including the use of R packages, and then
continues onto considerations and steps to take when wanting to
disseminate datasets as broadly as possible, while at the same time
respecting those whose data they are based on.

Part V -- Modelling -- begins with exploratory data analysis in
Chapter~\ref{sec-exploratory-data-analysis}. This is the critical
process of coming to understand the nature of a dataset, but not
something that typically finds itself into the final product. In
Chapter~\ref{sec-its-just-a-linear-model} the use of statistical models
to explore data is introduced.
Chapter~\ref{sec-causality-from-observational-data} is the first of
three applications of modelling. It focuses on attempts to make causal
claims from observational data and covers approaches such as
difference-in-differences, regression discontinuity, and instrumental
variables.
Chapter~\ref{sec-multilevel-regression-with-post-stratification} is the
second of the modelling applications chapters and focuses on multilevel
regression with post-stratification where we use a statistical model to
adjust a sample for known biases. Chapter~\ref{sec-text-as-data} is the
third and final modelling application and is focused on text-as-data.

Part VI -- Enrichment -- introduces various next steps that would
improve aspects of the workflow and approaches introduced in previous
chapters. Chapter~\ref{sec-deploying-models} goes through moving away
from your own computer and toward using the cloud and then discusses
deploying models through the use of packages, web applications, and
APIs. Chapter~\ref{sec-efficiency} discusses various alternatives to the
storage of data including feather and SQL; and also covers some ways to
improve the performance of your code. Finally,
Chapter~\ref{sec-concluding-remarks} offers some concluding remarks,
details some open problems, and suggests some next steps.

\hypertarget{pedagogy-and-key-features}{%
\section*{Pedagogy and key features}\label{pedagogy-and-key-features}}
\addcontentsline{toc}{section}{Pedagogy and key features}

You have to do the work. You should actively go through material and
code yourself. As S. King (2000) says `{[}a{]}mateurs sit and wait for
inspiration, the rest of us just get up and go to work'. Do not
passively read this book. My role is best described by Hamming (1996,
2--3):

\begin{quote}
I am, as it were, only a coach. I cannot run the mile for you; at best I
can discuss styles and criticize yours. You know you must run the mile
if the athletics course is to be of benefit to you---hence you must
think carefully about what you hear and read in this book if it is to be
effective in changing you---which must obviously be the purpose\ldots{}
\end{quote}

This book is structured around a dense 12-week course. It provides
enough material for advanced readers to be challenged, while
establishing a core that all readers should master. Typically courses
cover the material through to Chapter~\ref{sec-its-just-a-linear-model},
and then pick another couple of chapters that are of particular
interest.

From as early as Chapter~\ref{sec-fire-hose} you will have a
workflow---plan, simulate, acquire, model, and communicate---allowing
you to tell a convincing story with data. In each subsequent chapter you
will add aspects and depth to this workflow that will allow you to speak
with increasing sophistication and credibility. As this workflow expands
it addresses the skills that are typically sought in industry. For
instance, features such as: communication, ethics, reproducibility,
research question development, data collection, data cleaning, data
protection and dissemination, exploratory data analysis, statistical
modelling, and scaling.

One of the defining aspects of this book is that ethics and inequity
concerns are integrated throughout, rather than being clustered in one,
easily ignorable, chapter. These aspects are critical, yet it can be
difficult to immediately see their value, hence their tight integration.

This book is also designed to enable you to build a portfolio of work
that you could show to a potential employer. If you want an industry
job, then this is arguably the most important thing that you should be
doing. E. Robinson and Nolis (2020, 55) describe how a portfolio is a
collection of projects that show what you can do and is something that
can help be successful in a job search.

In the novel \emph{The Last Samurai} (DeWitt 2000, 326), a character
says:

\begin{quote}
{[}A{]} scholar should be able to look at any word in a passage and
instantly think of another passage where it occurred; \ldots{} {[}so
a{]} text was like a pack of icebergs each word a snowy peak with a huge
frozen mass of cross-references beneath the surface.
\end{quote}

In an analogous way, this book not only provides you with text and
instruction that is self-contained, but also helps develop critical
masses of knowledge on which expertise is built. No chapter positions
itself as the last word, instead they are written in relation to other
work.

Each chapter has the following features:

\begin{itemize}
\tightlist
\item
  A list of required materials that you should go through before you
  read that chapter. To be clear, you should first read that material
  and then return to this book. Each chapter also contains recommended
  materials for those who are particularly interested in the topic and
  want a starting place for further exploration.
\item
  A summary of the key concepts and skills that are developed in that
  chapter. Technical chapters additionally contain a list of the main
  packages and functions that are used in the chapter. The combination
  of these features acts as a checklist for your learning, and you
  should return to them after completing the chapter.
\item
  A series of short exercises that you should complete after going
  through the required materials, but before going through the chapter,
  to test your knowledge. After completing the chapter, you should go
  back through the exercises to make sure that you understand each
  aspect.
\item
  One or two tutorial questions are included at the end of each chapter
  to further encourage you to actively engage with the material. You
  could consider forming small groups to discuss your answers to these
  questions.
\end{itemize}

Some chapters additionally feature:

\begin{itemize}
\tightlist
\item
  A section called `Oh, you think we have good data on that!' which
  focuses on a particular setting, such as cause of death, in which it
  is often assumed that there is unimpeachable and unambiguous data but
  the reality tends to be quite far from that.
\item
  A section called `Shoulders of giants', which focuses on some of those
  who created the intellectual foundation on which we build.
\end{itemize}

Finally, a set of papers is included in Appendix~\ref{sec-papers}. If
you write these, you will be conducting original research on a topic
that is of interest to you. Although open-ended research may be new to
you, the extent to which you are able to: develop your own questions,
use quantitative methods to explore them, and communicates your
findings, is the measure of the success of this book.

\hypertarget{software-information-and-conventions}{%
\section*{Software information and
conventions}\label{software-information-and-conventions}}
\addcontentsline{toc}{section}{Software information and conventions}

The software that I use in this book is R (R Core Team 2021). This
language was chosen because it is open source, widely used, general
enough to cover the entire workflow, yet specific enough to have plenty
of well-developed features. I do not assume that you have used R before,
and so another reason for selecting R for this book is the community of
R users. The community is especially welcoming of new-comers and there
is a lot of complementary beginner-friendly material available.

If you do not have a programming language, then R is a great one to
start with. The ability to code is useful well beyond this book. If you
have a preferred programming language already, then it wouldn't hurt to
pick up R as well. That said, if you have a good reason to prefer
another open-source programming language (for instance you use Python
daily at work) then you may wish to stick with that. However, all
examples in this book are in R.

Please download R and R Studio onto your own computer. You can download
R for free here: http://cran.utstat.utoronto.ca/, and you can download R
Studio Desktop for free here:
https://rstudio.com/products/rstudio/download/\#download. Please also
create an account on R Studio Cloud: https://rstudio.cloud/. This will
allow you to run R in the cloud.

Packages are in typewriter text, for instance, \texttt{tidyverse}, while
functions are also in typewriter text, but include brackets, for
instance \texttt{dplyr::filter()}.

\hypertarget{acknowledgments}{%
\section*{Acknowledgments}\label{acknowledgments}}
\addcontentsline{toc}{section}{Acknowledgments}

Many people generously gave code, data, examples, guidance,
opportunities, thoughts, and time, that helped develop this book.

Thank you to David Grubbs and the team at CRC Press for taking a chance
on me and providing invaluable support.

Thank you to Michael Chong and Sharla Gelfand for greatly helping to
shape some of the approaches I advocate. However, both do much more than
that and contribute in an enormous way to the spirit of generosity that
characterizes the R community.

Thank you to Kelly Lyons for her support, guidance, mentorship, and
friendship. Every day she demonstrates what an academic should be, and
more broadly, what we should all aspire to be as a person.

Thank you to Greg Wilson for providing a structure to think about
teaching, for being the catalyst for this book, and for helpful comments
on drafts. Every day he provides an example of how to contribute to the
intellectual community.

Thank you to an anonymous reviewer and Isabella Ghement, who thoroughly
went through an early draft of this book and provided detailed feedback
that improved this book.

Thank you to Annie Collins who went through every word in this book,
improving many of them, and helped to sharpen my thinking on much of the
content covered in this book.

Thank you to Hareem Naveed for helpful feedback and encouragement. Her
industry experience was an invaluable resource as I grappled with
questions of coverage and focus.

Thank you to Leslie Root, who came up with the idea around `Oh, you
think we have good data on that!'.

Thank you to Ella Kaye, who suggested, and rightly insisted on, moving
to Quarto.

Thank you to Yanbo Tang, who assisted with Nancy Reid's `Shoulders of
giants' entry.

Thank you to Lauren Kennedy through whose generous sharing of code,
data, and countless conversations my thoughts about MRP have developed.

Thank you to my PhD supervisory panel John Tang, Martine Mariotti, Tim
Hatton, and Zach Ward who gave me the freedom to explore the
intellectual space that was of interest to me, the support to follow
through on those interests, and the guidance to ensure that it all
resulted in something tangible.

Thank you to Elle Côtè for enabling this book to be written.

This book has greatly benefited from the notes and teaching materials of
others that are freely available online, especially: Chris Bail, Scott
Cunningham, Andrew Heiss, Lisa Lendway, Grant McDermott, Nathan Matias,
David Mimno, and Ed Rubin. Thank you to these folks. The changed norm of
academics making their materials freely available online is a great one
and one that I hope the free online version of this book helps
contribute to.

Thank you to Samantha-Jo Caetano, who helped develop some of the
assessment items, and more generally developed my ideas around how the
material covered in this book should be taught. And also, to Lisa Romkey
and Alan Chong who allowed me to adapt some aspects of their rubric.

Thank you to those who contributed substantially to the development of
this book, including: A Mahfouz, Faria Khandaker, Keli Chiu, Paul
Hodgetts, and Thomas William Rosenthal. I discussed most aspects of this
book with them, and while they made specific contributions, they also
changed and sharpened the way that I thought about almost everything
covered here. Paul additionally made the art for this book.

Thank you to those who identified specific improvements, including:
Aaron Miller, Amy Farrow, Arsh Lakhanpal, Cesar Villarreal Guzman,
Flavia López, Hong Shi, Laura Cline, Lorena Almaraz De La Garza, Mounica
Thanam, Reem Alasadi, Wijdan Tariq, Yang Wu, and Yewon Han.

As at Christmas 2021 this book was a disparate collection of notes.
Thank you to Mum and Dad, who dropped everything and came over from the
other side of the world for two months to give me the opportunity to
re-write it all and put together a cohesive book.

Finally, thank you to Monica Alexander. Without you I would not have
written a book; I would not have even thought it possible. Many of the
best ideas in this book are yours, and those that are not, you made
better. Thank you for your inestimable help with writing this book,
providing the base on which it builds (remember in the library showing
me many times how to get certain rows in R!), giving me the time that I
needed to write, encouragement when it turned out that writing a book
just meant endlessly re-writing that which was perfect the day before,
reading everything in this book many times, making coffee or cocktails
as appropriate, and more.

You can contact me at: rohan.alexander@utoronto.ca.

\BeginKnitrBlock{flushright}

Rohan Alexander\\
Toronto, Canada\\
April 2022 \EndKnitrBlock{flushright}

\hypertarget{about-the-author}{%
\chapter*{About the author}\label{about-the-author}}
\addcontentsline{toc}{chapter}{About the author}

Rohan Alexander is an assistant professor at the University of Toronto
in Information and Statistical Sciences. He is also the assistant
director of CANSSI Ontario, a senior fellow at Massey College, a faculty
affiliate at the Schwartz Reisman Institute for Technology and Society,
and a co-lead of the Data Sciences Institute Thematic Program in
Reproducibility. He holds a PhD in Economics from the Australian
National University where he was supervised by John Tang (chair),
Martine Mariotti, Tim Hatton, and Zach Ward.

He is interested in using statistical models to try to understand the
world. And particularly how we get the data that go into those models;
whose data are systematically missing; how we clean, prepare, and tidy
data before they are modelled; the effects of all this on the
implications of our models; and how we can reproducibly share the
totality of this process. He tries to develop students that are skilled
not only in using statistical methods across various disciplines, but
also appreciate their limitations, and think deeply about the broader
contexts of their work.

He enjoys teaching and aims to help students from a wide range of
backgrounds learn how to use data to tell convincing stories. He teaches
in both the Faculty of Information and the Department of Statistical
Sciences at both undergraduate and graduate levels. He is a RStudio
Certified Tidyverse Trainer.

He is married to Monica Alexander and they have two children. He
probably spends too much money on books, and certainly too much time at
libraries. If you have any book recommendations of your own, then he'd
love to hear them.

\part{Foundations}

\hypertarget{sec-introduction}{%
\chapter{Telling stories with data}\label{sec-introduction}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{Counting the Countless}, (Keyes 2019)
\item
  Watch \emph{Data Science Ethics in 6 Minutes}, (Register 2020)
\end{itemize}

\hypertarget{on-telling-stories}{%
\section{On telling stories}\label{on-telling-stories}}

One of the first things that many parents regularly do when their
children are born is read stories to them. In doing so they carry on a
tradition that has occurred for millennia. Myths, fables, and fairy
tales can be seen and heard all around us. Not only are they
entertaining but they enable us to learn something about the world.
While \emph{The Very Hungry Caterpillar} (Carle 1969) may seem quite far
from the world of dealing with data, there are similarities. Both are
trying to tell a story and teaching us something about the world.

When using data, we try to tell a convincing story. It may be as
exciting as predicting elections, as banal as increasing internet
advertising click rates, as serious as finding the cause of a disease,
or as fun as forecasting basketball games. In any case the key elements
are the same. The English author, E. M. Forster, described the aspects
common to all novels as: story, people, plot, fantasy, prophecy,
pattern, and rhythm (Forster 1927). Similarly, when we tell stories with
data, there are common concerns, regardless of the setting:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the dataset? Who generated the dataset and why?
\item
  What is the process that underpins the dataset? Given that process,
  what is missing from the dataset or has been poorly measured? Could
  other datasets have been generated, and if so, how different could
  they have been to the one that we have?
\item
  What is the dataset trying to say, and how can we let it say this?
  What else could it say? How do we decide between these?
\item
  What are we hoping others will see from this dataset, and how can we
  convince them of this? How much work must we do to convince them?
\item
  Who is affected by the processes and outcomes related to this dataset?
  To what extent are they represented in the dataset, and have they been
  part of conducting the analysis?
\end{enumerate}

In the past, certain elements of telling stories with data were easier.
For instance, experimental design has a long and robust tradition within
agricultural and medical sciences, physics, and chemistry. Student's
t-distribution was identified in the early 1900s by a chemist, William
Sealy Gosset, who worked at Guinness, a beer manufacturer (Boland 1984).
It would have been straightforward for him to randomly sample the beer
and change one aspect at a time.

Many of the fundamentals of the statistical methods that we use today
were developed in such settings, where it was typically possible to
establish control groups and randomize; and in these settings there were
fewer ethical concerns. A story told with the resulting data is likely
to be fairly convincing.

Unfortunately, little of this applies these days, given the diversity of
settings to which statistical methods are applied. On the other hand, we
have many advantages. For instance, we have well-developed statistical
techniques, easier access to large datasets, and open-source statistical
languages such as R (R Core Team 2021). But the difficulty of conducting
traditional experiments means that we must also turn to other aspects to
tell a convincing story.

\hypertarget{workflow-components}{%
\section{Workflow components}\label{workflow-components}}

There are five core components to the workflow needed to tell stories
with data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Plan} and sketch an endpoint.
\item
  \textbf{Simulate} some reasonable data and consider that.
\item
  \textbf{Acquire} and prepare the real data.
\item
  \textbf{Explore} and understand the real dataset.
\item
  \textbf{Share} what was done and what was found.
\end{enumerate}

We begin by \textbf{planning and sketching an endpoint} because this
ensures that we think carefully about where we want to go. It forces us
to deeply consider our situation, acts to keep us focused and efficient,
and helps reduce scope creep. In \emph{Alice's Adventures in Wonderland}
(Carroll 1865), Alice asks the Cheshire Cat which way she should go. The
Cheshire Cat replies by asking where Alice would like to go. And when
Alice replies that she does not mind, so long as she gets somewhere, the
Cheshire Cat says then the direction does not matter because you will
always get somewhere if you `walk long enough'. The issue, in our case,
is that we typically cannot afford to walk aimlessly for long. While it
may be that the endpoint needs to change, it is important that this is a
deliberate, reasoned, decision. And that is only possible given an
initial target. There is no need to spend too much time on this to get a
lot of value from it. Often five minutes with paper and pen is enough.

The next step is to \textbf{simulate data} because that forces us into
the details. It helps with cleaning and preparing the dataset because it
focuses us on the classes in the dataset and the distribution of the
values that we expect. For instance, if we were interested in the effect
of age-groups on political preferences, then we may expect that our
age-group column would be a factor, with four possible values: `18-29',
`30-44', `45-59', `60+'. The process of simulation provides us with
clear features that our real dataset should satisfy. We could use these
features to define tests that would guide our data cleaning and
preparation. For instance, we could check our real dataset for
age-groups that are not one of those four values. When those tests pass,
we could be confident that our age-group column only contains values
that we expect.

Simulating data is also important when we turn to statistical modelling.
When we are at that stage, we are concerned with whether the model
reflects what is in the dataset. The issue is that if we go straight to
modelling the real dataset, then we do not know whether we have a
problem with our model. We initially simulate data so that we precisely
know the underlying data generation process. We then apply the model to
that simulated dataset. When we get out what we put in, then we know
that our model is performing appropriately, and can turn to the real
dataset. Without that initial application to simulated data, it would be
more difficult to have confidence in our model.

Simulation is often cheap---almost free given modern computing resources
and statistical programming languages---and fast. It provides `an
intimate feeling for the situation' (Hamming 1996, 239). The way to
proceed is to start with a simulation that just contains the essentials,
get that working, and to then complicate it.

\textbf{Acquiring and preparing the data} that we are interested in is
an often-overlooked stage of the workflow. This is surprising because it
can be one of the most difficult stages and requires many decisions to
be made. This is increasingly the subject of research. For instance, it
has been found that decisions made during this stage greatly affect
statistical results (Huntington-Klein et al. 2021).

At this stage of the workflow, it is common to feel a little
overwhelmed. Typically, the data we can acquire leave us a little
scared. There may be too little of it, in which case we worry about how
we are going to be able to make our statistical machinery work.
Alternatively, we may have the opposite problem and be worried about how
we can even begin to deal with such a large amount of data.

\begin{quote}
Perhaps all the dragons in our lives are princesses who are only waiting
to see us act, just once, with beauty and courage. Perhaps everything
that frightens us is, in its deepest essence, something helpless that
wants our love.

Rilke (1929)
\end{quote}

Developing comfort in this stage of the workflow unlocks the rest of it.
The dataset that is needed to tell a convincing story is in there, but
we need to iteratively remove everything that is not the data that we
need, and to then shape that which is.

After we have a dataset, we then want to \textbf{explore and understand
} certain relationships in that dataset. The use of statistical models
to understand the implications of our data is not free of bias, nor are
they `truth'; they do what we tell them to do. Within a workflow to tell
stories with data, statistical models are tools and approaches that we
use to explore our dataset, in the same way that we may use graphs and
tables. They are not something that will provide us with a definitive
result but will enable us to understand the dataset more clearly in a
particular way.

By the time we get to this step in the workflow, to a large extent, the
model will reflect the decisions that were made in earlier stages,
especially acquisition and cleaning, as much as it reflects any type of
underlying process. Sophisticated modelers know that their statistical
models are like the bit of the iceberg above the surface: they build on,
and are only possible due to, the majority that is underneath, in this
case, the data. But when an expert at the whole workflow uses modelling,
they recognize that the results that are obtained are additionally due
to choices about whose data matters, decisions about how to measure and
record the data, and other aspects that reflect the world as it is, well
before that data is available to their specific workflow.

Finally, we must \textbf{share} what we did and what we found, at as
high a fidelity as is possible. Talking about knowledge that only you
have, does not make you knowledgeable, and that includes knowledge that
only `past you' has. When communicating, we need to be clear about what
decisions we made, why we made them, our findings, and the weaknesses of
our approach. We are aiming to uncover something important (otherwise,
why bother) so we write down everything, in the first instance, although
this written communication may be supplemented with other forms of
communication later. There are so many decisions that we must make in
this workflow that we want to be sure that we are open about the entire
thing---start to finish. This means much more than just the statistical
modelling and creation of the graphs and tables, but everything. Without
this, stories based on data do not have any credibility.

The world is not a rational meritocracy where everything is carefully
and judiciously evaluated. Instead, we use shortcuts, hacks, and
heuristics, based on our experience. Unclear communication will render
even the best work moot, because it will not be thoroughly engaged with.
While there is a minimum when it comes to communication, there is no
upper limit to how impressive it can be. When it is the culmination of a
thought-out workflow, at best, it obtains a certain \emph{sprezzatura},
or studied carelessness. Achieving such mastery is the work of years.

\hypertarget{telling-stories-with-data}{%
\section{Telling stories with data}\label{telling-stories-with-data}}

A compelling story based on data can likely be told in around
ten-to-twenty pages. Much less than this, and it is likely too light on
some of the details. And while it is easy to write much more, often some
reflection enables succinctness or for multiple stories to be separated.
The best stories are typically based on research and independent
learning.

It is possible to tell convincing stories even when it is not possible
to conduct traditional experiments. These approaches do not rely on `big
data'---which is not a panacea (Meng 2018)---but instead on better using
the data that are available. A blend of theory and application, combined
with practical skills, a sophisticated workflow, and an appreciation for
what one does not know, is often enough to create lasting knowledge.

The best stories based on data tend to be multi-disciplinary. They take
from whatever field they need to, but almost always draw on statistics,
data visualization, computer science, experimental design, economics,
engineering, and information science (to name a few). As such, an
end-to-end workflow requires a blend of skills from these areas. The
best way to learn these skills is to use real-world data to conduct
research projects where you:

\begin{itemize}
\tightlist
\item
  obtain and clean relevant datasets;
\item
  develop research questions;
\item
  use statistical techniques to explore those questions; and
\item
  communicate in a meaningful way.
\end{itemize}

The key elements of telling convincing stories with data are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Communication.
\item
  Reproducibility.
\item
  Ethics.
\item
  Questions.
\item
  Measurement.
\item
  Data collection.
\item
  Data cleaning.
\item
  Exploratory data analysis.
\item
  Modelling.
\item
  Scaling.
\end{enumerate}

These elements are the foundation on which the workflow are built
(Figure~\ref{fig-iceberg}).

\begin{figure}

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{./figures/iceberg.png}

}

\caption{\label{fig-iceberg}The workflow builds on various elements}

\end{figure}

This is a lot to master, but \textbf{communication} is the most
important. Simple analysis, communicated well, is more valuable than
complicated analysis communicated poorly. This is because the latter
cannot be understood or trusted by others. A lack of clear communication
sometimes reflects a failure by the researcher to understand what is
going on, or even what they are doing. And so, while the level of the
analysis should match the dataset, instruments, task, and skillset, when
a trade-off is required between clarity and complication, it can be
sensible to err on the side of clarity.

Clear communication means writing in plain language with the help of
tables, graphs, and technical terms, in a way that brings the audience
along with you. It means setting out what was done and why, as well as
what was found. The minimum hurdle is doing this in a way that enables
another person to independently do what you did and find what you found.
One challenge is that as you immerse yourself in the data, it can be
difficult to remember what it was like when you first came to it. But
that is where most of your audience will be coming from. Learning to
provide an appropriate level of nuance and detail is especially
difficult but is made easier by trying to write for the audience's
benefit.

\textbf{Reproducibility} is required to create lasting knowledge about
the world. It means that everything that was done---all of it,
end-to-end---can be independently redone. Ideally, autonomous end-to-end
reproducibility is possible; anyone can get the code, data, and
environment, to verify everything that was done. Unfettered access to
code is almost always possible. While that is the default for data also,
it is not always reasonable. For instance, studies in psychology may
have small, personally identifying, samples. One way forward is to
openly share simulated data with similar properties, along with defining
a process by which the real data could be accessed, given appropriate
\emph{bona fides}.

Active consideration of \textbf{ethics} is needed because the dataset
likely concerns humans. This means considering things like: who is in
the dataset, who is missing, and why? To what extent will our story
perpetuate the past? And is this something that ought to happen? Even if
the dataset does not concern humans, the story is likely being put
together by humans, and we affect almost everything else. This means we
have a moral responsibility to use data ethically, with concern for
environmental impact, and inequity.

There are many definitions of ethics, but when it comes to telling
stories with data, at a minimum it means considering the full context of
the dataset (D'Ignazio and Klein 2020). In jurisprudence, a textual
approach to law means literally considering the words of the law as they
are printed, while a purposive approach means laws are interpreted
within a broader context. An ethical approach to telling stories with
data means adopting the latter approach, and considering the social,
cultural, historical, and political forces that shape our world, and
hence our data (Crawford 2021).

Curiosity provides internal motivation to explore a dataset, and
associated process, to a proper extent. \textbf{Questions} tend to beget
questions, and these usually improve and refine as the process of coming
to understand a dataset carries on. In contrast to the stock Popperian
approach to hypothesis testing often taught, questions are typically
developed through a continuous and evolving process (Franklin 2005). It
can be difficult to find an initial question. Selecting an area of
interest can help, as can sketching a broad claim with the intent of
evolving it into a specific question, and finally, bringing together two
different areas.

Developing a comfort and ease in the messiness of real-world data means
getting to ask new questions each time the data update. And knowing a
dataset in detail tends to surface unexpected groupings or values that
you can then work with subject-area experts to understand. Becoming a
bit of a `mongrel' by developing a base of knowledge across a variety of
areas is especially valuable, as is becoming comfortable with the
possibility of initially asking dumb questions.

\textbf{Measurement} and \textbf{data collection} are about deciding how
our world will become data. They are challenging. The world is so
vibrant that it is difficult to reduce it to something that is possible
to consistently measure and collect. Take, for instance, someone's
height. We can, probably, all agree that we should take our shoes off
before we measure height. But our height changes over the course of the
day. And measuring someone's height with a tape measure will give
different results to using a laser. If we are comparing heights between
people or over time, it therefore becomes important to measure at the
same time each day, using the same method. But that quickly becomes
unfeasible.

Most of the questions we are interested in will use data that are more
complicated than height. How do we measure how sad someone is? How do we
measure pain? Who decides what we will measure and how we will measure
it? There is a certain arrogance required to think that we can reduce
the world to a value and then compare these. Ultimately, we must, but it
is difficult to consistently define what is to be measured. This process
is not value-free. The only way to reasonably come to terms with this
brutal reduction is to deeply understand, and respect what we are
measuring and collecting. What is the central essence, and what can be
stripped away?

Pablo Picasso, the twentieth century Spanish painter, has a series of
drawings where he depicts the outline of an animal using only one line
(Figure~\ref{fig-lumpthedog}). Despite their simplicity, we recognize
which animal is being depicted---the drawing is sufficient to tell the
animal is a dog, not a cat. Could this be used to determine whether the
dog is sick? Probably not. We would likely want a more detailed drawing.
The decision as to which features should be measured and collected, and
which to ignore, turns on context and purpose.

\begin{figure}

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{./figures/lump.png}

}

\caption{\label{fig-lumpthedog}This drawing is clearly a dog, even
though it is just one line}

\end{figure}

\textbf{Data cleaning and preparation} is a critical part of using data.
We need to massage the data available to us into a dataset that we can
use. This requires making a lot of decisions. The data cleaning and
preparation stage is critical, and worthy of as much attention and care
as any other.

Following Kennedy et al. (2020) consider a survey that collected
information about gender using four options: `man', `woman', `prefer not
to say', `other', where `other' dissolved into an open textbox. When we
come to that dataset, we are likely to find that most responses are
either `man' or `woman'. We need to decide what to do about `prefer not
to say'. If we drop it from our dataset, then we are actively ignoring
these respondents. If we do not drop it, then it makes our analysis more
complicated. Similarly, we need to decide how to deal with the open text
responses. Again, we could drop these responses, but this ignores the
experiences of some of our respondents. Another option is to merge this
with `prefer not to say', but that shows a disregard for our
respondents, because they specifically did not choose that option.

There is no easy, nor always-correct, choice in many data cleaning and
preparation situations. It depends on context and purpose. Data cleaning
and preparation involves making many choices like this, and so it is
vital to record every step so that others can understand what was done
and why. Data never speak for themselves; they are the dummies of the
ventriloquists that cleaned and prepared them.

The process of coming to understand the look and feel of a dataset is
termed \textbf{exploratory data analysis} (EDA). This is an open-ended
process. We need to understand the shape of our dataset before we can
formally model it. The process of EDA is an iterative one that involves
producing summary statistics, graphs, tables, and sometimes even some
modelling. It is a process that never formally finishes and requires a
variety of skills.

It is difficult to delineate where EDA ends and formal statistical
modelling begins, especially when considering how beliefs and
understanding develop (Hullman and Gelman 2021). But at its core, `EDA
starts from the data', and involves immersing ourselves in it (Cook,
Reid, and Tanaka 2021). EDA is not something that is typically
explicitly part of our final story. But it has a central role in how we
come to understand the story we are telling. And so, it is critical that
all the steps taken during EDA are recorded and shared.

\textbf{Statistical modelling} has a long and robust history. Our
knowledge of statistics has been built over hundreds of years.
Statistics is not a series of dry theorems and proofs but is instead a
way of exploring the world. It is analogous to `a knowledge of foreign
languages or of algebra: it may prove of use at any time under any
circumstances' (Bowley 1901, 4). A statistical model is not a recipe to
be blindly followed in an if-this-then-that way but is instead a way of
understanding data (James et al. 2017). Modelling is usually required to
infer statistical patterns from data. More formally, `statistical
inference, or 'learning' as it is called in computer science, is the
process of using data to infer the distribution that generated the data'
(Wasserman 2005, 87).

Statistical significance is not the same as scientific significance, and
we are realizing the cost of what has been the dominant paradigm. It is
rarely appropriate to put our data through some arbitrary pass/fail
statistical test. Instead, the proper use for statistical modelling is
as a kind of echolocation. We listen to what comes back to us from the
model, to help learn about the shape of the world, while recognizing
that it is only one representation of the world.

The use of statistical programming languages, such as R, enables us to
rapidly \textbf{scale} our work. This refers to both inputs and outputs.
It is basically just as easy to consider 10 observations as 1,000, or
even 1,000,000. This enables us to more quickly see the extent to which
our stories apply. It is also the case that our outputs can be consumed
as easily by one person as by 10, or 100. Using an Application
Programming Interface (API) it is even possible for our stories to be
considered many thousands of times each second.

\hypertarget{how-do-our-worlds-become-data}{%
\section{How do our worlds become
data?}\label{how-do-our-worlds-become-data}}

\begin{quote}
There is the famous story by Eddington about some people who went
fishing in the sea with a net. Upon examining the size of the fish they
had caught, they decided there was a minimum size to the fish in the
sea! Their conclusion arose from the tool used and not from reality.

Hamming (1996, 177)
\end{quote}

To a certain extent we are wasting our time. We have a perfect model of
the world---it is the world! But it is too complicated. If we knew
perfectly how everything was affected by the uncountable factors that
influence it, then we could perfectly forecast a coin toss, a dice roll,
and every other seemingly random process each time. But we cannot.
Instead, we must simplify things to that which is plausibly measurable,
and it is that which we define as data. Our data are a simplification of
the messy, complex, world from which they were generated.

There are different approximations of `plausibly measurable'. Hence,
datasets are always the result of choices. We must decide whether they
are nonetheless reasonable for the task at hand. We use statistical
models to help us think deeply about, explore, and hopefully come to
better understand, our data.

Much of statistics is focused on considering, thoroughly, the data that
we have. And that was appropriate for when our data were predominately
agricultural, astronomical, or from the physical sciences. This is not
to say that systemic bias cannot exist or have an impact in non-human
contexts, but with the rise of data science, mostly because of the value
of its application to datasets generated by humans, we must also
actively consider what is not in our dataset. Who is systematically
missing from our dataset? Whose data does not fit nicely into the
approach that we are using and are hence is being inappropriately
simplified? If the process of the world becoming data necessarily
involves abstraction and simplification, then we need to be clear about
the points at which we can reasonably simplify, and those which would be
inappropriate, recognizing that this will be application specific.

The process of our world becoming data necessarily involves measurement.
Paradoxically, often those that do the measurement and are deeply
immersed in the details have less trust in the data than those that are
removed from it. Even seemingly clear tasks, such as measuring distance,
defining boundaries, and counting populations, are surprisingly
difficult in practice. Turning our world into data requires many
decisions and imposes much error. Among many other considerations, we
need to decide what will be measured, how accurately we will do this,
and who will be doing the measurement.

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Oh, you think we have good data on that!}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
An important example of how something seemingly simple quickly becomes
difficult is maternal mortality. That refers to the number of women who
die while pregnant, or soon after a termination, from a cause related to
the pregnancy or its management (WHO 2019). It is difficult but critical
to turn the tragedy of such a death into cause-specific data because
that helps mitigate future deaths. Some countries have well-developed
civil registration and vital statistics (CRVS). These collect data about
every death. But many countries do not have a CRVS and so not every
death is recorded. Even if a death is recorded, defining a cause of
death may be difficult, especially when there is a lack of qualified
medical personal or equipment. Maternal mortality is especially
difficult because there are typically many causes. Some CRVS have a
checkbox on the form to specify whether the death should be counted as
maternal mortality. But even some developed countries have only recently
adopted this. For instance, it was only introduced in the US in 2003,
and even in 2015 Alabama, California, and West Virginia had not adopted
the standard question (MacDorman and Declercq 2018).
\end{tcolorbox}

We typically use various instruments to turn the world into data. In
astronomy, the development of better telescopes, and eventually
satellites and probes, enabled new understanding of other worlds.
Similarly, we have new instruments for turning our own world into data
being developed each day. Where once a census was a
generational-defining event, now we have regular surveys, transactions
data available by the second, and almost all interactions on the
internet become data of some kind. The development of such instruments
has enabled exciting new stories.

Our world imperfectly becomes data. If we are to nonetheless use data to
learn about the world, then we need to actively seek to understand the
ways they are imperfect and the implications of those imperfections.

\hypertarget{what-is-data-science-and-how-should-we-use-it-to-learn-about-the-world}{%
\section{What is data science and how should we use it to learn about
the
world}\label{what-is-data-science-and-how-should-we-use-it-to-learn-about-the-world}}

There is no agreed definition of data science, but a lot of people have
tried. For instance, Wickham and Grolemund (2017) say it is `\ldots an
exciting discipline that allows you to turn raw data into understanding,
insight, and knowledge.' Similarly, Leek and Peng (2020) say it is
`\ldots the process of formulating a quantitative question that can be
answered with data, collecting and cleaning the data, analyzing the
data, and communicating the answer to the question to a relevant
audience.' Baumer, Kaplan, and Horton (2021) say it is `\ldots the
science of extracting meaningful information from data'. T.-A. Timbers,
Campbell, and Lee (2022) say they define data science as `the process of
generating insight from data through reproducible and auditable
processes'. Foster (1968) points very clearly to data science when he
says: `(s)tatistics are concerned with the processing and analysis of
masses of data and with the development of mathematical methods of
extracting information from data. Combine all this activity with
computer methods and you have something more than the sum of its parts.'

Craiu (2019) argues that the lack of certainty as to what data science
is might not matter because `\ldots who can really say what makes
someone a poet or a scientist?' He goes on to broadly say that a data
scientist is `\ldots someone with a data driven research agenda, who
adheres to or aspires to using a principled implementation of
statistical methods and uses efficient computation skills.'

In any case, alongside specific, technical, definitions, there is value
in having a simple definition, even if we lose a bit of specificity.
Probability is often informally defined as `counting things' (McElreath
2020, 10). In a similar informal sense, data science can be defined as
something like: humans measuring stuff, typically related to other
humans, and using sophisticated averaging to explain and predict.

That may sound a touch cute, but Francis Edgeworth, the nineteenth
century statistician and economist, considered statistics to be the
science `of those Means which are presented by social phenomena', so it
is in good company (Edgeworth 1885). In any case, one feature of this
definition is that it does not treat data as \emph{terra nullius}, or
nobody's land. Statisticians tend to see data as the result of some
process that we can never know, but that we try to use data to come to
understand. Many statisticians care deeply about data and measurement,
but there are many cases in statistics where data kind of just appear;
they belong to no one. But that is never actually the case.

Data is generated, and then must be gathered, cleaned, and prepared, and
these decisions matter. Every dataset is \emph{sui generis}, or a class
by itself, and so when you come to know one dataset well, you just know
one dataset, not all datasets.

Much of data science focuses on the `science', but it is important to
also focus on `data'. And that is another feature of that cutesy
definition of data science. A lot of data scientists are generalists,
who are interested in a broad range of problems. Often, the thing that
unites these is the need to gather, clean, and prepare messy data. And
often it is the specifics of those data that requires the most time,
that updates most often, and that are worthy of our most full attention.

Jordan (2019) describes being in a medical office and being given some
probability, based on prenatal initial screening, that his child, then a
fetus, had Down syndrome. By way of background, one can do a test to
know for sure, but that test comes with the risk of the fetus not
surviving, so this initial screening is done and then parents typically
use the probability of Down syndrome from that initial screening to
decide whether to do the conclusive test. Jordan (2019) found the
probabilities provided by the initial screening were being determined
based on a study done a decade earlier in the UK. The issue was that in
the ensuing 10 years, imaging technology had improved so the initial
screening was not expecting such high-resolution images, and there had
been a subsequent (false) increase in Down syndrome diagnoses from the
initial screening. The data was the problem.

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
The author of Jordan (2019), Dr Michael Jordan is Pehong Chen
Distinguished Professor at the University of California, Berkeley. After
taking a PhD in Cognitive Science from University of California, San
Diego, in 1985, he was appointed as an assistant professor at MIT, being
promoted to full professor in 1997, and in 1998 he moved to Berkeley.
One area of his research is statistical machine learning. One
particularly important paper is Blei, Ng, and Jordan (2003), which
enables text to be grouped together to define topics, and we cover this
in Chapter~\ref{sec-text-as-data}.
\end{tcolorbox}

It is not just the `science' bit that is hard, it is the `data' bit as
well. For instance, researchers went back and examined one of the most
popular text datasets in computer science, and they found that around 30
per cent of the data were inappropriately duplicated (Bandy and Vincent
2021). There is an entire field---linguistics---that specializes in
these types of datasets, and inappropriate use of data is one of the
dangers of any one field being hegemonic. The strength of data science
is that it brings together folks with a variety of backgrounds and
training to the task of learning about some dataset. It is not
constrained by what was done in the past. This means that we must go out
of our way to show respect for those who do not come from our own
tradition, but who are nonetheless as similarly interested in a dataset
as we are. Data science is multi-disciplinary and increasingly critical;
hence it must reflect our world. There is a pressing need a diversity of
backgrounds, of approaches, and of disciplines in data science.

Our world is messy, and so are our data. To successfully tell stories
with data you need to become comfortable with the fact that the process
will be difficult. Hannah Fry, the British mathematician, describes
spending six months rewriting code before it solved her problem
(Thornhill 2021). You need to learn to stick with it. You also need to
countenance failure, and you do this by developing resilience and having
intrinsic motivation. The world of data is about considering
possibilities and probabilities, and learning to make trade-offs between
them. There is almost never anything that we know for certain, and there
is no perfect analysis.

Ultimately, we are all just telling stories with data, but these stories
are increasingly among the most important in the world.

\hypertarget{exercises-and-tutorial}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial}}

\hypertarget{exercises}{%
\subsection{Exercises}\label{exercises}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  According to Register (2020) data decisions affect (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Real people.
  \item
    No one.
  \item
    Those in the training set.
  \item
    Those in the test set.
  \end{enumerate}
\item
  What is data science (in your own words)?
\item
  According to Keyes (2019) what is perhaps a more accurate definition
  of data science (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The inhumane reduction of humanity down to what can be counted.
  \item
    The quantitative analysis of large amounts of data for the purpose
    of decision-making.
  \item
    Data science is an inter-disciplinary field that uses scientific
    methods, processes, algorithms, and systems to extract knowledge and
    insights from many structural and unstructured data.
  \end{enumerate}
\item
  Imagine that you have a job in which including `race' and/or sexuality
  as explanatory variables improves the performance of your model. What
  types of issues would you consider when deciding whether to include
  these variable in your analysis (in your own words)?
\item
  Re-order the following steps of the workflow to be correct:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Simulate.
  \item
    Acquire.
  \item
    Share.
  \item
    Plan.
  \item
    Explore.
  \end{enumerate}
\item
  According to Crawford (2021), which of the following forces shape our
  world, and hence our data (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Political.
  \item
    Historical.
  \item
    Cultural.
  \item
    Social.
  \end{enumerate}
\item
  What is required to tell convincing stories (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Sophisticated workflow.
  \item
    Practical skills.
  \item
    Big data.
  \item
    Humility about one's own knowledge.
  \item
    Theory and application.
  \end{enumerate}
\item
  Why is ethics a key element of telling convincing stories (in your own
  words)?
\item
  Consider a survey that asked about gender with the following
  responses: `man: 879', `woman: 912', `non-binary: 10' `prefer not to
  say: 3', and `other: 1', which allowed respondents to enter their own
  text. What is the appropriate way to consider `prefer not to say'?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Drop them.
  \item
    Merge into `other'.
  \item
    Include them.
  \item
    It depends.
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial}{%
\subsection{Tutorial}\label{tutorial}}

The purpose of this tutorial is to clarify in your mind the difficulty
of measurement, even of seemingly simple things, and hence the
likelihood of measurement issues in more complicated areas.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Please obtain some seeds for a fast-growing plant such as radishes,
  mustard greens, or arugula. Plant the seeds and measure how much soil
  you used. Water them and measure the water you used. Each day take a
  note of any changes. More generally, measure and record as much as you
  can. Note your thoughts about the difficulty of measurement.
  Eventually your seeds will sprout, and you should measure how big they
  are. We will return to use the data that you put together.
\item
  While you are waiting for the seeds to sprout, and for one week only,
  please measure the length of your hair daily. Write a one-to-two-page
  paper about what you found and what you learned about the difficulty
  of measurement.
\end{enumerate}

\hypertarget{sec-fire-hose}{%
\chapter{Drinking from a fire hose}\label{sec-fire-hose}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{Data science as an atomic habit}, (Barrett 2021a)
\item
  Read \emph{This is how AI bias really happens---and why it's so hard
  to fix}, (Hao 2019)
\item
  Read \emph{The mundanity of excellence: An ethnographic report on
  stratification and Olympic swimmers}, (Chambliss 1989)
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  The statistical programming language R enables us to tell interesting
  stories using data. It is a language like any other, and the path to
  mastery can be slow.
\item
  The framework that we use to approach projects is: plan, simulate,
  gather, explore, and share.
\item
  The way to learn R is to start with a small project and break down
  what is required to achieve it into tiny steps, look at other people's
  code, and draw on that to achieve each step. Complete that project and
  move onto the next project. Each project you will get a little better.
\item
  The key is to start actively working regularly.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{janitor} (Firke 2020)
\item
  \texttt{opendatatoronto} (Gelfand 2020)
\item
  \texttt{tidyverse} (Wickham 2017):

  \begin{itemize}
  \tightlist
  \item
    \texttt{ggplot2} (Wickham 2016)
  \item
    \texttt{tidyr} (Wickham 2021c)
  \end{itemize}
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{\textless{}-} `assign'
\item
  \texttt{\textbar{}\textgreater{}} `pipe'
\item
  \texttt{+} `add'
\item
  \texttt{c()}
\item
  \texttt{citation()}
\item
  \texttt{class()}
\item
  \texttt{dplyr::arrange()}
\item
  \texttt{dplyr::filter()}
\item
  \texttt{dplyr::mutate()}
\item
  \texttt{dplyr::recode()}
\item
  \texttt{dplyr::rename()}
\item
  \texttt{dplyr::select()}
\item
  \texttt{dplyr::summarize()}
\item
  \texttt{ggplot2::geom\_bar()}
\item
  \texttt{ggplot2::geom\_point()}
\item
  \texttt{ggplot2::ggplot()}
\item
  \texttt{head()}
\item
  \texttt{janitor::clean\_names()}
\item
  \texttt{library()}
\item
  \texttt{names()}
\item
  \texttt{readr::read\_csv()}
\item
  \texttt{readr::write\_csv()}
\item
  \texttt{rep()}
\item
  \texttt{rpois()}
\item
  \texttt{runif()}
\item
  \texttt{sample()}
\item
  \texttt{set.seed()}
\item
  \texttt{stringr::str\_remove()}
\item
  \texttt{sum()}
\item
  \texttt{tail()}
\item
  \texttt{tidyr::separate()}
\end{itemize}

\hypertarget{hello-world}{%
\section{Hello, World!}\label{hello-world}}

The way to start, is to start. In this chapter we go through three
complete examples of the workflow advocated in this book. This means we
will: plan, simulate, acquire, explore, and share. If you are new to R,
then some of the code may be a bit unfamiliar to you. If you are new to
statistics, then some of the concepts may be unfamiliar. Do not worry.
It will all soon become familiar.

The only way to learn how to tell stories, is to start telling stories
yourself. This means that you should try to get these examples working.
Do the sketches yourself, type everything out yourself (using R Studio
Cloud if you are new to R and do not have it installed on your own
computer), and execute it all. It is important, and normal, to realize
that it will be challenging at the start.

\begin{quote}
Whenever you're learning a new tool, for a long time, you're going to
suck\ldots{} But the good news is that is typical; that's something that
happens to everyone, and it's only temporary.

Hadley Wickham as quoted by Barrett (2021a).
\end{quote}

You will be guided thoroughly here. Hopefully by experiencing the power
of telling stories with data, you will feel empowered to stick with it.

To get started, go to \href{https://rstudio.cloud/}{R Studio Cloud} and
create an account. As we are not doing anything too involved the free
version will be fine for now. Once you have an account and log in, then
it should look something like Figure~\ref{fig-02-rstudio_cloud-1}.

\begin{figure}

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{./figures/02-rstudio_cloud-1.png}

}

\caption{\label{fig-02-rstudio_cloud-1}Opening R Studio Cloud for the
first time}

\end{figure}

You will be in `Your Projects'. From here you should start a new project
(`New Project' -\textgreater{} `New RStudio Project')
(Figure~\ref{fig-02-rstudio_cloud-2}). You can give the project a name
by clicking on `Untitled Project' and replacing it .

\begin{figure}

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{./figures/02-rstudio_cloud-2.png}

}

\caption{\label{fig-02-rstudio_cloud-2}Opening a new R Studio project}

\end{figure}

We will now go through three worked examples: Australian elections,
Toronto homelessness, and neonatal mortality. These examples build
increasing complexity, but from the first one, we will be telling a
story with data.

\hypertarget{australian-elections}{%
\section{Australian elections}\label{australian-elections}}

Australia is a parliamentary democracy with 151 seats in the House of
Representatives, which is the lower house and that from which government
is formed. There are two major parties -- `Liberal' and `Labor' -- two
minor parties -- `National' and `Green' -- and many smaller parties and
independents. In this example we will create a graph of the number of
seats that each party won in the 2019 Federal Election.

\hypertarget{plan}{%
\subsection{Plan}\label{plan}}

For this example, we need to plan two aspects. The first is what the
dataset that we need will look like, and the second is what the final
graph will look like.

The basic requirement for the dataset is that it has the name of the
seat (sometimes called a `division' in Australia) and the party of the
person elected. So, a quick sketch of the dataset that we would need
could look something like Figure~\ref{fig-canadaexampledata}.

\begin{figure}

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{./figures/number-of-seats.png}

}

\caption{\label{fig-canadaexampledata}Quick sketch of a dataset that
could be useful for analyzing Australian elections}

\end{figure}

We also need to plan the graph that we are interested in. Given we want
to display the number of seats that each party won, a quick sketch of
what we might aim for is Figure~\ref{fig-canadaexampletable}.

\begin{figure}

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{./figures/possible-seats.png}

}

\caption{\label{fig-canadaexampletable}Quick sketch of a possible graph
of the number of seats won by each party}

\end{figure}

\hypertarget{simulate}{%
\subsection{Simulate}\label{simulate}}

We now simulate some data, to bring some specificity to our sketches.

To get started, within R Studio Cloud, make a new Quarto document
(`File' -\textgreater{} `New File' -\textgreater{} `Quarto
Document\ldots{}'). Give it a title, such as `Exploring the 2019
Australian Election', and add your name as author. Leave the other
options as their default, and then click `Create'. For this example, we
will put everything into this one Quarto document. You should save it as
`australian\_elections.qmd' (`File' -\textgreater{} `Save As\ldots{}').

Remove almost all the default content, and then beneath the heading
material create a new R code chunk (`Code' -\textgreater{} `Insert
Chunk') and add preamble documentation that explains:

\begin{itemize}
\tightlist
\item
  the purpose of the document;
\item
  the author and contact details;
\item
  when the file was written or last updated; and
\item
  pre-requisites that the file relies on.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Preamble \#\#\#\#}
\CommentTok{\# Purpose: Read in data from the 2019 Australian Election and make a}
\CommentTok{\# graph of the number of seats each party won.}
\CommentTok{\# Author: Rohan Alexander}
\CommentTok{\# Email: rohan.alexander@utoronto.ca}
\CommentTok{\# Date: 12 April 2022}
\CommentTok{\# Prerequisites: Need to know where to get Australian elections data.}
\end{Highlighting}
\end{Shaded}

In R, lines that start with `\#' are comments. This means that they are
not run as code by R, but are instead designed to be read by humans.
Each line of this preamble should start with a `\#'. Also make it clear
that this is the preamble section by surrounding that with `\#\#\#\#'.
The result should look like
Figure~\ref{fig-quarto-australian-elections-3}.

\begin{figure}

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{./figures/02-rstudio_cloud-3.png}

}

\caption{\label{fig-quarto-australian-elections-3}Screenshot of
australian\_elections.qmd after initial set-up and with a premable}

\end{figure}

After this we need to set-up the workspace. This involves installing and
loading any packages that will be needed. A package only needs to be
installed once for each computer, but needs to be loaded each time it is
to be used. In this case we are going to use \texttt{tidyverse} (Wickham
2017), \texttt{janitor} (Firke 2020), and \texttt{tidyr} (Wickham
2021c). They will need to be installed because this is the first time
they are being used, and then each will need to be loaded.

An example of installing the packages follows (excessive comments have
been added to be clear about what is going on; in general, this level of
commenting is unnecessary). Run this code by clicking the small green
arrow associated with the R code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Workspace set{-}up \#\#\#\#}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{) }\CommentTok{\# Only need to do this once per computer}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"janitor"}\NormalTok{) }\CommentTok{\# Only need to do this once per computer}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyr"}\NormalTok{) }\CommentTok{\# Only need to do this once per computer}
\end{Highlighting}
\end{Shaded}

Now that the packages are installed, they need to be loaded. As that
installation step only needs to be done once per computer, that code can
be commented out so that it is not accidentally run.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Workspace set{-}up \#\#\#\#}
\CommentTok{\# install.packages("tidyverse") \# Only need to do this once per computer}
\CommentTok{\# install.packages("janitor") \# Only need to do this once per computer}
\CommentTok{\# install.packages("tidyr") \# Only need to do this once per computer}

\FunctionTok{library}\NormalTok{(tidyverse) }\CommentTok{\# A collection of data{-}related packages}
\FunctionTok{library}\NormalTok{(janitor) }\CommentTok{\# Helps clean datasets}
\FunctionTok{library}\NormalTok{(tidyr) }\CommentTok{\# Helps make tidy datasets}
\end{Highlighting}
\end{Shaded}

We can render the entire document by clicking `Render'. When you do
this, you will be asked to install some packages, which you should agree
to. This will result in a html document.

For an introduction to the packages that were just installed, each
package contains a help file that provides information about them and
their functions. It can be accessed by appending a question mark before
the package name and then running that code in the console. For instance
\texttt{?tidyverse}.

To simulate our data, we need to create a dataset with two columns:
`Division' and `Party', and some values for each. In the case of
`Division' reasonable values would be a name of one of the 151
Australian divisions. In the case of `Party' reasonable values would be
one of the following five: `Liberal', `Labor', `National', `Green',
`Other'. Again, this code can be run by clicking the small green arrow
associated with the R code chunk.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_data }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \CommentTok{\# Use 1 through to 151 to represent each riding}
    \StringTok{\textquotesingle{}Riding\textquotesingle{}} \OtherTok{=} \DecValTok{1}\SpecialCharTok{:}\DecValTok{151}\NormalTok{,}
    \CommentTok{\# Randomly choose one of five options, with replacement, 151 times}
    \StringTok{\textquotesingle{}Party\textquotesingle{}} \OtherTok{=} \FunctionTok{sample}\NormalTok{(}
      \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}
        \StringTok{\textquotesingle{}Liberal\textquotesingle{}}\NormalTok{, }
        \StringTok{\textquotesingle{}Labor\textquotesingle{}}\NormalTok{, }
        \StringTok{\textquotesingle{}National\textquotesingle{}}\NormalTok{, }
        \StringTok{\textquotesingle{}Green\textquotesingle{}}\NormalTok{, }
        \StringTok{\textquotesingle{}Other\textquotesingle{}}
\NormalTok{      ),}
      \AttributeTok{size =} \DecValTok{151}\NormalTok{,}
      \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{    ))}

\NormalTok{simulated\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 151 x 2
   Riding Party  
    <int> <chr>  
 1      1 Green  
 2      2 Labor  
 3      3 Liberal
 4      4 Green  
 5      5 Other  
 6      6 Liberal
 7      7 Liberal
 8      8 Labor  
 9      9 Liberal
10     10 Green  
# ... with 141 more rows
\end{verbatim}

At a certain point, your code will not run and you will want to ask
others for help. We will discuss how to do this in
Chapter~\ref{sec-reproducible-workflows}, but for now, if you need help,
then you should create a GitHub Gist. The first step is to create an
account on \href{https://github.com}{GitHub}
(Figure~\ref{fig-githubone}). Thinking about an appropriate username is
important because this will become part of your professional profile. So
it would make sense to have a username that is professional, independent
of any course, and ideally related to your real name.

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/github_1.png}

}

\caption{\label{fig-githubone}GitHub sign-up screen}

\end{figure}

Then look for a `+' in the top right, and then select `New gist'
(Figure~\ref{fig-githubgistone}).

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/githubgistone.png}

}

\caption{\label{fig-githubgistone}New GitHub Gist}

\end{figure}

Then add all the code to that gist, not just the final bit that is
giving an error. And give it a meaningful filename that includes `.R' at
the end, for instance, `australian\_elections.R'. In
Figure~\ref{fig-githubgisttwo}, we have incorrect capitalization
\texttt{library(Tidyverse)}.

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/githubgisttwo.png}

}

\caption{\label{fig-githubgisttwo}Create a public GitHub Gist to share
code}

\end{figure}

Click `Create public gist'. We can then share the URL to this Gist,
explain what the problem is, and what we are trying to achieve. It will
be much easier to help, because all the code is available.

\hypertarget{acquire}{%
\subsection{Acquire}\label{acquire}}

Now we want to get the actual data. The data we need is from the
Australian Electoral Commission (AEC), which is the non-partisan agency
that organizes Australian federal elections. We can pass a page from
their website to \texttt{read\_csv()} from the \texttt{readr} package
(Wickham, Hester, and Bryan 2021). We do not need to explicitly load the
\texttt{readr} package because it is part of the \texttt{tidyverse}. The
\texttt{\textless{}-} or `assignment operator' is allocating the output
of \texttt{read\_csv()} to an object called `raw\_elections\_data'.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Read in the data \#\#\#\#}
\NormalTok{raw\_elections\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}
    \AttributeTok{file =}
      \StringTok{"https://results.aec.gov.au/24310/Website/Downloads/HouseMembersElectedDownload{-}24310.csv"}\NormalTok{,}
    \AttributeTok{show\_col\_types =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{skip =} \DecValTok{1}
\NormalTok{    ) }

\CommentTok{\# We have read the data from the AEC website. We may like}
\CommentTok{\# to save it in case something happens or they move it. }
\FunctionTok{write\_csv}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ raw\_elections\_data, }
  \AttributeTok{file =} \StringTok{"australian\_voting.csv"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We can take a quick look at the dataset using \texttt{head()} which will
show the first six rows, and \texttt{tail()} which will show the last
six rows.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(raw\_elections\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 8
  DivisionID DivisionNm StateAb CandidateID GivenNm   Surname   PartyNm  PartyAb
       <dbl> <chr>      <chr>         <dbl> <chr>     <chr>     <chr>    <chr>  
1        179 Adelaide   SA            33019 Steve     GEORGANAS Austral~ ALP    
2        197 Aston      VIC           33330 Alan      TUDGE     Liberal  LP     
3        198 Ballarat   VIC           32326 Catherine KING      Austral~ ALP    
4        103 Banks      NSW           33334 David     COLEMAN   Liberal  LP     
5        180 Barker     SA            33042 Tony      PASIN     Liberal  LP     
6        104 Barton     NSW           32681 Linda     BURNEY    Austral~ ALP    
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tail}\NormalTok{(raw\_elections\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 8
  DivisionID DivisionNm StateAb CandidateID GivenNm    Surname  PartyNm  PartyAb
       <dbl> <chr>      <chr>         <dbl> <chr>      <chr>    <chr>    <chr>  
1        152 Wentworth  NSW           33218 Dave       SHARMA   Liberal  LP     
2        153 Werriwa    NSW           32756 Anne Maree STANLEY  Austral~ ALP    
3        150 Whitlam    NSW           32632 Stephen    JONES    Austral~ ALP    
4        178 Wide Bay   QLD           33200 Llew       O'BRIEN  Liberal~ LNP    
5        234 Wills      VIC           32350 Peter      KHALIL   Austral~ ALP    
6        316 Wright     QLD           33376 Scott      BUCHHOLZ Liberal~ LNP    
\end{verbatim}

We need to clean the data so that we can use it. We are trying to make
it similar to the dataset that we thought we wanted in the planning
stage. While it is fine to move away from the plan, this needs to be a
deliberate, reasoned, decision. After reading in the dataset that we
saved, the first thing that we will do is adjust the names to make them
easier to type. Removing the spaces helps to type column names. We will
do this using \texttt{clean\_names()} from \texttt{janitor} (Firke 2020)
which changes the names into `snake\_case'.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Basic cleaning \#\#\#\#}
\NormalTok{raw\_elections\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"australian\_voting.csv"}\NormalTok{,}
           \AttributeTok{show\_col\_types =} \ConstantTok{FALSE}
\NormalTok{           )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Make the names easier to type}
\NormalTok{cleaned\_elections\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{clean\_names}\NormalTok{(raw\_elections\_data)}

\CommentTok{\# Have a look at the first six rows}
\FunctionTok{head}\NormalTok{(cleaned\_elections\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 8
  division_id division_nm state_ab candidate_id given_nm  surname   party_nm    
        <dbl> <chr>       <chr>           <dbl> <chr>     <chr>     <chr>       
1         179 Adelaide    SA              33019 Steve     GEORGANAS Australian ~
2         197 Aston       VIC             33330 Alan      TUDGE     Liberal     
3         198 Ballarat    VIC             32326 Catherine KING      Australian ~
4         103 Banks       NSW             33334 David     COLEMAN   Liberal     
5         180 Barker      SA              33042 Tony      PASIN     Liberal     
6         104 Barton      NSW             32681 Linda     BURNEY    Australian ~
# ... with 1 more variable: party_ab <chr>
\end{verbatim}

The names are faster to type because R Studio will auto-complete them.
To do this, we begin typing the name of a column and then use `tab' to
auto-complete it.

There are many columns in the dataset, and we are primarily interested
in two: `division\_nm', and `party\_nm'. We can choose certain columns
of interest using \texttt{select()} from \texttt{dplyr} (Wickham et al.
2020) which we loaded as part of the \texttt{tidyverse}. The `pipe
operator', \texttt{\textbar{}\textgreater{}}, pushes the output of one
line to be the first input of the function on the next line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  cleaned\_elections\_data }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# Select only certain columns}
  \FunctionTok{select}\NormalTok{(division\_nm,}
\NormalTok{         party\_nm}
\NormalTok{         )}

\CommentTok{\# Have a look at the first six rows}
\FunctionTok{head}\NormalTok{(cleaned\_elections\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 2
  division_nm party_nm              
  <chr>       <chr>                 
1 Adelaide    Australian Labor Party
2 Aston       Liberal               
3 Ballarat    Australian Labor Party
4 Banks       Liberal               
5 Barker      Liberal               
6 Barton      Australian Labor Party
\end{verbatim}

Some of the names of the columns are still not obvious because they are
abbreviated. We can look at the names of the columns with
\texttt{names()}. And we can change the names using \texttt{rename()}
from \texttt{dplyr} (Wickham et al. 2020).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(cleaned\_elections\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "division_nm" "party_nm"   
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  cleaned\_elections\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}
    \AttributeTok{division =}\NormalTok{ division\_nm,}
    \AttributeTok{elected\_party =}\NormalTok{ party\_nm}
\NormalTok{    )}

\FunctionTok{head}\NormalTok{(cleaned\_elections\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 2
  division elected_party         
  <chr>    <chr>                 
1 Adelaide Australian Labor Party
2 Aston    Liberal               
3 Ballarat Australian Labor Party
4 Banks    Liberal               
5 Barker   Liberal               
6 Barton   Australian Labor Party
\end{verbatim}

We will now look at this dataset, and the `elected\_party' column in
particular.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(cleaned\_elections\_data}\SpecialCharTok{$}\NormalTok{elected\_party)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Australian Labor Party" "Liberal"                "Australian Labor Party"
[4] "Liberal"                "Liberal"                "Australian Labor Party"
\end{verbatim}

Finally, we want to simplify the party names to match what we simulated,
using \texttt{recode()} from \texttt{dplyr} (Wickham et al. 2020).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\OtherTok{\textless{}{-}}
\NormalTok{  cleaned\_elections\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{elected\_party =}
      \FunctionTok{recode}\NormalTok{(}
\NormalTok{        elected\_party,}
        \StringTok{"Australian Labor Party"} \OtherTok{=} \StringTok{"Labor"}\NormalTok{,}
        \StringTok{"Liberal National Party"} \OtherTok{=} \StringTok{"Liberal"}\NormalTok{,}
        \StringTok{"The Nationals"} \OtherTok{=} \StringTok{"Nationals"}\NormalTok{,}
        \StringTok{"The Greens"} \OtherTok{=} \StringTok{"Greens"}\NormalTok{,}
        \StringTok{"Independent"} \OtherTok{=} \StringTok{"Other"}\NormalTok{,}
        \StringTok{"Katter\textquotesingle{}s Australian Party (KAP)"} \OtherTok{=} \StringTok{"Other"}\NormalTok{,}
        \StringTok{"Centre Alliance"} \OtherTok{=} \StringTok{"Other"}
\NormalTok{      )}
\NormalTok{  )}

\FunctionTok{head}\NormalTok{(cleaned\_elections\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 2
  division elected_party
  <chr>    <chr>        
1 Adelaide Labor        
2 Aston    Liberal      
3 Ballarat Labor        
4 Banks    Liberal      
5 Barker   Liberal      
6 Barton   Labor        
\end{verbatim}

Our data now matches our plan (Figure~\ref{fig-canadaexampledata})
pretty well. For every electoral division we have the party of the
person that won it.

Having now nicely cleaned the dataset, we should save it, so that we can
start with that cleaned dataset in the next stage. We should make sure
to save it under a new file name so we are not replacing the raw data,
and so that it is easy to identify the cleaned dataset later.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ cleaned\_elections\_data,}
  \AttributeTok{file =} \StringTok{"cleaned\_elections\_data.csv"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore}{%
\subsection{Explore}\label{explore}}

At this point we would like to explore the dataset that we created. One
way to better understand a dataset is to make a graph. In particular,
here we would like to build the graph that we planned in
Figure~\ref{fig-canadaexampletable}.

First, we read in the dataset that we just created.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Read in the data \#\#\#\#}
\NormalTok{cleaned\_elections\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}
    \AttributeTok{file =} \StringTok{"cleaned\_elections\_data.csv"}\NormalTok{,}
    \AttributeTok{show\_col\_types =} \ConstantTok{FALSE}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

We can get a quick count of how many seats each party won using
\texttt{count()} from \texttt{dplyr} (Wickham et al. 2020).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(elected\_party)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 2
  elected_party     n
  <chr>         <int>
1 Greens            1
2 Labor            68
3 Liberal          67
4 Nationals        10
5 Other             5
\end{verbatim}

To build the graph that we are interested in, we will rely on the
\texttt{ggplot2} package (Wickham 2016). The key aspect of this package
is that we build graphs by adding layers using `+', which we call the
`add operator'. In particular we will create a bar chart using
\texttt{geom\_bar()} from \texttt{ggplot2} (Wickham 2016).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ elected\_party)) }\SpecialCharTok{+} \CommentTok{\# aes abbreviates \textquotesingle{}aesthetics\textquotesingle{} and enables}
  \CommentTok{\#  us to specify the x axis variable}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./02-drinking_from_a_fire_hose_files/figure-pdf/unnamed-chunk-44-1.pdf}

}

\end{figure}

This accomplishes what we set out to do. But we can make it look a bit
nicer by modifying the default options (Figure~\ref{fig-canadanice}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ elected\_party)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# Make the theme neater}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# Swap the x and y axis to make parties easier to read}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Party"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of seats"}\NormalTok{) }\CommentTok{\# Make the labels more meaningful}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./02-drinking_from_a_fire_hose_files/figure-pdf/fig-canadanice-1.pdf}

}

\caption{\label{fig-canadanice}Number of seats won, by political party,
at the 2019 Australian Federal Election}

\end{figure}

\hypertarget{communicate}{%
\subsection{Communicate}\label{communicate}}

To this point we have downloaded some data, cleaned it, and made a
graph. We would typically need to communicate what we have done at some
length. In this case, we can write a few paragraphs about what we did,
why we did it, and what we found to conclude our workflow. An example
follows.

\begin{quote}
Australia is a parliamentary democracy with 151 seats in the House of
Commons, which is the house that forms government. There are two major
parties---`Liberal' and `Labor'---two minor parties---`Nationals' and
`Greens'---and many smaller parties. The 2019 Federal Election occurred
on 18 May, and more than 14 million votes were cast. We were interested
in the number of seats that were won by each party.

We downloaded the results, on a seat-specific basis, from the Australian
Electoral Commission website. We cleaned and tidied the dataset using
the statistical programming language R (R Core Team 2021) including the
packages \texttt{tidyverse} (Wickham et al. 2019a) and \texttt{janitor}
(Firke 2020). We then created a graph of the number of seats that each
political party won (Figure~\ref{fig-canadanice}).

We found that the Labor Party won 68 seats, followed by the Liberal
Party with 67 seats. The minor parties won the following number of
seats: Nationals, 10 seats and the Green Party, 1 seats. Finally,
candidates from five other parties were elected.

The distribution of seats is skewed toward the two major parties which
could reflect relatively stable preferences on the part of Australian
voters, or possibly inertia due to the benefits of already being a major
party such a national network and funding, or some other reason. A
better understanding the reasons for this distribution are of interest
in future work. While the dataset consists of everyone who voted, it
worth noting that in Australia some are systematically excluded from
voting; and it is much difficult for some to vote than others.
\end{quote}

\hypertarget{toronto-homelessness}{%
\section{Toronto homelessness}\label{toronto-homelessness}}

Toronto has a large homeless population (City of Toronto 2021). Freezing
winters mean it is important there are enough places in shelters. In
this example we will make a table of shelter usage in the second half of
2021 that compares average use in each month. Our expectation is that
there is greater usage in the colder months, for instance, December,
compared with warmer months, for instance, July.

\hypertarget{plan-1}{%
\subsection{Plan}\label{plan-1}}

The dataset that we are interested in would need to have date, the
shelter, and the number of beds that were occupied that night. A quick
sketch of a dataset that would work is
Figure~\ref{fig-torontohomelessdataplan}.

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{./figures/IMG_1817.png}

}

\caption{\label{fig-torontohomelessdataplan}Quick sketch of a dataset
that could be useful for understanding shelter usage in Toronto}

\end{figure}

We are interested in creating a table that has the monthly average
number of beds occupied each night. The table would probably look
something like Figure~\ref{fig-houselessexampletable}

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{./figures/IMG_1818.png}

}

\caption{\label{fig-houselessexampletable}Quick sketch of a table of the
average number of beds occupied each month}

\end{figure}

\hypertarget{simulate-1}{%
\subsection{Simulate}\label{simulate-1}}

The next step is to simulate some data that could resemble our dataset.
Simulation provides us with an opportunity to deeply think about our
data generating process.

Within R Studio Cloud make a new Quarto Document, save it, and make a
new R code chunk and add preamble documentation. Then install and/or
load the libraries that are needed. We will again use \texttt{tidyverse}
(Wickham 2017), \texttt{janitor} (Firke 2020), and \texttt{tidyr}
(Wickham 2021c). As those were installed earlier, they do not need to be
installed again. In this example we will also use \texttt{lubridate}
(Grolemund and Wickham 2011), which is part of the \texttt{tidyverse}
and so it does not need to be installed independently. We will also use
\texttt{opendatatoronto} (Gelfand 2020), and \texttt{knitr} (Xie 2021)
and these will need to be installed.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Preamble \#\#\#\#}
\CommentTok{\# Purpose: Get data about 2021 houseless shelter usage and make a table}
\CommentTok{\# Author: Rohan Alexander}
\CommentTok{\# Email: rohan.alexander@utoronto.ca}
\CommentTok{\# Date: 1 January 2022}
\CommentTok{\# Prerequisites: {-} }

\DocumentationTok{\#\#\#\# Workspace set{-}up \#\#\#\#}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"opendatatoronto"}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"lubridate"}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"knitr"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(lubridate)}
\FunctionTok{library}\NormalTok{(opendatatoronto)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidyr)}
\end{Highlighting}
\end{Shaded}

To add a bit more detail to the earlier example, libraries contain code
that other people have written. There are a few common ones that you
will see regularly, especially the \texttt{tidyverse}. To use a package,
we must first install it and then we need to load it. A package only
needs to be installed once per computer but must be loaded every time.
So, the packages that we installed earlier do not need to be reinstalled
here.

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Dr Robert Gentleman and Dr Ross Ihaka are the co-creators of R. After
taking a PhD in Statistics from the University of Washington in 1988,
Robert moved to the University of Auckland, then went onto various roles
including 23andMe and is now the Executive Director of the Center for
Computational Biomedicine at Harvard Medical School. Ross took a PhD in
\textbf{???} from the University of California, Berkeley, in
\textbf{???}, then moved to the University of Auckland where he remained
for his entire career \textbf{(VERIFY)}. He was awarded the Pickering
Medal in 2008 by the Royal Society of New Zealand Te Apārangi.
\end{tcolorbox}

Given that folks freely gave up their time to make R and the packages
that we use, it is important to cite them. To get the information that
is needed, we can use \texttt{citation()}. When run without any
arguments, that provides the citation information for R itself, and when
run with an argument that is the name of a package, it provides the
citation information for that package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{citation}\NormalTok{() }\CommentTok{\# Get the citation information for R}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

To cite R in publications use:

  R Core Team (2021). R: A language and environment for statistical
  computing. R Foundation for Statistical Computing, Vienna, Austria.
  URL https://www.R-project.org/.

A BibTeX entry for LaTeX users is

  @Manual{,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2021},
    url = {https://www.R-project.org/},
  }

We have invested a lot of time and effort in creating R, please cite it
when using it for data analysis. See also 'citation("pkgname")' for
citing R packages.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{citation}\NormalTok{(}\StringTok{\textquotesingle{}tidyverse\textquotesingle{}}\NormalTok{) }\CommentTok{\# Get the citation information for a particular package}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

  Wickham et al., (2019). Welcome to the tidyverse. Journal of Open
  Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686

A BibTeX entry for LaTeX users is

  @Article{,
    title = {Welcome to the {tidyverse}},
    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
    year = {2019},
    journal = {Journal of Open Source Software},
    volume = {4},
    number = {43},
    pages = {1686},
    doi = {10.21105/joss.01686},
  }
\end{verbatim}

Turning to the simulation, we need three columns: `date', `shelter', and
`occupancy'. This example will build on the earlier one by adding a seed
using \texttt{set.seed()}. A seed enables us to always generate the same
random data whenever we run the same code. Any integer can be used as
the seed. In this case the seed will be 853. If you use that as your
seed, then you should get the same random numbers as in this example. If
you use a different seed, then you should expect different random
numbers. Finally, we use \texttt{rep()} to repeat something a certain
number of times. For instance, we repeat `Shelter 1', 184 times which
accounts fora about half a year.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Simulate \#\#\#\#}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)   }

\NormalTok{simulated\_occupancy\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{date =} \FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2021{-}07{-}01"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{183}\NormalTok{), }\AttributeTok{times =} \DecValTok{3}\NormalTok{), }
    \CommentTok{\# Based on Dirk Eddelbuettel: https://stackoverflow.com/a/21502386}
    \AttributeTok{shelter =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{"Shelter 1"}\NormalTok{, }\AttributeTok{times =} \DecValTok{184}\NormalTok{), }
                \FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{"Shelter 2"}\NormalTok{, }\AttributeTok{times =} \DecValTok{184}\NormalTok{),}
                \FunctionTok{rep}\NormalTok{(}\AttributeTok{x =} \StringTok{"Shelter 3"}\NormalTok{, }\AttributeTok{times =} \DecValTok{184}\NormalTok{)),}
    \AttributeTok{number\_occupied =} 
      \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =} \DecValTok{184}\SpecialCharTok{*}\DecValTok{3}\NormalTok{,}
            \AttributeTok{lambda =} \DecValTok{30}\NormalTok{) }\CommentTok{\# Draw 552 times from the Poisson distribution}
\NormalTok{    )}

\FunctionTok{head}\NormalTok{(simulated\_occupancy\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  date       shelter   number_occupied
  <date>     <chr>               <int>
1 2021-07-01 Shelter 1              28
2 2021-07-02 Shelter 1              29
3 2021-07-03 Shelter 1              35
4 2021-07-04 Shelter 1              25
5 2021-07-05 Shelter 1              21
6 2021-07-06 Shelter 1              30
\end{verbatim}

In this simulation we first create a list of all the dates in 2021. We
repeat that list three times. We assume data for three shelters for
every day of the year. To simulate the number of beds that are occupied
each night, we draw from a Poisson distribution, assuming a mean number
of 30 beds occupied per shelter.

\hypertarget{acquire-1}{%
\subsection{Acquire}\label{acquire-1}}

We use data made available about Toronto homeless shelters by the City
of Toronto. The premise of the data is that each night at 4am a count is
made of the occupied beds. To access the data, we use
\texttt{opendatatoronto} (Gelfand 2020) and then save our own copy.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Acquire \#\#\#\#}
\CommentTok{\# Based on code from: }
\CommentTok{\# https://open.toronto.ca/dataset/daily{-}shelter{-}overnight{-}service{-}occupancy{-}capacity/}
\CommentTok{\# Thank you to Heath Priston for assistance}
\NormalTok{toronto\_shelters }\OtherTok{\textless{}{-}} 
  \CommentTok{\# Each package is associated with a unique id which can be found in }
  \CommentTok{\# \textquotesingle{}For Developers\textquotesingle{}:}
  \CommentTok{\# https://open.toronto.ca/dataset/daily{-}shelter{-}overnight{-}service{-}occupancy{-}capacity/}
  \FunctionTok{list\_package\_resources}\NormalTok{(}\StringTok{"21c83b32{-}d5a8{-}4106{-}a54f{-}010dbe49f6f2"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# Within that package, we are interested in the 2021 dataset}
  \FunctionTok{filter}\NormalTok{(name }\SpecialCharTok{==} \StringTok{"daily{-}shelter{-}overnight{-}service{-}occupancy{-}capacity{-}2021"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# Having reduce the dataset down to one row we can get the resource}
  \FunctionTok{get\_resource}\NormalTok{()}

\FunctionTok{write\_csv}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ toronto\_shelters, }
  \AttributeTok{file =} \StringTok{"toronto\_shelters.csv"}
\NormalTok{  )}

\FunctionTok{head}\NormalTok{(toronto\_shelters)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 32
    `_id` OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME        SHELTER_ID
    <dbl> <date>                   <dbl> <chr>                         <dbl>
1 7272806 2021-01-01                  24 COSTI Immigrant Services         40
2 7272807 2021-01-01                  24 COSTI Immigrant Services         40
3 7272808 2021-01-01                  24 COSTI Immigrant Services         40
4 7272809 2021-01-01                  24 COSTI Immigrant Services         40
5 7272810 2021-01-01                  24 COSTI Immigrant Services         40
6 7272811 2021-01-01                  24 COSTI Immigrant Services         40
# ... with 27 more variables: SHELTER_GROUP <chr>, LOCATION_ID <dbl>,
#   LOCATION_NAME <chr>, LOCATION_ADDRESS <chr>, LOCATION_POSTAL_CODE <chr>,
#   LOCATION_CITY <chr>, LOCATION_PROVINCE <chr>, PROGRAM_ID <dbl>,
#   PROGRAM_NAME <chr>, SECTOR <chr>, PROGRAM_MODEL <chr>,
#   OVERNIGHT_SERVICE_TYPE <chr>, PROGRAM_AREA <chr>, SERVICE_USER_COUNT <dbl>,
#   CAPACITY_TYPE <chr>, CAPACITY_ACTUAL_BED <dbl>, CAPACITY_FUNDING_BED <dbl>,
#   OCCUPIED_BEDS <dbl>, UNOCCUPIED_BEDS <dbl>, UNAVAILABLE_BEDS <dbl>, ...
\end{verbatim}

Not much needs to be done to this to make it similar to the dataset that
we were interested in (Figure~\ref{fig-torontohomelessdataplan}). We
need to change the names to make them easier to type using
\texttt{clean\_names()}, reduce the columns to only those that are
relevant using \texttt{select()}, and only keep the second half of the
year using \texttt{filter()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{toronto\_shelters\_clean }\OtherTok{\textless{}{-}} 
  \FunctionTok{clean\_names}\NormalTok{(toronto\_shelters) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(occupancy\_date, id, occupied\_beds) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(occupancy\_date }\SpecialCharTok{\textgreater{}=} \FunctionTok{as\_date}\NormalTok{(}\StringTok{"2021{-}07{-}01"}\NormalTok{))}

\FunctionTok{head}\NormalTok{(toronto\_shelters\_clean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  occupancy_date      id occupied_beds
  <date>           <dbl>         <dbl>
1 2021-12-27     7323151            50
2 2021-12-27     7323152            18
3 2021-12-27     7323153            28
4 2021-12-27     7323154            50
5 2021-12-27     7323155            NA
6 2021-12-27     7323156            NA
\end{verbatim}

All that remains is to save the cleaned dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ toronto\_shelters\_clean, }
  \AttributeTok{file =} \StringTok{"cleaned\_toronto\_shelters.csv"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore-1}{%
\subsection{Explore}\label{explore-1}}

First, we load the dataset that we just created.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Explore \#\#\#\#}
\NormalTok{toronto\_shelters\_clean }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}
    \StringTok{"cleaned\_toronto\_shelters.csv"}\NormalTok{,}
    \AttributeTok{show\_col\_types =} \ConstantTok{FALSE}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

The dataset contains records on a daily basis for each shelter. We are
interested in understanding average usage for each month. To do this, we
need to and add a month column, which we do using \texttt{month()} from
\texttt{lubridate} (Grolemund and Wickham 2011). By default,
\texttt{month()} provides the number of the month, and so we include two
arguments `label' and `abbr' to get the full name of the month. We
remove rows that do not have any data for the number of beds using
\texttt{drop\_na()} from \texttt{tidyr}. And we then create a summary
statistic on the basis of monthly groups, using \texttt{summarize()}
from \texttt{dplyr} (Wickham et al. 2020). We use \texttt{kable()} from
\texttt{knitr} (Xie 2021) to create a table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Based on code from Florence Vallée{-}Dubois and Lisa Lendway}
\NormalTok{toronto\_shelters\_clean }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{occupancy\_month =} \FunctionTok{month}\NormalTok{(occupancy\_date, }
                                 \AttributeTok{label =} \ConstantTok{TRUE}\NormalTok{, }
                                 \AttributeTok{abbr =} \ConstantTok{FALSE}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{drop\_na}\NormalTok{(occupied\_beds) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# We only want rows that have data}
  \FunctionTok{group\_by}\NormalTok{(occupancy\_month) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# We want to know the occupancy by month}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{number\_occupied =} \FunctionTok{mean}\NormalTok{(occupied\_beds)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lr@{}}
\toprule()
occupancy\_month & number\_occupied \\
\midrule()
\endhead
July & 29.67137 \\
August & 30.83975 \\
September & 31.65405 \\
October & 32.32991 \\
November & 33.26980 \\
December & 33.57806 \\
\bottomrule()
\end{longtable}

As with before, this looks fine, and achieves what we set out to do. But
we can make some tweaks to the defaults to make it look even better
(Table~\ref{tbl-homelessoccupancy}). We can add a caption, make the
column names easier to read, only show an appropriate level of decimal
places, and improve the formatting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{toronto\_shelters\_clean }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{occupancy\_month =} \FunctionTok{month}\NormalTok{(occupancy\_date, }
                                 \AttributeTok{label =} \ConstantTok{TRUE}\NormalTok{, }
                                 \AttributeTok{abbr =} \ConstantTok{FALSE}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{drop\_na}\NormalTok{(occupied\_beds) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# We only want rows that have data}
  \FunctionTok{group\_by}\NormalTok{(occupancy\_month) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# We want to know the occupancy by month}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{number\_occupied =} \FunctionTok{mean}\NormalTok{(occupied\_beds)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Month"}\NormalTok{, }\StringTok{"Average daily number of occupied beds"}\NormalTok{),}
        \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
        \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{linesep =} \StringTok{""}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-homelessoccupancy}{}
\begin{longtable}[]{@{}lr@{}}
\caption{\label{tbl-homelessoccupancy}Homeless shelter usage in Toronto
in 2021}\tabularnewline
\toprule()
Month & Average daily number of occupied beds \\
\midrule()
\endfirsthead
\toprule()
Month & Average daily number of occupied beds \\
\midrule()
\endhead
July & 29.7 \\
August & 30.8 \\
September & 31.7 \\
October & 32.3 \\
November & 33.3 \\
December & 33.6 \\
\bottomrule()
\end{longtable}

\hypertarget{communicate-1}{%
\subsection{Communicate}\label{communicate-1}}

We need to write a few brief paragraphs about what we did, why we did
it, and what we found to sum up our work. An example follows.

\begin{quote}
Toronto has a large homeless population. Freezing winters mean it is
critical there are enough places in shelters. We are interested to
understand how usage of shelters changes in colder months, compared with
warmer months.

We use data provided by the City of Toronto about Toronto homeless
shelter bed occupancy. Specifically, at 4am each night a count is made
of the occupied beds. We are interested in averaging this over the
month. We cleaned, tidied, and analyzed the dataset using the
statistical programming language R (R Core Team 2021) as well as the
packages \texttt{tidyverse} (Wickham 2017), \texttt{janitor} (Firke
2020), \texttt{tidyr} (Wickham 2021c), \texttt{opendatatoronto} (Gelfand
2020), \texttt{lubridate} (Grolemund and Wickham 2011), and
\texttt{knitr} (Xie 2021). We then made a table of the average number of
occupied beds each night for each month
(Table~\ref{tbl-homelessoccupancy}).

We found that the daily average number of occupied beds was higher in
December 2021 than July 2021, with 34 occupied beds in December,
compared with 30 in July (Table~\ref{tbl-homelessoccupancy}). More
generally, there was a steady increase in the daily average number of
occupied beds between July and December, with a slight overall increase
each month.

The dataset is on the basis of shelters, and so our results may be
skewed by changes that are specific to especially large or especially
small shelters. It may be that particular shelters are especially
attractive in colder months. Additionally, we were concerned with counts
of the number of occupied beds, but if the supply of beds changes over
the season, then an additional statistic of interest would be proportion
occupied.
\end{quote}

Although this example is only a few paragraphs, it could be reduced to
form an abstract, or increased to form a full report. The first
paragraph is a general overview, the second focuses on the data, the
third on the results, and the fourth is a discussion. Each of these
could be expanded to form sections of a short report.

\hypertarget{neonatal-mortality}{%
\section{Neonatal mortality}\label{neonatal-mortality}}

Neonatal mortality refers to a death that occurs within the first month
of life, and in particular, the neonatal mortality rate (NMR) is the
number of neonatal deaths per 1,000 live births (UN IGME 2021). Reducing
it is part of the third Sustainable Development Goal (Hug et al. 2019).
In this example we will create a graph of the estimated NMR for the past
fifty years for: Argentina, Australia, Canada, and Kenya.

\hypertarget{plan-2}{%
\subsection{Plan}\label{plan-2}}

For this example, we need to think about what our dataset should look
like, and what the graph should look like.

The dataset needs to have columns that specify the country, and the
year. It also needs to have a column with the NMR estimate for that year
for that country. Roughly, it should look like
Figure~\ref{fig-nmrexampledata}.

\begin{figure}

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{./figures/IMG_1812.png}

}

\caption{\label{fig-nmrexampledata}Quick sketch of a potentially useful
NMR dataset}

\end{figure}

We are interested to make a graph with year on the x-axis and estimated
NMR on the y-axis. Each country should have its own series similar to
Figure~\ref{fig-nmrexamplegraph}

\begin{figure}

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{./figures/IMG_1813.png}

}

\caption{\label{fig-nmrexamplegraph}Quick sketch of a graph of NMR by
country over time}

\end{figure}

\hypertarget{simulate-2}{%
\subsection{Simulate}\label{simulate-2}}

We would like to simulate some data that aligns with our plan. In this
case we will need three columns: country, year, and NMR.

Within R Studio Cloud, make a new Quarto Document and save it. Add
preamble documentation and set-up the workspace. We will use
\texttt{tidyverse} (Wickham 2017), \texttt{janitor} (Firke 2020), and
\texttt{lubridate} (Grolemund and Wickham 2011).

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Preamble \#\#\#\#}
\CommentTok{\# Purpose: Obtain and prepare data about neonatal mortality for four}
\CommentTok{\# countries for the past fifty years and create a graph.}
\CommentTok{\# Author: Rohan Alexander}
\CommentTok{\# Email: rohan.alexander@utoronto.ca}
\CommentTok{\# Date: 1 January 2022}
\CommentTok{\# Prerequisites: {-} }

\DocumentationTok{\#\#\#\# Workspace set{-}up \#\#\#\#}
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(lubridate)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

The code contained in libraries can change from time to time as the
authors update it and release new versions. We can see which version of
a package we are using with \texttt{packageVersion()}. For instance, we
are using version 1.3.1 of the \texttt{tidyverse} and version 2.1.0 of
\texttt{janitor}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{packageVersion}\NormalTok{(}\StringTok{\textquotesingle{}tidyverse\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] '1.3.1'
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{packageVersion}\NormalTok{(}\StringTok{\textquotesingle{}janitor\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] '2.1.0'
\end{verbatim}

To update the version of the package, we use \texttt{update.packages()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{update.packages}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

This does not need to be run, say, every day, but from time-to-time it
is worth updating packages. While many packages take care to ensure
backward compatibility, at a certain point this does not become
reasonable, and so it is important to be aware the updating packages can
result in old code needing to be updated.

Returning to the simulation, we repeat the name of each country 50 times
with \texttt{rep()}, and enable the passing of 50 years. Finally, we
draw from the uniform distribution with \texttt{runif()} to simulate an
estimated NMR value for that year for that country.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Simulate data \#\#\#\#}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{simulated\_nmr\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{country =} 
      \FunctionTok{c}\NormalTok{(}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Argentina\textquotesingle{}}\NormalTok{, }\DecValTok{50}\NormalTok{),}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, }\DecValTok{50}\NormalTok{),}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Canada\textquotesingle{}}\NormalTok{, }\DecValTok{50}\NormalTok{),}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Kenya\textquotesingle{}}\NormalTok{, }\DecValTok{50}\NormalTok{)}
\NormalTok{        ),}
    \AttributeTok{year =} 
      \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1971}\SpecialCharTok{:}\DecValTok{2020}\NormalTok{), }\DecValTok{4}\NormalTok{),}
    \AttributeTok{nmr =} 
      \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =} \DecValTok{200}\NormalTok{,}
            \AttributeTok{min =} \DecValTok{0}\NormalTok{, }
            \AttributeTok{max =} \DecValTok{100}\NormalTok{)}
\NormalTok{  )}

\FunctionTok{head}\NormalTok{(simulated\_nmr\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  country    year   nmr
  <chr>     <int> <dbl>
1 Argentina  1971 35.9 
2 Argentina  1972 12.0 
3 Argentina  1973 48.4 
4 Argentina  1974 31.6 
5 Argentina  1975  3.74
6 Argentina  1976 40.4 
\end{verbatim}

While this simulation works, it would be time-consuming and error-prone
if we decided that instead of fifty years, we were interested in
simulating, say, sixty years. One way to make this easier is to replace
all instances of 50 with a variable. An example follows.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Simulate data \#\#\#\#}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_years }\OtherTok{\textless{}{-}} \DecValTok{50}

\NormalTok{simulated\_nmr\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{country =} 
      \FunctionTok{c}\NormalTok{(}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Argentina\textquotesingle{}}\NormalTok{, number\_of\_years),}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, number\_of\_years),}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Canada\textquotesingle{}}\NormalTok{, number\_of\_years),}
        \FunctionTok{rep}\NormalTok{(}\StringTok{\textquotesingle{}Kenya\textquotesingle{}}\NormalTok{, number\_of\_years)}
\NormalTok{        ),}
    \AttributeTok{year =} 
      \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{number\_of\_years }\SpecialCharTok{+} \DecValTok{1970}\NormalTok{), }\DecValTok{4}\NormalTok{),}
    \AttributeTok{nmr =} 
      \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_years }\SpecialCharTok{*} \DecValTok{4}\NormalTok{,}
            \AttributeTok{min =} \DecValTok{0}\NormalTok{, }
            \AttributeTok{max =} \DecValTok{100}\NormalTok{)}
\NormalTok{  )}

\FunctionTok{head}\NormalTok{(simulated\_nmr\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  country    year   nmr
  <chr>     <dbl> <dbl>
1 Argentina  1971 35.9 
2 Argentina  1972 12.0 
3 Argentina  1973 48.4 
4 Argentina  1974 31.6 
5 Argentina  1975  3.74
6 Argentina  1976 40.4 
\end{verbatim}

The result will be the same, but now if we want to change from fifty to
sixty years is to make the change in one place.

We can have confidence in this simulated dataset because it is
relatively straight-forward, and we wrote the code for it. But when we
turn to the real dataset, it is more difficult to be sure that it is
what it claims to be. Even if we trust the data, it is important that we
can share that confidence with others. One way forward is to establish
some checks that prove our data are as they should be. For instance, we
expect that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  `country' is exclusively one of these four: `Argentina', `Australia',
  `Canada', or `Kenya'.
\item
  Conversely, `country' contains all those four countries.
\item
  `year' is no smaller than 1971 and no larger than 2020, and is an
  integer, not a letter or a number with decimal places.
\item
  `nmr' is a value somewhere between 0 and 1,000, and is a number.
\end{enumerate}

We can write a series of tests based on these features, that we expect
that dataset to pass.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tests for simulated data}
\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{country }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{==} \FunctionTok{c}\NormalTok{(}\StringTok{"Argentina"}\NormalTok{, }
                \StringTok{"Australia"}\NormalTok{, }
                \StringTok{"Canada"}\NormalTok{, }
                \StringTok{"Kenya"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE TRUE TRUE TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{country }\SpecialCharTok{|\textgreater{}} \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \FunctionTok{length}\NormalTok{() }\SpecialCharTok{==} \DecValTok{4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{year }\SpecialCharTok{|\textgreater{}} \FunctionTok{min}\NormalTok{() }\SpecialCharTok{==} \DecValTok{1971}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{year }\SpecialCharTok{|\textgreater{}} \FunctionTok{max}\NormalTok{() }\SpecialCharTok{==} \DecValTok{2020}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{nmr }\SpecialCharTok{|\textgreater{}} \FunctionTok{min}\NormalTok{() }\SpecialCharTok{\textgreater{}=} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{nmr }\SpecialCharTok{|\textgreater{}} \FunctionTok{max}\NormalTok{() }\SpecialCharTok{\textless{}=} \DecValTok{1000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_nmr\_data}\SpecialCharTok{$}\NormalTok{nmr }\SpecialCharTok{|\textgreater{}} \FunctionTok{class}\NormalTok{() }\SpecialCharTok{==} \StringTok{"numeric"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

Having passed these tests, we can have confidence in the simulated
dataset. More importantly, we can apply these tests to the real dataset.
This enables us to have greater confidence in that dataset and to share
that confidence with others.

\hypertarget{acquire-2}{%
\subsection{Acquire}\label{acquire-2}}

The UN Inter-agency Group for Child Mortality Estimation (IGME) provides
estimates of the NMR -- https://childmortality.org/ -- that we can
download and save.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Acquire data \#\#\#\#}
\NormalTok{raw\_igme\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}
    \AttributeTok{file =}
      \StringTok{"https://childmortality.org/wp{-}content/uploads/2021/09/UNIGME{-}2021.csv"}\NormalTok{,}
    \AttributeTok{show\_col\_types =} \ConstantTok{FALSE}\NormalTok{) }

\FunctionTok{write\_csv}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ raw\_igme\_data, }
  \AttributeTok{file =} \StringTok{"igme.csv"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We can take a quick look to get a better sense of it. We might be
interested in what the dataset seems to look like (using \texttt{head()}
and \texttt{tail()}), and what the names of the columns are (using
\texttt{names()}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(raw\_igme\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 29
  `Geographic area` Indicator Sex   `Wealth Quinti~` `Series Name` `Series Year`
  <chr>             <chr>     <chr> <chr>            <chr>         <chr>        
1 Afghanistan       Neonatal~ Total Total            Multiple Ind~ 2003         
2 Afghanistan       Neonatal~ Total Total            Multiple Ind~ 2003         
3 Afghanistan       Neonatal~ Total Total            Multiple Ind~ 2003         
4 Afghanistan       Neonatal~ Total Total            Multiple Ind~ 2003         
5 Afghanistan       Neonatal~ Total Total            Multiple Ind~ 2003         
6 Afghanistan       Neonatal~ Total Total            Afghanistan ~ 2010         
# ... with 23 more variables: `Regional group` <chr>, TIME_PERIOD <chr>,
#   OBS_VALUE <dbl>, COUNTRY_NOTES <chr>, CONNECTION <lgl>,
#   DEATH_CATEGORY <lgl>, CATEGORY <chr>, `Observation Status` <chr>,
#   `Unit of measure` <chr>, `Series Category` <chr>, `Series Type` <chr>,
#   STD_ERR <dbl>, REF_DATE <dbl>, `Age Group of Women` <chr>,
#   `Time Since First Birth` <chr>, DEFINITION <chr>, INTERVAL <dbl>,
#   `Series Method` <chr>, LOWER_BOUND <dbl>, UPPER_BOUND <dbl>, ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(raw\_igme\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "Geographic area"        "Indicator"              "Sex"                   
 [4] "Wealth Quintile"        "Series Name"            "Series Year"           
 [7] "Regional group"         "TIME_PERIOD"            "OBS_VALUE"             
[10] "COUNTRY_NOTES"          "CONNECTION"             "DEATH_CATEGORY"        
[13] "CATEGORY"               "Observation Status"     "Unit of measure"       
[16] "Series Category"        "Series Type"            "STD_ERR"               
[19] "REF_DATE"               "Age Group of Women"     "Time Since First Birth"
[22] "DEFINITION"             "INTERVAL"               "Series Method"         
[25] "LOWER_BOUND"            "UPPER_BOUND"            "STATUS"                
[28] "YEAR_TO_ACHIEVE"        "Model Used"            
\end{verbatim}

We would like to clean up the names and only keep the rows and columns
that we are interested in. Based on our plan, we are interested in rows
where `Sex' is `Total', `Series Name' is `UN IGME estimate', `Geographic
area' is one of `Argentina', `Australia', `Canada', and `Kenya', and the
`Indicator' is `Neonatal mortality rate'. After this we are interested
in just a few columns: `geographic\_area', `time\_period', and
`obs\_value'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_igme\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{clean\_names}\NormalTok{(raw\_igme\_data) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(sex }\SpecialCharTok{==} \StringTok{\textquotesingle{}Total\textquotesingle{}}\NormalTok{,}
\NormalTok{         series\_name }\SpecialCharTok{==} \StringTok{\textquotesingle{}UN IGME estimate\textquotesingle{}}\NormalTok{,}
\NormalTok{         geographic\_area }\SpecialCharTok{\%in\%} 
           \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Argentina\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Canada\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Kenya\textquotesingle{}}\NormalTok{),}
\NormalTok{         indicator }\SpecialCharTok{==} \StringTok{\textquotesingle{}Neonatal mortality rate\textquotesingle{}}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(geographic\_area,}
\NormalTok{         time\_period,}
\NormalTok{         obs\_value)}

\FunctionTok{head}\NormalTok{(cleaned\_igme\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  geographic_area time_period obs_value
  <chr>           <chr>           <dbl>
1 Argentina       1970-06          24.9
2 Argentina       1971-06          24.7
3 Argentina       1972-06          24.6
4 Argentina       1973-06          24.6
5 Argentina       1974-06          24.5
6 Argentina       1975-06          24.1
\end{verbatim}

Finally, we need to fix two final aspects: the class of `time\_period'
is character when we need it to be a year, and the name of `obs\_value'
should be `nmr' to be more informative.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_igme\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  cleaned\_igme\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{time\_period =} \FunctionTok{str\_remove}\NormalTok{(time\_period, }\StringTok{"{-}06"}\NormalTok{),}
         \AttributeTok{time\_period =} \FunctionTok{as.integer}\NormalTok{(time\_period)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(time\_period }\SpecialCharTok{\textgreater{}=} \DecValTok{1971}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{nmr =}\NormalTok{ obs\_value,}
         \AttributeTok{year =}\NormalTok{ time\_period,}
         \AttributeTok{country =}\NormalTok{ geographic\_area)}

\FunctionTok{head}\NormalTok{(cleaned\_igme\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  country    year   nmr
  <chr>     <int> <dbl>
1 Argentina  1971  24.7
2 Argentina  1972  24.6
3 Argentina  1973  24.6
4 Argentina  1974  24.5
5 Argentina  1975  24.1
6 Argentina  1976  23.3
\end{verbatim}

Finally, we can check that our dataset passes the tests that we
developed based on the simulated dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test the cleaned dataset}
\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{country }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{==} \FunctionTok{c}\NormalTok{(}\StringTok{"Argentina"}\NormalTok{, }
                \StringTok{"Australia"}\NormalTok{, }
                \StringTok{"Canada"}\NormalTok{, }
                \StringTok{"Kenya"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE TRUE TRUE TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{country }\SpecialCharTok{|\textgreater{}} \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \FunctionTok{length}\NormalTok{() }\SpecialCharTok{==} \DecValTok{4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{year }\SpecialCharTok{|\textgreater{}} \FunctionTok{min}\NormalTok{() }\SpecialCharTok{==} \DecValTok{1971}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{year }\SpecialCharTok{|\textgreater{}} \FunctionTok{max}\NormalTok{() }\SpecialCharTok{==} \DecValTok{2020}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{nmr }\SpecialCharTok{|\textgreater{}} \FunctionTok{min}\NormalTok{() }\SpecialCharTok{\textgreater{}=} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{nmr }\SpecialCharTok{|\textgreater{}} \FunctionTok{max}\NormalTok{() }\SpecialCharTok{\textless{}=} \DecValTok{1000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_igme\_data}\SpecialCharTok{$}\NormalTok{nmr }\SpecialCharTok{|\textgreater{}} \FunctionTok{class}\NormalTok{() }\SpecialCharTok{==} \StringTok{"numeric"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

All that remains is to save the nicely cleaned dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ cleaned\_igme\_data, }
  \AttributeTok{file =} \StringTok{"cleaned\_igme\_data.csv"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore-2}{%
\subsection{Explore}\label{explore-2}}

We would like to make a graph of estimated NMR using the cleaned
dataset. First, we read in the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\# Explore \#\#\#\#}
\NormalTok{cleaned\_igme\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}
    \AttributeTok{file =} \StringTok{"cleaned\_igme\_data.csv"}\NormalTok{,}
    \AttributeTok{show\_col\_types =} \ConstantTok{FALSE}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

We can now make the graph that we are interested in
(Figure~\ref{fig-nmrgraph}). We are interested in showing how NMR has
changed over time and the differences between countries.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_igme\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ nmr, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Neonatal Mortality Rate (NMR)"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./02-drinking_from_a_fire_hose_files/figure-pdf/fig-nmrgraph-1.pdf}

}

\caption{\label{fig-nmrgraph}Neonatal Mortality Rate (NMR), for
Argentina, Australia, Canada, and Kenya (1971-2020)}

\end{figure}

\hypertarget{communicate-2}{%
\subsection{Communicate}\label{communicate-2}}

To this point we downloaded some data, cleaned it, wrote some tests, and
made a graph. We would typically need to communicate what we have done
at some length. In this case, we will write a few paragraphs about what
we did, why we did it, and what we found.

\begin{quote}
Neonatal mortality refers to a death that occurs within the first month
of life. In particular, the neonatal mortality rate (NMR) is the number
of neonatal deaths per 1,000 live births (M. Alexander and Alkema 2018).
We obtain estimates for NMR for four countries---Argentina, Australia,
Canada, China, and Kenya---over the past fifty years.

The UN Inter-agency Group for Child Mortality Estimation (IGME) provides
estimates of the NMR at the website: https://childmortality.org/. We
downloaded their estimates then cleaned and tidied the dataset using the
statistical programming language R (R Core Team 2021).

We found considerable change in the estimated NMR over time and between
the four countries of interest (Figure~\ref{fig-nmrgraph}). We found
that the 1970s tended to be associated with reductions in the estimated
NMR. Australia and Canada were estimated to have a low NMR at that point
and remained there through 2020, with slight improvements. The estimates
for Argentina and Kenya continued to have substantial reductions through
2020.

Our results suggest considerable improvements in estimated NMR over
time. But it is worth emphasizing that estimates of the NMR are based on
a statistical model and underlying data. The paradox of data
availability is that often high-quality data are less easily available
for countries with worse outcomes. For instance, M. Alexander and Alkema
(2018) say `{[}t{]}here is large variability in the availability of data
on neonatal mortality'. Our conclusions are subject to the model that
underpins the estimates, and the quality of the underlying data and we
did not independently verify either of these.
\end{quote}

\hypertarget{exercises-and-tutorial-1}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-1}}

\hypertarget{exercises-1}{%
\subsection{Exercises}\label{exercises-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Following Barrett (2021a), please write a stack of four or five atomic
  habits, related to learning data science, that you could implement
  this week.
\item
  What is not one of the four challenges for mitigating bias mentioned
  in Hao (2019) (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Unknown unknowns.
  \item
    Imperfect processes.
  \item
    The definitions of fairness.
  \item
    Lack of social context.
  \item
    Disinterest given profit considerations.
  \end{enumerate}
\item
  When was the dataset that underpins Chambliss (1989) collected (pick
  one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    August 1983 to August 1984
  \item
    January 1983 to August 1984
  \item
    January 1983 to January 1984
  \item
    August 1983 to January 1984
  \end{enumerate}
\item
  When Chambliss (1989) talks of stratification, what is he talking
  about?
\item
  How does Chambliss (1989) define `excellence' (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Prolonged performance at world-class level.
  \item
    All Olympic medal winners.
  \item
    Consistent superiority of performance.
  \item
    All national-level athletes.
  \end{enumerate}
\item
  Think about the following quote from Chambliss (1989, 81) and list
  three small skills or activities that could help you achieve
  excellence in data science.
\end{enumerate}

\begin{quote}
Excellence is mundane. Superlative performance is really a confluence of
dozens of small skills or activities, each one learned or stumbled upon,
which have been carefully drilled into habit and then are fitted
together in a synthesized whole. There is nothing extraordinary or
super-human in any one of those actions; only the fact that they are
done consistently and correctly, and all together, produce excellence.
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Which of the following are arguments for \texttt{read\_csv()} from
  \texttt{readr} (Wickham, Hester, and Bryan 2021) (select all that
  apply)? (Hint: You can access the help for the function with
  \texttt{?readr::read\_csv()}.)

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `all\_cols'
  \item
    `file'
  \item
    `show\_col\_types'
  \item
    `number'
  \end{enumerate}
\item
  We used \texttt{rpois()} and \texttt{runif()} to draw from the Poisson
  and Uniform distributions, respectively. Which of the following can be
  used to draw from the Normal and Binomial distributions (select all
  that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{rnormal()} and \texttt{rbinom()}
  \item
    \texttt{rnorm()} and \texttt{rbinomial()}
  \item
    \texttt{rnormal()} and \texttt{rbinomial()}
  \item
    \texttt{rnorm()} and \texttt{rbinom()}
  \end{enumerate}
\item
  What is the result of \texttt{sample(x\ =\ letters,\ size\ =\ 2)} when
  the seed is set to `853'? What about when the seed is set to `1234'
  (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `\,``i'' ``q''\,' and `\,``p'' ``v''\,'
  \item
    `\,``e'' ``l''\,' and `\,``e'' ``r''\,'
  \item
    `\,``i'' ``q''\,' and `\,``e'' ``r''\,'
  \item
    `\,``e'' ``l''\,' and `\,``p'' ``v''\,'
  \end{enumerate}
\item
  Which function provides the recommended citation to cite R (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{cite(\textquotesingle{}R\textquotesingle{})}.
  \item
    \texttt{cite()}.
  \item
    \texttt{citation(\textquotesingle{}R\textquotesingle{})}.
  \item
    \texttt{citation()}.
  \end{enumerate}
\item
  How do we get the citation information for \texttt{opendatatoronto}
  (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    cite()
  \item
    citation()
  \item
    cite(`opendatatoronto')
  \item
    citation(`opendatatoronto')
  \end{enumerate}
\item
  Which argument needs to be changed to change the headings in
  \texttt{kable()} from \texttt{knitr} (Xie 2021) (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `booktabs'
  \item
    `col.names'
  \item
    `digits'
  \item
    `linesep'
  \item
    `caption'
  \end{enumerate}
\item
  Which function is used to update packages (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{update.packages()}
  \item
    \texttt{upgrade.packages()}
  \item
    \texttt{revise.packages()}
  \item
    \texttt{renovate.packages()}
  \end{enumerate}
\item
  What are some features that we might typically expect of a column that
  claimed to be a year (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The class is `character'.
  \item
    There are no negative numbers.
  \item
    There are letters in the column.
  \item
    Each entry has four digits.
  \end{enumerate}
\item
  Please consider the following code, add a small mistake to it, and
  then create a GitHub Gist that contains all of the code, and submit
  the URL.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{midwest }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ poptotal, }\AttributeTok{y =}\NormalTok{ popdensity, }\AttributeTok{color =}\NormalTok{ state)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tutorial-1}{%
\subsection{Tutorial}\label{tutorial-1}}

The purpose of this tutorial is to provide an opportunity to do a small
self-contained project. We will redo the Australian Elections worked
example, but for Canada.

Canada is a parliamentary democracy with 338 seats in the House of
Commons, which is the lower house and that from which government is
formed. There are two major parties -- `Liberal' and `Conservative' --
three minor parties -- `Bloc Québécois', `New Democratic', and `Green'
-- and many smaller parties and independents. In this example we will
create a graph of the number of seats that each party won in the 2019
Federal Election.

Begin by planning what the dataset that we need will look like, and what
the final graph will look like. The basic requirement for the dataset is
that it has the name of the seat (sometimes called a `riding' in Canada)
and the party of the person elected.

Please do a quick sketch of the dataset that we would need. And then do
a quick sketch of a graph that we might be interested in.

Then put together a Quarto Document that simulates some data. Add
preamble documentation, then load the packages that are needed:
\texttt{tidyverse}, \texttt{janitor}, and \texttt{tidyr}. Add numbers
for the riding, then use \texttt{sample()} to randomly choose one of six
options, with replacement, 338 times.

Next we need to get the actual data, from Elections Canada, and the file
that we need to download is:
``https://www.elections.ca/res/rep/off/ovr2019app/51/data\_donnees/table\_tableau11.csv''.

Clean the names, and then select the two columns that are of interest:
`electoral\_district\_name\_nom\_de\_circonscription', and
`elected\_candidate\_candidat\_elu'. Finally, rename the columns to
remove the French and simplify the names.

The column that we need is about the elected canadidates. That has the
surname of the elected candidate, followed by a comma, followed by their
first name, followed by a space, followed by the name of the party in
both English and French, separated by a slash. Break-up this column into
its pieces using \texttt{separate()} from \texttt{tidyr} (Wickham
2021c).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_elections\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  cleaned\_elections\_data }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# Separate the column into two based on the slash}
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ elected\_candidate,}
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}other\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}party\textquotesingle{}}\NormalTok{),}
           \AttributeTok{sep =} \StringTok{\textquotesingle{}/\textquotesingle{}}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \CommentTok{\# Remove the \textquotesingle{}other\textquotesingle{} column}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{other)}
\end{Highlighting}
\end{Shaded}

Then recode the party names from French to English to match what we
simulated.

At this point we can make a nice graph of the number of ridings won by
each party in the 2019 Canadian Federal Election.

\hypertarget{sec-r-essentials}{%
\chapter{R essentials}\label{sec-r-essentials}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{The Kitchen Counter Observatory}, (Healy 2020)
\item
  Read \emph{R for Data Science}, Chapter 5 `Data transformation',
  (Wickham and Grolemund 2017)
\item
  Read \emph{Data Feminism}, Chapter 6 `The Numbers Don't Speak for
  Themselves', (D'Ignazio and Klein 2020)
\item
  Read \emph{R generation}, (Thieme 2018)
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Understanding foundational aspects of R and R Studio.
\item
  Being able to use key \texttt{dplyr} verbs.
\item
  Know fundamentals of class and how to manipulate it.
\item
  Ability to simulate data.
\item
  Ability to make graphs in \texttt{ggplot2}.
\item
  Comfort with other aspects of the \texttt{tidyverse} including
  importing data, dataset manipulation, string manipulation, and
  factors.
\item
  Develop strategies for when things do not work.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{forcats} (Wickham 2020a)
\item
  \texttt{ggplot2} (Wickham 2016)
\item
  \texttt{haven} (Wickham and Miller 2020)
\item
  \texttt{stringr} (Wickham 2019e)
\item
  \texttt{tidyr} (Wickham 2021c)
\item
  \texttt{tidyverse} (Wickham et al. 2019a)
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{\textbar{}} `or'
\item
  \texttt{\&} `and'
\item
  \texttt{\textbar{}\textgreater{}} `pipe'
\item
  \texttt{\$} `extract'
\item
  \texttt{as.character()}
\item
  \texttt{as.integer()}
\item
  \texttt{c()}
\item
  \texttt{citation()}
\item
  \texttt{class()}
\item
  \texttt{dplyr::arrange()}
\item
  \texttt{dplyr::case\_when()}
\item
  \texttt{dplyr::count()}
\item
  \texttt{dplyr::filter()}
\item
  \texttt{dplyr::group\_by()}
\item
  \texttt{dplyr::if\_else()}
\item
  \texttt{dplyr::left\_join()}
\item
  \texttt{dplyr::mutate()}
\item
  \texttt{dplyr::pull()}
\item
  \texttt{dplyr::rename()}
\item
  \texttt{dplyr::select()}
\item
  \texttt{dplyr::slice()}
\item
  \texttt{dplyr::summarise()}
\item
  \texttt{forcats::as\_factor()}
\item
  \texttt{forcats::fct\_relevel()}
\item
  \texttt{function()}
\item
  \texttt{ggplot2::facet\_wrap()}
\item
  \texttt{ggplot2::geom\_density()}
\item
  \texttt{ggplot2::geom\_histogram()}
\item
  \texttt{ggplot2::geom\_point()}
\item
  \texttt{ggplot2::ggplot()}
\item
  \texttt{head()}
\item
  \texttt{janitor::clean\_names()}
\item
  \texttt{library()}
\item
  \texttt{lubridate::ymd()}
\item
  \texttt{max()}
\item
  \texttt{mean()}
\item
  \texttt{print()}
\item
  \texttt{readr::read\_csv()}
\item
  \texttt{rnorm()}
\item
  \texttt{round()}
\item
  \texttt{runif()}
\item
  \texttt{sample()}
\item
  \texttt{set.seed()}
\item
  \texttt{stringr::str\_detect()}
\item
  \texttt{stringr::str\_replace()}
\item
  \texttt{stringr::str\_squish()}
\item
  \texttt{sum()}
\item
  \texttt{tibble::tibble()}
\item
  \texttt{tidyr::pivot\_longer()}
\item
  \texttt{tidyr::pivot\_wider()}
\end{itemize}

\hypertarget{background}{%
\section{Background}\label{background}}

In this chapter we focus on foundational skills needed to use the
statistical programming language R (R Core Team 2021) to tell stories
with data. Some of it may not make sense at first, but these are skills
and approaches that we will often use. You should initially just go
through this chapter quickly, noting aspects that you do not understand.
And then come back to this chapter from time to time as you continue
through the rest of the book. That way you will see how the various bits
fit into context.

R is an open-source language for statistical programming. You can
download R for free from the
\href{https://cran.r-project.org}{Comprehensive R Archive Network}
(CRAN). R Studio is an Integrated Development Environment (IDE) for R
which makes the language easier to use and can be downloaded for free
from \href{https://www.rstudio.com/products/rstudio/}{R Studio}.

The past ten years or so have been characterized by the increased use of
the \texttt{tidyverse}. This is `\ldots an opinionated collection of R
packages designed for data science. All packages share an underlying
design philosophy, grammar, and data structures' (Wickham 2020b). There
are three distinctions to be clear about: the original R language,
typically referred to as `base'; the `tidyverse' which is a coherent
collection of packages that build on top of base, and other packages.

Essentially everything that we can do in the tidyverse, we can also do
in base. But, as the \texttt{tidyverse} was built especially for data
science it is often easier to use the tidyverse, especially when
learning. Additionally, often everything that we can do in the
tidyverse, we can also do with other packages. But, as the
\texttt{tidyverse} is a coherent collection of packages, it is often
easier to use the tidyverse, again, especially when learning. Eventually
there are cases where it makes sense to trade-off the convenience and
coherence of the \texttt{tidyverse} for some features of base or other
packages. Indeed, we will see that at various points later in this book.
For instance, the \texttt{tidyverse} can be slow, and so if one needs to
import thousands of CSVs then it can make sense to switch away from
\texttt{read\_csv()}. The appropriate use of base and non-tidyverse
packages, or even other languages, rather than dogmatic insistence on a
particular solution, is a sign of intellectual maturity.

Central to our use of the statistical programming language R is data,
and most of the data that we use will have humans at the heart of it.
Sometimes, dealing with human-centered data in this way can have a
numbing effect, resulting in over-generalization, and potentially
problematic work. Another sign of intellectual maturity is when it has
the opposite effect, increasing our awareness of our decision-making
processes and their consequences.

\begin{quote}
In practice, I find that far from distancing you from questions of
meaning, quantitative data forces you to confront them. The numbers draw
you in. Working with data like this is an unending exercise in humility,
a constant compulsion to think through what you can and cannot see, and
a standing invitation to understand what the measures really
capture---what they mean, and for whom.

Healy (2020)
\end{quote}

\hypertarget{broader-impacts}{%
\section{Broader impacts}\label{broader-impacts}}

\begin{quote}
``We shouldn't have to think about the societal impact of our work
because it's hard and other people can do it for us'' is a really bad
argument. I stopped doing CV {[}computer vision{]} research because I
saw the impact my work was having. I loved the work but the military
applications and privacy concerns eventually became impossible to
ignore. But basically all facial recognition work would not get
published if we took Broader Impacts sections seriously. There is almost
no upside and enormous downside risk. To be fair though I should have a
lot of humility here. For most of grad school I bought in to the myth
that science is apolitical and research is objectively moral and good no
matter what the subject is.

Joe Redmon, 20 February 2020
\end{quote}

Although the term `data science' is ubiquitous in academia, industry,
and even more generally, it is difficult to define. One deliberately
antagonistic definition of data science is `{[}t{]}he inhumane reduction
of humanity down to what can be counted' (Keyes 2019). While
purposefully controversial, this definition highlights one reason for
the increased demand for data science and quantitative methods over the
past decade---individuals and their behavior are now at the heart of it.
Many of the techniques have been around for many decades, but what makes
them popular now is this human focus.

Unfortunately, even though much of the work may be focused on
individuals, the issues of privacy and consent, and ethical concerns
more broadly, rarely seem front of mind. While there are some
exceptions, in general, even at the same time as claiming that AI,
machine learning, and data science are going to revolutionize society,
consideration of these types of issues appears to have been largely
treated as something that would be nice to have, rather than something
that we may like to think of before we embrace the revolution.

For the most part, these types of issues are not new. In the sciences,
there has been considerable recent ethical consideration around CRISPR
technology and gene editing (Brokowski and Adli 2019), but in an earlier
time similar conversations were had, for instance, about Wernher von
Braun being allowed to building rockets for the US (Neufeld 2002). In
medicine, of course, these concerns have been front-of-mind for some
time (Association and Medicine 1848). Data science seems determined to
have its own Tuskegee-moment rather than think about, and proactively
address with, these issues based on the experiences of other fields.

That said, there is some evidence that data scientists are beginning to
be more concerned about the ethics surrounding the practice. For
instance, NeurIPS, a prestigious machine learning conference, has
required a statement on ethics to accompany all submissions since 2020.

\begin{quote}
In order to provide a balanced perspective, authors are required to
include a statement of the potential broader impact of their work,
including its ethical aspects and future societal consequences. Authors
should take care to discuss both positive and negative outcomes.

NeurIPS 2020 Conference Call For Papers
\end{quote}

The purpose of ethical consideration and concern for the broader impact
of data science is not to prescriptively rule things in or out, but to
provide an opportunity to raise some issues that should be paramount.
The variety of data science applications, the relative youth of the
field, and the speed of change, mean that such considerations are
sometimes knowingly set aside, and this is acceptable to the rest of the
field. This contrasts with fields such as science, medicine,
engineering, and accounting. Possibly those fields are more self-aware
(Figure~\ref{fig-personalprobability}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/probability.png}

}

\caption{\label{fig-personalprobability}\href{https://xkcd.com/881/}{Probability},
from Randall Munroe's XKCD.}

\end{figure}

\hypertarget{r-r-studio-and-r-studio-cloud}{%
\section{R, R Studio, and R Studio
Cloud}\label{r-r-studio-and-r-studio-cloud}}

R and R Studio are complementary, but they are not the same thing. Dr
Liza Bolton, Assistant Professor, Teaching Stream, University of
Toronto, explains their relationship by analogy where R is like the
engine and R Studio is like the car. Although some of us use a car
engine directly, most of us use a car to interact with the engine.

\hypertarget{r}{%
\subsection{R}\label{r}}

\href{https://www.r-project.org/}{R} is an open-source and free
programming language that is focused on general statistics. Free in this
context does not refer to a price of zero, but instead to the freedom
that the creators give users to largely do what they want with it
(although it also does have a price of zero). This is in contrast with
an open-source programming language that is designed for general
purpose, such as Python, or an open-source programming language that is
focused on probability, such as Stan. It was created by Ross Ihaka and
Robert Gentleman at the University of Auckland in the 1990s, and traces
its provenance to S, which was developed at Bell Labs in the 1970s. It
is maintained by the R Core Team and changes to this `base' of code
occur methodically and with concern given to a variety of different
priorities.

Many people build on this stable base, to extend the capabilities of R
to better and more quickly suit their needs. They do this by creating
packages. Typically, although not always, a package is a collection of R
code, mostly functions, and this allows us to more easily do things that
we want to do. These packages are managed by repositories such as CRAN
and Bioconductor.

If you want to use a package then you first need to install it on your
computer, and then you need to load it when you want to use it. Dr.~Di
Cook, Professor of Business Analytics at Monash University, describes
this as analogous to a lightbulb. If you want light in your house, first
you need to fit a lightbulb, and then you need to turn the switch on.
Installing a package, say, \texttt{install.packages("tidyverse")}, is
akin to fitting a lightbulb into a socket---you only need to do this
once for each lightbulb. But then each time you want light you need to
turn on the switch to the lightbulb, which in the R packages case, means
calling the library, say, \texttt{library(tidyverse)}.

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Dr Di Cook is Professor of Business Analytics at Monash University.
After taking a PhD in statistics from Rutgers University in 1993 where
she focused on statistical graphics, she was appointed as an assistant
professor at Iowa State University, being promoted to full professor in
2005, and in 2015 she moved to Monash. One area of her research is data
visualisation, especially interactive and dynamic graphics. One
particularly important paper is Buja, Cook, and Swayne (1996) which
proposes a taxonomy of interactive data visualization and associated
software XGobi.
\end{tcolorbox}

To install a package on your computer (again, we will need to do this
only once per computer) we use \texttt{install.packages()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And then when we want to use the package, we use \texttt{library()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

Having downloaded it, we can open R and use it directly. It is primarily
designed to be interacted with through the command line. While this is
functional, it can be useful to have a richer environment than the
command line provides. In particular, it can be useful to install an
Integrated Development Environment (IDE), which is an application that
brings together various bits and pieces that will be used often. One
common IDE for R is R Studio, although others such as Visual Studio are
also used.

\hypertarget{r-studio}{%
\subsection{R Studio}\label{r-studio}}

R Studio is distinct to R, and they are different entities. R Studio
builds on top of R to make it easier to use R. This is in the same way
that one could use the internet from the command line, but most folks
use a browser such as Chrome, Firefox, or Safari.

R Studio is free in the sense that we do not pay for it. It is also free
in the sense of being able to take the code, modify it, and distribute
that code. But it is important to recognize that R Studio is a company
and so it is possible that the current situation could change. It can be
downloaded from \href{https://www.rstudio.com/products/rstudio/}{R
Studio}.

When we open R Studio it will look like Figure~\ref{fig-first}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/01.png}

}

\caption{\label{fig-first}Opening R Studio for the first time}

\end{figure}

The left pane is a console in which you can type and execute R code line
by line. Try it with 2+2 by clicking next to the prompt
`\textgreater{}', typing `2+2', and then pressing `return/enter'.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2} \SpecialCharTok{+} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4
\end{verbatim}

The pane on the top right has information about your environment. For
instance, when we create variables a list of their names and some
properties will appear there. Try to type the following code, replacing
my name with your name, next to the prompt, and again press enter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_name }\OtherTok{\textless{}{-}} \StringTok{"Rohan"}
\end{Highlighting}
\end{Shaded}

You should notice a new value in the environment pane with the variable
name and its value.

The pane in the bottom right is a file manager. At the moment it should
just have two files: an R History file and a R Project file. We will get
to what these are later, but for now we will create and save a file.

Run the following code, without worrying too much about the details for
now. And you should see a new `.rds' file in your list of files.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{saveRDS}\NormalTok{(}\AttributeTok{object =}\NormalTok{ my\_name, }\AttributeTok{file =} \StringTok{"my\_first\_file.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{r-studio-cloud}{%
\subsection{R Studio Cloud}\label{r-studio-cloud}}

While you can and should download R Studio to your own computer,
initially we will use \href{https://rstudio.cloud/}{R Studio Cloud}.
This is an online version that is provided by R Studio. We will use this
so that you can focus on getting comfortable with R and R Studio in an
environment that is consistent. This way you do not have to worry about
what computer you have or installation permissions, amongst other
things.

The free version of R Studio Cloud is free as is `no financial cost'.
The trade-off is that it is not very powerful, and it is sometimes slow,
but for the purposes of getting started it is enough.

\hypertarget{getting-started}{%
\section{Getting started}\label{getting-started}}

We will now start going through some code. It is important to actively
write this all out yourself.

While working line-by-line in the console is fine, it is easier to write
out a whole script that can then be run. We will do this by making an R
Script (`File' -\textgreater{} `New File' -\textgreater{} `R Script').
The console pane will fall to the bottom left and an R Script will open
in the top left. We will write some code that will get all of the
Australian federal politicians and then construct a small table about
the genders of the prime ministers. Some of this code will not make
sense at this stage, but just type it all out to get in the habit and
then run it. To run the whole script, we can click `Run' or we can
highlight certain lines and then click `Run' to just run those lines.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install the packages that we need}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"AustralianPoliticians"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the packages that we need to use this time}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(AustralianPoliticians)}

\CommentTok{\# Make a table of the counts of genders of the prime ministers}
\NormalTok{AustralianPoliticians}\SpecialCharTok{::}\FunctionTok{get\_auspol}\NormalTok{(}\StringTok{\textquotesingle{}all\textquotesingle{}}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Imports data from GitHub}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(wasPrimeMinister }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 2
  gender     n
  <chr>  <int>
1 female     1
2 male      29
\end{verbatim}

We can see that, as at the end of 2021, one female has been prime
minister (Julia Gillard), while the other 29 prime ministers were male.

One critical operator when programming is the `pipe':
\texttt{\textbar{}\textgreater{}}. We read this as `and then'. This
takes the output of a line of code and uses it as the first input to the
next line of code. It makes code easier to read.

The idea of the pipe is that we take a dataset, and then do something to
it. We used this in the earlier example. Another example follows where
we will look at the first six lines of a dataset by piping it to
\texttt{head()}. Notice that \texttt{head()} does not explicitly take
any arguments in this example. It knows which data to display because
the pipe does it implicitly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AustralianPoliticians}\SpecialCharTok{::}\FunctionTok{get\_auspol}\NormalTok{(}\StringTok{\textquotesingle{}all\textquotesingle{}}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Imports data from GitHub}
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 20
  uniqueID   surname allOtherNames          firstName commonName displayName    
  <chr>      <chr>   <chr>                  <chr>     <chr>      <chr>          
1 Abbott1859 Abbott  Richard Hartley Smith  Richard   <NA>       Abbott, Richard
2 Abbott1869 Abbott  Percy Phipps           Percy     <NA>       Abbott, Percy  
3 Abbott1877 Abbott  Macartney              Macartney Mac        Abbott, Mac    
4 Abbott1886 Abbott  Charles Lydiard Aubrey Charles   Aubrey     Abbott, Aubrey 
5 Abbott1891 Abbott  Joseph Palmer          Joseph    <NA>       Abbott, Joseph 
6 Abbott1957 Abbott  Anthony John           Anthony   Tony       Abbott, Tony   
# ... with 14 more variables: earlierOrLaterNames <chr>, title <chr>,
#   gender <chr>, birthDate <date>, birthYear <dbl>, birthPlace <chr>,
#   deathDate <date>, member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,
#   wikidataID <chr>, wikipedia <chr>, adb <chr>, comments <chr>
\end{verbatim}

We can save this R Script as `my\_first\_r\_script.R' (`File'
-\textgreater{} `Save As'). At this point, our workspace should look
something like Figure~\ref{fig-third}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/03.png}

}

\caption{\label{fig-third}After running an R Script}

\end{figure}

One thing to be aware of is that each R Studio Cloud workspace is
essentially a new computer. Because of this, we need to install any
package that we want to use for each workspace. For instance, before we
can use the \texttt{tidyverse}, we need to install it with
\texttt{install.packages("tidyverse")}. This contrasts with using one's
own computer.

A few final notes on R Studio Cloud:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In the Australian politician's example, we got our data from the
  website GitHub using an R package, but we can get data into a
  workspace from a local computer in a variety of ways. One way is to
  use the `upload' button in the `Files' panel.
\item
  R Studio Cloud allows some degree of collaboration. For instance, you
  can give someone else access to a workspace that you create. This
  could be useful for collaborating on an assignment, although it is not
  quite full featured yet and you cannot both be in the workspace at the
  same time, in contrast to Google Docs, for example.
\item
  There are a variety of weaknesses of R Studio Cloud, in particular the
  RAM limits. Additionally, like any web application, things break from
  time to time or go down.
\end{enumerate}

\hypertarget{the-dplyr-verbs}{%
\section{\texorpdfstring{The \texttt{dplyr}
verbs}{The dplyr verbs}}\label{the-dplyr-verbs}}

One of the key packages that we will use is the \texttt{tidyverse}
(Wickham et al. 2019b). The \texttt{tidyverse} is actually a package of
packages, which means when we install the \texttt{tidyverse}, we
actually install a whole bunch of different packages. The key package in
the \texttt{tidyverse} in terms of manipulating data is \texttt{dplyr}
(Wickham et al. 2020).

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Hadley Wickham is Chief Scientist at RStudio. After taking a PhD in
Statistics from Iowa State University in 2008 he was appointed as an
assistant professor at Rice University, and then became Chief Scientist
at RStudio in 2013. He developed the \texttt{tidyverse} collection of
packages (Wickham et al. 2019a), and has published many books including
Wickham and Grolemund (2017) and Wickham (2019a). He was awarded the
COPSS Presidents' Award in 2019.
\end{tcolorbox}

There are five \texttt{dplyr} functions that are regularly used, and we
will now go through each of these. These are commonly referred to as the
\texttt{dplyr} verbs.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{select()}
\item
  \texttt{filter()}
\item
  \texttt{arrange()}
\item
  \texttt{mutate()}
\item
  \texttt{summarise()} or equally \texttt{summarize()}
\end{enumerate}

We will also cover \texttt{group\_by()}, and \texttt{count()} here as
they are closely related.

As we have already installed the \texttt{tidyverse}, we just need to
load it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

And we will begin by again using some data about Australian politicians
from the \texttt{AustralianPoliticians} package (R. Alexander and
Hodgetts 2021).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AustralianPoliticians)}

\NormalTok{australian\_politicians }\OtherTok{\textless{}{-}} 
  \FunctionTok{get\_auspol}\NormalTok{(}\StringTok{\textquotesingle{}all\textquotesingle{}}\NormalTok{)}

\FunctionTok{head}\NormalTok{(australian\_politicians)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 20
  uniqueID   surname allOtherNames          firstName commonName displayName    
  <chr>      <chr>   <chr>                  <chr>     <chr>      <chr>          
1 Abbott1859 Abbott  Richard Hartley Smith  Richard   <NA>       Abbott, Richard
2 Abbott1869 Abbott  Percy Phipps           Percy     <NA>       Abbott, Percy  
3 Abbott1877 Abbott  Macartney              Macartney Mac        Abbott, Mac    
4 Abbott1886 Abbott  Charles Lydiard Aubrey Charles   Aubrey     Abbott, Aubrey 
5 Abbott1891 Abbott  Joseph Palmer          Joseph    <NA>       Abbott, Joseph 
6 Abbott1957 Abbott  Anthony John           Anthony   Tony       Abbott, Tony   
# ... with 14 more variables: earlierOrLaterNames <chr>, title <chr>,
#   gender <chr>, birthDate <date>, birthYear <dbl>, birthPlace <chr>,
#   deathDate <date>, member <dbl>, senator <dbl>, wasPrimeMinister <dbl>,
#   wikidataID <chr>, wikipedia <chr>, adb <chr>, comments <chr>
\end{verbatim}

\hypertarget{select}{%
\subsection{\texorpdfstring{\texttt{select()}}{select()}}\label{select}}

We use \texttt{select()} to pick particular columns of a dataset. For
instance, we might like to select the `firstName' column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(firstName) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 1
  firstName
  <chr>    
1 Richard  
2 Percy    
3 Macartney
4 Charles  
5 Joseph   
6 Anthony  
\end{verbatim}

In R, there are many ways to do things. Sometimes these are different
ways to do the same thing, and other times they are different ways to do
\emph{almost} the same thing. For instance, another way to pick a
particular column of a dataset is to use the `extract' operator
\texttt{\$}. This is from base, as opposed to \texttt{select()} which is
from the \texttt{tidyverse}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians}\SpecialCharTok{$}\NormalTok{firstName }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Richard"   "Percy"     "Macartney" "Charles"   "Joseph"    "Anthony"  
\end{verbatim}

The two appear similar---both pick the `firstName' column---but they
differ in the class of what they return. For the sake of completeness,
if we combine \texttt{select()} with \texttt{pull()} then we get the
same class of output as if we had used the extract operator.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(firstName) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pull}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Richard"   "Percy"     "Macartney" "Charles"   "Joseph"    "Anthony"  
\end{verbatim}

We can also use \texttt{select()} to remove columns, by negating the
column name.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{firstName) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 19
  uniqueID   surname allOtherNames commonName displayName earlierOrLaterN~ title
  <chr>      <chr>   <chr>         <chr>      <chr>       <chr>            <chr>
1 Abbott1859 Abbott  Richard Hart~ <NA>       Abbott, Ri~ <NA>             <NA> 
2 Abbott1869 Abbott  Percy Phipps  <NA>       Abbott, Pe~ <NA>             <NA> 
3 Abbott1877 Abbott  Macartney     Mac        Abbott, Mac <NA>             <NA> 
4 Abbott1886 Abbott  Charles Lydi~ Aubrey     Abbott, Au~ <NA>             <NA> 
5 Abbott1891 Abbott  Joseph Palmer <NA>       Abbott, Jo~ <NA>             <NA> 
6 Abbott1957 Abbott  Anthony John  Tony       Abbott, To~ <NA>             <NA> 
# ... with 12 more variables: gender <chr>, birthDate <date>, birthYear <dbl>,
#   birthPlace <chr>, deathDate <date>, member <dbl>, senator <dbl>,
#   wasPrimeMinister <dbl>, wikidataID <chr>, wikipedia <chr>, adb <chr>,
#   comments <chr>
\end{verbatim}

Finally, we can \texttt{select()} based on conditions. For instance, we
can \texttt{select()} all of the columns that start with, say, `birth'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"birth"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  birthDate  birthYear birthPlace  
  <date>         <dbl> <chr>       
1 NA              1859 Bendigo     
2 1869-05-14        NA Hobart      
3 1877-07-03        NA Murrurundi  
4 1886-01-04        NA St Leonards 
5 1891-10-18        NA North Sydney
6 1957-11-04        NA London      
\end{verbatim}

There are a variety of similar `selection helpers' including
\texttt{starts\_with()}, \texttt{ends\_with()}, and \texttt{contains()}.
More information about these is available in the help page for
\texttt{select()} which can be accessed by running \texttt{?select()}.

At this point, we will use \texttt{select()} to reduce the width of our
dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\OtherTok{\textless{}{-}}
\NormalTok{  australian\_politicians }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(uniqueID,}
\NormalTok{         surname,}
\NormalTok{         firstName,}
\NormalTok{         gender,}
\NormalTok{         birthDate,}
\NormalTok{         birthYear,}
\NormalTok{         deathDate,}
\NormalTok{         member,}
\NormalTok{         senator,}
\NormalTok{         wasPrimeMinister)}

\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 10
  uniqueID   surname firstName gender birthDate  birthYear deathDate  member
  <chr>      <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>
1 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0
2 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1
3 Abbott1877 Abbott  Macartney male   1877-07-03        NA 1960-12-30      0
4 Abbott1886 Abbott  Charles   male   1886-01-04        NA 1975-04-30      1
5 Abbott1891 Abbott  Joseph    male   1891-10-18        NA 1965-05-07      1
6 Abbott1957 Abbott  Anthony   male   1957-11-04        NA NA              1
# ... with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>
\end{verbatim}

\hypertarget{filter}{%
\subsection{\texorpdfstring{\texttt{filter()}}{filter()}}\label{filter}}

We use \texttt{filter()} to pick particular rows of a dataset. For
instance, we might be only interested in politicians that became prime
minister.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(wasPrimeMinister }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 30 x 10
   uniqueID    surname firstName gender birthDate  birthYear deathDate  member
   <chr>       <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>
 1 Abbott1957  Abbott  Anthony   male   1957-11-04        NA NA              1
 2 Barton1849  Barton  Edmund    male   1849-01-18        NA 1920-01-07      1
 3 Bruce1883   Bruce   Stanley   male   1883-04-15        NA 1967-08-25      1
 4 Chifley1885 Chifley Joseph    male   1885-09-22        NA 1951-06-13      1
 5 Cook1860    Cook    Joseph    male   1860-12-07        NA 1947-07-30      1
 6 Curtin1885  Curtin  John      male   1885-01-08        NA 1945-07-05      1
 7 Deakin1856  Deakin  Alfred    male   1856-08-03        NA 1919-10-07      1
 8 Fadden1894  Fadden  Arthur    male   1894-04-13        NA 1973-04-21      1
 9 Fisher1862  Fisher  Andrew    male   1862-08-29        NA 1928-10-22      1
10 Forde1890   Forde   Francis   male   1890-07-18        NA 1983-01-28      1
# ... with 20 more rows, and 2 more variables: senator <dbl>,
#   wasPrimeMinister <dbl>
\end{verbatim}

We could also give \texttt{filter()} two conditions. For instance, we
could look at politicians that become prime minister and were named
Joseph, using the `and' operator \texttt{\&}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(wasPrimeMinister }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ firstName }\SpecialCharTok{==} \StringTok{"Joseph"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 10
  uniqueID    surname firstName gender birthDate  birthYear deathDate  member
  <chr>       <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>
1 Chifley1885 Chifley Joseph    male   1885-09-22        NA 1951-06-13      1
2 Cook1860    Cook    Joseph    male   1860-12-07        NA 1947-07-30      1
3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA 1939-04-07      1
# ... with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>
\end{verbatim}

We get the same result if we use a comma instead of an ampersand.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(wasPrimeMinister }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, firstName }\SpecialCharTok{==} \StringTok{"Joseph"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 10
  uniqueID    surname firstName gender birthDate  birthYear deathDate  member
  <chr>       <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>
1 Chifley1885 Chifley Joseph    male   1885-09-22        NA 1951-06-13      1
2 Cook1860    Cook    Joseph    male   1860-12-07        NA 1947-07-30      1
3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA 1939-04-07      1
# ... with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>
\end{verbatim}

Similarly, we could look at politicians who were named, say, Myles or
Ruth using the `or' operator \texttt{\textbar{}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(firstName }\SpecialCharTok{==} \StringTok{"Myles"} \SpecialCharTok{|}\NormalTok{ firstName }\SpecialCharTok{==} \StringTok{"Ruth"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 10
  uniqueID     surname  firstName gender birthDate  birthYear deathDate  member
  <chr>        <chr>    <chr>     <chr>  <date>         <dbl> <date>      <dbl>
1 Coleman1931  Coleman  Ruth      female 1931-09-27        NA 2008-03-27      0
2 Ferricks1875 Ferricks Myles     male   1875-11-12        NA 1932-08-20      0
3 Webber1965   Webber   Ruth      female 1965-03-24        NA NA              0
# ... with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>
\end{verbatim}

We could also pipe the result. For instance we could pipe from
\texttt{filter()} to \texttt{select()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(firstName }\SpecialCharTok{==} \StringTok{"Ruth"} \SpecialCharTok{|}\NormalTok{ firstName }\SpecialCharTok{==} \StringTok{"Myles"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(firstName, surname)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 2
  firstName surname 
  <chr>     <chr>   
1 Ruth      Coleman 
2 Myles     Ferricks
3 Ruth      Webber  
\end{verbatim}

If we happen to know the particular row number that is of interest then
we could \texttt{filter()} to only that particular row. For instance,
say the row 853 was of interest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{row\_number}\NormalTok{() }\SpecialCharTok{==} \DecValTok{853}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 10
  uniqueID     surname  firstName gender birthDate  birthYear deathDate member
  <chr>        <chr>    <chr>     <chr>  <date>         <dbl> <date>     <dbl>
1 Jakobsen1947 Jakobsen Carolyn   female 1947-09-11        NA NA             1
# ... with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>
\end{verbatim}

There is also a dedicated function to do this, which is
\texttt{slice()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 10
  uniqueID     surname  firstName gender birthDate  birthYear deathDate member
  <chr>        <chr>    <chr>     <chr>  <date>         <dbl> <date>     <dbl>
1 Jakobsen1947 Jakobsen Carolyn   female 1947-09-11        NA NA             1
# ... with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>
\end{verbatim}

While this may seem somewhat esoteric, it is especially useful if we
would like to remove a particular row using negation, or duplicate
specific rows. For instance, we could remove the first row.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,782 x 10
   uniqueID    surname firstName gender birthDate  birthYear deathDate  member
   <chr>       <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>
 1 Abbott1869  Abbott  Percy     male   1869-05-14        NA 1940-09-09      1
 2 Abbott1877  Abbott  Macartney male   1877-07-03        NA 1960-12-30      0
 3 Abbott1886  Abbott  Charles   male   1886-01-04        NA 1975-04-30      1
 4 Abbott1891  Abbott  Joseph    male   1891-10-18        NA 1965-05-07      1
 5 Abbott1957  Abbott  Anthony   male   1957-11-04        NA NA              1
 6 Abel1939    Abel    John      male   1939-06-25        NA NA              1
 7 Abetz1958   Abetz   Eric      male   1958-01-25        NA NA              0
 8 Adams1943   Adams   Judith    female 1943-04-11        NA 2012-03-31      0
 9 Adams1951   Adams   Dick      male   1951-04-29        NA NA              1
10 Adamson1857 Adamson John      male   1857-02-18        NA 1922-05-02      0
# ... with 1,772 more rows, and 2 more variables: senator <dbl>,
#   wasPrimeMinister <dbl>
\end{verbatim}

We could also only, say, only keep the first three rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 10
  uniqueID   surname firstName gender birthDate  birthYear deathDate  member
  <chr>      <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>
1 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0
2 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1
3 Abbott1877 Abbott  Macartney male   1877-07-03        NA 1960-12-30      0
# ... with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>
\end{verbatim}

Finally, we could duplicate the first two rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\FunctionTok{n}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,785 x 10
   uniqueID   surname firstName gender birthDate  birthYear deathDate  member
   <chr>      <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>
 1 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0
 2 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1
 3 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0
 4 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1
 5 Abbott1877 Abbott  Macartney male   1877-07-03        NA 1960-12-30      0
 6 Abbott1886 Abbott  Charles   male   1886-01-04        NA 1975-04-30      1
 7 Abbott1891 Abbott  Joseph    male   1891-10-18        NA 1965-05-07      1
 8 Abbott1957 Abbott  Anthony   male   1957-11-04        NA NA              1
 9 Abel1939   Abel    John      male   1939-06-25        NA NA              1
10 Abetz1958  Abetz   Eric      male   1958-01-25        NA NA              0
# ... with 1,775 more rows, and 2 more variables: senator <dbl>,
#   wasPrimeMinister <dbl>
\end{verbatim}

\hypertarget{arrange}{%
\subsection{\texorpdfstring{\texttt{arrange()}}{arrange()}}\label{arrange}}

We use \texttt{arrange()} to change the order of the dataset based on
the values of particular columns. For instance, we could arrange the
politicians by their birthday.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(birthDate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,783 x 10
   uniqueID     surname  firstName gender birthDate  birthYear deathDate  member
   <chr>        <chr>    <chr>     <chr>  <date>         <dbl> <date>      <dbl>
 1 Braddon1829  Braddon  Edward    male   1829-06-11        NA 1904-02-02      1
 2 Ferguson1830 Ferguson John      male   1830-03-15        NA 1906-03-30      0
 3 Zeal1830     Zeal     William   male   1830-12-05        NA 1912-03-11      0
 4 Fraser1832   Fraser   Simon     male   1832-08-21        NA 1919-07-30      0
 5 Groom1833    Groom    William   male   1833-03-09        NA 1901-08-08      1
 6 Sargood1834  Sargood  Frederick male   1834-05-30        NA 1903-01-02      0
 7 Fysh1835     Fysh     Philip    male   1835-03-01        NA 1919-12-20      1
 8 Playford1837 Playford Thomas    male   1837-11-26        NA 1915-04-19      0
 9 Solomon1839  Solomon  Elias     male   1839-09-02        NA 1909-05-23      1
10 McLean1840   McLean   Allan     male   1840-02-03        NA 1911-07-13      1
# ... with 1,773 more rows, and 2 more variables: senator <dbl>,
#   wasPrimeMinister <dbl>
\end{verbatim}

We could modify \texttt{arrange()} with \texttt{desc()} to change from
ascending to descending order.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(birthDate))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,783 x 10
   uniqueID      surname firstName gender birthDate  birthYear deathDate member
   <chr>         <chr>   <chr>     <chr>  <date>         <dbl> <date>     <dbl>
 1 SteeleJohn19~ Steele~ Jordon    male   1994-10-14        NA NA             0
 2 Chandler1990  Chandl~ Claire    female 1990-06-01        NA NA             0
 3 Roy1990       Roy     Wyatt     male   1990-05-22        NA NA             1
 4 Thompson1988  Thomps~ Phillip   male   1988-05-07        NA NA             1
 5 Paterson1987  Paters~ James     male   1987-11-21        NA NA             0
 6 Burns1987     Burns   Joshua    male   1987-02-06        NA NA             1
 7 Smith1986     Smith   Marielle  female 1986-12-30        NA NA             0
 8 KakoschkeMoo~ Kakosc~ Skye      female 1985-12-19        NA NA             0
 9 Simmonds1985  Simmon~ Julian    male   1985-08-29        NA NA             1
10 Gorman1984    Gorman  Patrick   male   1984-12-12        NA NA             1
# ... with 1,773 more rows, and 2 more variables: senator <dbl>,
#   wasPrimeMinister <dbl>
\end{verbatim}

And we could arrange based on more than one column. For instance, if two
politicians have the same first name, then we could also arrange based
on their birthday.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(firstName, birthDate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,783 x 10
   uniqueID      surname firstName gender birthDate  birthYear deathDate  member
   <chr>         <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>
 1 Blain1894     Blain   Adair     male   1894-11-21        NA 1983-04-28      1
 2 Dein1889      Dein    Adam      male   1889-03-04        NA 1969-05-09      1
 3 Armstrong1909 Armstr~ Adam      male   1909-07-01        NA 1982-02-22      1
 4 Bandt1972     Bandt   Adam      male   1972-03-11        NA NA              1
 5 Ridgeway1962  Ridgew~ Aden      male   1962-09-18        NA NA              0
 6 Bennett1933   Bennett Adrian    male   1933-01-21        NA 2006-05-09      1
 7 Gibson1935    Gibson  Adrian    male   1935-11-03        NA 2015-04-30      1
 8 Wynne1850     Wynne   Agar      male   1850-01-15        NA 1934-05-12      1
 9 Robertson1882 Robert~ Agnes     female 1882-07-31        NA 1968-01-29      0
10 Pittard1902   Pittard Alan      male   1902-11-15        NA 1992-12-25      1
# ... with 1,773 more rows, and 2 more variables: senator <dbl>,
#   wasPrimeMinister <dbl>
\end{verbatim}

We could achieve the same result by piping between two instances of
\texttt{arrange()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(birthDate) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(firstName)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,783 x 10
   uniqueID      surname firstName gender birthDate  birthYear deathDate  member
   <chr>         <chr>   <chr>     <chr>  <date>         <dbl> <date>      <dbl>
 1 Blain1894     Blain   Adair     male   1894-11-21        NA 1983-04-28      1
 2 Dein1889      Dein    Adam      male   1889-03-04        NA 1969-05-09      1
 3 Armstrong1909 Armstr~ Adam      male   1909-07-01        NA 1982-02-22      1
 4 Bandt1972     Bandt   Adam      male   1972-03-11        NA NA              1
 5 Ridgeway1962  Ridgew~ Aden      male   1962-09-18        NA NA              0
 6 Bennett1933   Bennett Adrian    male   1933-01-21        NA 2006-05-09      1
 7 Gibson1935    Gibson  Adrian    male   1935-11-03        NA 2015-04-30      1
 8 Wynne1850     Wynne   Agar      male   1850-01-15        NA 1934-05-12      1
 9 Robertson1882 Robert~ Agnes     female 1882-07-31        NA 1968-01-29      0
10 Pittard1902   Pittard Alan      male   1902-11-15        NA 1992-12-25      1
# ... with 1,773 more rows, and 2 more variables: senator <dbl>,
#   wasPrimeMinister <dbl>
\end{verbatim}

When we use \texttt{arrange()} it is important to be clear about
precedence. For instance, changing to birthday and then first name would
give a different arrangement.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(birthYear, firstName)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,783 x 10
   uniqueID    surname firstName gender birthDate birthYear deathDate  member
   <chr>       <chr>   <chr>     <chr>  <date>        <dbl> <date>      <dbl>
 1 Edwards1842 Edwards Richard   male   NA             1842 1915-10-29      1
 2 Sawers1844  Sawers  William   male   NA             1844 1916-05-19      1
 3 Barker1846  Barker  Stephen   male   NA             1846 1924-06-21      0
 4 Corser1852  Corser  Edward    male   NA             1852 1928-07-31      1
 5 Lee1856     Lee     Henry     male   NA             1856 1927-08-12      1
 6 Grant1857   Grant   John      male   NA             1857 1928-05-19      0
 7 Palmer1859  Palmer  Albert    male   NA             1859 1919-08-14      1
 8 Riley1859   Riley   Edward    male   NA             1859 1943-07-21      1
 9 Abbott1859  Abbott  Richard   male   NA             1859 1940-02-28      0
10 Kennedy1860 Kennedy Thomas    male   NA             1860 1929-02-16      1
# ... with 1,773 more rows, and 2 more variables: senator <dbl>,
#   wasPrimeMinister <dbl>
\end{verbatim}

A nice way to arrange by a variety of columns is to use
\texttt{across()}. It enables us to use the `selection helpers' such as
\texttt{starts\_with()} that were mentioned in association with
\texttt{select()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(firstName, birthYear))) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 10
  uniqueID      surname  firstName gender birthDate  birthYear deathDate  member
  <chr>         <chr>    <chr>     <chr>  <date>         <dbl> <date>      <dbl>
1 Blain1894     Blain    Adair     male   1894-11-21        NA 1983-04-28      1
2 Armstrong1909 Armstro~ Adam      male   1909-07-01        NA 1982-02-22      1
3 Bandt1972     Bandt    Adam      male   1972-03-11        NA NA              1
4 Dein1889      Dein     Adam      male   1889-03-04        NA 1969-05-09      1
5 Ridgeway1962  Ridgeway Aden      male   1962-09-18        NA NA              0
6 Bennett1933   Bennett  Adrian    male   1933-01-21        NA 2006-05-09      1
# ... with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{\textquotesingle{}birth\textquotesingle{}}\NormalTok{))) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 10
  uniqueID     surname  firstName gender birthDate  birthYear deathDate  member
  <chr>        <chr>    <chr>     <chr>  <date>         <dbl> <date>      <dbl>
1 Braddon1829  Braddon  Edward    male   1829-06-11        NA 1904-02-02      1
2 Ferguson1830 Ferguson John      male   1830-03-15        NA 1906-03-30      0
3 Zeal1830     Zeal     William   male   1830-12-05        NA 1912-03-11      0
4 Fraser1832   Fraser   Simon     male   1832-08-21        NA 1919-07-30      0
5 Groom1833    Groom    William   male   1833-03-09        NA 1901-08-08      1
6 Sargood1834  Sargood  Frederick male   1834-05-30        NA 1903-01-02      0
# ... with 2 more variables: senator <dbl>, wasPrimeMinister <dbl>
\end{verbatim}

\hypertarget{mutate}{%
\subsection{\texorpdfstring{\texttt{mutate()}}{mutate()}}\label{mutate}}

We use \texttt{mutate()} when we want to make a new column. For
instance, perhaps we want to make a new column that is 1 if a person was
both a member and a senator and 0 otherwise. That is to say that our new
column would denote politicians that served in both the upper and the
lower house.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\OtherTok{\textless{}{-}} 
\NormalTok{  australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{was\_both =} \FunctionTok{if\_else}\NormalTok{(member }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ senator }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}

\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(member, senator, was\_both)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,783 x 3
   member senator was_both
    <dbl>   <dbl>    <dbl>
 1      0       1        0
 2      1       1        1
 3      0       1        0
 4      1       0        0
 5      1       0        0
 6      1       0        0
 7      1       0        0
 8      0       1        0
 9      0       1        0
10      1       0        0
# ... with 1,773 more rows
\end{verbatim}

We could use \texttt{mutate()} with math, such as addition and
subtraction. For instance, we could calculate the age that the
politicians are (or would have been) in 2022.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\OtherTok{\textless{}{-}} 
\NormalTok{  australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age =} \DecValTok{2022} \SpecialCharTok{{-}}\NormalTok{ lubridate}\SpecialCharTok{::}\FunctionTok{year}\NormalTok{(birthDate))}

\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,783 x 2
   uniqueID     age
   <chr>      <dbl>
 1 Abbott1859    NA
 2 Abbott1869   153
 3 Abbott1877   145
 4 Abbott1886   136
 5 Abbott1891   131
 6 Abbott1957    65
 7 Abel1939      83
 8 Abetz1958     64
 9 Adams1943     79
10 Adams1951     71
# ... with 1,773 more rows
\end{verbatim}

There are a variety of functions that are especially useful when
constructing new columns. These include \texttt{log()} which will
compute the natural logarithm, \texttt{lead()} which will bring values
up by one row, \texttt{lag()} which will push values down by one row,
and \texttt{cumsum()} which creates a cumulative sum of the column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, age) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{log\_age =} \FunctionTok{log}\NormalTok{(age)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  uniqueID     age log_age
  <chr>      <dbl>   <dbl>
1 Abbott1859    NA   NA   
2 Abbott1869   153    5.03
3 Abbott1877   145    4.98
4 Abbott1886   136    4.91
5 Abbott1891   131    4.88
6 Abbott1957    65    4.17
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, age) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lead\_age =} \FunctionTok{lead}\NormalTok{(age)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  uniqueID     age lead_age
  <chr>      <dbl>    <dbl>
1 Abbott1859    NA      153
2 Abbott1869   153      145
3 Abbott1877   145      136
4 Abbott1886   136      131
5 Abbott1891   131       65
6 Abbott1957    65       83
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, age) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lag\_age =} \FunctionTok{lag}\NormalTok{(age)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  uniqueID     age lag_age
  <chr>      <dbl>   <dbl>
1 Abbott1859    NA      NA
2 Abbott1869   153      NA
3 Abbott1877   145     153
4 Abbott1886   136     145
5 Abbott1891   131     136
6 Abbott1957    65     131
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, age) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(age)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{cumulative\_age =} \FunctionTok{cumsum}\NormalTok{(age)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  uniqueID     age cumulative_age
  <chr>      <dbl>          <dbl>
1 Abbott1869   153            153
2 Abbott1877   145            298
3 Abbott1886   136            434
4 Abbott1891   131            565
5 Abbott1957    65            630
6 Abel1939      83            713
\end{verbatim}

As we have in earlier examples, we can also use \texttt{mutate()} in
combination with \texttt{across()}. This includes the potential use of
the selection helpers. For instance, we could count the number of
characters in both the first and last names at the same time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(firstName, surname), str\_count)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, firstName, surname)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,783 x 3
   uniqueID   firstName surname
   <chr>          <int>   <int>
 1 Abbott1859         7       6
 2 Abbott1869         5       6
 3 Abbott1877         9       6
 4 Abbott1886         7       6
 5 Abbott1891         6       6
 6 Abbott1957         7       6
 7 Abel1939           4       4
 8 Abetz1958          4       5
 9 Adams1943          6       5
10 Adams1951          4       5
# ... with 1,773 more rows
\end{verbatim}

Finally, we use \texttt{case\_when()} when we need to make a new column
on the basis of more than two conditional statements (in contrast to
\texttt{if\_else()} from our first \texttt{mutate()} example). For
instance, we may have some years and want to group them into decades.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year\_of\_birth =}\NormalTok{ lubridate}\SpecialCharTok{::}\FunctionTok{year}\NormalTok{(birthDate),}
         \AttributeTok{decade\_of\_birth =} 
           \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1929} \SpecialCharTok{\textasciitilde{}} \StringTok{"pre{-}1930"}\NormalTok{,}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1939} \SpecialCharTok{\textasciitilde{}} \StringTok{"1930s"}\NormalTok{,}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1949} \SpecialCharTok{\textasciitilde{}} \StringTok{"1940s"}\NormalTok{,}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1959} \SpecialCharTok{\textasciitilde{}} \StringTok{"1950s"}\NormalTok{,}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1969} \SpecialCharTok{\textasciitilde{}} \StringTok{"1960s"}\NormalTok{,}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1979} \SpecialCharTok{\textasciitilde{}} \StringTok{"1970s"}\NormalTok{,}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1989} \SpecialCharTok{\textasciitilde{}} \StringTok{"1980s"}\NormalTok{,}
\NormalTok{             year\_of\_birth }\SpecialCharTok{\textless{}=} \DecValTok{1999} \SpecialCharTok{\textasciitilde{}} \StringTok{"1990s"}\NormalTok{,}
             \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"Unknown or error"}
\NormalTok{             )}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, year\_of\_birth, decade\_of\_birth)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,783 x 3
   uniqueID   year_of_birth decade_of_birth 
   <chr>              <dbl> <chr>           
 1 Abbott1859            NA Unknown or error
 2 Abbott1869          1869 pre-1930        
 3 Abbott1877          1877 pre-1930        
 4 Abbott1886          1886 pre-1930        
 5 Abbott1891          1891 pre-1930        
 6 Abbott1957          1957 1950s           
 7 Abel1939            1939 1930s           
 8 Abetz1958           1958 1950s           
 9 Adams1943           1943 1940s           
10 Adams1951           1951 1950s           
# ... with 1,773 more rows
\end{verbatim}

We could accomplish this with a series of \texttt{if\_else()}
statements, but \texttt{case\_when()} is more clear. The cases are
evaluated in order and as soon as there is a match \texttt{case\_when()}
does not continue to the remainder of the cases. So it can be useful to
have a catch-all at the end that will signal if there is a potential
issue that we might like to know about.

\hypertarget{summarise}{%
\subsection{\texorpdfstring{\texttt{summarise()}}{summarise()}}\label{summarise}}

We use \texttt{summarise()} when we would like to make new, condensed,
summary variables. For instance, perhaps we would like to know the
minimum, average, and maximum of some column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{youngest =} \FunctionTok{min}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{oldest =} \FunctionTok{max}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{average =} \FunctionTok{mean}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
  youngest oldest average
     <dbl>  <dbl>   <dbl>
1       28    193    101.
\end{verbatim}

As an aside, \texttt{summarise()} and \texttt{summarize()} are
equivalent and we can use either.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{youngest =} \FunctionTok{min}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{oldest =} \FunctionTok{max}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{average =} \FunctionTok{mean}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
  youngest oldest average
     <dbl>  <dbl>   <dbl>
1       28    193    101.
\end{verbatim}

By default, \texttt{summarise()} will provide one row of output for a
whole dataset. For instance, in the earlier example we found the
youngest, oldest, and average across all politicians. However, we can
create more groups in our dataset using \texttt{group\_by()}. And we can
then apply another function within the context of those groups. We could
use many functions on the basis of groups, but the \texttt{summarise()}
function is particularly powerful in conjunction with
\texttt{group\_by()}. For instance, we could group by gender, and then
get age-based summary statistics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{youngest =} \FunctionTok{min}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{oldest =} \FunctionTok{max}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{average =} \FunctionTok{mean}\NormalTok{(age, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  gender youngest oldest average
  <chr>     <dbl>  <dbl>   <dbl>
1 female       32    140    66.0
2 male         28    193   106. 
\end{verbatim}

Similarly, we could look at youngest, oldest, and mean age at death by
gender.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{days\_lived =}\NormalTok{ deathDate }\SpecialCharTok{{-}}\NormalTok{ birthDate) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(days\_lived)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{min\_days =} \FunctionTok{min}\NormalTok{(days\_lived),}
    \AttributeTok{mean\_days =} \FunctionTok{mean}\NormalTok{(days\_lived) }\SpecialCharTok{|\textgreater{}} \FunctionTok{round}\NormalTok{(),}
    \AttributeTok{max\_days =} \FunctionTok{max}\NormalTok{(days\_lived)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  gender min_days   mean_days  max_days  
  <chr>  <drtn>     <drtn>     <drtn>    
1 female 14856 days 28857 days 35560 days
2 male   12380 days 27376 days 36416 days
\end{verbatim}

And so we learn that female members of parliament on average lived
slightly longer than male members of parliament.

We can use \texttt{group\_by()} on the basis of more than one group. For
instance, we could look at the average number of days lived by gender
and by house.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{days\_lived =}\NormalTok{ deathDate }\SpecialCharTok{{-}}\NormalTok{ birthDate) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(days\_lived)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender, member) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{min\_days =} \FunctionTok{min}\NormalTok{(days\_lived),}
    \AttributeTok{mean\_days =} \FunctionTok{mean}\NormalTok{(days\_lived) }\SpecialCharTok{|\textgreater{}} \FunctionTok{round}\NormalTok{(),}
    \AttributeTok{max\_days =} \FunctionTok{max}\NormalTok{(days\_lived)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 5
# Groups:   gender [2]
  gender member min_days   mean_days  max_days  
  <chr>   <dbl> <drtn>     <drtn>     <drtn>    
1 female      0 21746 days 29517 days 35560 days
2 female      1 14856 days 27538 days 33442 days
3 male        0 13619 days 27133 days 36416 days
4 male        1 12380 days 27496 days 36328 days
\end{verbatim}

We can use \texttt{count()} to create counts by groups. For instance,
the number of politicians by gender.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 2
# Groups:   gender [2]
  gender     n
  <chr>  <int>
1 female   240
2 male    1543
\end{verbatim}

In addition to the \texttt{count()}, we could calculate a proportion.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{proportion =}\NormalTok{ n}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sum}\NormalTok{(n)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 3
  gender     n proportion
  <chr>  <int>      <dbl>
1 female   240      0.135
2 male    1543      0.865
\end{verbatim}

Using \texttt{count()} is essentially the same as using
\texttt{group\_by()} and then \texttt{summarise()} with \texttt{n()},
and we get the same result in that way.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 2
  gender     n
  <chr>  <int>
1 female   240
2 male    1543
\end{verbatim}

And there is a comparably helpful function that acts similarly
to\texttt{mutate()}, which is \texttt{add\_count()}. The difference is
that the number will be added in a new column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{australian\_politicians }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{add\_count}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(uniqueID, gender, n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,783 x 3
# Groups:   gender [2]
   uniqueID   gender     n
   <chr>      <chr>  <int>
 1 Abbott1859 male    1543
 2 Abbott1869 male    1543
 3 Abbott1877 male    1543
 4 Abbott1886 male    1543
 5 Abbott1891 male    1543
 6 Abbott1957 male    1543
 7 Abel1939   male    1543
 8 Abetz1958  male    1543
 9 Adams1943  female   240
10 Adams1951  male    1543
# ... with 1,773 more rows
\end{verbatim}

\hypertarget{base}{%
\section{Base}\label{base}}

While the \texttt{tidyverse} was established relatively recently to help
with data science, R existed long before this. There is a host of
functionality that is built into R especially around the core needs of
programming and statisticians.

In particular, we will cover:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{class()}
\item
  Data simulation
\item
  \texttt{function()}, \texttt{for()}, and \texttt{apply()}
\end{enumerate}

There is no need to install or load any additional packages, as this
functionality comes with R.

\hypertarget{class}{%
\subsection{\texorpdfstring{\texttt{class()}}{class()}}\label{class}}

In everyday usage `a, b, c, \ldots{}' are letters and `1, 2, 3,\ldots{}'
are numbers. And we use letters and numbers differently; for instance,
we do not add or subtract letters. Similarly, R needs to have some way
of distinguishing different classes of content and to define the
properties that each class has, `how it behaves, and how it relates to
other types of objects' (Wickham 2019a).

Classes have a hierarchy. For instance, we are `human', which is itself
`animal'. All `humans' are `animals', but not all `animals' are
`humans'. Similarly, all integers are numbers, but not all numbers are
integers. We can find out the class of an object in R with
\texttt{class()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_number }\OtherTok{\textless{}{-}} \DecValTok{8}
\FunctionTok{class}\NormalTok{(a\_number)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "numeric"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_letter }\OtherTok{\textless{}{-}} \StringTok{"a"}
\FunctionTok{class}\NormalTok{(a\_letter)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "character"
\end{verbatim}

The classes that we cover here are `numeric', `character', `factor',
`date', and `data.frame'.

The first thing to know is that, in the same way that a frog can become
a prince, we can sometimes change the class of an object in R. For
instance, we could start with a `numeric', change it to a `character'
with \texttt{as.character()}, and then a `factor' with
\texttt{as.factor()}. But if we tried to make it into a `date' with
\texttt{as.Date()} we would get an error because no all numbers have the
properties that are needed to be a date.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_number }\OtherTok{\textless{}{-}} \DecValTok{8}
\NormalTok{a\_number}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(a\_number)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "numeric"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_number }\OtherTok{\textless{}{-}} \FunctionTok{as.character}\NormalTok{(a\_number)}
\NormalTok{a\_number}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "8"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(a\_number)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_number }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(a\_number)}
\NormalTok{a\_number}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 8
Levels: 8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(a\_number)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "factor"
\end{verbatim}

Compared with `numeric' and `character' classes, the `factor' class
might be less familiar. A `factor' is used for categorical data that can
only take certain values (Wickham 2019a). For instance, typical usage of
a `factor' variable would be a binary, such as `day' or `night'. It is
also often used for age-groups, such as `18-29', `30-44', `45-60', `60+'
(as opposed to age, which would often be a `numeric'); and sometimes for
level of education: `less than high school', `high school', `college',
`undergraduate degree', `postgraduate degree'. We can find the allowed
levels for a `factor' using \texttt{levels()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age\_groups }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}
  \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}18{-}29\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}30{-}44\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}45{-}60\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}60+\textquotesingle{}}\NormalTok{)}
\NormalTok{)}
\NormalTok{age\_groups}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 18-29 30-44 45-60 60+  
Levels: 18-29 30-44 45-60 60+
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(age\_groups)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{levels}\NormalTok{(age\_groups)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "18-29" "30-44" "45-60" "60+"  
\end{verbatim}

Dates are an especially tricky class and quickly become complicated.
Nonetheless, at a foundational level, we can use \texttt{as.Date()} to
convert a character that looks like a `date' into an actual `date'. This
enables us to, say, perform addition and subtraction, when we would not
be able to do that with a `character'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{looks\_like\_a\_date\_but\_is\_not }\OtherTok{\textless{}{-}} \StringTok{"2022{-}01{-}01"}
\NormalTok{looks\_like\_a\_date\_but\_is\_not}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "2022-01-01"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(looks\_like\_a\_date\_but\_is\_not)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{is\_a\_date }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(looks\_like\_a\_date\_but\_is\_not)}
\NormalTok{is\_a\_date}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "2022-01-01"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(is\_a\_date)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Date"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{is\_a\_date }\SpecialCharTok{+} \DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "2022-01-04"
\end{verbatim}

The final class that we discuss here is `data.frame'. This looks like a
spreadsheet and is commonly used to store the data that we will analyze.
Formally, `a data frame is a list of equal-length vectors' (Wickham
2019a). It will have column and row names which we can see using
\texttt{colnames()} and \texttt{rownames()}, although often the names of
the rows are just numbers.

To illustrate this, we use the `ResumeNames' dataset from \texttt{AER}
(Kleiber and Zeileis 2008). This package can be installed in the same
way as any other package from CRAN. This dataset comprises
cross-sectional data about resume content, especially the name used on
the resume, and associated information about whether the candidate
received a call-back for 4,870 fictitious resumes. The dataset was
created by Bertrand and Mullainathan (2004) who sent fictitious resumes
in response to job advertisements in Boston and Chicago that differed in
whether the resume was assigned a `very African American sounding name
or a very White sounding name'. They found considerable discrimination
whereby `White names receive 50 percent more callbacks for interviews'.
Hangartner, Kopp, and Siegenthaler (2021) generalize this using an
online Swiss platform and find that immigrants and minority ethnic
groups are contacted less by recruiters, as are women when the
profession is men-dominated, and vice versa.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"AER"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(AER)}
\FunctionTok{data}\NormalTok{(}\StringTok{"ResumeNames"}\NormalTok{, }\AttributeTok{package =} \StringTok{"AER"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ResumeNames }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     name gender ethnicity quality call    city jobs experience honors
1 Allison female      cauc     low   no chicago    2          6     no
2 Kristen female      cauc    high   no chicago    3          6     no
3 Lakisha female      afam     low   no chicago    1          6     no
4 Latonya female      afam    high   no chicago    4          6     no
5  Carrie female      cauc    high   no chicago    3         22     no
6     Jay   male      cauc     low   no chicago    2          6    yes
  volunteer military holes school email computer special college minimum equal
1        no       no   yes     no    no      yes      no     yes       5   yes
2       yes      yes    no    yes   yes      yes      no      no       5   yes
3        no       no    no    yes    no      yes      no     yes       5   yes
4       yes       no   yes     no   yes      yes     yes      no       5   yes
5        no       no    no    yes   yes      yes      no      no    some   yes
6        no       no    no     no    no       no     yes     yes    none   yes
      wanted requirements reqexp reqcomm reqeduc reqcomp reqorg
1 supervisor          yes    yes      no      no     yes     no
2 supervisor          yes    yes      no      no     yes     no
3 supervisor          yes    yes      no      no     yes     no
4 supervisor          yes    yes      no      no     yes     no
5  secretary          yes    yes      no      no     yes    yes
6      other           no     no      no      no      no     no
                          industry
1                    manufacturing
2                    manufacturing
3                    manufacturing
4                    manufacturing
5 health/education/social services
6                            trade
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(ResumeNames)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "data.frame"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(ResumeNames)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "name"         "gender"       "ethnicity"    "quality"      "call"        
 [6] "city"         "jobs"         "experience"   "honors"       "volunteer"   
[11] "military"     "holes"        "school"       "email"        "computer"    
[16] "special"      "college"      "minimum"      "equal"        "wanted"      
[21] "requirements" "reqexp"       "reqcomm"      "reqeduc"      "reqcomp"     
[26] "reqorg"       "industry"    
\end{verbatim}

We can examine the class of the vectors, i.e.~the columns, that make-up
a data frame by specifying the column name.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{jobs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "integer"
\end{verbatim}

Sometimes it is helpful to be able to change the classes of many columns
at once. We can do this by using \texttt{mutate()} and
\texttt{across()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{ethnicity)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ResumeNames }\OtherTok{\textless{}{-}}\NormalTok{ ResumeNames }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(name, gender, ethnicity), as.character)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{head}\NormalTok{()}

\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{gender)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "character"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(ResumeNames}\SpecialCharTok{$}\NormalTok{ethnicity)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "character"
\end{verbatim}

There are many ways for code to not run but having an issue with the
class is always among the first things to check. Common issues are
variables that we think should be `character' or `numeric' actually
being `factor'. And variables that we think should be `numeric' actually
being `character'.

\hypertarget{simulating-data}{%
\subsection{Simulating data}\label{simulating-data}}

Simulating data is a key skill for telling believable stories with data.
In order to simulate data, we need to be able to randomly draw from
statistical distributions and other collections. R has a variety of
functions to make this easier, including: the normal distribution,
\texttt{rnorm()}; the uniform distribution, \texttt{runif()}; the
Poisson distribution, \texttt{rpois()}; the binomial distribution,
\texttt{rbinom()}; and many others. To randomly sample from a collection
of items, we can use \texttt{sample()}.

When dealing with randomness, the need for reproducibility makes it
important, paradoxically, that the randomness is repeatable. That is to
say, another person needs to be able to draw the random numbers that we
draw. We do this by setting a seed for our random draws using
\texttt{set.seed()}.

We could get observations from the standard normal distribution and put
the those into a data frame.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_observations }\OtherTok{\textless{}{-}} \DecValTok{5}

\NormalTok{simulated\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{person =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{number\_of\_observations),}
    \AttributeTok{std\_normal\_observations =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_observations,}
                                    \AttributeTok{mean =} \DecValTok{0}\NormalTok{,}
                                    \AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{    )}

\NormalTok{simulated\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  person std_normal_observations
1      1             -0.35980342
2      2             -0.04064753
3      3             -1.78216227
4      4             -1.12242282
5      5             -1.00278400
\end{verbatim}

We could then add draws from the uniform, Poisson, and binomial
distributions, using \texttt{cbind()} to bring the columns of the
original dataset and the new one together.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_data }\OtherTok{\textless{}{-}}
  \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{uniform\_observations =} 
      \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_observations, }\AttributeTok{min =} \DecValTok{0}\NormalTok{, }\AttributeTok{max =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{poisson\_observations =} 
      \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_observations, }\AttributeTok{lambda =} \DecValTok{100}\NormalTok{),}
    \AttributeTok{binomial\_observations =} 
      \FunctionTok{rbinom}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_observations, }\AttributeTok{size =} \DecValTok{2}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{cbind}\NormalTok{(simulated\_data)}

\NormalTok{simulated\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  uniform_observations poisson_observations binomial_observations person
1            9.6219155                   81                     2      1
2            7.2269016                   91                     1      2
3            0.8252921                   84                     1      3
4            1.0379810                  100                     1      4
5            3.0942004                   97                     1      5
  std_normal_observations
1             -0.35980342
2             -0.04064753
3             -1.78216227
4             -1.12242282
5             -1.00278400
\end{verbatim}

Finally, we will add a favorite color to each observation with
\texttt{sample()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{favorite\_color =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{" white "}\NormalTok{), }
                             \AttributeTok{size =}\NormalTok{ number\_of\_observations,}
                             \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{cbind}\NormalTok{(simulated\_data)}

\NormalTok{simulated\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  favorite_color uniform_observations poisson_observations
1           blue            9.6219155                   81
2           blue            7.2269016                   91
3           blue            0.8252921                   84
4         white             1.0379810                  100
5           blue            3.0942004                   97
  binomial_observations person std_normal_observations
1                     2      1             -0.35980342
2                     1      2             -0.04064753
3                     1      3             -1.78216227
4                     1      4             -1.12242282
5                     1      5             -1.00278400
\end{verbatim}

We set the option `replace' to `TRUE' because we are only choosing
between two items, but each time we choose we want the possibility that
either are chosen. Depending on the simulation we may need to think
about whether `replace' should be `TRUE' or `FALSE'. Another useful
optional argument in \texttt{sample()} is to adjust the probability with
which each item is drawn. The default is that all options are equally
likely, but we could specify particular probabilities if we wanted to
with `prob'. As always with functions, we can find more in the help
file, for instance \texttt{?sample}.

\hypertarget{function-for-and-apply}{%
\subsection{\texorpdfstring{\texttt{function()}, \texttt{for()}, and
\texttt{apply()}}{function(), for(), and apply()}}\label{function-for-and-apply}}

R `is a functional programming language' (Wickham 2019a). This means
that we foundationally write, use, and compose functions, which are
collections of code that accomplish something specific.

There are a lot of functions in R that other people have written, and we
can use. Almost any common statistical or data science task that we
might need to accomplish likely already has a function that has been
written by someone else and made available to us, either as part of the
base R installation or a package. But we will need to write our own
functions from time to time, especially for more-specific tasks. We
define a function using \texttt{function()}, and then assign a name. We
will likely need to include some inputs and outputs for the function.
Inputs are specified between round brackets. The specific task that the
function is to accomplish goes between braces.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{print\_names }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(some\_names) \{}
  \FunctionTok{print}\NormalTok{(some\_names)}
\NormalTok{\}}

\FunctionTok{print\_names}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"rohan"}\NormalTok{, }\StringTok{"monica"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "rohan"  "monica"
\end{verbatim}

We can specify defaults for the inputs in case the person using the
function does not supply them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{print\_names }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{some\_names =} \FunctionTok{c}\NormalTok{(}\StringTok{"edward"}\NormalTok{, }\StringTok{"hugo"}\NormalTok{)) \{}
  \FunctionTok{print}\NormalTok{(some\_names)}
\NormalTok{\}}

\FunctionTok{print\_names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "edward" "hugo"  
\end{verbatim}

One common scenario is that we want to apply a function multiple times.
Like many programming languages, we can use a \texttt{for()} loop for
this. The look of a \texttt{for()} loop in R is similar to
\texttt{function()}, in that we define what we are iterating over in the
round brackets, and the function to apply in braces.

Because R is a programming language that is focused on statistics, we
are often interested in arrays or matrices. We use \texttt{apply()} to
apply a function to rows (`MARGIN = 1') or columns (`MARGIN = 2').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  favorite_color uniform_observations poisson_observations
1           blue            9.6219155                   81
2           blue            7.2269016                   91
3           blue            0.8252921                   84
4         white             1.0379810                  100
5           blue            3.0942004                   97
  binomial_observations person std_normal_observations
1                     2      1             -0.35980342
2                     1      2             -0.04064753
3                     1      3             -1.78216227
4                     1      4             -1.12242282
5                     1      5             -1.00278400
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apply}\NormalTok{(}\AttributeTok{X =}\NormalTok{ simulated\_data, }\AttributeTok{MARGIN =} \DecValTok{2}\NormalTok{, }\AttributeTok{FUN =}\NormalTok{ unique)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$favorite_color
[1] "blue"    " white "

$uniform_observations
[1] "9.6219155" "7.2269016" "0.8252921" "1.0379810" "3.0942004"

$poisson_observations
[1] " 81" " 91" " 84" "100" " 97"

$binomial_observations
[1] "2" "1"

$person
[1] "1" "2" "3" "4" "5"

$std_normal_observations
[1] "-0.35980342" "-0.04064753" "-1.78216227" "-1.12242282" "-1.00278400"
\end{verbatim}

\hypertarget{making-graphs-with-ggplot2}{%
\section{\texorpdfstring{Making graphs with
\texttt{ggplot2}}{Making graphs with ggplot2}}\label{making-graphs-with-ggplot2}}

If the key package in the \texttt{tidyverse} in terms of manipulating
data is \texttt{dplyr} (Wickham et al. 2020), then the key package in
the \texttt{tidyverse} in terms of creating graphs is \texttt{ggplot2}
(Wickham 2016).\texttt{ggplot2} works by defining layers which build to
form a graph, based around the `grammar of graphics' (hence, the `gg').
Instead of the pipe operator (\texttt{\textbar{}\textgreater{}}) ggplot
uses the add operator \texttt{+}. As part of the \texttt{tidyverse}
collection of packages, \texttt{ggplot2} does not need to be explicitly
installed or loaded if the \texttt{tidyverse} has been loaded.

There are three key aspects that need to be specified to build a graph
with \texttt{ggplot2}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data;
\item
  Aesthetics / mapping; and
\item
  Type.
\end{enumerate}

To get started we will obtain some GDP data for countries in the
Organisation for Economic Co-operation and Development (OECD) (OECD
2022).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{oecd\_gdp }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://stats.oecd.org/sdmx{-}json/data/DP\_LIVE/.QGDP.../OECD?contentType=csv\&detail=code\&separator=comma\&csv{-}lang=en"}\NormalTok{)}

\FunctionTok{write\_csv}\NormalTok{(oecd\_gdp, }\StringTok{\textquotesingle{}inputs/data/oecd\_gdp.csv\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 8
  LOCATION INDICATOR SUBJECT MEASURE  FREQUENCY TIME  Value `Flag Codes`
  <chr>    <chr>     <chr>   <chr>    <chr>     <chr> <dbl> <chr>       
1 OECD     QGDP      TOT     PC_CHGPP A         1962   5.70 <NA>        
2 OECD     QGDP      TOT     PC_CHGPP A         1963   5.20 <NA>        
3 OECD     QGDP      TOT     PC_CHGPP A         1964   6.38 <NA>        
4 OECD     QGDP      TOT     PC_CHGPP A         1965   5.35 <NA>        
5 OECD     QGDP      TOT     PC_CHGPP A         1966   5.75 <NA>        
6 OECD     QGDP      TOT     PC_CHGPP A         1967   3.96 <NA>        
\end{verbatim}

We are interested, firstly, in making a bar chart of GDP change in the
third quarter of 2021 for ten countries: Australia, Canada, Chile,
Indonesia, Germany, Great Britain, New Zealand, South Africa, Spain, and
the US.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oecd\_gdp\_most\_recent }\OtherTok{\textless{}{-}} 
\NormalTok{  oecd\_gdp }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(TIME }\SpecialCharTok{==} \StringTok{"2021{-}Q3"}\NormalTok{,}
\NormalTok{         SUBJECT }\SpecialCharTok{==} \StringTok{"TOT"}\NormalTok{,}
\NormalTok{         LOCATION }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"AUS"}\NormalTok{, }\StringTok{"CAN"}\NormalTok{, }\StringTok{"CHL"}\NormalTok{, }\StringTok{"DEU"}\NormalTok{, }\StringTok{"GBR"}\NormalTok{,}
                         \StringTok{"IDN"}\NormalTok{, }\StringTok{"ESP"}\NormalTok{, }\StringTok{"NZL"}\NormalTok{, }\StringTok{"USA"}\NormalTok{, }\StringTok{"ZAF"}\NormalTok{),}
\NormalTok{         MEASURE }\SpecialCharTok{==} \StringTok{"PC\_CHGPY"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{european =} \FunctionTok{if\_else}\NormalTok{(LOCATION }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"DEU"}\NormalTok{, }\StringTok{"GBR"}\NormalTok{, }\StringTok{"ESP"}\NormalTok{),}
                             \StringTok{"European"}\NormalTok{,}
                             \StringTok{"Not european"}\NormalTok{),}
         \AttributeTok{hemisphere =} \FunctionTok{if\_else}\NormalTok{(LOCATION }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"CAN"}\NormalTok{, }\StringTok{"DEU"}\NormalTok{, }\StringTok{"GBR"}\NormalTok{, }\StringTok{"ESP"}\NormalTok{, }\StringTok{"USA"}\NormalTok{),}
                             \StringTok{"Northern Hemisphere"}\NormalTok{,}
                             \StringTok{"Southern Hemisphere"}\NormalTok{),}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

We start with \texttt{ggplot()} and specify a mapping/aesthetic, which
in this case means specifying the x-axis and the y-axis. The first
argument in \texttt{ggplot()} is the data we want to visualize, so we
can use the pipe operator at this stage as usual.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oecd\_gdp\_most\_recent }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ LOCATION, }\AttributeTok{y =}\NormalTok{ Value))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./03-r_essentials_files/figure-pdf/unnamed-chunk-128-1.pdf}

}

\end{figure}

Now we need to specify the type of graph that we are interested in. In
this case we want a bar chart and we do this by adding
\texttt{geom\_bar()} using \texttt{+}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oecd\_gdp\_most\_recent }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ LOCATION, }\AttributeTok{y =}\NormalTok{ Value)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./03-r_essentials_files/figure-pdf/unnamed-chunk-130-1.pdf}

}

\end{figure}

We can color the bars by whether the country is European by adding
another aesthetic, `fill'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oecd\_gdp\_most\_recent }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ LOCATION, }\AttributeTok{y =}\NormalTok{ Value, }\AttributeTok{fill =}\NormalTok{ european)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./03-r_essentials_files/figure-pdf/unnamed-chunk-132-1.pdf}

}

\end{figure}

Finally, we could make it look nicer by: adding labels, \texttt{labs()};
changing the color, \texttt{scale\_fill\_brewer()}; and the background,
\texttt{theme\_classic()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oecd\_gdp\_most\_recent }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ LOCATION, }\AttributeTok{y =}\NormalTok{ Value, }\AttributeTok{fill =}\NormalTok{ european)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Quarterly change in GDP for ten OECD countries in 2021Q3"}\NormalTok{, }
       \AttributeTok{x =} \StringTok{"Countries"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Change (\%)"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Is European?"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./03-r_essentials_files/figure-pdf/unnamed-chunk-134-1.pdf}

}

\end{figure}

Facets enable us to that we create subplots that focus on specific
aspects of our data. They are invaluable because they allow us to add
another variable to a graph without having to make a 3D graph. We use
\texttt{facet\_wrap()} to add facets and specify the variable that we
would like to facet by. In this case, we facet by hemisphere.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oecd\_gdp\_most\_recent }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ LOCATION, }\AttributeTok{y =}\NormalTok{ Value, }\AttributeTok{fill =}\NormalTok{ european)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Quarterly change in GDP for ten OECD countries in 2021Q3"}\NormalTok{, }
       \AttributeTok{x =} \StringTok{"Countries"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Change (\%)"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Is European?"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{hemisphere, }
              \AttributeTok{scales =} \StringTok{"free\_x"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./03-r_essentials_files/figure-pdf/unnamed-chunk-136-1.pdf}

}

\end{figure}

\hypertarget{exploring-the-tidyverse}{%
\section{\texorpdfstring{Exploring the
\texttt{tidyverse}}{Exploring the tidyverse}}\label{exploring-the-tidyverse}}

We have focused on two aspects of the \texttt{tidyverse}:
\texttt{dplyr}, and \texttt{ggplot2}. However, the \texttt{tidyverse}
comprises a variety of different packages and functions. We will now go
through four common aspects:

\begin{itemize}
\tightlist
\item
  Importing data and \texttt{tibble()};
\item
  Joining and pivoting datasets;
\item
  String manipulation and \texttt{stringr};
\item
  Factor variables and \texttt{forcats}.
\end{itemize}

However, the first task is to deal with the nomenclature, and in
particular to be specific about what is `tidy' about the `tidyverse'.
The name refers to tidy data, and the benefit of that is that while
there are a variety of ways for data to be messy, tidy data satisfy
three rules. This means the structure of the datasets is consistent
regardless of the specifics, and makes it easier to apply functions that
expect certain types of input. Tidy data refers to a dataset where
(Wickham and Grolemund 2017; Wickham 2014, 4):

\begin{itemize}
\tightlist
\item
  Every variable is in a column of its own.
\item
  Every observation is in its own row.
\item
  Every value is in its own cell.
\end{itemize}

Table~\ref{tbl-nottidydata} is not tidy because age and hair share a
column. Table~\ref{tbl-tidydata} is its tidy counterpart.

\hypertarget{tbl-nottidydata}{}
\begin{longtable}[]{@{}lll@{}}
\caption{\label{tbl-nottidydata}Example of data that are not
tidy}\tabularnewline
\toprule()
Person & Variable & Value \\
\midrule()
\endfirsthead
\toprule()
Person & Variable & Value \\
\midrule()
\endhead
Rohan & Age & 35 \\
Rohan & Hair & Black \\
Monica & Age & 35 \\
Monica & Hair & Blonde \\
Edward & Age & 2 \\
Edward & Hair & Brown \\
Hugo & Age & 0 \\
Hugo & Hair & None \\
\bottomrule()
\end{longtable}

\hypertarget{tbl-tidydata}{}
\begin{longtable}[]{@{}lrl@{}}
\caption{\label{tbl-tidydata}Example of tidy data}\tabularnewline
\toprule()
Person & Age & Hair \\
\midrule()
\endfirsthead
\toprule()
Person & Age & Hair \\
\midrule()
\endhead
Rohan & 35 & Black \\
Monica & 35 & Blonde \\
Edward & 2 & Brown \\
Hugo & 0 & None \\
\bottomrule()
\end{longtable}

\hypertarget{importing-data-and-tibble}{%
\subsection{\texorpdfstring{Importing data and
\texttt{tibble()}}{Importing data and tibble()}}\label{importing-data-and-tibble}}

There are a variety of ways to get data into R so that we can use it.
For CSV files, there is \texttt{read\_csv()} from \texttt{readr}
(Wickham, Hester, and Bryan 2021), and for dta files, there is
\texttt{read\_dta()} from \texttt{haven} (Wickham and Miller 2020).

CSVs are a common format and have many advantages including the fact
that they typically do not modify the data. Each column is separated by
a comma, and each row is a record. We can provide \texttt{read\_csv()}
with a URL or a local file to read. There are a variety of different
options that can be passed to \texttt{read\_csv()} including the ability
to specify whether the dataset has column names, the types of the
columns, and how many lines to skip. If we do not specify the types of
the columns then \texttt{read\_csv()} will make a guess by looking at
the dataset.

We use \texttt{read\_dta()} to read .dta files, which are commonly
produced by the statistical program Stata. This means that they are
common in fields such as sociology, political science, and economics.
This format separates the data from its labels and so we typically
reunite these using \texttt{to\_factor()} from \texttt{labelled}
(Larmarange 2021). \texttt{haven} is part of the \texttt{tidyverse}, but
is not automatically loaded by default, in contrast to a package such as
\texttt{ggplot2}, and so we would need to run \texttt{library(haven)}.

Typically a dataset enters R as a `data.frame'. While this can be
useful, another helpful class for a dataset is `tibble'. These can be
created using \texttt{tibble()} from the \texttt{tibble} package which
is part of the \texttt{tidyverse}. A tibble is a data frame, with some
particular changes that make it easier to work with, including not
converting strings to factors by default, showing the class of columns,
and printing nicely.

We can make a tibble manually, if need be, for instance, when we
simulate data. But we typically import data directly as a tibble, for
instance, when we use \texttt{read\_csv()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{people\_as\_dataframe }\OtherTok{\textless{}{-}} 
  \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{names =} \FunctionTok{c}\NormalTok{(}\StringTok{"rohan"}\NormalTok{, }\StringTok{"monica"}\NormalTok{),}
             \AttributeTok{website =} \FunctionTok{c}\NormalTok{(}\StringTok{"rohanalexander.com"}\NormalTok{, }\StringTok{"monicaalexander.com"}\NormalTok{),}
             \AttributeTok{fav\_color =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{" white "}\NormalTok{)}
\NormalTok{             )}
\FunctionTok{class}\NormalTok{(people\_as\_dataframe)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "data.frame"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{people\_as\_dataframe}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   names             website fav_color
1  rohan  rohanalexander.com      blue
2 monica monicaalexander.com    white 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{people\_as\_tibble }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{names =} \FunctionTok{c}\NormalTok{(}\StringTok{"rohan"}\NormalTok{, }\StringTok{"monica"}\NormalTok{),}
         \AttributeTok{website =} \FunctionTok{c}\NormalTok{(}\StringTok{"rohanalexander.com"}\NormalTok{, }\StringTok{"monicaalexander.com"}\NormalTok{),}
         \AttributeTok{fav\_color =} \FunctionTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\StringTok{" white "}\NormalTok{)}
\NormalTok{         )}
\NormalTok{people\_as\_tibble}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 3
  names  website             fav_color
  <chr>  <chr>               <chr>    
1 rohan  rohanalexander.com  "blue"   
2 monica monicaalexander.com " white "
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(people\_as\_tibble)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "tbl_df"     "tbl"        "data.frame"
\end{verbatim}

\hypertarget{dataset-manipulation-with-joins-and-pivots}{%
\subsection{Dataset manipulation with joins and
pivots}\label{dataset-manipulation-with-joins-and-pivots}}

There are two dataset manipulations that are often needed: joins and
pivots.

We often have a situation where we have two, or more, datasets and we
are interested in combining them. We can join datasets together in a
variety of ways. A common way is to use \texttt{left\_join()} from
\texttt{dplyr} (Wickham et al. 2020). This is most useful where there is
one main dataset that we are using and there is another dataset with
some useful variables that we want to add to that. The critical aspect
is that we have a column or columns that we can use to link the two
datasets. Here we will create two tibbles and then join them on the
basis of names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{main\_dataset }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{names =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}rohan\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}monica\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}edward\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}hugo\textquotesingle{}}\NormalTok{),}
    \AttributeTok{status =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}adult\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}adult\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}child\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}infant\textquotesingle{}}\NormalTok{)}
\NormalTok{  )}
\NormalTok{main\_dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 2
  names  status
  <chr>  <chr> 
1 rohan  adult 
2 monica adult 
3 edward child 
4 hugo   infant
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{supplementary\_dataset }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{names =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}rohan\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}monica\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}edward\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}hugo\textquotesingle{}}\NormalTok{),}
    \AttributeTok{favorite\_food =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}pasta\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}salmon\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}pizza\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}milk\textquotesingle{}}\NormalTok{)}
\NormalTok{  )}
\NormalTok{supplementary\_dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 2
  names  favorite_food
  <chr>  <chr>        
1 rohan  pasta        
2 monica salmon       
3 edward pizza        
4 hugo   milk         
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{main\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  main\_dataset }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{left\_join}\NormalTok{(supplementary\_dataset, }\AttributeTok{by =} \StringTok{"names"}\NormalTok{)}

\NormalTok{main\_dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 3
  names  status favorite_food
  <chr>  <chr>  <chr>        
1 rohan  adult  pasta        
2 monica adult  salmon       
3 edward child  pizza        
4 hugo   infant milk         
\end{verbatim}

There are a variety of other options to join datasets, including
\texttt{inner\_join()}, \texttt{right\_join()}, and
\texttt{full\_join()}.

Another common dataset manipulation task is pivoting them. Datasets tend
to be either long or wide. Generally, in the \texttt{tidyverse}, and
certainly for \texttt{ggplot2}, we need long data. To go from one to the
other we use \texttt{pivot\_longer()} and \texttt{pivot\_wider()} from
\texttt{tidyr} (Wickham 2021c).

We will create some wide data on whether `mark' or `lauren' won a
running race in each of three years.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pivot\_example\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{year =} \FunctionTok{c}\NormalTok{(}\DecValTok{2019}\NormalTok{, }\DecValTok{2020}\NormalTok{, }\DecValTok{2021}\NormalTok{),}
         \AttributeTok{mark =} \FunctionTok{c}\NormalTok{(}\StringTok{"first"}\NormalTok{, }\StringTok{"second"}\NormalTok{, }\StringTok{"first"}\NormalTok{),}
         \AttributeTok{lauren =} \FunctionTok{c}\NormalTok{(}\StringTok{"second"}\NormalTok{, }\StringTok{"first"}\NormalTok{, }\StringTok{"second"}\NormalTok{))}

\NormalTok{pivot\_example\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 3
   year mark   lauren
  <dbl> <chr>  <chr> 
1  2019 first  second
2  2020 second first 
3  2021 first  second
\end{verbatim}

This dataset is in wide format at the moment. To get it into long
format, we need a column that specifies the person, and another that
specifies the result. We use \texttt{pivot\_longer()} to achieve this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_pivoted\_longer }\OtherTok{\textless{}{-}} 
\NormalTok{  pivot\_example\_data }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"mark"}\NormalTok{, }\StringTok{"lauren"}\NormalTok{),}
              \AttributeTok{names\_to =} \StringTok{"person"}\NormalTok{,}
               \AttributeTok{values\_to =} \StringTok{"position"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(data\_pivoted\_longer)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
   year person position
  <dbl> <chr>  <chr>   
1  2019 mark   first   
2  2019 lauren second  
3  2020 mark   second  
4  2020 lauren first   
5  2021 mark   first   
6  2021 lauren second  
\end{verbatim}

Occasionally, we need to go from long data to wide data. We use
\texttt{pivot\_wider()} to do this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_pivoted\_wider }\OtherTok{\textless{}{-}} 
\NormalTok{  data\_pivoted\_longer }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =} \StringTok{"person"}\NormalTok{,}
                     \AttributeTok{values\_from =} \StringTok{"position"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(data\_pivoted\_wider)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 3
   year mark   lauren
  <dbl> <chr>  <chr> 
1  2019 first  second
2  2020 second first 
3  2021 first  second
\end{verbatim}

\hypertarget{string-manipulation-and-stringr}{%
\subsection{\texorpdfstring{String manipulation and
\texttt{stringr}}{String manipulation and stringr}}\label{string-manipulation-and-stringr}}

In R we often create a string with double quotes, although using single
quotes works too. For instance \texttt{c("a",\ "b")} consists of two
strings `a' and `b', that are contained in a character vector. There are
a variety of ways to manipulate strings in R and we focus on
\texttt{stringr} (Wickham 2019e). This is automatically loaded when we
load the \texttt{tidyverse}.

If we want to look for whether a string contains certain content, then
we can use \texttt{str\_detect()}. And if we want to remove or change
some particular content then we can use \texttt{str\_remove()} or
\texttt{str\_replace()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_of\_strings }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{names =} \FunctionTok{c}\NormalTok{(}\StringTok{"rohan alexander"}\NormalTok{, }
              \StringTok{"monica alexander"}\NormalTok{, }
              \StringTok{"edward alexander"}\NormalTok{, }
              \StringTok{"hugo alexander"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{dataset\_of\_strings }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{is\_rohan =} \FunctionTok{str\_detect}\NormalTok{(names, }\StringTok{"rohan"}\NormalTok{),}
         \AttributeTok{make\_howlett =} \FunctionTok{str\_replace}\NormalTok{(names, }\StringTok{"alexander"}\NormalTok{, }\StringTok{"howlett"}\NormalTok{),}
         \AttributeTok{remove\_rohan =} \FunctionTok{str\_remove}\NormalTok{(names, }\StringTok{"rohan"}\NormalTok{)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 4
  names            is_rohan make_howlett   remove_rohan      
  <chr>            <lgl>    <chr>          <chr>             
1 rohan alexander  TRUE     rohan howlett  " alexander"      
2 monica alexander FALSE    monica howlett "monica alexander"
3 edward alexander FALSE    edward howlett "edward alexander"
4 hugo alexander   FALSE    hugo howlett   "hugo alexander"  
\end{verbatim}

There are a variety of other functions that are often especially useful
in data cleaning. For instance, we can use \texttt{str\_length()} to
find out how long a string is, and \texttt{str\_c()} to bring strings
together.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_of\_strings }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{length\_is =} \FunctionTok{str\_length}\NormalTok{(}\AttributeTok{string =}\NormalTok{ names),}
         \AttributeTok{name\_and\_length =} \FunctionTok{str\_c}\NormalTok{(names, length\_is, }\AttributeTok{sep =} \StringTok{" {-} "}\NormalTok{)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 3
  names            length_is name_and_length      
  <chr>                <int> <chr>                
1 rohan alexander         15 rohan alexander - 15 
2 monica alexander        16 monica alexander - 16
3 edward alexander        16 edward alexander - 16
4 hugo alexander          14 hugo alexander - 14  
\end{verbatim}

Finally, \texttt{separate()} from \texttt{tidyr}, although not part of
\texttt{stringr}, is indispensable for string manipulation. It turns one
character column into many.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataset\_of\_strings }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ names,}
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"first"}\NormalTok{, }\StringTok{"last"}\NormalTok{),}
           \AttributeTok{sep =} \StringTok{" "}\NormalTok{,}
           \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 3
  names            first  last     
  <chr>            <chr>  <chr>    
1 rohan alexander  rohan  alexander
2 monica alexander monica alexander
3 edward alexander edward alexander
4 hugo alexander   hugo   alexander
\end{verbatim}

\hypertarget{factor-variables-and-forcats}{%
\subsection{\texorpdfstring{Factor variables and
\texttt{forcats}}{Factor variables and forcats}}\label{factor-variables-and-forcats}}

A factor is a collection of strings that are categories. Sometimes there
will be an inherent ordering. For instance, the days of the week have an
order -- Monday, Tuesday, Wednesday, \ldots{} -- which is not
alphabetical. But there is no requirement for that to be the case, for
instance gender: female, male, and other; or pregnancy status: pregnant
or not pregnant. Factors feature prominently in base R. They can be
useful because they ensure that only appropriate strings are allowed.
For instance, if `days\_of\_the\_week' was a factor variable then
`January' would not be allowed. But they can add a great deal of
complication, and so they have a less prominent role in
\texttt{tidyverse}. Nonetheless taking advantage of factors is useful in
certain circumstances. For instance, when plotting the days of the week
we probably want them in the usual ordering than in the alphabetical
ordering that would result if we had them as a character variable. While
factors are built into base R, one \texttt{tidyverse} package that is
especially useful when using factors is \texttt{forcats} (Wickham
2020a).

Sometimes we have a character vector, and we will want it ordered in a
particular way. The default is that a character vector is ordered
alphabetically, but we may not want that. For instance, the days of the
week would look strange on a graph if they were alphabetically ordered:
Friday, Monday, Saturday, Sunday, Thursday, Tuesday, and Wednesday!

The way to change the ordering is to change the variable from a
character to a factor. We can use \texttt{fct\_relevel()} from
\texttt{forcats} (Wickham 2020a) to specify an ordering.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{days\_data }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{days =}
      \FunctionTok{c}\NormalTok{(}
        \StringTok{"Monday"}\NormalTok{,}
        \StringTok{"Tuesday"}\NormalTok{,}
        \StringTok{"Wednesday"}\NormalTok{,}
        \StringTok{"Thursday"}\NormalTok{,}
        \StringTok{"Friday"}\NormalTok{,}
        \StringTok{"Saturday"}\NormalTok{,}
        \StringTok{"Sunday"}
\NormalTok{      ),}
    \AttributeTok{some\_value =} \FunctionTok{c}\NormalTok{(}\FunctionTok{sample.int}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\NormalTok{  )}

\NormalTok{days\_data }\OtherTok{\textless{}{-}}
\NormalTok{  days\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{days\_as\_factor =} \FunctionTok{factor}\NormalTok{(days),}
    \AttributeTok{days\_as\_factor =} \FunctionTok{fct\_relevel}\NormalTok{(}
\NormalTok{      days,}
      \StringTok{"Monday"}\NormalTok{,}
      \StringTok{"Tuesday"}\NormalTok{,}
      \StringTok{"Wednesday"}\NormalTok{,}
      \StringTok{"Thursday"}\NormalTok{,}
      \StringTok{"Friday"}\NormalTok{,}
      \StringTok{"Saturday"}\NormalTok{,}
      \StringTok{"Sunday"}
\NormalTok{    )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

And we can compare the results by graphing first with the original
character vector on the x-axis, and then another graph with the factor
vector on the x-axis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{days\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ days, }\AttributeTok{y =}\NormalTok{ some\_value)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}

\NormalTok{days\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ days\_as\_factor, }\AttributeTok{y =}\NormalTok{ some\_value)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{./03-r_essentials_files/figure-pdf/unnamed-chunk-158-1.pdf}

}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{./03-r_essentials_files/figure-pdf/unnamed-chunk-158-2.pdf}

}

\end{figure}

\hypertarget{exercises-and-tutorial-2}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-2}}

\hypertarget{exercises-2}{%
\subsection{Exercises}\label{exercises-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is R?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A open-source statistical programming language
  \item
    A programming language created by Guido van Rossum
  \item
    A closed source statistical programming language
  \item
    An integrated development environment (IDE)
  \end{enumerate}
\item
  What are three advantages of R? What are three disadvantages?
\item
  What is R Studio?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    An integrated development environment (IDE).
  \item
    A closed source paid program.
  \item
    A programming language created by Guido van Rossum
  \item
    A statistical programming language.
  \end{enumerate}
\item
  What is the class of the output of \texttt{2\ +\ 2} (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    character
  \item
    factor
  \item
    numeric
  \item
    date
  \end{enumerate}
\item
  Say we had run:
  \texttt{my\_name\ \textless{}-\ \textquotesingle{}Rohan\textquotesingle{}}.
  What would be the result of running \texttt{print(my\_name)} (pick
  one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `Edward'
  \item
    `Monica'
  \item
    `Hugo'
  \item
    `Rohan'
  \end{enumerate}
\item
  Say we had a dataset with two columns: `name', and `age'. Which verb
  should we use to pick just `name' (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{tidyverse::select()}
  \item
    \texttt{tidyverse::mutate()}
  \item
    \texttt{tidyverse::filter()}
  \item
    \texttt{tidyverse::rename()}
  \end{enumerate}
\item
  Say we had loaded \texttt{AustralianPoliticians} and
  \texttt{tidyverse} and then run the following code:
  \texttt{australian\_politicians\ \textless{}-\ AustralianPoliticians::get\_auspol(\textquotesingle{}all\textquotesingle{})}.
  How could we select all of the columns that end with `Name' (pick
  one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{australian\_politicians\ \textbar{}\textgreater{}\ select(contains("Name"))}
  \item
    \texttt{australian\_politicians\ \textbar{}\textgreater{}\ select(starts\_with("Name"))}
  \item
    \texttt{australian\_politicians\ \textbar{}\textgreater{}\ select(matches("Name"))}
  \item
    \texttt{australian\_politicians\ \textbar{}\textgreater{}\ select(ends\_with("Name"))}
  \end{enumerate}
\item
  Under what circumstances, in terms of the names of the columns, would
  the use of \texttt{contains()} potentially give different answers to
  using \texttt{ends\_with()} in the above question?
\item
  Which of the following are not \texttt{tidyverse} verbs (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{select()}
  \item
    \texttt{filter()}
  \item
    \texttt{arrange()}
  \item
    \texttt{mutate()}
  \item
    \texttt{visualize()}
  \end{enumerate}
\item
  Which function would make a new column (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{select()}
  \item
    \texttt{filter()}
  \item
    \texttt{arrange()}
  \item
    \texttt{mutate()}
  \item
    \texttt{visualize()}
  \end{enumerate}
\item
  Which function would focus on particular rows (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{select()}
  \item
    \texttt{filter()}
  \item
    \texttt{arrange()}
  \item
    \texttt{mutate()}
  \item
    \texttt{summarise()}
  \end{enumerate}
\item
  Which combination of two functions could provide a mean of a dataset,
  by sex (pick two)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{summarise()}
  \item
    \texttt{filter()}
  \item
    \texttt{arrange()}
  \item
    \texttt{mutate()}
  \item
    \texttt{group\_by()}
  \end{enumerate}
\item
  Assume a variable called `age' is an integer. Which line of code would
  create a column that is its exponential (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{mutate(exp\_age\ =\ exponential(age))}
  \item
    \texttt{mutate(exp\_age\ =\ exponent(age))}
  \item
    \texttt{mutate(exp\_age\ =\ exp(age))}
  \item
    \texttt{mutate(exp\_age\ =\ expon(age))}
  \end{enumerate}
\item
  Assume a column called `age'. Which line of code could create a column
  that contains the value from five rows above?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{mutate(five\_before\ =\ lag(age))}
  \item
    \texttt{mutate(five\_before\ =\ lead(age))}
  \item
    \texttt{mutate(five\_before\ =\ lag(age,\ n\ =\ 5))}
  \item
    \texttt{mutate(five\_before\ =\ lead(age,\ n\ =\ 5))}
  \end{enumerate}
\item
  What would be the output of
  \texttt{class(\textquotesingle{}edward\textquotesingle{})} (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `numeric'
  \item
    `character'
  \item
    `data.frame'
  \item
    `vector'
  \end{enumerate}
\item
  Which function would enable us to draw once from three options `blue,
  white, red', with 10 per cent probability on `blue' and `white', and
  the remainder on `red'?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{sample(c(\textquotesingle{}blue\textquotesingle{},\ \textquotesingle{}white\textquotesingle{},\ \textquotesingle{}red\textquotesingle{}),\ prob\ =\ c(0.1,\ 0.1,\ 0.8))}
  \item
    \texttt{sample(c(\textquotesingle{}blue\textquotesingle{},\ \textquotesingle{}white\textquotesingle{},\ \textquotesingle{}red\textquotesingle{}),\ size\ =\ 1)}
  \item
    \texttt{sample(c(\textquotesingle{}blue\textquotesingle{},\ \textquotesingle{}white\textquotesingle{},\ \textquotesingle{}red\textquotesingle{}),\ size\ =\ 1,\ prob\ =\ c(0.8,\ 0.1,\ 0.1))}
  \item
    \texttt{sample(c(\textquotesingle{}blue\textquotesingle{},\ \textquotesingle{}white\textquotesingle{},\ \textquotesingle{}red\textquotesingle{}),\ size\ =\ 1,\ prob\ =\ c(0.1,\ 0.1,\ 0.8))}
  \end{enumerate}
\item
  Which code simulates 10,000 draws from a normal distribution with a
  mean of 27 and a standard deviation of 3 (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{rnorm(10000,\ mean\ =\ 27,\ sd\ =\ 3)}
  \item
    \texttt{rnorm(27,\ mean\ =\ 10000,\ sd\ =\ 3)}
  \item
    \texttt{rnorm(3,\ mean\ =\ 10000,\ sd\ =\ 27)}
  \item
    \texttt{rnorm(27,\ mean\ =\ 3,\ sd\ =\ 1000)}
  \end{enumerate}
\item
  What are the three key aspects of the grammar of graphics (select
  all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    data
  \item
    aesthetics
  \item
    type
  \item
    \texttt{geom\_histogram()}
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial-2}{%
\subsection{Tutorial}\label{tutorial-2}}

\begin{quote}
I think we should be suspicious when we find ourselves attracted to
data---very, very thin and weak data---that seem to justify beliefs that
have held great currency in lots of societies throughout history, in a
way that is conducive to the oppression of large segments of the
population

Amia Srinivasan, 22 September 2021
\end{quote}

Reflect on the quote from Amia Srinivasan, Chichele Professor of Social
and Political Theory, All Souls College, Oxford, and D'Ignazio and Klein
(2020), especially Chapter 6, and spend at least two pages discussing
them in relation to a dataset that you are familiar with.

\hypertarget{sec-reproducible-workflows}{%
\chapter{Reproducible workflows}\label{sec-reproducible-workflows}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{What has happened down here is the winds have changed},
  (Gelman 2016)
\item
  Read \emph{Good enough practices in scientific computing}, (Wilson et
  al. 2017)
\item
  Watch \emph{Overcoming barriers to sharing code}, (M. Alexander 2021)
\item
  Watch \emph{Make a reprex\ldots{} Please}, (Gelfand 2021)
\item
  Read \emph{The tidyverse style guide}, (Wickham 2021b)
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Reproducibility is a requirement for data science, and this implies
  sharing data, code, and environment.
\item
  Reproducibility is enhanced by using Quarto, R Projects, and Git and
  GitHub.

  \begin{itemize}
  \tightlist
  \item
    Quarto involves marking text as a certain type, and then building a
    document that could contain R code.
  \item
    R Projects enable a file structure that is not dependent on a
    directory set-up that is user-specific.
  \item
    Git and GitHub make it easier to share code and data.

    \begin{itemize}
    \tightlist
    \item
      Get the latest changes from GitHub: `Pull'.
    \item
      Add your updates: `Add' and `Commit'.
    \item
      Push your changes to GitHub: `Push'.
    \end{itemize}
  \end{itemize}
\item
  Restart R often (`Session' -\textgreater{} `Restart R and Clear
  Output').
\item
  Debugging is a skill, that improves with practice.
\item
  One key debugging skill is being able to make a reproducible example
  that reproduces the issue for others.
\item
  Appropriate code structure and comments are a critical aspect of
  reproducibility because they help others understand what was done.
\end{itemize}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\begin{quote}
Suppose you have cancer and you have to choose between a black box AI
surgeon that cannot explain how it works but has a 90\% cure rate and a
human surgeon with an 80\% cure rate. Do you want the AI surgeon to be
illegal?

Geoffrey Hinton, 20 February 2020.
\end{quote}

\begin{quote}
The number one thing to keep in mind about machine learning is that
performance is evaluated on samples from one dataset, but the model is
used in production on samples that may not necessarily follow the same
characteristics\ldots{} The finance industry has a saying for this:
``past performance is no guarantee of future results''. Your model
scoring X on your test dataset doesn't mean it will perform at level X
on the next N situations it encounters in the real world. The future may
not be like the past.

So when asking the question, ``would you rather use a model that was
evaluated as 90\% accurate, or a human that was evaluated as 80\%
accurate'', the answer depends on whether your data is typical per the
evaluation process. Humans are adaptable, models are not. If significant
uncertainty is involved, go with the human. They may have inferior
pattern recognition capabilities (versus models trained on enormous
amounts of data), but they understand what they do, they can reason
about it, and they can improvise when faced with novelty

If every possible situation is known and you want to prioritize
scalability and cost-reduction, go with the model. Models exist to
encode and operationalize human cognition in well-understood situations.
(``well understood'' meaning either that it can be explicitly described
by a programmer, or that you can amass a dataset that densely samples
the distribution of possible situations -- which must be static)

François Chollet, 20 February 2020.
\end{quote}

If science is about systematically building and organizing knowledge in
terms of testable explanations and predictions, then data science takes
this and focuses on data. This means that building, organizing, and
sharing knowledge is a critical aspect. Creating knowledge, once, in a
way that only you can do it, does not meet this standard. Hence, the
need for reproducible data science workflows.

M. Alexander (2019a) talks about how reproducible research means it can
be exactly redone, given all the materials used. This underscores the
importance of providing the code, data, and environment. The minimum
expectation is that another person is independently able to use your
code, data, and environment, to get your results, including figures and
tables. Ironically there are different definitions of reproducibility
between disciplines. Barba (2018) surveys a variety of disciplines and
concludes that the predominant language usage implies the following
definitions: Reproducible research is when `{[}a{]}uthors provide all
the necessary data and the computer codes to run the analysis again,
re-creating the results.' A replication is a study `that arrives at the
same scientific findings as another study, collecting new data (possibly
with different methods) and completing new analyses.'

Regardless of what it is specifically called, Gelman (2016) identifies
how large an issue this is in various social sciences. The problem with
work that is not reproducible, is that it does not contribute to our
stock of knowledge about the world. Since Gelman (2016), a great deal of
work has been done in many social sciences and the situation has
improved a little, but much work remains. And the situation is similar
in the life sciences (Heil et al. 2021) and computer science (Pineau et
al. 2021).

Some of the examples that Gelman (2016) talks about, which turned out to
not reproduce, such as himmicanes and power pose, are not that important
in the scheme of things. But at the same time, we saw, and continue to
see, similar approaches being used in areas with big impacts. For
instance, many governments have created `nudge' units that implement
public policy (Sunstein and Reisch 2017). Governments are increasingly
using algorithms that they do not make open (Chouldechova et al. 2018).
And Herndon, Ash, and Pollin (2014) document how a paper in economics
that was used by governments to justify austerity policies following the
Global Financial Crisis, turned out to not be reproducible.

At a minimum, and with few exceptions, we must release our code,
datasets, and environment. Without these, it is difficult to know what a
finding speaks to (Miyakawa 2020). More banally, we also do not know if
there are mistakes or aspects that were inadvertently overlooked (Merali
2010; Hillel 2017; Silver 2020). Increasingly, we consider a paper to be
an advertisement, and for the associated code, data, and environment to
be the actual work. Steve Jobs, a co-founder of Apple, talked about how
the best craftsmen ensure that even the aspects of their work that no
one else will ever see are as well-finished and high-quality as the
aspects that are public facing (Isaacson 2011). The same is true in data
science, where often one of the distinguishing aspects of high-quality
work is that the README and code comments are as polished as the
abstract of the associated paper.

Workflows exist within a cultural and social context, which imposes an
additional reason for the need for them to be reproducible. For
instance, Y. Wang and Kosinski (2018) use deep neural networks to train
a model to distinguish between gay and heterosexual men. (Murphy (2017)
provides a summary of the paper, the associated issues, and comments
from its authors.) To do this, Y. Wang and Kosinski (2018, 248) needed a
dataset of photos of folks that were `adult, Caucasian, fully visible,
and of a gender that matched the one reported on the user's profile'.
They verified this using Amazon Mechanical Turk, an online platform that
pays workers a small amount of money to complete specific tasks. The
instructions provided to the Mechanical Turk workers for this task
specify that Obama, who had a white mother and a black father, should be
classified as `Black'; and that Latino is an ethnicity, rather than a
race (Mattson 2017). The classification task may seem objective, but,
perhaps unthinkingly, echoes the views of Americans with a certain class
and background.

This is just one specific concern about one part of the Y. Wang and
Kosinski (2018) workflow. Broader concerns are raised by others
including Gelman, Mattson, and Simpson (2018). The main issue is that
statistical models are specific to the data on which they were trained.
And the only reason that we can identify likely issues in the model of
Y. Wang and Kosinski (2018) is because, despite not releasing the
specific dataset that they used, they were nonetheless open about their
procedure. For our work to be credible, it needs to be reproducible by
others.

Some of the steps that we can take to make our work more reproducible
include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensure the entire workflow is documented. This may involve addressing
  questions such as:

  \begin{itemize}
  \tightlist
  \item
    How was the raw dataset obtained and is access likely to be
    persistent and available to others?
  \item
    What specific steps are being taken to transform the raw data in the
    data that were analyzed, and how can this be made available to
    others?
  \item
    What analysis has been done, and how clearly can this be shared?
  \item
    How has the final paper or report been built and to what extent can
    others follow that process themselves?
  \end{itemize}
\item
  Not worrying about perfect reproducibility initially, but instead
  focusing on trying to improve with each successive project. For
  instance, each of the following requirements are increasingly more
  onerous and there is no need to be concerned about not being able to
  the last, until we can do the first:

  \begin{itemize}
  \tightlist
  \item
    Can you run your entire workflow again?
  \item
    Can `another person' run your entire workflow again?
  \item
    Can `future-you' run your entire workflow again?
  \item
    Can `future-another-person' run your entire workflow again?
  \end{itemize}
\item
  Including a detailed discussion about the limitations of the dataset
  and the approach in the final paper or report.
\end{enumerate}

The workflow that we follow is illustrated in Figure~\ref{fig-workflow}.
But it can be more concisely summarized as: `Think an awful lot, mostly
read and write, sometimes code'.

\begin{figure}

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{./figures/IMG_1847.png}

}

\caption{\label{fig-workflow}Workflow for telling stories with data}

\end{figure}

There are various tools that we can use at the different stages that
will improve the reproducibility of this workflow. This includes Quarto,
R Projects, and Git and GitHub.

\hypertarget{quarto}{%
\section{Quarto}\label{quarto}}

\hypertarget{getting-started-1}{%
\subsection{Getting started}\label{getting-started-1}}

Quarto integrates code and natural language in a way that is called
`literate programming' (D. E. Knuth 1984). It uses a mark-up language
similar to HyperText Markup Language (HTML) or LaTeX, in comparison to a
`What You See Is What You Get' (WYSIWYG) language, such as Microsoft
Word. This means that all the aspects are consistent, for instance, all
top-level heading will look the same. But it means that we use symbols
to designate how we would like certain aspects to appear. And it is only
when we build the mark-up that we get to see what it looks like. A
visual editor option can also be used which hides the need for the user
to do this mark-up themselves.

Quarto replaced R Markdown, which was a variant of Markdown specifically
designed to allow R code chunks to be included. One advantage of
literate programming is that we get a `live' document in which code
executes and then forms part of the document. Another advantage of
Quarto is that very similar code can compile into a variety of
documents, including HTML pages and PDFs. Quarto also has default
options set up for including title, author, and date sections. One
disadvantage is that it can take a while for a document to compile
because all the code needs to run. N. Tierney (2022) provides an
especially useful and detailed Quarto usage guide.

We can create a new Quarto document within R Studio (`File'
-\textgreater{} `New File' -\textgreater{} `Quarto Document\ldots{}').

\hypertarget{essential-commands}{%
\subsection{Essential commands}\label{essential-commands}}

Essential markdown commands include those for emphasis, headers, lists,
links, and images. A reminder of these is included in R Studio (`Help'
-\textgreater{} `Markdown Quick Reference'). It is your choice as to
whether you want to use the visual or source editor. But either way, it
is good to understand these essentials because it will not always be
possible to use a visual editor, for instance if you are quickly looking
at a Quarto document in GitHub.

\begin{itemize}
\tightlist
\item
  Emphasis: \texttt{*italic*}, \texttt{**bold**}
\item
  Headers (these go on their own line with a blank line before and
  after): \texttt{\#\ First\ level\ header}
  \texttt{\#\#\ Second\ level\ header}
  \texttt{\#\#\#\ Third\ level\ header}
\item
  Unordered list, with sub-lists:
\end{itemize}

\begin{verbatim}
* Item 1
* Item 2
    + Item 2a
    + Item 2b
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Ordered list, with sub-lists:
\end{itemize}

\begin{verbatim}
1. Item 1
2. Item 2
3. Item 3
    + Item 3a
    + Item 3b
\end{verbatim}

\begin{itemize}
\tightlist
\item
  URLs can be added by linking text
  \texttt{{[}the\ address\ of\ this\ book{]}(https://www.tellingstorieswithdata.com)}
  results in \href{https://www.tellingstorieswithdata.com}{the address
  of this book}.
\item
  A paragraph is created by leaving a blank line.
\end{itemize}

\begin{verbatim}
A paragraph about some idea, nicely spaced from the following paragraph.

Another paragraph about another idea, nicely spaced from the earlier paragraph.
\end{verbatim}

Once we have added some aspects, then we may want to see the actual
document. To build the document click `Render'.

\hypertarget{r-chunks}{%
\subsection{R chunks}\label{r-chunks}}

We can include code for R and many other languages in code chunks within
a Quarto document. Then when we render the document, the code will run
and be included in the document.

To create an R chunk, we start with three backticks and then within
curly braces we tell Quarto that this is an R chunk. Anything inside
this chunk will be considered R code and run as such. For instance, we
could load the \texttt{tidyverse} and \texttt{AER} and make a graph of
the number of times a survey respondent visited the doctor in the past
two weeks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\NormalTok{library(tidyverse)}
\NormalTok{library(AER)}

\NormalTok{data("DoctorVisits", package = "AER")}

\NormalTok{DoctorVisits |\textgreater{}}
\NormalTok{  ggplot(aes(x = illness)) +}
\NormalTok{  geom\_histogram(stat = "count")}
\NormalTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

The output of that code is Figure~\ref{fig-doctervisits}.

\begin{figure}

{\centering \includegraphics{./04-workflow_files/figure-pdf/fig-doctervisits-1.pdf}

}

\caption{\label{fig-doctervisits}Number of illnesses in the past two
weeks, based on the 1977--1978 Australian Health Survey}

\end{figure}

There are various evaluation options that are available in chunks. We
include these, each on a new line, by opening the line with the
chunk-specific comment delimiter `\#\textbar{}' and then the option.
Helpful options include:

\begin{itemize}
\tightlist
\item
  \texttt{echo:\ false}: run the code and include the output, but do not
  print the code in the document.
\item
  \texttt{include:\ false}: run the code but do not output anything and
  do not print the code in the document.
\item
  \texttt{eval:\ false}: do not run the code, and hence do not include
  the outputs, but do print the code in the document.
\item
  \texttt{warning:\ false}: do not display warnings.
\item
  \texttt{message:\ false}: do not display messages.
\end{itemize}

For instance, we could include the output, but not the code, and
suppress any warnings.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\NormalTok{\#| echo: false}
\NormalTok{\#| warning: false}

\NormalTok{library(tidyverse)}
\NormalTok{library(AER)}

\NormalTok{data("DoctorVisits", package = "AER")}

\NormalTok{DoctorVisits \%\textgreater{}\%}
\NormalTok{  ggplot(aes(x = visits)) +}
\NormalTok{  geom\_histogram(stat = "count")}
\NormalTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

It is important to leave a blank line on either side of an R chunk,
otherwise it may not run properly. It is also important that lower case
is used, i.e.~`true' not `TRUE'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Most people did not visit a doctor in the past week.}

\NormalTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\NormalTok{\#| echo: false}
\NormalTok{\#| warning: false}

\NormalTok{library(tidyverse)}
\NormalTok{library(AER)}

\NormalTok{data("DoctorVisits", package = "AER")}

\NormalTok{DoctorVisits \%\textgreater{}\%}
\NormalTok{  ggplot(aes(x = visits)) +}
\NormalTok{  geom\_histogram(stat = "count")}
\NormalTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}

\NormalTok{There were some people that visited a doctor once, and then very few people that visited two or more times.}
\end{Highlighting}
\end{Shaded}

It is also important that the Quarto document itself loads any datasets
that are needed. It is not enough that they are in the environment. This
is because the Quarto document evaluates the code in the document when
it is built, not necessarily the environment.

\hypertarget{top-matter}{%
\subsection{Top matter}\label{top-matter}}

Top matter consists of defining aspects such as the title, author, and
date. It is contained within three dashes at the top of a Quarto
document. For instance, the following would specify a title, date that
automatically updated to the date the document was rendered, and an
author.

\begin{verbatim}
---
title: "My document"
author: "Rohan Alexander"
date: "19 April 2022"
format: html
---
\end{verbatim}

An abstract is a short summary of the paper, and we could add that to
the top matter as well.

\begin{verbatim}
---
title: "My document"
author: "Rohan Alexander"
date: "19 April 2022"
abstract: "This is my abstract."
format: html
---
\end{verbatim}

By default, Quarto will create an HTML document, but we can change the
output format to produce a PDF. This uses LaTeX in the background and
may require the installation of supporting packages. In particular it is
common to need to first install \texttt{tinytex} (Xie 2019).

\begin{verbatim}
---
title: "My document"
author: "Rohan Alexander"
date: "19 April 2022"
abstract: "This is my abstract."
format: pdf
---
\end{verbatim}

We can include references by specifying a BibTeX file in the top matter
and then calling it within the text, as needed.

\begin{verbatim}
---
title: "My document"
author: "Rohan Alexander"
date: 1 January 2022
format: pdf
abstract: "This is my abstract."
bibliography: bibliography.bib
---
\end{verbatim}

We would need to make a separate file called `bibliography.bib' and save
it next to the Quarto file. In the BibTeX file we need an entry for the
item that is to be referenced. For instance, the citation for R can be
obtained with \texttt{citation()} and this can be added to the
`bibliography.bib' file. Similarly, the citation for a package can be
found by including the package name, for instance
\texttt{citation(\textquotesingle{}tidyverse\textquotesingle{})}. It can
be helpful to use Google Scholar, or doi2bib, to get citations for books
or articles.

\begin{verbatim}
@Manual{,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2021},
    url = {https://www.R-project.org/},
  }
@Article{,
    title = {Welcome to the {tidyverse}},
    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
    year = {2019},
    journal = {Journal of Open Source Software},
    volume = {4},
    number = {43},
    pages = {1686},
    doi = {10.21105/joss.01686},
  }
\end{verbatim}

We need to create a unique key that we use to refer to this item in the
text. This can be anything, provided it is unique, but meaningful ones
can be easier to remember, for instance `citeR'.

\begin{verbatim}
@Manual{citeR,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2021},
    url = {https://www.R-project.org/},
  }
@Article{citetidyverse,
    title = {Welcome to the {tidyverse}},
    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
    year = {2019},
    journal = {Journal of Open Source Software},
    volume = {4},
    number = {43},
    pages = {1686},
    doi = {10.21105/joss.01686},
  }
\end{verbatim}

To cite R in the Quarto document we include \texttt{@citeR}, which would
put the brackets around the year, like this: R Core Team (2021) or
\texttt{{[}@citeR{]}}, which would put the brackets around the whole
thing, like this: (R Core Team 2021).

The reference list at the end of the paper is automatically built based
on calling the BibTeX file and including the references in the paper. At
the end of the Quarto document, including a heading `\# References' and
the actual citations will be included after that. When the Quarto file
is rendered, Quarto sees these in the content, goes to BibTeX to get the
reference details that it needs, builds the reference list, and then
adds it to the end of the rendered document.

\hypertarget{cross-references}{%
\subsection{Cross-references}\label{cross-references}}

It can be useful to cross-reference figures, tables, and equations. This
makes it easier to refer to them in the text. To do this for a figure we
refer to the name of the R chunk that creates or contains the figure.
For instance, if we had the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\NormalTok{\#| label: fig{-}uniquename}
\NormalTok{\#| fig{-}cap: Number of illnesses in the past two weeks, based on the 1977{-}{-}1978 Australian Health Survey}
\NormalTok{\#| echo: true}
\NormalTok{\#| warning: false}

\NormalTok{library(tidyverse)}
\NormalTok{library(AER)}

\NormalTok{data("DoctorVisits", package = "AER")}

\NormalTok{DoctorVisits |\textgreater{}}
\NormalTok{  ggplot(aes(x = illness)) +}
\NormalTok{  geom\_histogram(stat = "count")}
\NormalTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

Then \texttt{(@fig-uniquename)} would produce:
(Figure~\ref{fig-uniquename}) as the name of the R chunk is
\texttt{fig-uniquename}. We need to add `fig' to the start of the chunk
name so that Quarto knows that this is a figure. We then include a
`fig-cap:' in the R chunk that specifies a caption.

\begin{figure}

{\centering \includegraphics{./04-workflow_files/figure-pdf/fig-uniquename-1.pdf}

}

\caption{\label{fig-uniquename}Number of illnesses in the past two
weeks, based on the 1977--1978 Australian Health Survey}

\end{figure}

We can take a similar approach to cross-reference tables. For instance,
\texttt{(@tbl-docvisittable)} will produce:
(Table~\ref{tbl-docvisittable}). In this case we specify `tbl' at the
start of the label so that Quarto knows that it is a table. And we
specify a caption for the table with `tbl-cap:'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\NormalTok{\#| label: tbl{-}docvisittable}
\NormalTok{\#| echo: true}
\NormalTok{\#| tbl{-}cap: "Number of visits to the doctor in the past two weeks, based on the 1977{-}{-}1978 Australian Health Survey"}

\NormalTok{DoctorVisits |\textgreater{} }
\NormalTok{  count(visits) |\textgreater{} }
\NormalTok{  knitr::kable()}
\NormalTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-docvisittable}{}
\begin{longtable}[]{@{}rr@{}}
\caption{\label{tbl-docvisittable}Number of visits to the doctor in the
past two weeks, based on the 1977--1978 Australian Health
Survey}\tabularnewline
\toprule()
visits & n \\
\midrule()
\endfirsthead
\toprule()
visits & n \\
\midrule()
\endhead
0 & 4141 \\
1 & 782 \\
2 & 174 \\
3 & 30 \\
4 & 24 \\
5 & 9 \\
6 & 12 \\
7 & 12 \\
8 & 5 \\
9 & 1 \\
\bottomrule()
\end{longtable}

Finally, we can also cross-reference equations. To that we need to add a
tag \texttt{\{\#eq-macroidentity\}} which we then reference.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$$}
\NormalTok{Y = C + I + G + (X {-} M)}
\NormalTok{$$ \{\#eq{-}macroidentity\}}
\end{Highlighting}
\end{Shaded}

For instance, we then use \texttt{@eq-macroidentity} to produce
Equation~\ref{eq-macroidentity}.

\begin{equation}\protect\hypertarget{eq-macroidentity}{}{
Y = C + I + G + (X - M)
}\label{eq-macroidentity}\end{equation}

When using cross-references, it is important that the labels are
relatively simple. In general, try to keep the names simple but unique,
avoid punctuation and stick to letters and hyphens. Do not use
underbars, because that can cause an error.

\hypertarget{r-projects-and-file-structure}{%
\section{R projects and file
structure}\label{r-projects-and-file-structure}}

Projects are widely used in software development and exist to keeps all
the files (data, analysis, report, etc) associated with a particular
project together and related to each other. An R project can be created
in R Studio `File' -\textgreater{} `New Project', then select `Empty
project', name the project and decide where to save it. For instance, a
project focused on maternal mortality, may be called
`maternalmortality', and might be saved within a folder of other
projects. The use of R projects enables `reliable, polite behavior
across different computers or users and over time.' (Jennifer Bryan and
Hester 2020). This is because it removes the context of that folder from
its broader existence. So files exist in relation to the base of the R
project, not the base of the computer.

Once a project has been created, a new file with the extension `.RProj'
will appear in that folder. As an example, of a folder with an R
Project, an example Quarto document, and an appropriate file structure
is available
\href{https://github.com/RohanAlexander/starter_folder}{here}. That can
be downloaded: `Code' -\textgreater{} `Download ZIP'.

The main advantage of using an R Project is that we are more easily able
to reference other files in a self-contained way. That means when others
want to reproduce our work, they know that all the file references and
structure should not need to be changed. It means that files are
referenced in relation to where the `.Rproj' file is. For instance,
instead of reading a csv from, say,
\texttt{"\textasciitilde{}/Documents/projects/book/data/"} you can read
it in from \texttt{book/data/}. It may be that someone else does not
have a `projects' folder, and so the former would not work for them,
while the latter would.

The use of R projects is required to meet the minimal level of
reproducibility. The use of functions such as \texttt{setwd()}, and
computer-specific file paths, bind work to a specific computer in a way
that is not appropriate. Trisovic et al. (2022) describe the use of
absolute paths, rather than relative paths, as a common error that they
had to correct in their large-scale study of R code, uploaded to the
Harvard Dataverse, that underpins research papers.

There are a variety of ways to set-up a folder. A variant of Wilson et
al. (2017) that is often useful is shown in the
\href{https://github.com/RohanAlexander/starter_folder}{example}. Here
we have an `inputs' folder that contains raw data (which should never be
modified (Wilson et al. 2017)) and literature related to the project
(which cannot be modified). An `outputs' folder contains data that we
create using R, as well as the paper that we are writing. And a
`scripts' folder is what modifies the raw data and saves it into
`outputs'. We will do most of our work in `scripts', and the Quarto file
for the paper in `outputs'. Useful other aspects include a `README.md'
which will specify overview details about the project, and a LICENSE.
Another helpful variant of this project skeleton is provided by Mineault
and The Good Research Code Handbook Community (2021).

\hypertarget{version-control}{%
\section{Version control}\label{version-control}}

We implement version control through a combination of Git and GitHub.
There are a variety of reasons for this including:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  enhance the reproducibility of work by making it easier to share code
  and data;
\item
  make it easier to share work;
\item
  improve workflow by encouraging systematic approaches; and
\item
  make it easier to work in teams.
\end{enumerate}

Git is a version control system. The way one often starts doing version
control is to have various versions of the one file: `first\_go.R',
`first\_go-fixed.R', `first\_go-fixed-with-mons-edits.R'. But this soon
becomes cumbersome. One often soon turns to dates, for instance:
`2022-01-01-analysis.R', `2022-01-02-analysis.R',
`2022-01-03-analysis.R', etc. While this keeps a record it can be
difficult to search when we need to go back, because it can be difficult
to remember the date some change was made. In any case, it quickly gets
unwieldy for a project that is being regularly worked on.

Instead of this, we use Git so that we can have one version of the file,
say, `analysis.R' and then use Git to keep a record of the changes to
that file, and a snapshot of that file at a given point in time. We
determine when Git takes that snapshot, and when we take that snapshot.
We additionally include a message saying what changed between this
snapshot and the last. In that way, there is only ever one version of
the file, but the history can be more easily searched.

One complication is that Git was designed for teams of software
developers. As such, while it works, it can be a little ungainly for
non-developers. But in general it is the case that Git has been usefully
adapted for data science, even when the only collaborator one may have
is one's future self (Jennifer Bryan 2018).

GitHub, GitLab, and various other companies offer easier-to-use services
that build on Git. While there are tradeoffs, we introduce GitHub here
because it is the predominant platform (Eghbal 2020, 21). Git and GitHub
are built into R Studio Cloud, which provides a nice option if you have
issues with local installation. One of the initial challenging aspects
of Git is the terminology. Folders are called `repos'. Creating a
snapshot is called a `commit'. One gets used to it eventually, but
feeling confused initially is normal. Jenny Bryan (2020) is especially
useful for setting up and using Git and GitHub.

\hypertarget{git}{%
\subsection{Git}\label{git}}

We first need to git check whether Git is installed. Open R Studio, go
to the Terminal, type the following, and then enter/return.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git} \AttributeTok{{-}{-}version}
\end{Highlighting}
\end{Shaded}

If you get a version number, then you are done
(Figure~\ref{fig-gitone}).

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/04-git-terminal-access-cropped.png}

}

\caption{\label{fig-gitone}Using Terminal to check whether Git is
installed in R Studio}

\end{figure}

Git is pre-installed in R Studio Cloud, it should be pre-installed on
Mac, and it may be pre-installed on Windows. If you do not get a version
number in response, then you need to install it. To do that you should
follow the instructions specific to your operating system in Jenny Bryan
(2020, chap. 5).

Given Git is installed we need to tell it our username and email. We
need to do this because Git adds this information whenever we take a
`snapshot', or to use Git's language, whenever we make a commit.

Again, within the Terminal, type the following, replacing the details
with yours, and then enter/return after each line.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{git}\NormalTok{ config }\AttributeTok{{-}{-}global}\NormalTok{ user.name }\StringTok{\textquotesingle{}Rohan Alexander\textquotesingle{}}
\FunctionTok{git}\NormalTok{ config }\AttributeTok{{-}{-}global}\NormalTok{ user.email }\StringTok{\textquotesingle{}rohan.alexander@utoronto.ca\textquotesingle{}}
\FunctionTok{git}\NormalTok{ config }\AttributeTok{{-}{-}global} \AttributeTok{{-}{-}list}
\end{Highlighting}
\end{Shaded}

When this set-up has been done properly, the values that you entered for
`user.name' and `user.email' will be returned after the last line
(Figure~\ref{fig-gitone-setup}).

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/04-git-setup-cropped.png}

}

\caption{\label{fig-gitone-setup}Adding a username and email address to
Git in R Studio}

\end{figure}

These details--username and email address--will be public. There are
various ways to hide the email address if necessary, and GitHub provides
instructions about this. Jenny Bryan (2020, chap. 7) provides more
detailed instructions about this step, and a trouble-shooting guide.

\hypertarget{github}{%
\subsection{GitHub}\label{github}}

Now that Git is set-up, we need to set-up GitHub. We created an account
in Chapter~\ref{sec-fire-hose}, which we use again here. After being
signed in we first need to make a new folder, which is called a `repo'
in Git. Look for a `+' in the top right, and then select `New
Repository' (Figure~\ref{fig-githubtwo}).

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/github_2.png}

}

\caption{\label{fig-githubtwo}Start process of creating a new
repository}

\end{figure}

At this point we can add a sensible name for the repo. Leave it as
`public' for now, because it can always be deleted later. And check the
box to `Initialize this repository with a README'. Change `Add
.gitignore' to R. After that, click `Create repository'
(Figure~\ref{fig-githubthree}).

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/github_3.png}

}

\caption{\label{fig-githubthree}Creating a new repository in GitHub}

\end{figure}

This will take us to a screen that is fairly empty, but the details that
we need are in the green `Clone or Download' button, which we can copy
by clicking the clipboard (Figure~\ref{fig-githubfour}).

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/github_4.png}

}

\caption{\label{fig-githubfour}Copy the URL of the new repository}

\end{figure}

Now returning to R Studio, in R Studio Cloud, we create a `New Project'
using `New Project from Git Repository'. It will ask for the URL that we
just copied (Figure~\ref{fig-githubnewproject}). If you are using a
local machine, then this same step is accomplished through the menu:
`File' -\textgreater{} `New Project\ldots{}' -\textgreater{} `Version
Control' -\textgreater{} `Git', then paste in the URL, give the folder a
meaningful name, check `Open in new session', then `Create Project'.

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/04-github-new-project.png}

}

\caption{\label{fig-githubnewproject}Adding the project to R Studio
Cloud}

\end{figure}

At this point, a new folder has been created locally that we can use. We
will want to be able to push it back to GitHub, and for that we will
need to use a Personal Access Token (PAT) to link our R Studio Workspace
with our GitHub account. To create a PAT, while signed into GitHub in
the browser, run \texttt{usethis::create\_github\_token()} in your R
session. GitHub will open in the browser with various options filled out
(Figure~\ref{fig-githubpat}). It can be useful to give the PAT an
informative name by replacing `Note', for instance `PAT for R Studio',
then `Generate token'.

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/04-githubpat.png}

}

\caption{\label{fig-githubpat}Creating a PAT}

\end{figure}

We only have one shot to copy this token, and if we make a mistake then
we will need to generate a new one. Do not include the PAT in any R
script or Quarto document. Instead run
\texttt{gitcreds::gitcreds\_set()}, which will then prompt you to add
your PAT in the console.

To use GitHub for a project that we are actively working on we follow a
procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The first thing to do is almost always to get any changes with `pull'.
  To do this, open the Git pane in R Studio, and click the blue down
  arrow. This gets any changes to the folder, as it is on GitHub, into
  our own version of the folder.
\item
  We can then make our changes to our copy of the folder. For instance,
  we could update the README, and then save it as normal.
\item
  Once this is done, we need to `add', `commit', and `push'. In the Git
  pane in R Studio, select the files to be added. This adds them to the
  staging area. Then click `Commit' (Figure~\ref{fig-githubadd}). A new
  window will open. Add a commit message which is informative about the
  change that was made, and then click `Commit' in that new window
  (Figure~\ref{fig-githubcommit}). Finally, click `Push' to send the
  changes to GitHub.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/04-github-add.png}

}

\caption{\label{fig-githubadd}Adding files to be committed}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/04-github-commit.png}

}

\caption{\label{fig-githubcommit}Making a commit}

\end{figure}

There are a few common pain-points when it comes to Git and GitHub. We
recommend committing and pushing regularly, especially when you are new
to version control. This increases the number of snapshots that you
could come back if needed. All commits should have an informative commit
message. If you are new to version control, then the expectation of a
good commit message is that it contains a short summary of the change,
followed by a blank line, and then an explanation of the change
including what the change is, and why it is being made. For instance, if
your commit adds graphs to a paper, then a commit message could be:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Add graphs}

\NormalTok{Graphs of unemployment and inflation added into Data section.}
\end{Highlighting}
\end{Shaded}

There is some evidence of a relationship between overall quality and
commit behavior (Sprint and Conci 2019). In an ideal scenario the commit
messages act as a kind of journal of the project.

Git and GitHub were designed for software developers, rather than data
scientists. GitHub limits the size of the files it will consider to
100MB, and even 50MB will prompt a warning. Data science projects
regularly involve datasets that are larger than this. In
Chapter~\ref{sec-store-and-share} we discuss the use of data deposits,
which can be especially useful when a project is completed, but when we
are actively working on a project it can be useful to ignore the file,
at least as far as Git and GitHub are concerned. We do this using a
`.gitignore' file, in which we list all of the files that we do not want
to track using Git. The
\href{https://github.com/RohanAlexander/starter_folder}{starter folder}
contains an example `.gitignore' file. And it can be helpful to run
\texttt{usethis::git\_vaccinate()}, which will add a variety of files to
a global `.gitignore' file in case you forget to do it on a
project-basis.

We used the Git pane in R Studio which removed the need to use the
Terminal, but it did not remove the need to go to GitHub and set-up a
new project. Having set-up Git and GitHub, we can further improve this
aspect of our workflow with \texttt{usethis} (Wickham and Bryan 2020).

First check that Git is set-up with \texttt{usethis::git\_sitrep()}.
This should print information about the username and email. We can use
\texttt{usethis::use\_git\_config()} to update these details if needed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usethis}\SpecialCharTok{::}\FunctionTok{use\_git\_config}\NormalTok{(}
  \AttributeTok{user.name =} \StringTok{"Rohan Alexander"}\NormalTok{, }
  \AttributeTok{user.email =} \StringTok{"rohan.alexander@utoronto.ca"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Rather than starting a new project in GitHub, and then adding it
locally, we can now use \texttt{usethis::use\_git()} to initiate it and
commit the files. Having committed, we can use
\texttt{usethis::use\_github()} to push to GitHub, which will create the
folder on GitHub as well.

\hypertarget{using-r-in-practice}{%
\section{Using R in practice}\label{using-r-in-practice}}

\hypertarget{dealing-with-errors}{%
\subsection{Dealing with errors}\label{dealing-with-errors}}

\begin{quote}
When you are programming, eventually your code will break, when I say
eventually, I mean like probably 10 or 20 times a day.

Gelfand (2021)
\end{quote}

Everyone who uses R, or any programming language for that matter, has
trouble find them at some point. This is normal. Programming is hard. At
some point code will not run or will throw an error. This happens to
everyone. It is common to get frustrated, but to move forward we develop
strategies to work through the issues:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If you are getting an error message, then sometimes it will be useful.
  Try to read it carefully to see if there is anything of use in it.
\item
  Try to search, say on Google, for the error message. It can be useful
  to include `tidyverse' or `in R' in the search to help make the
  results more appropriate. Sometimes Stack Overflow results can be
  useful.
\item
  Look at the help file for the function, by putting `?' before the
  function, for instance, \texttt{?pivot\_wider()}. A common issue is to
  use a slightly incorrect argument name or format, such as accidentally
  including a string instead of an object name.
\item
  Look at where the error is happening and remove code until the error
  is resolved, and then slowly add code back again.
\item
  Check the class of the object, with \texttt{class()}, for instance,
  \texttt{class(data\_set\$data\_column)}. Ensure that it is what it
  expected.
\item
  Restart R (`Session' -\textgreater{} `Restart R and Clear Output') and
  load everything again.
\item
  Restart the computer.
\item
  Search for what you are trying to do, rather than the error, being
  sure to include `tidyverse' or `in R' in the search to help make the
  results more appropriate. For instance, `save PDF of graph in R made
  using ggplot'. Sometimes there are relevant blog posts or Stack
  Overflow answers that will help.
\item
  Making a small, self-contained, reproducible example `reprex' to see
  if the issue can be isolated and to enable others to help.
\end{enumerate}

More generally, while this is rarely possible to do, it is almost always
helpful to take a break and come back the next day.

\hypertarget{reproducible-examples}{%
\subsection{Reproducible examples}\label{reproducible-examples}}

\begin{quote}
No one can advise or help you---no one. There is only one thing you
should do. Go into yourself.

Rilke (1929)
\end{quote}

Asking for help is a skill like any other. We get better at it with
practice. It is important to try not to say `this doesn't work', `I
tried everything', `your code does not work', or `here is the error
message, what do I do?'. In general, it is not possible to help based on
these comments, because there are too many possible issues. You need to
make it easy for others to help you. This involves a few steps.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Provide a small, self-contained, example of your data, and code, and
  detail what is going wrong.
\item
  Document what you have tried so far, including which Stack Overflow
  and R Studio Community pages have you looked at, and why are they not
  quite what you are after?
\item
  Be clear about the outcome that you would like.
\end{enumerate}

Begin by creating a minimal REPRoducible EXample, a `reprex'. This is
code that contains what is needed to reproduce the error, but only what
is needed. This means that the code it likely a smaller, simpler,
version that nonetheless reproduces the error.

Sometimes this process enables one to solve the problem. If it does not,
then it gives someone else a fighting chance of being able to help. It
is important to recognize that there is almost no chance that you have
got a problem that someone has not addressed before. It is more likely
that the main difficulty is in trying to communicate what you are trying
to do and what is happening, in a way that allows others to recognize
both. Developing tenacity is important.

To develop reproducible examples, \texttt{reprex} (Jennifer Bryan et al.
2019) is especially useful. To use it we:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load the \texttt{reprex} package: \texttt{library(reprex)}.
\item
  Highlight, and copy, the code that is giving issues.
\item
  Run \texttt{reprex()} in the Console.
\end{enumerate}

If the code is self-contained, then it will preview in the Viewer. If it
is not, then it will error, and the code needs to be re-written so that
it is self-contained.

If you need data to reproduce the error, then you should use data that
is built into R. There are a large number of datasets that are built
into R and can be seen using \texttt{library(help\ =\ "datasets")}. But
if possible, you should use a common option such as `mtcars' or
`faithful'. Combining a reprex with a GitHub Gist that was introduced in
Chapter~\ref{sec-fire-hose}, increases the chances that someone is able
to help you.

\hypertarget{mentality}{%
\subsection{Mentality}\label{mentality}}

\begin{quote}
(Y)ou are a real, valid, \emph{competent} user and programmer no matter
what IDE you develop in or what tools you use to make your work work for
you

(L)et's break down the gates, there's enough room for everyone

Sharla Gelfand, 10 March 2020.
\end{quote}

If you write code, then you are a programmer regardless of how you do
it, what you are using it for, or who you are. But there are a few
traits that one tends to notice great programmers have in common.

\begin{itemize}
\tightlist
\item
  Focused: Often having an aim to `learn R' or something similar tends
  to be problematic, because there is no real end point to that. It
  tends to be more efficient to have smaller, more specific goals, such
  as `make a histogram about the 2022 Australian Election with
  \texttt{ggplot}'. This is something that can be focused on and
  achieved in a few hours. The issue with goals that are more nebulous,
  such as `I want to learn R', is that it becomes easy to get lost on
  tangents, much more difficult to get help. This can be demoralizing
  and lead to folks quitting too early.
\item
  Curious: It is almost always useful to have a go. In general, the
  worst that happens is that you waste your time. You can rarely break
  something irreparably with code. If you want to know what happens if
  you pass a `vector' instead of a `dataframe' to \texttt{ggplot()} then
  try it.
\item
  Pragmatic: At the same time, it can be useful to stick within
  reasonable bounds, and make one small change each time. For instance,
  say you want to run some regressions, and are curious about the
  possibility of using the \texttt{tidymodels} package (Kuhn and Wickham
  2020) instead of \texttt{lm()}. A pragmatic way to proceed is to use
  one aspect from the \texttt{tidymodels} package initially and then
  make another change next time.
\item
  Tenacious: Again, this is a balancing act. There are always unexpected
  problems and issues with every project. On the one hand, persevering
  despite these is a good tendency. But on the other hand, sometimes one
  does need to be prepared to give up on something if it does not seem
  like a break-through is possible. Here mentors can be useful as they
  tend to be a better judge of what is reasonable. It is also where
  appropriate planning is useful.
\item
  Planned: It is almost always useful to excessively plan what you are
  going to do. For instance, you may want to make a histogram of the
  2019 Canadian Election. You should plan the steps that are needed and
  even to sketch out how each step might be implemented. For instance,
  the first step is to get the data. What packages might be useful?
  Where might the data be? What is the back-up plan if the data do not
  exist there?
\item
  Done is better than perfect: We all have various perfectionist
  tendencies to a certain extent, but it can be useful to initially try
  to turn them off to a certain extent. In the first instance, try to
  write code that works, especially in the early days. You can always
  come back and improve aspects of it. But it is important to actually
  ship. Ugly code that gets the job done, is better than beautiful code
  that is never finished.
\end{itemize}

\hypertarget{code-comments-and-style}{%
\subsection{Code comments and style}\label{code-comments-and-style}}

Code must be commented (Lee 2018). Comments should focus on why certain
code was written, (and to a lesser extent, why a common option is not
selected).

There is no one way to write code, especially in R. However, there are
some general guidelines that will make it easier for you even if you are
just working on your own. It is important to recognize that most
projects will evolve over time, and one purpose served by code comments
are as `{[}m{]}essages left for your future self (or near-future others)
{[}that{]} help retrace and justify your decisions' (Bowers 2011).

Comments in R can be added by including the \# symbol. We do not have to
put a comment at the start of the line, it can be midway through. In
general, we do not need to comment what every aspect of your code is
doing but we should comment parts that are not obvious. For instance, if
we read in some value then we may like to comment where it is coming
from.

You should comment why you are doing something (Wickham 2021b). What are
you trying to achieve?

You must comment to explain weird things. Like if you are removing some
specific row, say row 27, then why are you removing that row? It may
seem obvious in the moment, but future-you in six months will not
remember.

You should break your code into sections. For instance, setting up the
workspace, reading in datasets, manipulating and cleaning the dataset,
analyzing the datasets, and finally producing tables and figures. Each
of these should be separated with comments explaining what is going on,
and sometimes into separate files, depending on the length.

Additionally, at the top of each file it is important to basic
information, such as the purpose of the file, and pre-requisites or
dependencies, the date, the author and contact information, and finally
and red-flags or todos.

At the very least every R script needs a preamble and a clear
demarcation of sections.

\begin{verbatim}
#### Preamble ####
# Purpose: Brief sentence about what this script does
# Author: Your name
# Data: The date it was written
# Contact: Add your email
# License: Think about how your code may be used
# Pre-requisites: 
# - Maybe you need some data or some other script to have been run?


#### Workspace setup ####
# do not keep the install.packages line - comment out if need be
# Load libraries
library(tidyverse)

# Read in the raw data. 
raw_data <- readr::read_csv("inputs/data/raw_data.csv")


#### Next section ####
...
\end{verbatim}

\hypertarget{exercises-and-tutorial-3}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-3}}

\hypertarget{exercises-3}{%
\subsection{Exercises}\label{exercises-3}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  According to M. Alexander (2019a) research is reproducible if (pick
  one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    It is published in peer-reviewed journals.
  \item
    All of the materials used in the study are provided.
  \item
    It can be reproduced exactly without the authors providing
    materials.
  \item
    It can be reproduced exactly, given all the materials used in the
    study.
  \end{enumerate}
\item
  According to the timeline of Gelman (2016), a) when did Paul Meehl
  identify various issues; and b) when did null hypothesis significance
  testing (NHST) become controversial (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    1970s-1980s; 1990s-2000.
  \item
    1960s-1970s; 1980s-1990.
  \item
    1970s-1980s; 1980s-1990.
  \item
    1960s-1970s; 1990s-2000.
  \end{enumerate}
\item
  Which of the following are components of the project layout
  recommended by Wilson et al. (2017) (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    requirements.txt
  \item
    doc
  \item
    data
  \item
    LICENSE
  \item
    CITATION
  \item
    README
  \item
    src
  \item
    results
  \end{enumerate}
\item
  Based on M. Alexander (2021) please write a paragraph about some of
  the barriers you overcame, or still face, with regard to sharing code
  that you wrote.
\item
  According to Gelfand (2021), what is the key part of `If you need help
  getting unstuck, the first step is to create a reprex, or reproducible
  example. The goal of a reprex is to package your problematic code in
  such a way that other people can run it and feel your pain. Then,
  hopefully, they can provide a solution and put you out of your
  misery.' (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    package your problematic code
  \item
    other people can run it and feel your pain
  \item
    the first step is to create a reprex
  \item
    they can provide a solution and put you out of your misery
  \end{enumerate}
\item
  According to Gelfand (2021), what are the three key aspects of a
  reprex (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    data
  \item
    only the libraries that are necessary and all the libraries that are
    necessary
  \item
    relevant code and only relevant code
  \end{enumerate}
\item
  According to Wickham (2021b) for naming files, how would these files
  `00\_get\_data.R', `get data.R' be classified (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    bad; bad.
  \item
    good; bad.
  \item
    bad; good.
  \item
    good; good.
  \end{enumerate}
\item
  Which of the following would result in bold text in Quarto (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{**bold**}
  \item
    \texttt{\#\#bold\#\#}
  \item
    \texttt{*bold*}
  \item
    \texttt{\#bold\#}
  \end{enumerate}
\item
  Which option would hide the warnings in a Quarto R chunk (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{echo:\ false}
  \item
    \texttt{include:\ false}
  \item
    \texttt{eval:\ false}
  \item
    \texttt{warning\ =\ false}
  \item
    \texttt{message\ =\ false}
  \end{enumerate}
\item
  Which options would run the R code chunk and display the results, but
  not show the code in a Quarto R chunk (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{echo:\ false}
  \item
    \texttt{include:\ false}
  \item
    \texttt{eval:\ false}
  \item
    \texttt{warning\ =\ false}
  \item
    \texttt{message\ =\ false}
  \end{enumerate}
\item
  Why are R Projects important (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    They help with reproducibility.
  \item
    They make it easier to share code.
  \item
    They make your workspace more organized.
  \item
    They ensure reproducibility.
  \end{enumerate}
\item
  Please discuss a circumstance in which an R Project would be useful.
\item
  Consider this sequence: `\texttt{git\ pull}, \texttt{git\ status},
  \_\_\_\_\_\_\_\_, \texttt{git\ status},
  \texttt{git\ commit\ -m\ "My\ message"}, \texttt{git\ push}'. What is
  the missing step (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{git\ add\ -A}.
  \item
    \texttt{git\ status}.
  \item
    \texttt{git\ pull}.
  \item
    \texttt{git\ push}.
  \end{enumerate}
\item
  Assuming the libraries and datasets have been loaded, what is the
  mistake in this code:
  \texttt{DoctorVisits\ \textbar{}\textgreater{}\ select("visits")}
  (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{"visits"}
  \item
    \texttt{DoctorVisits}
  \item
    \texttt{select}
  \item
    \texttt{\textbar{}\textgreater{}}
  \end{enumerate}
\item
  What is a reprex and why is it important to be able to make one
  (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A reproducible example that enables your error to be reproduced.
  \item
    A reproducible example that helps others help you.
  \item
    A reproducible example during the construction of which you may
    solve your own problem.
  \item
    A reproducible example that demonstrates you have actually tried to
    help yourself.
  \end{enumerate}
\item
  The following code produces an error. Please use \texttt{reprex}
  (Jennifer Bryan et al. 2019) to build a reproducible example that you
  could use to get help with it, and submit the reprex using a GitHub
  Gist. You should simplify many aspects including reducing the number
  of libraries, changing the dataset, and simplying the
  \texttt{filter()} and \texttt{mutate()}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{oecd\_gdp }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://stats.oecd.org/sdmx{-}json/data/DP\_LIVE/.QGDP.../OECD?contentType=csv\&detail=code\&separator=comma\&csv{-}lang=en"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(oecd\_gdp)}

\FunctionTok{library}\NormalTok{(forcats)}
\FunctionTok{library}\NormalTok{(dplyr)}

\NormalTok{oecd\_gdp\_most\_recent }\OtherTok{\textless{}{-}} 
\NormalTok{  oecd\_gdp }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(TIME }\SpecialCharTok{==} \StringTok{"2021{-}Q3"}\NormalTok{,}
\NormalTok{         SUBJECT }\SpecialCharTok{==} \StringTok{"TOT"}\NormalTok{,}
\NormalTok{         LOCATION }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"AUS"}\NormalTok{, }\StringTok{"CAN"}\NormalTok{, }\StringTok{"CHL"}\NormalTok{, }\StringTok{"DEU"}\NormalTok{, }\StringTok{"GBR"}\NormalTok{,}
                         \StringTok{"IDN"}\NormalTok{, }\StringTok{"ESP"}\NormalTok{, }\StringTok{"NZL"}\NormalTok{, }\StringTok{"USA"}\NormalTok{, }\StringTok{"ZAF"}\NormalTok{),}
\NormalTok{         MEASURE }\SpecialCharTok{==} \StringTok{"PC\_CHGPY"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{european =} \FunctionTok{if\_else}\NormalTok{(LOCATION }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"DEU"}\NormalTok{, }\StringTok{"GBR"}\NormalTok{, }\StringTok{"ESP"}\NormalTok{),}
                             \StringTok{"European"}\NormalTok{,}
                             \StringTok{"Not european"}\NormalTok{),}
         \AttributeTok{hemisphere =} \FunctionTok{if\_else}\NormalTok{(LOCATION }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"CAN"}\NormalTok{, }\StringTok{"DEU"}\NormalTok{, }\StringTok{"GBR"}\NormalTok{, }\StringTok{"ESP"}\NormalTok{, }\StringTok{"USA"}\NormalTok{),}
                             \StringTok{"Northern Hemisphere"}\NormalTok{,}
                             \StringTok{"Southern Hemisphere"}\NormalTok{),}
\NormalTok{         )}

\FunctionTok{library}\NormalTok{(ggplot)}
\FunctionTok{library}\NormalTok{(patchwork)}

\NormalTok{oecd\_gdp\_most\_recent }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ LOCATION, }\AttributeTok{y =}\NormalTok{ Value)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tutorial-3}{%
\subsection{Tutorial}\label{tutorial-3}}

Please put together a small Quarto file that downloads a dataset using
\texttt{opendatatoronto}, cleans it, and makes a graph. Then exchange it
with someone else. Ask them to both read the code and to run it, and to
then provide you with feedback about both aspects. Write a one-to-two
pages of single-spaced content, about the comments that you received and
changes that you could make going forward.

\hypertarget{paper}{%
\subsection{Paper}\label{paper}}

At about this point, Paper One Appendix~\ref{sec-paper-one} would be
appropriate.

\part{Communication}

\hypertarget{sec-on-writing}{%
\chapter{On writing}\label{sec-on-writing}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{On Writing Well}, (any edition is fine) Parts I
  `Principles', and II `Methods', (Zinsser 1976)
\item
  Read \emph{Publication, publication}, (G. King 2006)
\item
  Read one of the following well-written quantitative papers:

  \begin{itemize}
  \tightlist
  \item
    \emph{Asset prices in an exchange economy}, (Lucas Jr 1978)
  \item
    \emph{Individuals, institutions, and innovation in the debates of
    the French Revolution}, (Barron et al. 2018)
  \item
    \emph{Modeling: optimal marathon performance on the basis of
    physiological factors}, (Joyner 1991)
  \item
    \emph{On reproducible econometric research}, (Koenker and Zeileis
    2009)
  \item
    \emph{Prevented mortality and greenhouse gas emissions from
    historical and projected nuclear power}, (Kharecha and Hansen 2013)
  \item
    \emph{Seeing like a market}, (Fourcade and Healy 2017)
  \item
    \emph{Simpson's paradox and the hot hand in basketball}, (Wardrop
    1995)
  \item
    \emph{Smoking and carcinoma of the lung}, (Doll and Hill 1950)
  \item
    \emph{Some studies in machine learning using the game of checkers},
    (Samuel 1959)
  \item
    \emph{Statistical methods for assessing agreement between two
    methods of clinical measurement}, (Bland and Altman 1986)
  \item
    \emph{The mundanity of excellence: An ethnographic report on
    stratification and Olympic swimmers}, (Chambliss 1989)
  \item
    \emph{The probable error of a mean}, (Student 1908)
  \end{itemize}
\item
  Read one of the following articles from \emph{The New Yorker}:

  \begin{itemize}
  \tightlist
  \item
    \emph{Funny Like a Guy}, Tad Friend, 4 April 2011
  \item
    \emph{Going the Distance}, David Remnick, 19 January 2014
  \item
    \emph{How the First Gravitational Waves Were Found}, Nicola Twilley,
    11 February 2016
  \item
    \emph{Happy Feet}, Alexandra Jacobs, 7 September 2009
  \item
    \emph{Levels of the Game}, John McPhee, 31 May 1969
  \item
    \emph{Reporting from Hiroshima}, John Hersey, 23 August 1946
  \item
    \emph{The Catastrophist}, Elizabeth Kolbert, 22 June 2009
  \item
    \emph{The Quiet German}, George Packer, 24 November 2014
  \end{itemize}
\item
  Read one of the following articles from miscellaneous publications:

  \begin{itemize}
  \tightlist
  \item
    \emph{Blades of Glory}, Holly Anderson
  \item
    \emph{Born to Run}, Walt Harrington
  \item
    \emph{Dropped}, Jason Fagone
  \item
    \emph{Federer as Religious Experience}, David Foster Wallace
  \item
    \emph{Generation Why?}, Zadie Smith
  \item
    \emph{One hundred years of arm bars}, David Samuels
  \item
    \emph{Out in the Great Alone}, Brian Phillips
  \item
    \emph{Pearls Before Breakfast}, Gene Weingarten
  \item
    \emph{The Cult of `Jurassic Park'}, Bryan Curtis
  \item
    \emph{The House that Hova Built}, Zadie Smith
  \item
    \emph{The Re-Education of Chris Copeland}, Flinder Boyd
  \item
    \emph{The Sea of Crisis}, Brian Phillips
  \end{itemize}
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  To get better at writing, write, ideally every day.
\item
  Write for the reader.
\item
  Have one message that you want to communicate.
\item
  Get to a first draft as quickly as possible.
\item
  Rewrite brutally.
\item
  Remove as many words as possible.
\end{itemize}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

\begin{quote}
If you want to be a writer, you must do two things above all others:
read a lot and write a lot. There's no way around these two things that
I'm aware of, no shortcut.

S. King (2000, 145)
\end{quote}

\begin{quote}
{[}T{]}he duty of a scientist is not only to find new things, but to
communicate them successfully in at least three forms: 1) Writing papers
and books. 2) Prepared public talks. 3) Impromptu talks.

Hamming (1996, 65)
\end{quote}

\begin{quote}
People who need to write: founders, VCs, lawyers, software engineers,
designers, painters, data scientists, musicians, filmmakers, creative
directors, physical trainers, teachers, writers. Learn to write.

Sahil Lavingia, 3 February 2020.
\end{quote}

\begin{quote}
Writing well has done just as much for me as knowing how to code. I'd
add that if you're intimidated by writing, start a blog and write often
about something you're interested in. You'll get better. At least that's
what I've done for the past 10 years. :)

Vicki Boykis, 3 February 2020.
\end{quote}

We predominately tell stories with data by writing them down. Writing
allows us to communicate efficiently. It is also a way to work out what
we believe and allows us to get feedback on our ideas. Effective papers
are tightly written and well-organized, which makes the story flow and
easy to follow. Proper sentence structure, spelling, vocabulary, and
grammar are important because they remove distractions and enable each
aspect of the story to be clearly articulated. Effective papers
demonstrate understanding of their topic by confidently using relevant
terms and techniques appropriately, and considering issues without being
overly verbose. Graphs, tables, and references are used to enhance both
the story and its credibility.

This chapter is about writing. By the end of it, you will have a better
idea of how to write short, detailed, quantitative papers that
communicate what you want them to, and do not waste the reader's time.
We write for the reader, not for ourselves. Specifically, we write to be
useful to the reader. This means clearly communicating something new,
true, and important (Graham 2020). That said, the greatest benefit of
writing nonetheless often accrues to the writer, even when we write for
our audience. This is because the process of writing is a way to work
out what we think and how we came to believe it.

\hypertarget{re-writing}{%
\section{Re-writing}\label{re-writing}}

\begin{quote}
The way to do a piece of writing is three or four times over, never
once. For me, the hardest part comes first, getting
something---anything---out in front of me. Sometimes in a nervous frenzy
I just fling words as if I were flinging mud at a wall. Blurt out, heave
out, babble out something---anything---as a first draft. With that, you
have a achieved a sort of nucleus. Then, as you work it over and alter
it, you begin to shape sentences that score higher with the ear and the
eye. Edit it again---top to bottom. The chances are that about now
you'll be seeing something that you are sort of eager for others to see.
And all that takes times. What I have left out is the interstitial time.
You finish that first awful blurting, and then you put the thing aside.
You get in your car and drive home. On the way, your mind is still
knitting at the words. You think of a better way to say something, a
good phrase to correct a certain problem. Without that drafted
version---if it did not exist---you obviously would not be thinking of
things that would improve it. In short, you may be actually writing only
two or three hours a day, but your mind, in one way or another, is
working on it twenty-four hours a day---yes, while you sleep---but only
if some sort of draft of earlier version already exists. Until it
exists, writing has not really begun.

McPhee (2017, 159)
\end{quote}

The process of writing is a process of re-writing. And the critical task
is to get to a first draft as quickly as possible. A complete first
draft of a five-to-fifteen-page quantitative paper can be done in a day.
Until that complete first draft exists, it is useful to try to not to
delete, or even revise, anything that was written, regardless of how bad
it may seem. Just write.

One of the most intimidating things in the world is a blank page, and we
deal with this by immediately adding headings such as: `Introduction',
`Data', `Model', `Results', and `Discussion'. And then adding fields in
the top matter for the various bits and pieces that are needed, such as
`title', `date', `author', and `abstract'. This creates a generic
outline, which will play the role of \emph{mise en place} for the paper.
By way of background, \emph{mise en place} is a critical preparatory
phase of cooking in a professional kitchen, when the ingredients that
will be needed during service are sorted, prepared, and arranged for
easy access, ensuring everything that is needed is available without
unnecessary delay. Putting together an outline plays the same role when
writing quantitative papers, and is akin to placing on the counter, the
ingredients that we will use to prepare dinner (McPhee 2017).

Having established this generic outline, we need to develop an
understanding of what we are exploring through thinking deeply about our
answer to our research question. In theory, we develop a research
question, answer it, and then we do all the writing; but that rarely
actually happens (Franklin 2005). Instead, we typically have some idea
of the question, and our answer, and these become less vague as we
write. This is because it is through the process of writing that we
refine our thinking (S. King 2000, 131). Having put down some thoughts
about the research question, we can start to add dot points in each of
the sections, adding sub-sections with informative sub-headings as
needed. We then go back and expand those dot points into paragraphs. We
do all this as part of a web of other researchers and other influences,
including our environment, and this all serves to influence our thinking
(Latour 1996).

While writing the first draft it is important to ignore the feeling that
one is not good enough, or that it is impossible. Just write. We need
words on paper, even if they are bad, and the first draft is when we
accomplish this. Remove all distractions and focus on writing, even if
it needs to be forced. Perfectionism is the enemy, and should be set
aside. Sometimes this can be accomplished by getting up very early to
write, by creating a deadline, forming a writing group, or with a glass
or two of wine. One friend puts her baby to sleep to the sound of her
typing, with the result being that she must keep typing otherwise the
baby will wake up. Creating a sense of urgency can be useful and one
option is rather than adding proper citations as we go, which could slow
us down, just add something like `{[}TODO: CITE R HERE{]}'. Do similar
with graphs and tables. That is, include textual descriptions such as
`{[}TODO: ADD GRAPH THAT SHOWS EACH COUNTRY OVER TIME HERE{]}' instead
of actual graphs and tables. Focus on adding content, even if it is bad.
When this is all done, a first draft exists.

This first draft will be poorly written and far from great. But it is by
writing a bad first draft that we can get to a good second draft, a
great third draft, and eventually excellence (Lamott 1994, 20). That
first draft will be too long, it will not make sense, it will contain
claims that cannot be supported, and some claims that should not be. If
you are not embarrassed by your first draft, then you have not written
it quickly enough. Having focused on adding content while writing the
first draft, when turning that into a second draft, we use the `delete'
key extensively, as well as `cut' and `paste'. Printing out the paper
and using a red pen to move or remove words, sentences, and entire
paragraphs, is especially helpful. The process of going from a first
draft to a second draft is best done in one sitting, to help with flow
and consistency of the story. One aspect of this first re-write is
enhancing the story that we want to tell. And another aspect is taking
out everything that is not the story (S. King 2000, 57).

As we go through what was written in each of the sections, we try to
bring some sense to it, with special consideration to how it supports
our story. This revision process is the essence of writing (McPhee 2017,
160). We should also fix the references, and add the real graphs and
tables. As part of this re-writing process, the paper's central message
tends to develop, and our answers to the research questions tend to
become clearer. At this point, aspects such as the introduction can be
returned to and, finally, the abstract. Typos and other issues affect
the credibility of the work, and so it is important that these are fixed
as part of the second draft.

We now have a paper that is sensible. The job is to now make it
brilliant. Print it out again, and again go through it on paper. It is
especially important to brutally remove everything that does not
contribute to the story. At about this stage, we may be starting to get
too close to the paper. We write for our reader, and so this is a great
opportunity to give it to someone else for their comments. We ask them
for feedback that enables us to better understand the weak parts of the
story. After addressing these, it can be helpful to go through the paper
once more, this time reading it aloud. A paper is never `done' and it is
more that at a certain point we either run out of time or become sick of
the sight of it.

\hypertarget{answering-questions}{%
\section{Answering questions}\label{answering-questions}}

Both qualitative and quantitative approaches have their place, but here
we focus on quantitative approaches. Qualitative research is important
as well, and often the most interesting work has a little of both. When
conducting quantitative analysis, we are subject to issues such as data
quality, scales, measurement, and sources. We are often especially
interested in trying to tease out causality. Regardless, we are trying
to learn something about the world. Our research questions need to take
this all into account.

Broadly, there are two ways to go about research:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  data-first; or
\item
  question-first.
\end{enumerate}

\hypertarget{data-first}{%
\subsection{Data-first}\label{data-first}}

When being data-first, the main issue is working out the questions that
can be reasonably answered with the available data. When deciding what
these are, it is useful to consider:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Theory: Is there a reasonable expectation that there is something
  causal that could be determined? For instance, if the question
  involves charting the stock market, then it might be better to
  consider haruspicy instead because at least that way we would have
  something to eat. Questions usually need to have some plausible
  theoretical underpinning to help avoid spurious relationships.
\item
  Importance: There are plenty of trivial questions that can be
  answered, but it important to not waste our time or that of the
  reader. Having an important question can also help with motivation
  when we find ourselves in, say, the fourth straight week of cleaning
  data and de-bugging code. It can also make it easier to attract
  talented employees and funding. That said, a balance is needed; it is
  important that the question has a decent chance of being answered. And
  so attacking a generational-defining question might be best broken up
  into smaller chunks.
\item
  Availability: Is there a reasonable expectation of additional data
  being available in the future? This could allow us to answer related
  questions and turn this one paper into a research agenda.
\item
  Iteration: Is this something that could be run multiple times, or is
  it a once-off analysis? If it is the former, then it becomes possible
  to start answering specific research questions and then iterate. But
  if we can only get access to the data once then we need to think about
  broader questions.
\end{enumerate}

There is a saying, sometimes attributed to Xiao-Li Meng, that all of
statistics is a missing data problem. And so paradoxically, another way
to ask data-first questions to think about which data we do not have.
For instance, returning to the neonatal and maternal mortality examples
discussed in, respectively, Chapter~\ref{sec-introduction} and
Chapter~\ref{sec-fire-hose}, the fundamental problem is that we do not
have perfect and complete data about cause of death. If we did, then we
could count the number of relevant deaths. Having established there is a
missing data problem, we can take a data-driven approach by looking at
the data we do have, and then ask research questions that speak to the
extent that we can use that to approximate our hypothetical perfect and
complete dataset.

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Xiao-Li Meng is the Whipple V. N. Jones Professor of Statistics at
Harvard University. After taking a PhD in Statistics from Harvard
University in 1990 he was appointed as an assistant professor at the
University of Chicago where he was promoted to professor in 2000. He
moved to Harvard in 2001, serving as chair of the statistics department
between 2004 and 2012. He was awarded the COPSS Presidents' Award in
2001.
\end{tcolorbox}

One way that some researchers are data-first, is that they develop a
particular expertise in the data of some geographical or historical
circumstance. For instance, they may be especially knowledgeable about
the present-day UK, or late nineteenth century Japan. They then look at
the questions that other researchers are asking in other circumstances,
and bring their data to that question. For instance, it is common to see
a particular question initially asked for the US, and then a host of
researchers answer that same question for the UK, Canada, Australia, and
many other countries.

A variant of data-driven research is model-driven research. Here a
researcher becomes an expert on some particular statistical approach and
then applies that approach whenever there are appropriate datasets.

\hypertarget{question-first}{%
\subsection{Question-first}\label{question-first}}

When trying to be question-first, there is the inverse issue of being
concerned about data availability. The `FINER framework' is used in
medicine to help guide the development of research questions. It
recommends asking questions that are: Feasible, Interesting, Novel,
Ethical, and Relevant (Hulley 2007). Farrugia et al. (2010) builds on
FINER with PICOT, which recommends additional considerations:
Population, Intervention, Comparison group, Outcome of interest, and
Time.

It can feel overwhelming trying to write out a question. One way to go
about it is to ask a very specific question. Another is to decide
whether we are interested in descriptive, predictive, inferential, or
causal analysis. These then lead to different types of questions, for
instance, descriptive analysis: `What does \(x\) look like?'; predictive
analysis: `What will happen to \(x\)?'; inferential: `How can we explain
\(x\)?'; and causal: `What impact does \(x\) have on \(y\)?'. Each of
these have a role to play. Since the credibility revolution (Angrist and
Pischke 2010) causal questions answered with a particular approach have
been predominant. But, more generally, almost all questions can be
re-framed as causal, and the nature of the question being asked matters
less than being genuinely interested in answering it.

Often time will be constrained, possibly in interesting ways and these
can guide the specifics of the research question. If we are interested
in the effect of a celebrity's tweets on the stock market, then that can
be done just by looking at stock's price before and after they tweet.
But what if we are interested in the effect of a cancer drug on long
term outcomes? If the effect takes 20 years, then we must either wait a
while, or we need to look at people who were treated twenty years ago,
but then we have selection effects and different circumstances to if we
were to administer the drug today. Often the only reasonable thing to do
is to build a statistical model, but then we need adequate sample sizes,
etc.

When answering questions usually, the creation of a counterfactual is
crucial. A counterfactual is an if-then statement in which the `if' is
false. Consider the example of Humpty Dumpty in \emph{Through the
Looking-Glass} (Carroll 1871).

\begin{quote}
`What tremendously easy riddles you ask!' Humpty Dumpty growled out. `Of
course I don't think so! Why, if ever I did fall off---which there's no
chance of---but if I did---' Here he pursed his lips and looked so
solemn and grand that Alice could hardly help laughing. `If I did fall,'
he went on, `The King has promised me---with his very own mouth-to-to-'
\end{quote}

Humpty is satisfied with what would happen if he were to fall off, even
though he is similarly satisfied that this would never happen. It is
this comparison group that often determines the answer to a question.
For instance, in Chapter~\ref{sec-causality-from-observational-data} we
consider the effect of VO2 max on a cyclist's endurance. If we compare
over the general population then it is an important variable, but if we
only compare over well-trained athletes, then it is less important,
because of selection.

Two aspects of the data to be especially aware of when deciding on a
research question are selection bias, and measurement bias.

Selection bias occurs when the results depend on who is in the sample.
One of the pernicious aspects of selection bias is that we need to know
about its existence in order to do anything about it. But many default
diagnostics will not identify selection bias. In an A/B testing set-up,
which we discuss in Chapter~\ref{sec-hunt-data}, A/A testing can help to
identify selection bias. And more generally, comparing the properties of
the sample, such as age-group, gender, and education, with
characteristics of the population. But the fundamental problem with
selection bias and observational data, is that, as Dr Jill Sheppard,
Lecturer, Australian National University, says, people who respond to
surveys are weird. And this weirdness likely generalizes to almost any
method of collecting data.

Selection bias pervades every aspect of our analysis. Even a sample that
starts off as representative, may become selected over time. For
instance, the survey panels used for polling, discussed in
Chapter~\ref{sec-farm-data}, need to be updated from time to time
because the folks who do not get anything out of it stop responding.
Another bias to be aware of is measurement bias, which when the results
are affected by how the data were collected. For instance, if we were to
ask respondents their income, then we may get different answers
in-person, compared with an online survey.

We will typically be interested in using data to answer our question and
it is important that we are clear about specifics. For instance, we
might be interested in the effect of smoking on life expectancy. In that
case, there is some true effect, which is can never know, and that true
effect is called the `estimand' (Little and Lewis 2021). Defining the
estimand at some point in the paper, ideally in the introduction is
critical (Lundberg, Johnson, and Stewart 2021). An `estimator' is a
process of by which we use the data that we have available to generate
an `estimate' of the `estimand'.

Bueno de Mesquita and Fowler (2021, 94) describe the relationship
between an estimate and an estimand as: \[
\mbox{Estimate = Estimand + Bias + Noise}
\]

Bias refers to issues with an estimator systematically providing
estimates that are different from the estimand, while noise refers to
non-systematic differences. For instance, consider a standard normal
distribution. We might be interested in understanding the mean, which
would be our estimand. We know (in a way that we can never with real
data) that the estimand is 0. Let us draw ten times from that
distribution. One estimator we could use to produce an estimate is: sum
the draws and divide by the number of draws. Another is sum the draws,
divide by the number of draws, and then add half. To be more specific,
here we will simulate this situation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_draws }\OtherTok{\textless{}{-}} \DecValTok{100}

\NormalTok{draws }\OtherTok{\textless{}{-}} 
  \FunctionTok{rnorm}\NormalTok{(number\_of\_draws, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}

\NormalTok{estimator\_one }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(draws) }\SpecialCharTok{/}\NormalTok{ number\_of\_draws}
\NormalTok{estimator\_two }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(draws) }\SpecialCharTok{/}\NormalTok{ number\_of\_draws) }\SpecialCharTok{+} \FloatTok{0.5}

\NormalTok{estimator\_one}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.124929
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estimator\_two}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.375071
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#}

\CommentTok{\# set.seed(853)}
\CommentTok{\# }
\CommentTok{\# library(tidyverse)}
\CommentTok{\# }
\CommentTok{\# estimand \textless{}{-} }
\CommentTok{\#   tibble(}
\CommentTok{\#     draws = rnorm(110, mean = 0, sd = 1),}
\CommentTok{\#     number = c(rep(x = 10, times = 10), rep(x = 100, times = 100))}
\CommentTok{\#   ) |\textgreater{}}
\CommentTok{\#   group\_by(number) |\textgreater{} }
\CommentTok{\#   summarise(}
\CommentTok{\#     estimator\_one = sum(draws) / number,}
\CommentTok{\#     estimator\_two = (sum(draws) / number) + 0.5}
\CommentTok{\# )}
\CommentTok{\#   }
\CommentTok{\# }
\CommentTok{\# draws \textless{}{-} }
\CommentTok{\#   rnorm(number\_of\_draws, mean = 0, sd = 1)}
\CommentTok{\# }
\CommentTok{\# estimator\_one \textless{}{-} sum(draws) / number\_of\_draws}
\CommentTok{\# estimator\_two \textless{}{-} (sum(draws) / number\_of\_draws) + 0.5}
\CommentTok{\# }
\CommentTok{\# estimator\_one}
\CommentTok{\# estimator\_two}
\end{Highlighting}
\end{Shaded}

As the number of draws increases, the effect of noise is removed and our
estimates illustrate the bias of our estimators. In this example, we
know that we should not add half, but when considering real data it can
be more difficult to know what to do. Hence the importance of being
clear about what the estimand is, before turning to generating
estimates.

\hypertarget{writing}{%
\section{Writing}\label{writing}}

\begin{quote}
I had not indeed published anything before I commenced ``The
Professor'', but in many a crude effort, destroyed almost as soon as
composed, I had got over any such taste as I might once have had for
ornamented and redundant composition, and come to prefer what was plain
and homely.

\emph{The Professor} (Brontë 1857)
\end{quote}

We discuss the following components: title, abstract, introduction,
data, results, discussion, figures, tables, equations, and technical
terms. Throughout all sections of a paper it is important that we are as
brief and specific as possible.

\hypertarget{title}{%
\subsection{Title}\label{title}}

A title is the first opportunity that we have to engage our reader in
our story. Ideally, we are able to tell our reader exactly what we
found. Effective titles are critical because otherwise papers will be
ignored by readers. While a title does not have to be `cute', it does
need to be effective. This means it needs to make the story clear.

One example of a title that is good enough is `On the 2016 Brexit
referendum'. This title is useful because the reader at least knows what
the paper will be about. But it is not particular informative or
enticing. A slightly better variant could be `On the ``Vote Leave''
outcome in the 2016 Brexit referendum'. This variant adds specificity
which is particularly informative. Finally, another variant would be
`Vote Leave outperforms in rural areas in the 2016 Brexit referendum:
Evidence from a Bayesian hierarchical model'. Here the reader knows the
approach of the paper and also the main take-away.

We will consider a few examples of particularly effective titles. Hug et
al. (2019) uses `National, regional, and global levels and trends in
neonatal mortality between 1990 and 2017, with scenario-based
projections to 2030: a systematic analysis'. Here it is clear what the
paper is about and the methods that are used. R. Alexander and Alexander
(2021) uses `The Increased Effect of Elections and Changing Prime
Ministers on Topics Discussed in the Australian Federal Parliament
between 1901 and 2018'. While the method used in that paper is not clear
from the title, the main finding is, along with a good deal of
information about what the content will be. And finally, M. J.
Alexander, Kiang, and Barbieri (2018) uses `Trends in Black and White
Opioid Mortality in the United States, 1979--2015'.

A title is often among the last aspects of a paper to be finalized.
While getting through the first draft, we would typically just use a
working title that is good enough to get the job done. We then refine it
over the course of redrafting. The title needs to reflect the final
story of the paper, and this is not usually something that we know at
the start. We are interested in striking a balance between getting our
reader interested enough to read the paper, and conveying enough of the
content so as to be useful (Hayot 2014). Two perfect examples are
Macaulay (1848) \emph{The History of England from the Accession of James
the Second}, and Churchill (1956) \emph{A History of the
English-Speaking Peoples}. Both are clear about what the content is,
and, for their target audience, spark interest.

One specific approach is the form: `Exciting content: Specific content',
for instance, `Returning to their roots: Examining the performance of
``Vote Leave'' in the 2016 Brexit referendum'. Kennedy and Gelman (2020)
provide a particularly nice example of this approach with `Know your
population and know your model: Using model-based regression and
poststratification to generalize findings beyond the observed sample',
as does Craiu (2019) with `The Hiring Gambit: In Search of the Twofer
Data Scientist'. A close variant of this is `A question? And an answer'.
For instance, Cahill, Weinberger, and Alkema (2020) with `What increase
in modern contraceptive use is needed in FP2020 countries to reach 75\%
demand satisfied by 2030? An assessment using the Accelerated Transition
Method and Family Planning Estimation Model'. As one gains experience
with this variant, it becomes possible to know when it is appropriate to
drop the answer part yet remain effective, such as Briggs (2021) with
`Why Does Aid Not Target the Poorest?'. Another specific approach is
`Specific content then broad content' or the inverse. For instance,
`Rurality, elites, and support for ``Vote Leave'' in the 2016 Brexit
referendum' or `Support for ``Vote Leave'' in the 2016 Brexit
referendum, rurality and elites'. This approach is used by Tolley and
Paquet (2021) with `Gender, municipal party politics, and Montreal's
first woman mayor'.

\hypertarget{abstract}{%
\subsection{Abstract}\label{abstract}}

For a five-to-fifteen-page paper, a good abstract is a three to five
sentence paragraph. For a longer paper the abstract can be slightly
longer. The abstract needs to specify the story of the paper, and the
objective of an abstract is to convey what was done and why it matters.
To do this an abstract typically touches on the context of the work, its
objectives, approach, and findings.

More specifically, a good recipe for an abstract is: first sentence:
specify the general area of the paper and encourage the reader; second
sentence: specify the dataset and methods at a general level; third
sentence: specify the headline result; and a fourth sentence about
implications.

We see this pattern in a variety of abstracts. For instance, Tolley and
Paquet (2021) draw in the reader with their first sentence by mentioning
the election of the first woman mayor in 400 years. The second sentence
is clear about what is done in the paper. The third paper tells the
reader how it is done i.e.~a survey, and the fourth sentence adds some
detail. The fifth and final sentence makes the main take-away from the
paper clear.

\begin{quote}
In 2017, Montreal elected Valérie Plante, the first woman mayor in the
city's 400-year history. Using this election as a case study, we show
how gender did and did not influence the outcome. A survey of Montreal
electors suggests that gender was not a salient factor in vote choice.
Although gender did not matter much for voters, it did shape the
organization of the campaign and party. We argue that Plante's victory
can be explained in part by a strategy that showcased a less
leader-centric party and a degendered campaign that helped counteract
stereotypes about women's unsuitability for positions of political
leadership.
\end{quote}

Similarly, Beauregard and Sheppard (2021) make the broader environment
clear within the first two sentences, and the specific contribution of
this paper to that environment. The third and fourth sentences makes the
data source clear and also the main findings. The fifth and sixth
sentences add specificity here that would be of interest to likely
readers of this abstract i.e.~academic political science experts. And
then the final sentence makes it clear the position of the authors.

\begin{quote}
Previous research on support for gender quotas focuses on attitudes
toward gender equality and government intervention as explanations. We
argue the role of attitudes toward women in understanding support for
policies aiming to increase the presence of women in politics is
ambivalent---both hostile and benevolent forms of sexism contribute in
understanding support, albeit in different ways. Using original data
from a survey conducted on a probability-based sample of Australian
respondents, our findings demonstrate that hostile sexists are more
likely to oppose increasing of women's presence in politics through the
adoption of gender quotas. Benevolent sexists, on the other hand, are
more likely to support these policies than respondents exhibiting low
levels of benevolent sexism. We argue this is because benevolent sexism
holds that women are pure and need protection; they do not have what it
takes to succeed in politics without the assistance of quotas. Finally,
we show that while women are more likely to support quotas, ambivalent
sexism has the same relationship with support among both women and men.
These findings suggest that aggregate levels of public support for
gender quotas do not necessarily represent greater acceptance of gender
equality generally.
\end{quote}

Another excellent example of an abstract is Sides, Vavreck, and Warshaw
(2021). In just five sentences they make it clear what they do, how they
do it, what they find, and why it is important.

\begin{quote}
We provide a comprehensive assessment of the influence of television
advertising on United States election outcomes from 2000--2018. We
expand on previous research by including presidential, Senate, House,
gubernatorial, Attorney General, and state Treasurer elections and using
both difference-in-differences and border-discontinuity research designs
to help identify the causal effect of advertising. We find that
televised broadcast campaign advertising matters up and down the ballot,
but it has much larger effects in down-ballot elections than in
presidential elections. Using survey and voter registration data from
multiple election cycles, we also show that the primary mechanism for ad
effects is persuasion, not the mobilization of partisans. Our results
have implications for the study of campaigns and elections as well as
voter decision making and information processing.
\end{quote}

And finally, Briggs (2021) begins with a claim that seems unquestionably
true. In the second sentence he then claims to have found that it is
false. The third sentence specifies the extent of this claim, and the
fourth sentence details how he comes to this position, before providing
more detail. The final two sentences speak broad implications and
importance.

\begin{quote}
Foreign-aid projects typically have local effects, so they need to be
placed close to the poor if they are to reduce poverty. I show that,
conditional on local population levels, World Bank (WB) project aid
targets richer parts of countries. This relationship holds over time and
across world regions. I test five donor-side explanations for pro-rich
targeting using a pre-registered conjoint experiment on WB Task Team
Leaders (TTLs). TTLs perceive aid-receiving governments as most
interested in targeting aid politically and controlling implementation.
They also believe that aid works better in poorer or more remote areas,
but that implementation in these areas is uniquely difficult. These
results speak to debates in distributive politics, international
bargaining over aid, and principal-agent issues in international
organizations. The results also suggest that tweaks to WB incentive
structures to make ease of project implementation less important may
encourage aid to flow to poorer parts of countries.
\end{quote}

The journal \emph{Nature} provides a guide for constructing an abstract.
They recommend a structure that results in an abstract of six parts,
that add up to around 200 words.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  A basic introductory sentence that is comprehensible to a wide
  audience.
\item
  A more detailed sentence about background that is relevant to likely
  readers.
\item
  A sentence that states the general problem.
\item
  Sentences that summarize and then explain the main results.
\item
  A sentence about general context.
\item
  And finally, a sentence about the broader perspective.
\end{enumerate}

It is critical that the first sentence of an abstract is not vacuous.
Assuming the reader continued past the title, this first sentence is the
next opportunity that we have to implore them to keep reading our paper.
And then the second sentence of the abstract, and so on. Work and
re-work the abstract until it is so good that you would be fine if that
is the only thing that was read; because that will often be the case.

\hypertarget{introduction-2}{%
\subsection{Introduction}\label{introduction-2}}

An introduction needs to be self-contained and convey everything that a
reader needs to know. It is important to recognize that we are not
writing a mystery story. Instead, we want to give-away the most
important points in the introduction. For a five-to-fifteen-page paper,
an introduction may be two or three paragraphs of main content. Hayot
(2014, 90) describes the goal of an introduction is to engage the
reader, locate them in some discipline and background, and then tell
them what happens in the rest of the paper. It is completely
reader-focused.

The introduction should set the scene and give the reader some
background. For instance, we typically start a little broader. This
provides some context to the paper. We then describe how the paper fits
into that context, and give some high-level results, especially focused
on the one key result that is the main part of the story. We provide
more detail here than we provided in the abstract, but not the full
extent. And the final bit of main content is to broadly discuss next
steps. Finally, we finish the introduction with an additional short
final paragraph that highlights the structure of the paper.

As an example (with made-up details):

\begin{quote}
The UK Conservative Party has always done well in rural electorates. And
the 2016 Brexit vote was no different with a significant different in
support between rural and urban areas. But even by the standard of rural
support for conservative issues, support for `Vote Leave' was unusually
strong with `Vote Leave' being most heavily supported in the East
Midlands and the East of England, while the strongest support for
`Remain' was in Greater London.

In this paper we look at why the performance of `Vote Leave' in the 2016
Brexit referendum was so correlated with rurality. We construct a model
in which support for `Vote Leave' at a voting area level, is explained
by the number of farms in the area, the average internet connectivity,
and the median age. We find that as the median age of an area increases,
the likelihood that an area supported `Vote Leave' decreases by 14
percentage points. Future work could look at the effect of having a
Conservative MP which would allow a more nuanced understanding of these
effects.

The remainder of this paper is structured as follows: Section 2
discusses the data, Section 3 discusses the model, Section 4 presents
the results, and finally Section 5 discusses our findings and some
weaknesses.
\end{quote}

The introduction needs to be self-contained and tell your reader
everything that they need to know. A reader should be able to only read
the introduction and have an accurate picture of all the major aspects
that they would if they were to read the whole paper. It would be rare
to include graphs or tables in the introduction. An introduction always
closes with the structure of the paper. For instance (and this is just a
rough guide) an introduction for a 10-page paper, should probably be
about 3 or 4 paragraphs, or 10 per cent, but it depends on specifics.

\hypertarget{data}{%
\subsection{Data}\label{data}}

Robert Caro, Lyndon B. Johnson's (LBJ) biographer, describes the
importance of conveying `a sense of place' when writing biography (Caro
2019, 141). This he defines as `the physical setting in which a book's
action is occurring: to see it clearly enough, in sufficient detail, so
that he feels as if he himself were present while the action is
occurring.' He provides the following example:

\begin{quote}
When Rebekah walked out the front door of that little house, there was
nothing---a roadrunner streaking behind some rocks with something long
and wet dangling from his beak, perhaps, or a rabbit disappearing around
a bush so fast that all she really saw was the flash of a white
tail---but otherwise nothing. There was no movement except for the
ripple of the leaves in the scattered trees, no sound except for the
constant whisper of the wind\ldots{} If Rebekah climbed, almost in
desperation, the hill in the back of the house, what she saw from its
crest was more hills, an endless vista of hills, hills on which there
was visible not a single house\ldots{} hills on which nothing moved,
empty hills with, above them, empty sky; a hawk circling silently
overhead was an event. Bus most of all, there was nothing human, no one
to talk to.

Caro (2019, 146)
\end{quote}

How thoroughly we can imagine the circumstances of Rebekah Baines
Johnson, who was LBJ's mother. When writing our papers, we need to
achieve that same sense of place, for our data, as Caro is able to
provide for the Hill county. We do this by being as explicit as
possible. We typically have a whole section about it and this is
designed to show the reader, as closely as possible, the actual data
that underpin our story.

When writing the data section, we are beginning our answer to the
critical question about our claim, which is, how is it possible to know
this? (McPhee 2017, 78). The preeminent example of a data section is
provided by Doll and Hill (1950), who are interested in the effect of
smoking between control and treatment groups. They begin by clearly
describing their dataset. They then use tables to display relevant
cross-tabs. And use graphs to contrast their groups.

In the data section we need to thoroughly discuss the variables in the
dataset that we are using. If there are other datasets that could have
been used, but were not, then these should be mentioned and our choices
justified. If variables were constructed or combined, then this process
and motivation should be explained.

To get a sense of the data, it is important that the reader is able to
understand what the data that underpin the results look like. This means
that we should graph the actual data that are used in our analysis, or
as close to them as possible. And we should also include tables of
summary statistics. If the dataset was created from some other source,
then it can also help to include an example of that original source. For
instance, if the dataset was created from survey responses then the
survey form should be included, potentially in an appendix.

Some judgment is required when it comes to the figures and tables in the
data section. While it is important that the reader has the opportunity
to understand the details, it may be that some are better placed in an
appendix. Figure and tables are a critical aspect of convincing people
of a story. In a graph we can show the data and then let the reader
decide for themselves. And using a table, we can more easily summarize
our dataset. At the very least, every variable needs to be shown in a
graph and summarized in a table. Figures and tables should be numbered
and then cross-referenced in the text, for instance, `Figure 1
shows\ldots{}', `Table 1 describes\ldots{}'. For every graph and table
there should be extensive accompanying text that describes their main
aspects, and adds additional detail.

We discuss the components of graphs and tables, including titles and
labels, in Chapter~\ref{sec-static-communication}. But here we will
discuss captions, as they are between text and the graph or table.
Captions need to be informative and self-contained. As Cleveland (1994,
57) says, the `interplay between graph, caption, and text is a delicate
one', however the reader should be able to read only the caption and
understand what the graph or table shows. A caption that is two or three
lines long would is not necessarily inappropriate. And all aspects of
the graph or table should be explained. For instance, consider
Figure~\ref{fig-bowleygraphisnice} and
Figure~\ref{fig-bowleytableisnice} both from Bowley (1901, 151), which
are exceptionally clear, and self-contained.

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/HANDIRIS3.png}

}

\caption{\label{fig-bowleygraphisnice}Example of a well-captioned
figure}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{./figures/HANDIRIS1.png}

}

\caption{\label{fig-bowleytableisnice}Example of a well-captioned table}

\end{figure}

The choice between a table and a graph comes down to how much
information is to be conveyed. In general, if there is specific
information that should be considered, such as a summary statistic, then
a table is a good option, while if we are interested in the reader
making comparisons and understanding trends then a graph is a good
option (Gelman, Pasarica, and Dodhia 2002).

Finally, if there is relevant literature then we should discuss it
throughout the paper as appropriate. For instance, when there is
literature relevant to the data then it should be discussed in this
section, literature relevant to the model, results, or discussion should
be mentioned as appropriate in those sections. It is rarely necessary to
have a separate literature review section.

\hypertarget{model}{%
\subsection{Model}\label{model}}

We will often build a statistical model that we will use to explore the
data, and we often have a specific section about this. At a minimum it
is important to clearly specify equation/s that describe the model being
used, and explain their components with plain language and
cross-references.

The model section typically begins with the model being written out,
explained, and justified. Depending on the expected reader, some
background may be needed. After specifying the model with appropriate
mathematical notation and cross-referencing it, the components of the
model are then typically defined and explained. It is especially
important to define each aspect of the notation. This helps convince the
reader that the model was well-chosen and enhances the credibility of
the paper. The model's variables should correspond to those that were
discussed in the data section, making a clear link between the two
sections.

There should be some discussion of how features enter the model and why.
For instance, some examples could include, why use ages rather than
age-groups, why does state/province have a levels effect, and why is
gender a categorical variable. In general, we are trying to convey a
sense that this is the model for the situation. We want the reader to
understand how the aspects that were discussed in the data section
assert themselves in the modelling decisions that were made.

The model section should close with some discussion of the assumptions
that underpin the model, and a brief discussion of alternative models,
or variants, and strengths and weaknesses made clear. It should be clear
in the reader's mind why it was this model that was chosen.

At some point in this section, it is usually appropriate to specify the
software that was used to run the model, and to provide some evidence of
thought about the circumstances in which the model may not be
appropriate. The later point would typically be expanded on in the
discussion. And there should be evidence of model validation and
checking, model convergence, and/or diagnostic issues. Again, there is a
balance needed here, and some of this content may be more appropriate
placed in appendices.

When technical terms are used, they should be briefly explained in plain
language for readers who might not be familiar with it. For instance, M.
Alexander (2019b) integrates an explanation of the Gini coefficient that
brings the reader along.

\begin{quote}
To look at the concentration of baby names, let's calculate the Gini
coefficient for each country, sex and year. The Gini coefficient
measures dispersion or inequality among values of a frequency
distribution. It can take any value between 0 and 1. In the case of
income distributions, a Gini coefficient of 1 would mean one person has
all the income. In this case, a Gini coefficient of 1 would mean that
all babies have the same name. In contrast, a Gini coefficient of 0
would mean names are evenly distributed across all babies.
\end{quote}

\hypertarget{results}{%
\subsection{Results}\label{results}}

Two excellent examples of results sections provided by Kharecha and
Hansen (2013) and Kiang et al. (2021). In the results section, we want
to communicate the outcomes of the model in a clear way and without too
much in the way of discussion of implications. The results section
likely requires summary statistics, tables, and graphs. Each of those
aspects should be cross-referenced and have text associated with them
that details what is seen in them. This section should strictly relay
results; that is, we are interested in what the results are, rather than
what they mean.

This section would also typically include table/s of coefficient
estimates based on the modelling that we used to further explore the
data. Various features of the estimates should be discussed, and
differences between the models explained. It may be that different
subsets of the data are considered separately. Again, all graphs and
tables need to have plain language text accompany them. A rough guide is
that the amount of text should be at least equal to the amount of space
taken up by the tables and graphs. For instance, if a full page is used
to display a table of coefficient estimates, then that should be
cross-referenced and accompanied by at least a full page of text about
that table.

\hypertarget{discussion}{%
\subsection{Discussion}\label{discussion}}

A discussion section may be the final section of a paper and would
typically have four or five sub-sections.

The discussion section would typically begin with a sub-section that
comprises a one- or two-paragraph summary of what was done in the paper.
This would be followed by two or three sub-sections that are devoted to
the key things that we learn about the world from this paper. For
instance, there are typically a few implications that come from the
modelling results. These few sub-sections are the main opportunity to
justify or detail the implications of the story being told in the paper.
Typically, these sub-sections do not see newly introduced graphs or
tables, but are instead focused on what we learn from those that were
introduced in earlier sections. It may be that some of the results are
discussed in relation to what others have found, and differences could
be attempted to be reconciled here.

Following these sub-sections of what we learn about the world, we would
typically have a sub-section focused on some of the weaknesses of what
was done. This could concern aspects such as the data that were used,
the approach, and the model. And the final sub-section is typically a
few paragraphs that specify what is left to learn, and how future work
could proceed.

In general, we would expect this section to take at least twenty-five
per cent of the total paper. For instance, in an eight-page paper, we
would expect at least two pages of discussion.

\hypertarget{brevity-typos-and-grammar}{%
\subsection{Brevity, typos, and
grammar}\label{brevity-typos-and-grammar}}

Brevity is important. Partly this is because we write for the reader,
and the reader has other priorities. But it is also because as the
writer it focuses us to consider what our most important points are, how
we can best support them, and where our arguments are weakest. Jean
Chrétien, the former Canadian Prime Minister, describes how `{[}t{]}o
allow me to get to the heart of an issue quickly, I asked the officials
to summarize their documents in two or three pages and attach the rest
of the materials as background information. I soon discovered that this
was a problem only for those who didn't really know what they were
talking about.' (Chrétien 2007, 105).

This experience is not unique to Canada. For instance, Oliver Letwin,
the former British Conservative Cabinet member, describes there as being
`a huge amount of terrible guff, at huge, colossal, humongous length
coming from some departments' and how he asked `for them to be one
quarter of the length' (Hughes and Rutter 2016). He found that the
departments were able to accommodate this request without losing
anything important.

This experience is also not new. For instance, Churchill asked for
brevity during the Second World War, saying `the discipline of setting
out the real points concisely will prove an aid to clearer thinking'.
And the letter from Szilard and Einstein to FDR that was the catalyst
for the Manhattan Project was only two pages.

This experience is also not unique to academia. For instance, one of the
foundations of Amazon, which is one of the world's largest companies, is
clear writing. Specifically, instead of PowerPoint presentations, Jeff
Bezos asked for `{[}w{]}ell structured, narrative text\ldots{}
{[}which{]} forces better thought and better understanding of what's
more important than what, and how things are related.'

Zinsser (1976) goes further and describes `the secret of good writing'
being `to strip every sentence to its cleanest components.' Every
sentence should be simplified to its essence. And every word that does
not contribute should be removed.

Typos and other grammatical mistakes affect the credibility of claims.
If the reader cannot trust us to use a spell-checker, then why should
they trust us to use logistic regression? Microsoft Word and Google Docs
are useful here for their spell-checkers: copy/paste from the Quarto
document, look for the red and green lines, and fix them in the Quarto
document.

We are not worried about the n-th degree of grammatical content.
Instead, we are interested in grammar and sentence structure that occurs
in conversational language use (S. King 2000, 118). The way to develop
that comfort is by reading a lot and asking others to read your work
also. One small aspect to check that will regularly come up is that any
number from on to ten should be written out in words, while 11 and over
should be written in numbers.

Unnecessary words, typos, and grammatical issues should be removed from
papers with a fanatical zeal.

\hypertarget{rules}{%
\subsection{Rules}\label{rules}}

A variety of authors have established rules for writing, including
famously, Orwell (1946), which were reimagined by The Economist (2013).
And Fiske and Kuriwaki (2021) have a list of rules for scientific
papers. A further reimagining, focused on telling stories with data,
could be:

\begin{itemize}
\tightlist
\item
  Focus on the reader and their needs. Everything else is comment.
\item
  Establish a logical structure and rely on that structure to tell the
  story.
\item
  Write a first draft as quickly as possible.
\item
  Re-write that extensively and without favor.
\item
  Aim to be concise and direct. Remove as many words as possible.
\item
  Using words precisely. Stock-markets rise or fall, not improve or
  worsen.
\item
  Use short sentence where possible.
\item
  Avoid jargon.
\item
  Write as though your work will be on the front page of a newspaper.
  Because it could be.
\end{itemize}

\hypertarget{exercises-and-tutorial-4}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-4}}

\hypertarget{exercises-4}{%
\subsection{Exercises}\label{exercises-4}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  According to the Introduction of Zinsser (1976), whose picture hangs
  in Zinsser's office?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Charlotte Bronte
  \item
    E. M. Forster
  \item
    E. B. White
  \item
    Stephen King
  \end{enumerate}
\item
  According to Chapter 2 of Zinsser (1976), what is the secret to good
  writing?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Correct sentence structure and grammar.
  \item
    The use of long words, adverbs, and passive voice.
  \item
    Thorough planning.
  \item
    Strip every sentence to its cleanest components.
  \end{enumerate}
\item
  According to Chapter 2 of Zinsser (1976), what must a writer
  constantly ask?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    What am I trying to say?
  \item
    Who am I writing for?
  \item
    How can this be re-written?
  \item
    Why does this matter?
  \end{enumerate}
\item
  Which two repeated words, for instance in Chapter 3, characterize the
  advice of Zinsser (1976)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Re-write, re-write.
  \item
    Remove, remove.
  \item
    Simplify, simplify.
  \item
    Less, less.
  \end{enumerate}
\item
  According to Chapter 5 of Zinsser (1976), a writer should never say
  anything in writing that they wouldn't say in?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Private
  \item
    Public
  \item
    Conversation
  \item
    Speeches
  \end{enumerate}
\item
  According to Chapter 6 of Zinsser (1976), what are the only tools that
  a writer has?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Papers
  \item
    Words
  \item
    Paragraphs
  \item
    Sentences
  \end{enumerate}
\item
  According to G. King (2006), what is the key task of subheadings (pick
  one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Enable a reader who randomly falls asleep but keeps turning pages to
    know where they are.
  \item
    Be broad and sweeping so that a reader is impressed by the
    importance of the paper.
  \item
    Use acronyms to integrate the paper into the literature.
  \end{enumerate}
\item
  According to G. King (2006), what is the maximum length of an abstract
  (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Two hundred words.
  \item
    Two hundred and fifty words.
  \item
    One hundred words.
  \item
    One hundred and fifty words.
  \end{enumerate}
\item
  According to G. King (2006), in a paper, raw computer output should be
  (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Commented out.
  \item
    Not included.
  \item
    Included.
  \end{enumerate}
\item
  According to G. King (2006), if our standard error was 0.05 then which
  of the following specificity for a coefficient would be silly (select
  all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    2.7182818
  \item
    2.718282
  \item
    2.72
  \item
    2.7
  \item
    2.7183
  \item
    2.718
  \item
    3
  \item
    2.71828
  \end{enumerate}
\item
  When should we try not to use the `delete' key (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    While writing the first draft.
  \item
    While writing the second draft.
  \item
    While writing the third draft.
  \item
    The `delete' key should always be used.
  \end{enumerate}
\item
  How long should a first draft take to write of a five-to-fifteen-page
  paper (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    One hour
  \item
    One day
  \item
    One week
  \item
    One month
  \end{enumerate}
\item
  What is a key aspect of the re-drafting process (select all that
  apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Going through it with a red pen to remove unneeded words.
  \item
    Printing the paper and reading a physical copy.
  \item
    Cutting and pasting to enhance flow.
  \item
    Reading it aloud.
  \item
    Exchanging it with others.
  \end{enumerate}
\item
  What are three features of a good research question (write a paragraph
  or two)?
\item
  What are some of the challenges of being `data-first' (write a
  paragraph or two)?
\item
  What are some of the challenges of being `question-first' (write a
  paragraph or two)?
\item
  What is a counterfactual (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    If-then statements in which the if does not happen.
  \item
    If-then statements in which the if happens.
  \item
    Statements that are either true or false.
  \item
    Statements that are neither true or false.
  \end{enumerate}
\item
  Which of the following is the best title (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    ``Problem Set 1''
  \item
    ``Unemployment''
  \item
    ``Examining England's Unemployment (2010-2020)''
  \item
    ``England's Unemployment Increased between 2010 and 2020''
  \end{enumerate}
\item
  Which of the following is the best title (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    ``Problem Set 2''
  \item
    ``Standard errors''
  \item
    ``On standard errors with small samples''
  \end{enumerate}
\item
  Which word/s can be removed from the following sentence without
  substantially affecting its meaning (select all that apply)? `Like
  many parents, when our children were born, one of the first things
  that my wife and I did regularly was read stories to them.'

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    first
  \item
    regularly
  \item
    stories
  \end{enumerate}
\item
  Please write a new title for Fourcade and Healy (2017).
\item
  Please write a new title for the first article from the list of
  articles at the start of this chapter from \emph{The New Yorker} that
  you read.
\item
  Please write a new title for the other article from the list of
  articles at the start of this chapter from \emph{The New Yorker} that
  you read.
\item
  Please write a new four-sentence abstract for Chambliss (1989)
\item
  Please write a new four-sentence abstract for Doll and Hill (1950).
\item
  Please write an abstract for the first article from the list of
  `miscellaneous' articles that you read.
\item
  Please write an abstract for the other article from the list of
  `miscellaneous' articles that you read.
\item
  Using only the 1000-most popular words in the English language --
  https://xkcd.com/simplewriter/ -- re-write the following so that it
  retains its original meaning:
\end{enumerate}

\begin{quote}
When using data, we try to tell a convincing story. It may be as
exciting as predicting elections, as banal as increasing internet
advertising click rates, as serious as finding the cause of a disease,
or as fun as forecasting basketball games. In any case the key elements
are the same.
\end{quote}

\hypertarget{tutorial-4}{%
\subsection{Tutorial}\label{tutorial-4}}

Caro (2019, xii) writes at least one thousand words almost every day. In
this tutorial we will write every day for a week. Please pick one of the
papers specified in the required materials and complete the following
tasks:

\begin{itemize}
\tightlist
\item
  Day 1: Transcribe, by writing each word yourself, the entire
  introduction.
\item
  Day 2:(This idea comes from McPhee (2017, 186).) Re-write the
  introduction so that it is five lines (or 10 per cent, whichever is
  less) shorter.
\item
  Day 3: Transcribe, by writing each word yourself, the abstract.
\item
  Day 4: Re-write a new, four-sentence, abstract for the paper.
\item
  Day 5: (This idea comes from comes from Chelsea Parlett-Pelleriti.)
  Write a second version of your new abstract using only the one
  thousand most popular words in the English language:
  https://xkcd.com/simplewriter/.
\item
  Day 6: Detail three points about the way the paper is written that you
  like
\item
  Day 7: Detail one point about the way the paper is written that you do
  not like.
\end{itemize}

Please use Quarto to produce a single PDF for the whole week. Make
judicious use of headings and sub-headings to structure your submission.
Submit the PDF.

\hypertarget{sec-static-communication}{%
\chapter{Static communication}\label{sec-static-communication}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{R for Data Science}, Chapter 28 `Graphics for
  communication', (Wickham and Grolemund 2017).
\item
  Read \emph{Data Visualization: A Practical Introduction}, Chapters 3
  `Make a plot', 4 `Show the right numbers', and 5 `Graph tables, add
  labels, make notes', (Healy 2018).
\item
  Read \emph{Testing Statistical Charts: What Makes a Good Graph?},
  (Vanderplas, Cook, and Hofmann 2020).
\item
  Read \emph{Data Feminism}, Chapter 3 `On Rational, Scientific,
  Objective Viewpoints from Mythical, Imaginary, Impossible
  Standpoints', (D'Ignazio and Klein 2020).
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Knowing the importance of showing the reader the actual dataset, or as
  close as is possible.
\item
  Using a variety of different graph options, including bar charts,
  scatterplots, line plots, and histograms.
\item
  Knowing how to use tables to show part of a dataset, communicate
  summary statistics, and display regression results.
\item
  Approaching maps as a type of a graph.
\item
  Comfort with geocoding places.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{datasauRus} (Locke and D'Agostino McGowan 2018)
\item
  \texttt{ggmap} (Kahle and Wickham 2013)
\item
  \texttt{kableExtra} (Zhu 2020)
\item
  \texttt{knitr} (Xie 2021)
\item
  \texttt{maps}
\item
  \texttt{modelsummary} (Arel-Bundock 2021a)
\item
  \texttt{opendatatoronto} (Gelfand 2020)
\item
  \texttt{patchwork} (Pedersen 2020)
\item
  \texttt{tidyverse} (Wickham et al. 2019a)
\item
  \texttt{viridis} (Garnier et al. 2021)
\item
  \texttt{WDI} (Arel-Bundock 2021b)
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{ggmap::get\_googlemap()}
\item
  \texttt{ggmap::get\_stamenmap()}
\item
  \texttt{ggmap::ggmap()}
\item
  \texttt{ggplot2::coord\_map()}
\item
  \texttt{ggplot2::facet\_wrap()}
\item
  \texttt{ggplot2::geom\_abline()}
\item
  \texttt{ggplot2::geom\_bar()}
\item
  \texttt{ggplot2::geom\_boxplot()}
\item
  \texttt{ggplot2::geom\_dotplot()}
\item
  \texttt{ggplot2::geom\_freqpoly()}
\item
  \texttt{ggplot2::geom\_histogram()}
\item
  \texttt{ggplot2::geom\_jitter()}
\item
  \texttt{ggplot2::geom\_line()}
\item
  \texttt{ggplot2::geom\_path()}
\item
  \texttt{ggplot2::geom\_point()}
\item
  \texttt{ggplot2::geom\_polygon()}
\item
  \texttt{ggplot2::geom\_smooth()}
\item
  \texttt{ggplot2::geom\_step()}
\item
  \texttt{ggplot2::ggplot()}
\item
  \texttt{ggplot2::ggsave()}
\item
  \texttt{ggplot2::labeller()}
\item
  \texttt{ggplot2::labs()}
\item
  \texttt{ggplot2::map\_data()}
\item
  \texttt{ggplot2::scale\_color\_brewer()}
\item
  \texttt{ggplot2::scale\_colour\_viridis\_d()}
\item
  \texttt{ggplot2::scale\_fill\_brewer()}
\item
  \texttt{ggplot2::scale\_fill\_viridis()}
\item
  \texttt{ggplot2::stat\_qq()}
\item
  \texttt{ggplot2::stat\_qq\_line()}
\item
  \texttt{ggplot2::theme()}
\item
  \texttt{ggplot2::theme\_bw()}
\item
  \texttt{ggplot2::theme\_classic()}
\item
  \texttt{ggplot2::theme\_linedraw()}
\item
  \texttt{ggplot2::theme\_minimal()}
\item
  \texttt{kableExtra::add\_header\_above()}
\item
  \texttt{knitr::kable()}
\item
  \texttt{lm()}
\item
  \texttt{maps::map()}
\item
  \texttt{modelsummary::datasummary()}
\item
  \texttt{modelsummary::datasummary\_balance()}
\item
  \texttt{modelsummary::datasummary\_correlation()}
\item
  \texttt{modelsummary::datasummary\_skim()}
\item
  \texttt{modelsummary::modelsummary()}
\item
  \texttt{WDI::WDI()}
\item
  \texttt{WDI::WDIsearch()}
\end{itemize}

\hypertarget{introduction-3}{%
\section{Introduction}\label{introduction-3}}

When telling stories with data, we would like the data to do much of the
work of convincing our reader. The paper is the medium, and the data are
the message. To that end, we want to try to show our reader the data
that allowed us to come to our understanding of the story. We use
graphs, tables, and maps to help achieve this.

The critical task is to show the actual data that underpin our analysis,
or as close to it as we can. For instance, if our dataset consists of
2,500 responses to a survey, then at some point in our paper we would
expect a graph that contains 2,500 points. To do this we build graphs
using \texttt{ggplot2} (Wickham 2016). We will go through a variety of
different options here including bar charts, scatterplots, line plots,
and histograms.

In contrast to the role of graphs, which is to show the actual data, or
as close to it as possible, the role of tables is typically to show an
extract of the dataset or convey various summary statistics. We will
build tables using \texttt{knitr} (Xie 2021) and \texttt{kableExtra}
(Zhu 2020) initially, and then \texttt{modelsummary} (Arel-Bundock
2021a).

Finally, we cover maps as a variant of graphs that are used to show a
particular type of data. We will build static maps using \texttt{ggmap}
(Kahle and Wickham 2013), having obtained the geocoded data that we need
using \texttt{tidygeocoder} (Cambon and Belanger 2021).

\hypertarget{graphs}{%
\section{Graphs}\label{graphs}}

Graphs are a critical aspect of compelling stories told with data.

\begin{quote}
Graphs allow us to explore data to see overall patterns and to see
detailed behavior; no other approach can compete in revealing the
structure of data so thoroughly. Graphs allow us to view complex
mathematical models fitted to data, and they allow us to assess the
validity of such models.

Cleveland (1994, 5)
\end{quote}

In a way, the graphing of data is an information coding process where we
create a glyph, or purposeful mark, that we mean to convey information
to our audience. The audience must decode our glyph. The success of our
graph turns on how much information is lost in this process. It is the
decoding that is the critical aspect (Cleveland 1994, 221), which means
that we are creating graphs for the audience. If nothing else is
possible, the most important feature is to convey as much of the actual
data as possible.

To see why this is important we begin by using the dataset
`datasaurus\_dozen' from \texttt{datasauRus} (Locke and D'Agostino
McGowan 2018). After installing and loading the necessary packages, we
can take a quick look at the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}datasauRus\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(datasauRus)}

\FunctionTok{head}\NormalTok{(datasaurus\_dozen)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  dataset     x     y
  <chr>   <dbl> <dbl>
1 dino     55.4  97.2
2 dino     51.5  96.0
3 dino     46.2  94.5
4 dino     42.8  91.4
5 dino     40.8  88.3
6 dino     38.7  84.9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datasaurus\_dozen }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(dataset)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 13 x 2
   dataset        n
   <chr>      <int>
 1 away         142
 2 bullseye     142
 3 circle       142
 4 dino         142
 5 dots         142
 6 h_lines      142
 7 high_lines   142
 8 slant_down   142
 9 slant_up     142
10 star         142
11 v_lines      142
12 wide_lines   142
13 x_shape      142
\end{verbatim}

We can see that the dataset consists of values for `x' and `y', which
should be plotted on the x-axis and y-axis, respectively. We can further
see that there are thirteen different values in the variable `dataset'
including: ``dino'', ``star'', ``away'', and ``bullseye''. We will focus
on those four and generate summary statistics for each
(Table~\ref{tbl-datasaurussummarystats}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# From Julia Silge: }
\CommentTok{\# https://juliasilge.com/blog/datasaurus{-}multiclass/}
\NormalTok{datasaurus\_dozen }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(dataset }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"dino"}\NormalTok{, }\StringTok{"star"}\NormalTok{, }\StringTok{"away"}\NormalTok{, }\StringTok{"bullseye"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(dataset) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(x, y),}
                   \FunctionTok{list}\NormalTok{(}\AttributeTok{mean =}\NormalTok{ mean,}
                        \AttributeTok{sd =}\NormalTok{ sd)),}
            \AttributeTok{x\_y\_cor =} \FunctionTok{cor}\NormalTok{(x, y)) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Dataset"}\NormalTok{, }
                  \StringTok{"x mean"}\NormalTok{, }
                  \StringTok{"x sd"}\NormalTok{, }
                  \StringTok{"y mean"}\NormalTok{, }
                  \StringTok{"y sd"}\NormalTok{, }
                  \StringTok{"correlation"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{linesep =} \StringTok{""}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-datasaurussummarystats}{}
\begin{longtable}[]{@{}lrrrrr@{}}
\caption{\label{tbl-datasaurussummarystats}Mean and standard deviation
for four `datasaurus' datasets}\tabularnewline
\toprule()
Dataset & x mean & x sd & y mean & y sd & correlation \\
\midrule()
\endfirsthead
\toprule()
Dataset & x mean & x sd & y mean & y sd & correlation \\
\midrule()
\endhead
away & 54.3 & 16.8 & 47.8 & 26.9 & -0.1 \\
bullseye & 54.3 & 16.8 & 47.8 & 26.9 & -0.1 \\
dino & 54.3 & 16.8 & 47.8 & 26.9 & -0.1 \\
star & 54.3 & 16.8 & 47.8 & 26.9 & -0.1 \\
\bottomrule()
\end{longtable}

Despite the similarities of the summary statistics, it turns out the
different `datasets' are actually very different beasts when we graph
the actual data (Figure~\ref{fig-datasaurusgraph}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datasaurus\_dozen }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(dataset }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"dino"}\NormalTok{, }\StringTok{"star"}\NormalTok{, }\StringTok{"away"}\NormalTok{, }\StringTok{"bullseye"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{y, }\AttributeTok{colour=}\NormalTok{dataset)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(dataset), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{colour =} \StringTok{"Dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-datasaurusgraph-1.pdf}

}

\caption{\label{fig-datasaurusgraph}Graph of four `datasaurus' datasets}

\end{figure}

This is a variant of the famous `Anscombe's Quartet'. The key takeaway
is that it is important to plot the actual data and not rely on summary
statistics. The `anscombe' dataset is built into R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(anscombe)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  x1 x2 x3 x4   y1   y2    y3   y4
1 10 10 10  8 8.04 9.14  7.46 6.58
2  8  8  8  8 6.95 8.14  6.77 5.76
3 13 13 13  8 7.58 8.74 12.74 7.71
4  9  9  9  8 8.81 8.77  7.11 8.84
5 11 11 11  8 8.33 9.26  7.81 8.47
6 14 14 14  8 9.96 8.10  8.84 7.04
\end{verbatim}

It consists of six observations for four different datasets, again with
x and y values for each observation. We need to manipulate this dataset
with \texttt{pivot\_longer()} to get it into a `tidy format'.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# From Nick Tierney: }
\CommentTok{\# https://www.njtierney.com/post/2020/06/01/tidy{-}anscombe/}
\CommentTok{\# Code from pivot\_longer() vignette.}

\NormalTok{tidy\_anscombe }\OtherTok{\textless{}{-}} 
\NormalTok{  anscombe }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_longer}\NormalTok{(}\FunctionTok{everything}\NormalTok{(),}
               \AttributeTok{names\_to =} \FunctionTok{c}\NormalTok{(}\StringTok{".value"}\NormalTok{, }\StringTok{"set"}\NormalTok{),}
               \AttributeTok{names\_pattern =} \StringTok{"(.)(.)"}
\NormalTok{               )}
\end{Highlighting}
\end{Shaded}

We can again first create some summary statistics
(Table~\ref{tbl-anscombesummarystats}) and then graph the data
(Figure~\ref{fig-anscombegraph}). And we again see the importance of
graphing the actual data, rather than relying on summary statistics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_anscombe }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(set) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(x, y),}
                   \FunctionTok{list}\NormalTok{(}\AttributeTok{mean =}\NormalTok{ mean, }\AttributeTok{sd =}\NormalTok{ sd)),}
            \AttributeTok{x\_y\_cor =} \FunctionTok{cor}\NormalTok{(x, y)) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Dataset"}\NormalTok{, }
                  \StringTok{"x mean"}\NormalTok{, }
                  \StringTok{"x sd"}\NormalTok{, }
                  \StringTok{"y mean"}\NormalTok{, }
                  \StringTok{"y sd"}\NormalTok{, }
                  \StringTok{"correlation"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{linesep =} \StringTok{""}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-anscombesummarystats}{}
\begin{longtable}[]{@{}lrrrrr@{}}
\caption{\label{tbl-anscombesummarystats}Mean and standard deviation for
Anscombe}\tabularnewline
\toprule()
Dataset & x mean & x sd & y mean & y sd & correlation \\
\midrule()
\endfirsthead
\toprule()
Dataset & x mean & x sd & y mean & y sd & correlation \\
\midrule()
\endhead
1 & 9 & 3.3 & 7.5 & 2 & 0.8 \\
2 & 9 & 3.3 & 7.5 & 2 & 0.8 \\
3 & 9 & 3.3 & 7.5 & 2 & 0.8 \\
4 & 9 & 3.3 & 7.5 & 2 & 0.8 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_anscombe }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{colour =}\NormalTok{ set)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(set), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{colour =} \StringTok{"Dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-anscombegraph-1.pdf}

}

\caption{\label{fig-anscombegraph}Recreation of Anscombe's Quartet}

\end{figure}

We can add two specific evaluation options in an R Markdown chunk to
have two graphs appear side-by-side
(\textbf{?@fig-anscombegraphsidebyside}). These are `out-width:
``49\%''\,' and `fig-show: ``hold''\,'. Another helpful option is
`fig.align = ``center''\,' which ensures the graph is placed in the
horizonal center of the page.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_anscombe }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{colour =}\NormalTok{ set)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(set), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{colour =} \StringTok{"Dataset"}\NormalTok{)}

\NormalTok{tidy\_anscombe }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{colour =}\NormalTok{ set)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(set), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{colour =} \StringTok{"Dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{./06-static_communication_files/figure-pdf/fig-anscombegraphsidebyside-1.pdf}

}

\caption{\label{fig-anscombegraphsidebyside-1}Two variants of Anscombe's
Quartet}

}

\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{./06-static_communication_files/figure-pdf/fig-anscombegraphsidebyside-2.pdf}

}

\caption{\label{fig-anscombegraphsidebyside-2}Two variants of Anscombe's
Quartet}

}

\end{minipage}%

\end{figure}

\hypertarget{bar-charts}{%
\subsection{Bar charts}\label{bar-charts}}

We typically use a bar chart when we have a categorical variable that we
want to focus on. We saw an example of this in
Chapter~\ref{sec-fire-hose} where we constructed a graph of the number
of occupied beds. The geom that we primarily use is
\texttt{geom\_bar()}, but there are many variants to cater for specific
situations.

We will use a dataset from the 1997-2001 British Election Panel Study
that was put together by Fox and Andersen (2006).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Vincent Arel Bundock provides access to this dataset.}
\NormalTok{beps }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}
    \AttributeTok{file =} 
    \StringTok{"https://vincentarelbundock.github.io/Rdatasets/csv/carData/BEPS.csv"}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(beps)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 11
   ...1 vote    age economic.cond.n~ economic.cond.h~ Blair Hague Kennedy Europe
  <dbl> <chr> <dbl>            <dbl>            <dbl> <dbl> <dbl>   <dbl>  <dbl>
1     1 Libe~    43                3                3     4     1       4      2
2     2 Labo~    36                4                4     4     4       4      5
3     3 Labo~    35                4                4     5     2       3      3
4     4 Labo~    24                4                2     2     1       3      4
5     5 Labo~    41                2                2     1     1       4      6
6     6 Labo~    47                3                4     4     4       2      4
# ... with 2 more variables: political.knowledge <dbl>, gender <chr>
\end{verbatim}

The dataset consists of which party the person supports, along with
various demographic, economic, and political variables. In particular,
we have the age of the respondents. We begin by creating age-groups from
the ages, and making a bar chart of the age-groups using
\texttt{geom\_bar()} (Figure~\ref{fig-bepfitst}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beps }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age\_group =} 
           \FunctionTok{case\_when}\NormalTok{(age }\SpecialCharTok{\textless{}} \DecValTok{35} \SpecialCharTok{\textasciitilde{}} \StringTok{"\textless{}35"}\NormalTok{,}
\NormalTok{                     age }\SpecialCharTok{\textless{}} \DecValTok{50} \SpecialCharTok{\textasciitilde{}} \StringTok{"35{-}49"}\NormalTok{,}
\NormalTok{                     age }\SpecialCharTok{\textless{}} \DecValTok{65} \SpecialCharTok{\textasciitilde{}} \StringTok{"50{-}64"}\NormalTok{,}
\NormalTok{                     age }\SpecialCharTok{\textless{}} \DecValTok{80} \SpecialCharTok{\textasciitilde{}} \StringTok{"65{-}79"}\NormalTok{,}
\NormalTok{                     age }\SpecialCharTok{\textless{}} \DecValTok{100} \SpecialCharTok{\textasciitilde{}} \StringTok{"80{-}99"}
\NormalTok{                     ),}
         \AttributeTok{age\_group =} \FunctionTok{factor}\NormalTok{(age\_group,}
                            \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"\textless{}35"}\NormalTok{,}
                                       \StringTok{"35{-}49"}\NormalTok{,}
                                       \StringTok{"50{-}64"}\NormalTok{,}
                                       \StringTok{"65{-}79"}\NormalTok{,}
                                       \StringTok{"80{-}99"}
\NormalTok{                                       )}
\NormalTok{                            )}
\NormalTok{         )}

\NormalTok{beps }\SpecialCharTok{|\textgreater{}}  
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-bepfitst-1.pdf}

}

\caption{\label{fig-bepfitst}Distribution of ages in the 1997-2001
British Election Panel Study}

\end{figure}

By default, \texttt{geom\_bar()} has created a count of the number of
times each age-group appears in the dataset. It does this because the
default `stat' for \texttt{geom\_bar()} is `count'. This saves us from
having to create that statistic ourselves. But if we had already
constructed a count (for instance, with
\texttt{beps\ \textbar{}\textgreater{}\ count(age)}), then we could also
specify a column of values for the y-axis and then use
\texttt{stat\ =\ "identity"}.

We may also like to consider different groupings of the data, for
instance, looking at age-groups by which party the respondent supports
(Figure~\ref{fig-bepsecond}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-bepsecond-1.pdf}

}

\caption{\label{fig-bepsecond}Distribution of ages, and vote preference,
in the 1997-2001 British Election Panel Study}

\end{figure}

The default is that these different groups are stacked, but they can be
placed side-by-side with \texttt{position\ =\ "dodge"}
(Figure~\ref{fig-bepthird}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-bepthird-1.pdf}

}

\caption{\label{fig-bepthird}Distribution of age-groups, and vote
preference, in the 1997-2001 British Election Panel Study}

\end{figure}

At this point, we may like to address the general look of the graph.
There are various themes that are built into \texttt{ggplot2}. Some of
these include \texttt{theme\_bw()}, \texttt{theme\_classic()},
\texttt{theme\_dark()}, and \texttt{theme\_minimal()}. A full list is
available at the \texttt{ggplot2}
\href{https://github.com/rstudio/cheatsheets/blob/main/data-visualization.pdf}{cheatsheet}.
We can use these themes by adding them as a layer
(Figure~\ref{fig-bepthemes}). Here we can use \texttt{patchwork}
(Pedersen 2020) to bring together multiple graphs. To do this we assign
the graph to a name, and then use `+' to signal which should be next to
each other, `/' to signal which would be on top, and brackets for
precedence.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(patchwork)}

\NormalTok{theme\_bw }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}

\NormalTok{theme\_classic }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}

\NormalTok{theme\_dark }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_dark}\NormalTok{()}

\NormalTok{theme\_minimal }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}

\NormalTok{(theme\_bw }\SpecialCharTok{+}\NormalTok{ theme\_classic) }\SpecialCharTok{/}\NormalTok{ (theme\_dark }\SpecialCharTok{+}\NormalTok{ theme\_minimal)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-bepthemes-1.pdf}

}

\caption{\label{fig-bepthemes}Distribution of age-groups, and vote
preference, in the 1997-2001 British Election Panel Study, illustrating
different themes}

\end{figure}

We can install themes from other packages, including \texttt{ggthemes}
(Arnold 2021), and \texttt{hrbrthemes} (Rudis 2020). And we can also
build our own.

The default labels use dby \texttt{ggplot2} are from the name of the
relevant variable, and it is often useful to add more detail. We could
add a title and caption at this point. A caption can be useful to add
information about the source of the dataset. A title can be useful when
the graph is going to be considered outside of the context of our paper.
But in the case of a graph that will be included in a paper, the need to
cross-reference all graphs that are in a paper means that included a
title within \texttt{labs()} is unnecessary
(Figure~\ref{fig-withnicelabels}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Distribution of age{-}groups, and vote preference, in}
\StringTok{       the 1997{-}2001 British Election Panel Study"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Source: 1997{-}2001 British Election Panel Study."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-withnicelabels-1.pdf}

}

\caption{\label{fig-withnicelabels}Distribution of age-groups, and vote
preference, in the 1997-2001 British Election Panel Study}

\end{figure}

We use facets to create `many little graphics that are variations of a
single graphic' (L. Wilkinson 2005, 219). They are especially useful
when we want to specifically compare across some variable, but have
already used color. For instance, we may be interested to explain vote,
by age and gender (Figure~\ref{fig-facets}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(gender))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-facets-1.pdf}

}

\caption{\label{fig-facets}Distribution of age-group by gender, and vote
preference, in the 1997-2001 British Election Panel Study}

\end{figure}

We could change \texttt{facet\_wrap()} to wrap vertically instead of
horizontally with \texttt{dir\ =\ "v"}. Alternatively, we could specify
a number of rows, say \texttt{nrow\ =\ 2}, or a number of columns, say
\texttt{ncol\ =\ 2}. Additionally, by default, both facets will have the
same scales. We could enable both facets to have different scales with
\texttt{scales\ =\ "free"}, or just the x-axis
\texttt{scales\ =\ "free\_x"}, or just the y-axis
\texttt{scales\ =\ "free\_y"} (Figure~\ref{fig-facetsfancy}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(gender),}
             \AttributeTok{dir =} \StringTok{"v"}\NormalTok{,}
             \AttributeTok{scales =} \StringTok{"free"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-facetsfancy-1.pdf}

}

\caption{\label{fig-facetsfancy}Distribution of age-group by gender, and
vote preference, in the 1997-2001 British Election Panel Study}

\end{figure}

Finally, we can change the labels of the facets using
\texttt{labeller()} (Figure~\ref{fig-facetsfancylabels}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{female =} \StringTok{"Female"}\NormalTok{, }\AttributeTok{male =} \StringTok{"Male"}\NormalTok{)}

\NormalTok{beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(gender),}
             \AttributeTok{dir =} \StringTok{"v"}\NormalTok{,}
             \AttributeTok{scales =} \StringTok{"free"}\NormalTok{,}
             \AttributeTok{labeller =} \FunctionTok{labeller}\NormalTok{(}\AttributeTok{gender =}\NormalTok{ new\_labels))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-facetsfancylabels-1.pdf}

}

\caption{\label{fig-facetsfancylabels}Distribution of age-group by
gender, and vote preference, in the 1997-2001 British Election Panel
Study}

\end{figure}

There are a variety of different ways to change the colors, and many
palettes are available including from \texttt{RColorBrewer} (Neuwirth
2014), which we specify with \texttt{scale\_fill\_brewer()}, and
\texttt{viridis} (Garnier et al. 2021), which we specify with
\texttt{scale\_fill\_viridis()} and is particularly focused on
color-blind palettes (Figure~\ref{fig-usecolor}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(viridis)}
\FunctionTok{library}\NormalTok{(patchwork)}

\NormalTok{RColorBrewerBrBG }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Blues"}\NormalTok{)}

\NormalTok{RColorBrewerSet2 }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}

\NormalTok{viridis }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of respondents"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_fill\_viridis}\NormalTok{(}\AttributeTok{discrete =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{viridismagma }\OtherTok{\textless{}{-}} 
\NormalTok{  beps }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{fill =}\NormalTok{ vote)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group of respondent"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Voted for"}\NormalTok{) }\SpecialCharTok{+}
   \FunctionTok{scale\_fill\_viridis}\NormalTok{(}\AttributeTok{discrete =} \ConstantTok{TRUE}\NormalTok{, }
                      \AttributeTok{option =} \StringTok{"magma"}\NormalTok{)}

\NormalTok{(RColorBrewerBrBG }\SpecialCharTok{+}\NormalTok{ RColorBrewerSet2) }\SpecialCharTok{/}
\NormalTok{  (viridis }\SpecialCharTok{+}\NormalTok{ viridismagma)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-usecolor-1.pdf}

}

\caption{\label{fig-usecolor}Distribution of age-group and vote
preference, in the 1997-2001 British Election Panel Study}

\end{figure}

Details of the variety of palettes available in \texttt{RColorBrewer}
and \texttt{viridis} are available in their help files. Many different
palettes are available, and we can also build our own. That said, color
is something to be considered with a great deal of care and it should
only be added to increase the amount of information that is communicated
(Cleveland 1994). Colors should not be added to graphs
unnecessarily---that is to say, they must play some role. Typically,
that role is to distinguish different groups, and that implies making
the colors dissimilar. Colors may also be appropriate if there is some
relationship between the color and the variable, for instance if making
a graph of sales of, say, mangoes and raspberries, it could help the
reader if the colors were yellow and red, respectively (Franconeri et
al. 2021, 121).

\hypertarget{scatterplots}{%
\subsection{Scatterplots}\label{scatterplots}}

We are often interested in the relationship between two variables. We
can use scatterplots to show this information. Unless there is a good
reason to move to a different option, a scatterplot is almost always the
best choice (Weissgerber et al. 2015). Indeed, `among all forms of
statistical graphics, the scatterplot may be considered the most
versatile and generally useful invention in the entire history of
statistical graphics.' (Friendly and Wainer 2021, 121) To illustrate
scatterplots, we use \texttt{WDI} (Arel-Bundock 2021b) to download some
economic indicators from the World Bank, and in particular
\texttt{WDIsearch()} to find the unique key that need to pass to
\texttt{WDI()} to facilitate the download.

\begin{quote}
\textbf{Oh, you think we have good data on that!} Gross Domestic Product
(GDP) `combines in a single figure, and with no double counting, all the
output (or production) carried out by all the firms, non-profit
institutions, government bodies and households in a given country during
a given period, regardless of the type of goods and services produced,
provided that the production takes place within the country's economic
territory' (OECD (2014), p.~15). The modern concept was developed by
Simon Kuznets and is widely used and reported. There is a certain
comfort in having a definitive and concrete single number to describe
something as complicated as the entire economic activity of a country.
And it is crucial that we have such summary statistics. But as with any
summary statistic, its strength is also its weakness. A single number
necessarily loses information about constituent components, and these
distributional differences are critical. It highlights short term
economic progress over longer term improvements. And `the quantitative
definiteness of the estimates makes it easy to forget their dependence
upon imperfect data and the consequently wide margins of possible error
to which both totals and components are liable' (Kuznets 1941, xxvi).
Reliance on any one summary measure of economic performance presents a
misguided picture not only of a country's economy, but also of its
peoples.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{\textquotesingle{}WDI\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(WDI)}
\FunctionTok{WDIsearch}\NormalTok{(}\StringTok{"gdp growth"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     indicator              name                                    
[1,] "5.51.01.10.gdp"       "Per capita GDP growth"                 
[2,] "6.0.GDP_growth"       "GDP growth (annual %)"                 
[3,] "NV.AGR.TOTL.ZG"       "Real agricultural GDP growth rates (%)"
[4,] "NY.GDP.MKTP.KD.ZG"    "GDP growth (annual %)"                 
[5,] "NY.GDP.MKTP.KN.87.ZG" "GDP growth (annual %)"                 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{WDIsearch}\NormalTok{(}\StringTok{"inflation"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     indicator              name                                               
[1,] "FP.CPI.TOTL.ZG"       "Inflation, consumer prices (annual %)"            
[2,] "FP.FPI.TOTL.ZG"       "Inflation, food prices (annual %)"                
[3,] "FP.WPI.TOTL.ZG"       "Inflation, wholesale prices (annual %)"           
[4,] "NY.GDP.DEFL.87.ZG"    "Inflation, GDP deflator (annual %)"               
[5,] "NY.GDP.DEFL.KD.ZG"    "Inflation, GDP deflator (annual %)"               
[6,] "NY.GDP.DEFL.KD.ZG.AD" "Inflation, GDP deflator: linked series (annual %)"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{WDIsearch}\NormalTok{(}\StringTok{"population, total"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          indicator                name 
      "SP.POP.TOTL" "Population, total" 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{WDIsearch}\NormalTok{(}\StringTok{"Unemployment, total"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     indicator          
[1,] "SL.UEM.TOTL.NE.ZS"
[2,] "SL.UEM.TOTL.ZS"   
     name                                                                 
[1,] "Unemployment, total (% of total labor force) (national estimate)"   
[2,] "Unemployment, total (% of total labor force) (modeled ILO estimate)"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{WDI}\NormalTok{(}\AttributeTok{indicator =} \FunctionTok{c}\NormalTok{(}\StringTok{"FP.CPI.TOTL.ZG"}\NormalTok{,}
                    \StringTok{"NY.GDP.MKTP.KD.ZG"}\NormalTok{,}
                    \StringTok{"SP.POP.TOTL"}\NormalTok{,}
                    \StringTok{"SL.UEM.TOTL.NE.ZS"}
\NormalTok{                    ),}
      \AttributeTok{country =} \FunctionTok{c}\NormalTok{(}\StringTok{"AU"}\NormalTok{, }\StringTok{"ET"}\NormalTok{, }\StringTok{"IN"}\NormalTok{, }\StringTok{"US"}\NormalTok{)}
\NormalTok{      )}
\end{Highlighting}
\end{Shaded}

We may like to change the names to be more meaningful, and only keep the
columns that we need.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{inflation =}\NormalTok{ FP.CPI.TOTL.ZG,}
         \AttributeTok{gdp\_growth =}\NormalTok{ NY.GDP.MKTP.KD.ZG,}
         \AttributeTok{population =}\NormalTok{ SP.POP.TOTL,}
         \AttributeTok{unemployment\_rate =}\NormalTok{ SL.UEM.TOTL.NE.ZS}
\NormalTok{         ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{iso2c)}

\FunctionTok{head}\NormalTok{(world\_bank\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 6
  country    year inflation gdp_growth population unemployment_rate
  <chr>     <dbl>     <dbl>      <dbl>      <dbl>             <dbl>
1 Australia  1960     3.73       NA      10276477                NA
2 Australia  1961     2.29        2.48   10483000                NA
3 Australia  1962    -0.319       1.29   10742000                NA
4 Australia  1963     0.641       6.21   10950000                NA
5 Australia  1964     2.87        6.98   11167000                NA
6 Australia  1965     3.41        5.98   11388000                NA
\end{verbatim}

To get started we can use \texttt{geom\_point()} to make a scatterplot
showing GDP growth and inflation, by country
(Figure~\ref{fig-scattorplot}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-scattorplot-1.pdf}

}

\caption{\label{fig-scattorplot}Relationship between inflation and GDP
growth for Australia, Ethiopia, India, and the US}

\end{figure}

As with bar charts, we change the theme, and update the labels
(Figure~\ref{fig-scatterplotnicer}), although again, we would normally
not need both a caption and a title and would just use one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Relationship between inflation and GDP growth"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-scatterplotnicer-1.pdf}

}

\caption{\label{fig-scatterplotnicer}Relationship between inflation and
GDP growth for Australia, Ethiopia, India, and the US}

\end{figure}

Here we use `color' instead of `fill' because we are using dots rather
than bars. This also then slightly affects how we change the palette
(Figure~\ref{fig-scatterplotnicercolor}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(patchwork)}

\NormalTok{RColorBrewerBrBG }\OtherTok{\textless{}{-}}
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Blues"}\NormalTok{)}

\NormalTok{RColorBrewerSet2 }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}

\NormalTok{viridis }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_viridis\_d}\NormalTok{()}

\NormalTok{viridismagma }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_viridis\_d}\NormalTok{(}\AttributeTok{option =} \StringTok{"magma"}\NormalTok{)}

\NormalTok{RColorBrewerBrBG }\SpecialCharTok{/} 
\NormalTok{  RColorBrewerSet2 }\SpecialCharTok{/}
\NormalTok{  viridis }\SpecialCharTok{/}
\NormalTok{  viridismagma}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-scatterplotnicercolor-1.pdf}

}

\caption{\label{fig-scatterplotnicercolor}Relationship between inflation
and GDP growth for Australia, Ethiopia, India, and the US}

\end{figure}

The points of a scatterplot sometimes overlap. We can address this
situation in one of two ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Adding a degree of transparency to our dots with `alpha'
  (Figure~\ref{fig-alphaplot}). The value for `alpha' can vary between
  0, which is fully transparent, and 1, which is completely opaque.
\item
  Adding a small about of noise, which slightly moves the points, using
  \texttt{geom\_jitter()} (Figure~\ref{fig-jitterplot}). By default, the
  movement is uniform in both directions, but we can specify which
  direction movement occurs with `width' or `height'. The decision
  between these two options turns on the degree to which exact accuracy
  matters, and the number of points.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-alphaplot-1.pdf}

}

\caption{\label{fig-alphaplot}Relationship between inflation and GDP
growth for Australia, Ethiopia, India, and the US}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-jitterplot-1.pdf}

}

\caption{\label{fig-jitterplot}Relationship between inflation and GDP
growth for Australia, Ethiopia, India, and the US}

\end{figure}

A common use case for a scatterplot is to illustrate a relationship
between two variables. It can be useful to add a line of best fit using
\texttt{geom\_smooth()} (Figure~\ref{fig-scattorplottwo}). By default,
\texttt{geom\_smooth()} will use LOESS smoothing is used for datasets
with less than 1,000 observations, but we can specify the relationship
using `method', change the color with `color' and remove standard errors
with `se'. Using \texttt{geom\_smooth()} adds a layer to the graph, and
so it inherits aesthetics from \texttt{ggplot()}. For instance, that is
why we initially have one line for each country in
Figure~\ref{fig-scattorplottwo}). We could overwrite that by specifying
a particular color, which we do in the third graph of
Figure~\ref{fig-scattorplottwo}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{defaults }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{straightline }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =}\NormalTok{ lm, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{onestraightline }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{y =}\NormalTok{ inflation, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =}\NormalTok{ lm, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{defaults }\SpecialCharTok{/} 
\NormalTok{  straightline }\SpecialCharTok{/}
\NormalTok{  onestraightline}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-scattorplottwo-1.pdf}

}

\caption{\label{fig-scattorplottwo}Relationship between inflation and
GDP growth for Australia, Ethiopia, India, and the US}

\end{figure}

\hypertarget{line-plots}{%
\subsection{Line plots}\label{line-plots}}

We can use a line plot when we have variables that should be joined
together, for instance, an economic time series. We will continue with
the dataset from the World Bank and focus on US GDP growth using
\texttt{geom\_line()} (Figure~\ref{fig-lineplot}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-lineplot-1.pdf}

}

\caption{\label{fig-lineplot}US GDP growth (1961-2020)}

\end{figure}

As before, we can adjust the theme, say with \texttt{theme\_minimal()}
and labels with \texttt{labs()} (Figure~\ref{fig-lineplottwo}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-lineplottwo-1.pdf}

}

\caption{\label{fig-lineplottwo}US GDP growth (1961-2020)}

\end{figure}

We can use a slight variant of \texttt{geom\_line()},
\texttt{geom\_step()} to focus attention on the change from year to year
(Figure~\ref{fig-stepplot}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_step}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"GDP growth"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-stepplot-1.pdf}

}

\caption{\label{fig-stepplot}US GDP growth (1961-2020)}

\end{figure}

The Phillips curve is the name given to plot of the relationship between
unemployment and inflation over time. An inverse relationship is
sometimes found in the data, for instance in the UK between 1861 and
1957 (Phillips 1958). We have a variety of ways to investigate this
including:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Adding a second line to our graph. For instance, we could add
  inflation (Figure~\ref{fig-notphillips}). This may require use to use
  \texttt{pivot\_longer()} to ensure that the data are in tidy format.
\item
  Using \texttt{geom\_path()} to links values in the order they appear
  in the dataset. In Figure~\ref{fig-phillipsmyboy}) we show a Phillips
  curve for the US between 1960 and 2020.
  Figure~\ref{fig-phillipsmyboy}) does not appear to show any clear
  relationship between unemployment and inflation.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{population, }\SpecialCharTok{{-}}\NormalTok{gdp\_growth) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"inflation"}\NormalTok{, }\StringTok{"unemployment\_rate"}\NormalTok{),}
               \AttributeTok{names\_to =} \StringTok{"series"}\NormalTok{,}
               \AttributeTok{values\_to =} \StringTok{"value"}
\NormalTok{               ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ value, }\AttributeTok{color =}\NormalTok{ series)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Value"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Economic indicator"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{, }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Inflation"}\NormalTok{, }\StringTok{"Unemployment"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-notphillips-1.pdf}

}

\caption{\label{fig-notphillips}Unemployment and inflation for the US
(1960-2020)}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"United States"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ unemployment\_rate, }\AttributeTok{y =}\NormalTok{ inflation)) }\SpecialCharTok{+}
  \FunctionTok{geom\_path}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Unemployment rate"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-phillipsmyboy-1.pdf}

}

\caption{\label{fig-phillipsmyboy}Phillips curve for the US (1960-2020)}

\end{figure}

\hypertarget{histograms}{%
\subsection{Histograms}\label{histograms}}

A histogram is useful to show the shape of a continuous variable and
works by constructing counts of the number of observations in different
subsets of the support, called `bins'. In Figure~\ref{fig-hisogramone})
we examine the distribution of GDP in Ethiopia.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"Ethiopia"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-hisogramone-1.pdf}

}

\caption{\label{fig-hisogramone}Distribution of GDP in Ethiopia
(1960-2020)}

\end{figure}

And again we can add a theme and labels (Figure~\ref{fig-hisogramtwo}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"Ethiopia"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-hisogramtwo-1.pdf}

}

\caption{\label{fig-hisogramtwo}Distribution of GDP in Ethiopia
(1960-2020)}

\end{figure}

The key component determining the shape of a histogram is the number of
bins. This can be specified in one of two ways
(Figure~\ref{fig-hisogrambins}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  specifying the number of `bins' to include, or
\item
  specifying how wide they should be with `binwidth'.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{twobins }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"Ethiopia"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{fivebins }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"Ethiopia"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{twentybins }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"Ethiopia"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{20}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{halfbinwidth }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"Ethiopia"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{twobinwidth }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"Ethiopia"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{fivebinwidth }\OtherTok{\textless{}{-}} 
\NormalTok{  world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"Ethiopia"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{)}

\NormalTok{(twobins }\SpecialCharTok{+}\NormalTok{ fivebins }\SpecialCharTok{+}\NormalTok{ twentybins) }\SpecialCharTok{/} 
\NormalTok{  (halfbinwidth }\SpecialCharTok{+}\NormalTok{ twobinwidth }\SpecialCharTok{+}\NormalTok{ fivebinwidth)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-hisogrambins-1.pdf}

}

\caption{\label{fig-hisogrambins}Distribution of GDP in Ethiopia
(1960-2020)}

\end{figure}

The histogram is smoothing the data, and the number of bins affects how
much smoothing occurs. When there are only two bins then the data are
very smooth, but we have lost a great deal of accuracy. More
specifically, `the histogram estimator is a piecewise constant function
where the height of the function is proportional to the number of
observations in each bin' (Wasserman 2005, 303). Too few bins result in
a biased estimator, while too many bins results in an estimator with
high variance. Our decision as to the number of bins, or their width, is
concerned with trying to balance bias and variance. This will depend on
a variety of concerns including the subject matter and the goal
(Cleveland 1994, 135).

Finally, while we can use `fill' to distinguish between different types
of observations, it can get quite messy. It is usually better to give
away showing the distribution with columns and instead trace the outline
of the distribution, using \texttt{geom\_freqpoly()}
(Figure~\ref{fig-freq}) or to build it up using dots with
\texttt{geom\_dotplot()} (Figure~\ref{fig-dotplot}) .

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{color =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_freqpoly}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-freq-1.pdf}

}

\caption{\label{fig-freq}Distribution of GDP in four countries
(1960-2020)}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gdp\_growth, }\AttributeTok{group =}\NormalTok{ country, }\AttributeTok{fill =}\NormalTok{ country)) }\SpecialCharTok{+}
  \FunctionTok{geom\_dotplot}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}histodot\textquotesingle{}}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"GDP"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-dotplot-1.pdf}

}

\caption{\label{fig-dotplot}Distribution of GDP in four countries
(1960-2020)}

\end{figure}

\hypertarget{boxplots}{%
\subsection{Boxplots}\label{boxplots}}

Boxplots are almost never an appropriate choice because they hide the
distribution of data, rather than show it. Unless we need to compare the
summary statistics of many variables at once, then they should almost
never be used. This is because the same boxplot can apply to very
different distributions. To see this, consider some simulated data from
the beta distribution of two types. One type of data contains draws from
two beta distributions: one that is right skewed and another that is
left skewed. The other type of data contains draws from a beta
distribution with no skew.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{both\_left\_and\_right\_skew }\OtherTok{\textless{}{-}} 
  \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rbeta}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{),}
    \FunctionTok{rbeta}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{    )}

\NormalTok{no\_skew }\OtherTok{\textless{}{-}} 
  \FunctionTok{rbeta}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\NormalTok{beta\_distributions }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{observation =} \FunctionTok{c}\NormalTok{(both\_left\_and\_right\_skew, no\_skew),}
    \AttributeTok{source =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Left and right skew"}\NormalTok{, }\DecValTok{1000}\NormalTok{),}
               \FunctionTok{rep}\NormalTok{(}\StringTok{"No skew"}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{               )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We can first compare the boxplots of the two series
(Figure~\ref{fig-boxplotfirst}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_distributions }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ source, }\AttributeTok{y =}\NormalTok{ observation)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-boxplotfirst-1.pdf}

}

\caption{\label{fig-boxplotfirst}Data drawn from beta distributions with
different parameters}

\end{figure}

But if we plot the actual data then we can see how different they are
(Figure~\ref{fig-freqpolyofdistributions}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_distributions }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ observation, }\AttributeTok{color =}\NormalTok{ source)) }\SpecialCharTok{+}
  \FunctionTok{geom\_freqpoly}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.05}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-freqpolyofdistributions-1.pdf}

}

\caption{\label{fig-freqpolyofdistributions}Data drawn from beta
distributions with different parameters}

\end{figure}

One way forward, if a boxplot must be included, is to include the actual
data as a layer on top of the boxplot. For instance, in
Figure~\ref{fig-bloxplotandoverlay}) we show the distribution of
inflation across the four countries.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ country, }\AttributeTok{y =}\NormalTok{ inflation)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.15}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Country"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Inflation"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: World Bank."}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-bloxplotandoverlay-1.pdf}

}

\caption{\label{fig-bloxplotandoverlay}Distribution of unemployment data
for four countries (1960-2020)}

\end{figure}

\hypertarget{tables}{%
\section{Tables}\label{tables}}

Tables are critical for telling a compelling story. Tables can
communicate less information than a graph, but they can do so at a high
fidelity. We primarily use tables in three ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To show some of our actual dataset, for which we use \texttt{kable()}
  from \texttt{knitr} (Xie 2021), alongside \texttt{kableExtra} (Zhu
  2020).
\item
  To communicate summary statistics, for which we use
  \texttt{modelsummary} (Arel-Bundock 2021a).
\item
  To display regression results, for which we also use
  \texttt{modelsummary} (Arel-Bundock 2021a).
\end{enumerate}

\hypertarget{showing-part-of-a-dataset}{%
\subsection{Showing part of a dataset}\label{showing-part-of-a-dataset}}

We illustrate showing part of a dataset using \texttt{kable()} from
\texttt{knitr} and drawing on \texttt{kableExtra} for enhancement. We
again use the World Bank dataset that we downloaded earlier.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{head}\NormalTok{(world\_bank\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 6
  country    year inflation gdp_growth population unemployment_rate
  <chr>     <dbl>     <dbl>      <dbl>      <dbl>             <dbl>
1 Australia  1960     3.73       NA      10276477                NA
2 Australia  1961     2.29        2.48   10483000                NA
3 Australia  1962    -0.319       1.29   10742000                NA
4 Australia  1963     0.641       6.21   10950000                NA
5 Australia  1964     2.87        6.98   11167000                NA
6 Australia  1965     3.41        5.98   11388000                NA
\end{verbatim}

To begin, we can display the first ten rows with the default
\texttt{kable()} settings.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1515}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0758}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2727}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
country
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
year
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
inflation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
gdp\_growth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
population
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
unemployment\_rate
\end{minipage} \\
\midrule()
\endhead
Australia & 1960 & 3.7288136 & NA & 10276477 & NA \\
Australia & 1961 & 2.2875817 & 2.483271 & 10483000 & NA \\
Australia & 1962 & -0.3194888 & 1.294468 & 10742000 & NA \\
Australia & 1963 & 0.6410256 & 6.214949 & 10950000 & NA \\
Australia & 1964 & 2.8662420 & 6.978540 & 11167000 & NA \\
Australia & 1965 & 3.4055728 & 5.980893 & 11388000 & NA \\
Australia & 1966 & 3.2934132 & 2.381966 & 11651000 & NA \\
Australia & 1967 & 3.4782609 & 6.303650 & 11799000 & NA \\
Australia & 1968 & 2.5210084 & 5.095103 & 12009000 & NA \\
Australia & 1969 & 3.2786885 & 7.043526 & 12263000 & NA \\
\bottomrule()
\end{longtable}

In order to be able to cross-reference it in text, we need to add a
caption with `caption'. We can also make the column names more
information with `col.names' and specify the number of digits to be
displayed (Table~\ref{tbl-gdpfirst}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(    }
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Country"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Inflation"}\NormalTok{, }\StringTok{"GDP growth"}\NormalTok{, }\StringTok{"Population"}\NormalTok{, }\StringTok{"Unemployment rate"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-gdpfirst}{}
\begin{longtable}[]{@{}lrrrrr@{}}
\caption{\label{tbl-gdpfirst}First ten rows of a dataset of economic
indicators for Australia, Ethiopia, India, and the US}\tabularnewline
\toprule()
Country & Year & Inflation & GDP growth & Population & Unemployment
rate \\
\midrule()
\endfirsthead
\toprule()
Country & Year & Inflation & GDP growth & Population & Unemployment
rate \\
\midrule()
\endhead
Australia & 1960 & 3.7 & NA & 10276477 & NA \\
Australia & 1961 & 2.3 & 2.5 & 10483000 & NA \\
Australia & 1962 & -0.3 & 1.3 & 10742000 & NA \\
Australia & 1963 & 0.6 & 6.2 & 10950000 & NA \\
Australia & 1964 & 2.9 & 7.0 & 11167000 & NA \\
Australia & 1965 & 3.4 & 6.0 & 11388000 & NA \\
Australia & 1966 & 3.3 & 2.4 & 11651000 & NA \\
Australia & 1967 & 3.5 & 6.3 & 11799000 & NA \\
Australia & 1968 & 2.5 & 5.1 & 12009000 & NA \\
Australia & 1969 & 3.3 & 7.0 & 12263000 & NA \\
\bottomrule()
\end{longtable}

When producing PDFs, the `booktabs' option makes a host of small changes
to the default display and results in tables that look better
(Table~\ref{tbl-gdpbookdtabs}). When using `booktabs' we additionally
should specify `linesep' otherwise \texttt{kable()} adds a small space
every five lines. (None of this will show up for html output.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Country"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Inflation"}\NormalTok{, }\StringTok{"GDP growth"}\NormalTok{, }\StringTok{"Population"}\NormalTok{, }\StringTok{"Unemployment rate"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }
    \AttributeTok{linesep =} \StringTok{""}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-gdpbookdtabs}{}
\begin{longtable}[]{@{}lrrrrr@{}}
\caption{\label{tbl-gdpbookdtabs}First ten rows of a dataset of economic
indicators for Australia, Ethiopia, India, and the US}\tabularnewline
\toprule()
Country & Year & Inflation & GDP growth & Population & Unemployment
rate \\
\midrule()
\endfirsthead
\toprule()
Country & Year & Inflation & GDP growth & Population & Unemployment
rate \\
\midrule()
\endhead
Australia & 1960 & 3.7 & NA & 10276477 & NA \\
Australia & 1961 & 2.3 & 2.5 & 10483000 & NA \\
Australia & 1962 & -0.3 & 1.3 & 10742000 & NA \\
Australia & 1963 & 0.6 & 6.2 & 10950000 & NA \\
Australia & 1964 & 2.9 & 7.0 & 11167000 & NA \\
Australia & 1965 & 3.4 & 6.0 & 11388000 & NA \\
Australia & 1966 & 3.3 & 2.4 & 11651000 & NA \\
Australia & 1967 & 3.5 & 6.3 & 11799000 & NA \\
Australia & 1968 & 2.5 & 5.1 & 12009000 & NA \\
Australia & 1969 & 3.3 & 7.0 & 12263000 & NA \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Country"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Inflation"}\NormalTok{, }\StringTok{"GDP growth"}\NormalTok{, }\StringTok{"Population"}\NormalTok{, }\StringTok{"Unemployment rate"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-gdpbookdtabsnolinesep}{}
\begin{longtable}[]{@{}lrrrrr@{}}
\caption{\label{tbl-gdpbookdtabsnolinesep}First ten rows of a dataset of
economic indicators for Australia, Ethiopia, India, and the
US}\tabularnewline
\toprule()
Country & Year & Inflation & GDP growth & Population & Unemployment
rate \\
\midrule()
\endfirsthead
\toprule()
Country & Year & Inflation & GDP growth & Population & Unemployment
rate \\
\midrule()
\endhead
Australia & 1960 & 3.7 & NA & 10276477 & NA \\
Australia & 1961 & 2.3 & 2.5 & 10483000 & NA \\
Australia & 1962 & -0.3 & 1.3 & 10742000 & NA \\
Australia & 1963 & 0.6 & 6.2 & 10950000 & NA \\
Australia & 1964 & 2.9 & 7.0 & 11167000 & NA \\
Australia & 1965 & 3.4 & 6.0 & 11388000 & NA \\
Australia & 1966 & 3.3 & 2.4 & 11651000 & NA \\
Australia & 1967 & 3.5 & 6.3 & 11799000 & NA \\
Australia & 1968 & 2.5 & 5.1 & 12009000 & NA \\
Australia & 1969 & 3.3 & 7.0 & 12263000 & NA \\
\bottomrule()
\end{longtable}

We can specify the alignment of the columns using a character vector of
`l' (left), `c' (centre), and `r' (right) (Table~\ref{tbl-gdpalign}).
Additionally, we can change the formatting. For instance, we could
specify groupings for numbers that are at least one thousand using
`format.args = list(big.mark = ``,'')'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year =} \FunctionTok{as.factor}\NormalTok{(year)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kable}\NormalTok{(}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Country"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Inflation"}\NormalTok{, }\StringTok{"GDP growth"}\NormalTok{, }
                  \StringTok{"Population"}\NormalTok{, }\StringTok{"Unemployment rate"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }
    \AttributeTok{linesep =} \StringTok{""}\NormalTok{,}
    \AttributeTok{align =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{),}
    \AttributeTok{format.args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{big.mark =} \StringTok{","}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-gdpalign}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1493}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0746}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1642}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1791}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1642}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2687}}@{}}
\caption{\label{tbl-gdpalign}First ten rows of a dataset of economic
indicators for Australia, Ethiopia, India, and the US}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Country
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Year
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Inflation
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
GDP growth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Population
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Unemployment rate
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Country
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Year
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Inflation
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
GDP growth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Population
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Unemployment rate
\end{minipage} \\
\midrule()
\endhead
Australia & 1960 & 3.7 & NA & 10,276,477 & NA \\
Australia & 1961 & 2.3 & 2.5 & 10,483,000 & NA \\
Australia & 1962 & -0.3 & 1.3 & 10,742,000 & NA \\
Australia & 1963 & 0.6 & 6.2 & 10,950,000 & NA \\
Australia & 1964 & 2.9 & 7.0 & 11,167,000 & NA \\
Australia & 1965 & 3.4 & 6.0 & 11,388,000 & NA \\
Australia & 1966 & 3.3 & 2.4 & 11,651,000 & NA \\
Australia & 1967 & 3.5 & 6.3 & 11,799,000 & NA \\
Australia & 1968 & 2.5 & 5.1 & 12,009,000 & NA \\
Australia & 1969 & 3.3 & 7.0 & 12,263,000 & NA \\
\bottomrule()
\end{longtable}

We can use \texttt{kableExtra} (Zhu 2020) to add extra functionality to
\texttt{kable}. For instance, we could add a row that groups some of the
columns (Table~\ref{tbl-gdpalign}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(kableExtra)}

\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Country"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"Inflation"}\NormalTok{, }\StringTok{"GDP growth"}\NormalTok{, }
                  \StringTok{"Population"}\NormalTok{, }\StringTok{"Unemployment rate"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }
    \AttributeTok{linesep =} \StringTok{""}\NormalTok{,}
    \AttributeTok{align =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{),}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{add\_header\_above}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{" "} \OtherTok{=} \DecValTok{2}\NormalTok{, }\StringTok{"Economic indicators"} \OtherTok{=} \DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-gdpkableextra}{}
\begin{table}
\caption{\label{tbl-gdpkableextra}First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US }

\centering
\begin{tabular}{llccrr}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{4}{c}{Economic indicators} \\
\cmidrule(l{3pt}r{3pt}){3-6}
Country & Year & Inflation & GDP growth & Population & Unemployment rate\\
\midrule
Australia & 1960 & 3.7 & NA & 10276477 & NA\\
Australia & 1961 & 2.3 & 2.5 & 10483000 & NA\\
Australia & 1962 & -0.3 & 1.3 & 10742000 & NA\\
Australia & 1963 & 0.6 & 6.2 & 10950000 & NA\\
Australia & 1964 & 2.9 & 7.0 & 11167000 & NA\\
Australia & 1965 & 3.4 & 6.0 & 11388000 & NA\\
Australia & 1966 & 3.3 & 2.4 & 11651000 & NA\\
Australia & 1967 & 3.5 & 6.3 & 11799000 & NA\\
Australia & 1968 & 2.5 & 5.1 & 12009000 & NA\\
Australia & 1969 & 3.3 & 7.0 & 12263000 & NA\\
\bottomrule
\end{tabular}
\end{table}

Another especially nice way to build tables is to use \texttt{gt}
(Iannone, Cheng, and Schloerke 2020).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gt)}

\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{gt}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{lrrrrr}
\toprule
country & year & inflation & gdp\_growth & population & unemployment\_rate \\ 
\midrule
Australia & 1960 & 3.7288136 & NA & 10276477 & NA \\ 
Australia & 1961 & 2.2875817 & 2.483271 & 10483000 & NA \\ 
Australia & 1962 & -0.3194888 & 1.294468 & 10742000 & NA \\ 
Australia & 1963 & 0.6410256 & 6.214949 & 10950000 & NA \\ 
Australia & 1964 & 2.8662420 & 6.978540 & 11167000 & NA \\ 
Australia & 1965 & 3.4055728 & 5.980893 & 11388000 & NA \\ 
Australia & 1966 & 3.2934132 & 2.381966 & 11651000 & NA \\ 
Australia & 1967 & 3.4782609 & 6.303650 & 11799000 & NA \\ 
Australia & 1968 & 2.5210084 & 5.095103 & 12009000 & NA \\ 
Australia & 1969 & 3.2786885 & 7.043526 & 12263000 & NA \\ 
 \bottomrule
\end{longtable}

Again, we can add a caption and more informative column labels
(Table~\ref{tbl-dsdfweasdf}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{gt}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{cols\_label}\NormalTok{(}
      \AttributeTok{country =} \StringTok{"Country"}\NormalTok{,}
      \AttributeTok{year =} \StringTok{"Year"}\NormalTok{,}
      \AttributeTok{inflation =} \StringTok{"Inflation"}\NormalTok{,}
      \AttributeTok{gdp\_growth =} \StringTok{"GDP growth"}\NormalTok{,}
      \AttributeTok{population =} \StringTok{"Population"}\NormalTok{,}
      \AttributeTok{unemployment\_rate =} \StringTok{"Unemployment rate"}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-dsdfweasdf}{}
\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}
\caption{\label{tbl-dsdfweasdf}First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the US }
{lrrrrr}
\toprule
Country & Year & Inflation & GDP growth & Population & Unemployment rate \\ 
\midrule
Australia & 1960 & 3.7288136 & NA & 10276477 & NA \\ 
Australia & 1961 & 2.2875817 & 2.483271 & 10483000 & NA \\ 
Australia & 1962 & -0.3194888 & 1.294468 & 10742000 & NA \\ 
Australia & 1963 & 0.6410256 & 6.214949 & 10950000 & NA \\ 
Australia & 1964 & 2.8662420 & 6.978540 & 11167000 & NA \\ 
Australia & 1965 & 3.4055728 & 5.980893 & 11388000 & NA \\ 
Australia & 1966 & 3.2934132 & 2.381966 & 11651000 & NA \\ 
Australia & 1967 & 3.4782609 & 6.303650 & 11799000 & NA \\ 
Australia & 1968 & 2.5210084 & 5.095103 & 12009000 & NA \\ 
Australia & 1969 & 3.2786885 & 7.043526 & 12263000 & NA \\ 
 \bottomrule
\end{longtable}

\hypertarget{communicating-summary-statistics}{%
\subsection{Communicating summary
statistics}\label{communicating-summary-statistics}}

We can use \texttt{datasummary()} from \texttt{modelsummary} to create
tables of summary statistics from our dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(modelsummary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Attaching package: 'modelsummary'
\end{verbatim}

\begin{verbatim}
The following object is masked from 'package:gt':

    escape_latex
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{datasummary\_skim}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lrrrrrrr>{}r}
\toprule
  & Unique (\#) & Missing (\%) & Mean & SD & Min & Median & Max &   \\
\midrule
year & 61 & 0 & \num{1990.0} & \num{17.6} & \num{1960.0} & \num{1990.0} & \num{2020.0} & \includegraphics[width=0.67in, height=0.17in]{06-static_communication_files/figure-latex//hist_f7037a6a980a.pdf}\\
inflation & 238 & 3 & \num{6.1} & \num{6.3} & \num{-9.8} & \num{4.3} & \num{44.4} & \includegraphics[width=0.67in, height=0.17in]{06-static_communication_files/figure-latex//hist_f7032d419b00.pdf}\\
gdp\_growth & 220 & 10 & \num{4.2} & \num{3.7} & \num{-11.1} & \num{3.9} & \num{13.9} & \includegraphics[width=0.67in, height=0.17in]{06-static_communication_files/figure-latex//hist_f703fd4dce6.pdf}\\
population & 244 & 0 & \num{304177482.9} & \num{380093166.9} & \num{10276477.0} & \num{147817291.5} & \num{1380004385.0} & \includegraphics[width=0.67in, height=0.17in]{06-static_communication_files/figure-latex//hist_f7032bc92b4e.pdf}\\
unemployment\_rate & 104 & 52 & \num{6.0} & \num{1.9} & \num{1.2} & \num{5.7} & \num{10.9} & \includegraphics[width=0.67in, height=0.17in]{06-static_communication_files/figure-latex//hist_f7036527de55.pdf}\\
\bottomrule
\end{tabular}
\end{table}

By default, \texttt{datasummary()} summarizes the `numeric' variables,
but we can ask for the `categorical' variables
(Table~\ref{tbl-testdatasummary}). Additionally we can add
cross-references in the same way as \texttt{kable()}, that is, include a
title and then cross-reference the name of the R chunk.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{datasummary\_skim}\NormalTok{(}\AttributeTok{type =} \StringTok{"categorical"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-testdatasummary}{}
\begin{table}
\caption{\label{tbl-testdatasummary}Summary of categorical economic indicator variables for four countries }

\centering
\begin{tabular}[t]{lrr}
\toprule
country & N & \%\\
\midrule
Australia & 61 & \num{25.0}\\
Ethiopia & 61 & \num{25.0}\\
India & 61 & \num{25.0}\\
United States & 61 & \num{25.0}\\
\bottomrule
\end{tabular}
\end{table}

We can create a table that shows the correlation between variables using
\texttt{datasummary\_correlation()} (Table~\ref{tbl-correlationtable}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world\_bank\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{datasummary\_correlation}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-correlationtable}{}
\begin{table}
\caption{\label{tbl-correlationtable}Correlation between the economic indicator variables for four countries (Australia, Ethiopia, India, and the US) }

\centering
\begin{tabular}[t]{lrrrrr}
\toprule
  & year & inflation & gdp\_growth & population & unemployment\_rate\\
\midrule
year & 1 & . & . & . & .\\
inflation & \num{.00} & 1 & . & . & .\\
gdp\_growth & \num{.10} & \num{.00} & 1 & . & .\\
population & \num{.24} & \num{.07} & \num{.15} & 1 & .\\
unemployment\_rate & \num{-.13} & \num{-.14} & \num{-.31} & \num{-.35} & 1\\
\bottomrule
\end{tabular}
\end{table}

We typically need a table of descriptive statistics that we could add to
our paper (Table~\ref{tbl-descriptivestats}). This contrasts with
Table~\ref{tbl-testdatasummary}) which would likely not be included in a
paper. We can add a note about the source of the data using `notes'.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{datasummary\_balance}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{country,}
                    \AttributeTok{data =}\NormalTok{ world\_bank\_data,}
                    \AttributeTok{notes =} \StringTok{"Data source: World Bank."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-descriptivestats}{}
\begin{table}
\caption{\label{tbl-descriptivestats}Descriptive statistics for the inflation and GDP dataset }

\centering
\begin{tabular}[t]{lrrrrrrrr}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{Australia (N=61)} & \multicolumn{2}{c}{Ethiopia (N=61)} & \multicolumn{2}{c}{India (N=61)} & \multicolumn{2}{c}{United States (N=61)} \\
\cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5} \cmidrule(l{3pt}r{3pt}){6-7} \cmidrule(l{3pt}r{3pt}){8-9}
  & Mean & Std. Dev. & Mean  & Std. Dev.  & Mean   & Std. Dev.   & Mean    & Std. Dev.   \\
\midrule
year & 1990.0 & 17.8 & 1990.0 & 17.8 & 1990.0 & 17.8 & 1990.0 & 17.8\\
inflation & 4.7 & 3.8 & 8.7 & 10.4 & 7.4 & 5.0 & 3.7 & 2.8\\
gdp\_growth & 3.4 & 1.8 & 5.9 & 6.4 & 5.0 & 3.3 & 2.9 & 2.2\\
population & 17244215.9 & 4328625.6 & 55662437.9 & 27626912.1 & 888774544.9 & 292997809.4 & 255028733.1 & 45603604.8\\
unemployment\_rate & 6.8 & 1.7 & 2.6 & 0.9 & 3.5 & 1.4 & 6.0 & 1.6\\
\bottomrule
\multicolumn{9}{l}{\rule{0pt}{1em}Data source: World Bank.}\\
\end{tabular}
\end{table}

\hypertarget{display-regression-results}{%
\subsection{Display regression
results}\label{display-regression-results}}

Finally, one common reason for needing a table is to report regression
results. We will do this using \texttt{modelsummary()} from
\texttt{modelsummary} (Arel-Bundock 2021a).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ gdp\_growth }\SpecialCharTok{\textasciitilde{}}\NormalTok{ inflation, }
                  \AttributeTok{data =}\NormalTok{ world\_bank\_data)}

\FunctionTok{modelsummary}\NormalTok{(first\_model)}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lc}
\toprule
  & Model 1\\
\midrule
(Intercept) & \num{4.157}\\
 & (\num{0.352})\\
inflation & \num{-0.002}\\
 & (\num{0.041})\\
\midrule
Num.Obs. & \num{218}\\
R2 & \num{0.000}\\
R2 Adj. & \num{-0.005}\\
AIC & \num{1195.1}\\
BIC & \num{1205.3}\\
Log.Lik. & \num{-594.554}\\
F & \num{0.002}\\
\bottomrule
\end{tabular}
\end{table}

We can put a variety of different of different models together
(Table~\ref{tbl-twomodels}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{second\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ gdp\_growth }\SpecialCharTok{\textasciitilde{}}\NormalTok{ inflation }\SpecialCharTok{+}\NormalTok{ country, }
                  \AttributeTok{data =}\NormalTok{ world\_bank\_data)}

\NormalTok{third\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ gdp\_growth }\SpecialCharTok{\textasciitilde{}}\NormalTok{ inflation }\SpecialCharTok{+}\NormalTok{ country }\SpecialCharTok{+}\NormalTok{ population, }
                  \AttributeTok{data =}\NormalTok{ world\_bank\_data)}

\FunctionTok{modelsummary}\NormalTok{(}\FunctionTok{list}\NormalTok{(first\_model, second\_model, third\_model))}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-twomodels}{}
\begin{table}
\caption{\label{tbl-twomodels}Explaining GDP as a function of inflation }

\centering
\begin{tabular}[t]{lccc}
\toprule
  & Model 1 & Model 2 & Model 3\\
\midrule
(Intercept) & \num{4.157} & \num{3.728} & \num{3.668}\\
 & (\num{0.352}) & (\num{0.495}) & (\num{0.494})\\
inflation & \num{-0.002} & \num{-0.075} & \num{-0.072}\\
 & (\num{0.041}) & (\num{0.041}) & (\num{0.041})\\
countryEthiopia &  & \num{2.872} & \num{2.716}\\
 &  & (\num{0.757}) & (\num{0.758})\\
countryIndia &  & \num{1.854} & \num{-0.561}\\
 &  & (\num{0.655}) & (\num{1.520})\\
countryUnited States &  & \num{-0.524} & \num{-1.176}\\
 &  & (\num{0.646}) & (\num{0.742})\\
population &  &  & \num{0.000}\\
 &  &  & (\num{0.000})\\
\midrule
Num.Obs. & \num{218} & \num{218} & \num{218}\\
R2 & \num{0.000} & \num{0.110} & \num{0.123}\\
R2 Adj. & \num{-0.005} & \num{0.093} & \num{0.102}\\
AIC & \num{1195.1} & \num{1175.7} & \num{1174.5}\\
BIC & \num{1205.3} & \num{1196.0} & \num{1198.2}\\
Log.Lik. & \num{-594.554} & \num{-581.844} & \num{-580.266}\\
F & \num{0.002} & \num{6.587} & \num{5.939}\\
\bottomrule
\end{tabular}
\end{table}

We can adjust the number of significant digits
(Table~\ref{tbl-twomodelstwo}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{modelsummary}\NormalTok{(}\FunctionTok{list}\NormalTok{(first\_model, second\_model, third\_model),}
             \AttributeTok{fmt =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-twomodelstwo}{}
\begin{table}
\caption{\label{tbl-twomodelstwo}Two models of GDP as a function of inflation }

\centering
\begin{tabular}[t]{lccc}
\toprule
  & Model 1 & Model 2 & Model 3\\
\midrule
(Intercept) & \num{4.2} & \num{3.7} & \num{3.7}\\
 & (\num{0.4}) & (\num{0.5}) & (\num{0.5})\\
inflation & \num{0.0} & \num{-0.1} & \num{-0.1}\\
 & (\num{0.0}) & (\num{0.0}) & (\num{0.0})\\
countryEthiopia &  & \num{2.9} & \num{2.7}\\
 &  & (\num{0.8}) & (\num{0.8})\\
countryIndia &  & \num{1.9} & \num{-0.6}\\
 &  & (\num{0.7}) & (\num{1.5})\\
countryUnited States &  & \num{-0.5} & \num{-1.2}\\
 &  & (\num{0.6}) & (\num{0.7})\\
population &  &  & \num{0.0}\\
 &  &  & (\num{0.0})\\
\midrule
Num.Obs. & \num{218} & \num{218} & \num{218}\\
R2 & \num{0.000} & \num{0.110} & \num{0.123}\\
R2 Adj. & \num{-0.005} & \num{0.093} & \num{0.102}\\
AIC & \num{1195.1} & \num{1175.7} & \num{1174.5}\\
BIC & \num{1205.3} & \num{1196.0} & \num{1198.2}\\
Log.Lik. & \num{-594.554} & \num{-581.844} & \num{-580.266}\\
F & \num{0.002} & \num{6.587} & \num{5.939}\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{maps}{%
\section{Maps}\label{maps}}

In many ways maps can be thought of as another type of graph, where the
x-axis is latitude, the y-axis is longitude, and there is some outline
or a background image. We have seen this type of set-up are used to this
type of set-up, for instance, in the \texttt{ggplot2} setting, this is
quite familiar.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_polygon}\NormalTok{( }\CommentTok{\# First draw an outline}
    \AttributeTok{data =}\NormalTok{ some\_data, }
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ latitude, }
        \AttributeTok{y =}\NormalTok{ longitude,}
        \AttributeTok{group =}\NormalTok{ group}
\NormalTok{        )) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{( }\CommentTok{\# Then add points of interest}
    \AttributeTok{data =}\NormalTok{ some\_other\_data, }
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ latitude, }
        \AttributeTok{y =}\NormalTok{ longitude)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

And while there are some small complications, for the most part it is as
straight-forward as that. The first step is to get some data. There is
some geographic data built into \texttt{ggplot2}, and there is
additional information in the `world.cities' dataset from \texttt{maps}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(maps)}

\NormalTok{france }\OtherTok{\textless{}{-}} \FunctionTok{map\_data}\NormalTok{(}\AttributeTok{map =} \StringTok{"france"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(france)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      long      lat group order region subregion
1 2.557093 51.09752     1     1   Nord      <NA>
2 2.579995 51.00298     1     2   Nord      <NA>
3 2.609101 50.98545     1     3   Nord      <NA>
4 2.630782 50.95073     1     4   Nord      <NA>
5 2.625894 50.94116     1     5   Nord      <NA>
6 2.597699 50.91967     1     6   Nord      <NA>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{french\_cities }\OtherTok{\textless{}{-}} 
\NormalTok{  world.cities }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(country.etc }\SpecialCharTok{==} \StringTok{"France"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(french\_cities)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             name country.etc    pop   lat long capital
1       Abbeville      France  26656 50.12 1.83       0
2         Acheres      France  23219 48.97 2.06       0
3            Agde      France  23477 43.33 3.46       0
4            Agen      France  34742 44.20 0.62       0
5 Aire-sur-la-Lys      France  10470 50.64 2.39       0
6 Aix-en-Provence      France 148622 43.53 5.44       0
\end{verbatim}

With that information in hand, we can then create a map of France that
shows the larger cities. We use \texttt{geom\_polygon()} from
\texttt{ggplot2} to draw shapes by connecting points within groups. And
\texttt{coord\_map()} adjusts for the fact that we are making a 2D map
to represent a world that is 3D.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_polygon}\NormalTok{(}\AttributeTok{data =}\NormalTok{ france,}
               \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ long,}
                   \AttributeTok{y =}\NormalTok{ lat,}
                   \AttributeTok{group =}\NormalTok{ group),}
               \AttributeTok{fill =} \StringTok{"white"}\NormalTok{, }
               \AttributeTok{colour =} \StringTok{"grey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_map}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ french\_cities}\SpecialCharTok{$}\NormalTok{long, }
                 \AttributeTok{y =}\NormalTok{ french\_cities}\SpecialCharTok{$}\NormalTok{lat),}
             \AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{,}
             \AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Longitude"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Latitude"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/unnamed-chunk-89-1.pdf}

}

\end{figure}

As is often the case with R, there are many different ways to get
started creating static maps. We have seen how they can be built using
only \texttt{ggplot2}, but \texttt{ggmap} brings additional
functionality (Kahle and Wickham 2013).

There are two essential components to a map:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  a border or background image (sometimes called a tile); and
\item
  something of interest within that border, or on top of that tile.
\end{enumerate}

In \texttt{ggmap}, we use an open-source option for our tile, Stamen
Maps. And we use plot points based on latitude and longitude.

\hypertarget{australian-polling-places}{%
\subsection{Australian polling places}\label{australian-polling-places}}

In Australia people go to specific locations, called booths, to vote.
These booths have latitudes and longitudes and so we can plot these. One
reason we may like to do this is to notice patterns over geographies.

To get started we need to get a tile. We are going to use \texttt{ggmap}
to get a tile from Stamen Maps, which builds on OpenStreetMap
(openstreetmap.org). The main argument to this function is to specify a
bounding box. This requires two latitudes - one for the top of the box
and one for the bottom of the box - and two longitudes - one for the
left of the box and one for the right of the box. It can be useful to
use Google Maps, or an alternative, to find the values of these that you
need. The bounding box provides the coordinates of the edges that you
are interested in. In this case we have provided it with coordinates
such that it will be centered around Canberra, Australia, which is a
small city that was created for the purposes of being the capital.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggmap)}

\NormalTok{bbox }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{left =} \FloatTok{148.95}\NormalTok{, }\AttributeTok{bottom =} \SpecialCharTok{{-}}\FloatTok{35.5}\NormalTok{, }\AttributeTok{right =} \FloatTok{149.3}\NormalTok{, }\AttributeTok{top =} \SpecialCharTok{{-}}\FloatTok{35.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once you have defined the bounding box, then the function
\texttt{get\_stamenmap()} will get the tiles in that area. The number of
tiles that it needs to get depends on the zoom, and the type of tiles
that it gets depends on the maptype. We have used a black-and-white type
of map but the helpfile specifies others. At this point we can the map
to maps to \texttt{ggmap()} and it will plot the tile! It will be
actively downloading these tiles, and so it needs an internet
connection.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{canberra\_stamen\_map }\OtherTok{\textless{}{-}} \FunctionTok{get\_stamenmap}\NormalTok{(bbox, }\AttributeTok{zoom =} \DecValTok{11}\NormalTok{, }\AttributeTok{maptype =} \StringTok{"toner{-}lite"}\NormalTok{)}

\FunctionTok{ggmap}\NormalTok{(canberra\_stamen\_map)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/unnamed-chunk-93-1.pdf}

}

\end{figure}

Once we have a map then we can use \texttt{ggmap()} to plot it. Now we
want to get some data that we plot on top of our tiles. We will just
plot the location of the polling places, based on which `division' it
is. This is available
\href{https://results.aec.gov.au/20499/Website/Downloads/HouseTppByPollingPlaceDownload-20499.csv}{here}.
The Australian Electoral Commission (AEC) is the official government
agency that is responsible for elections in Australia.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in the booths data for each year}
\NormalTok{booths }\OtherTok{\textless{}{-}}
\NormalTok{  readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{(}
    \StringTok{"https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload{-}24310.csv"}\NormalTok{,}
    \AttributeTok{skip =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{guess\_max =} \DecValTok{10000}
\NormalTok{  )}

\FunctionTok{head}\NormalTok{(booths)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 15
  State DivisionID DivisionNm PollingPlaceID PollingPlaceTypeID PollingPlaceNm  
  <chr>      <dbl> <chr>               <dbl>              <dbl> <chr>           
1 ACT          318 Bean                93925                  5 Belconnen BEAN ~
2 ACT          318 Bean                93927                  5 BLV Bean PPVC   
3 ACT          318 Bean                11877                  1 Bonython        
4 ACT          318 Bean                11452                  1 Calwell         
5 ACT          318 Bean                 8761                  1 Chapman         
6 ACT          318 Bean                 8763                  1 Chisholm        
# ... with 9 more variables: PremisesNm <chr>, PremisesAddress1 <chr>,
#   PremisesAddress2 <chr>, PremisesAddress3 <chr>, PremisesSuburb <chr>,
#   PremisesStateAb <chr>, PremisesPostCode <chr>, Latitude <dbl>,
#   Longitude <dbl>
\end{verbatim}

This dataset is for the whole of Australia, but as we are just going to
plot the area around Canberra we filter to that and only to booths that
are geographic (the AEC has various options for people who are in
hospital, or not able to get to a booth, etc, and these are still
`booths' in this dataset).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reduce the booths data to only rows with that have latitude and longitude}
\NormalTok{booths\_reduced }\OtherTok{\textless{}{-}}
\NormalTok{  booths }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(State }\SpecialCharTok{==} \StringTok{"ACT"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(PollingPlaceID, DivisionNm, Latitude, Longitude) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Longitude)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Remove rows that do not have a geography}
  \FunctionTok{filter}\NormalTok{(Longitude }\SpecialCharTok{\textless{}} \DecValTok{165}\NormalTok{) }\CommentTok{\# Remove Norfolk Island}
\end{Highlighting}
\end{Shaded}

Now we can use \texttt{ggmap} in the same way as before to plot our
underlying tiles, and then build on that using \texttt{geom\_point()} to
add our points of interest.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggmap}\NormalTok{(canberra\_stamen\_map,}
      \AttributeTok{extent =} \StringTok{"normal"}\NormalTok{,}
      \AttributeTok{maprange =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ booths\_reduced,}
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Longitude,}
                 \AttributeTok{y =}\NormalTok{ Latitude,}
                 \AttributeTok{colour =}\NormalTok{ DivisionNm),) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{name =} \StringTok{"2019 Division"}\NormalTok{, }\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_map}\NormalTok{(}
    \AttributeTok{projection =} \StringTok{"mercator"}\NormalTok{,}
    \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\FunctionTok{attr}\NormalTok{(map, }\StringTok{"bb"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{ll.lon, }\FunctionTok{attr}\NormalTok{(map, }\StringTok{"bb"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{ur.lon),}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\FunctionTok{attr}\NormalTok{(map, }\StringTok{"bb"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{ll.lat, }\FunctionTok{attr}\NormalTok{(map, }\StringTok{"bb"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{ur.lat)}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Longitude"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Latitude"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{panel.grid.major =} \FunctionTok{element\_blank}\NormalTok{(),}
        \AttributeTok{panel.grid.minor =} \FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/unnamed-chunk-99-1.pdf}

}

\end{figure}

We may like to save the map so that we do not have to draw it every
time, and we can do that in the same way as any other graph, using
\texttt{ggsave()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"map.pdf"}\NormalTok{, }\AttributeTok{width =} \DecValTok{20}\NormalTok{, }\AttributeTok{height =} \DecValTok{10}\NormalTok{, }\AttributeTok{units =} \StringTok{"cm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, the reason that we used Stamen Maps and OpenStreetMap is
because it is open source, but we could have also used Google Maps. This
requires you to first register a credit card with Google, and specify a
key, but with low usage should be free. Using Google Maps,
\texttt{get\_googlemap()}, brings some advantages over
\texttt{get\_stamenmap()}, for instance it will attempt to find a
placename, rather than needing to specify a bounding box.

\hypertarget{us-troop-deployment}{%
\subsection{US troop deployment}\label{us-troop-deployment}}

Let us see another example of a static map, this time using data on US
military deployments from \texttt{troopdata} (Flynn 2021). We can access
data about US overseas military bases back to the start of the Cold War
using \texttt{get\_basedata()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"troopdata"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(troopdata)}

\NormalTok{bases }\OtherTok{\textless{}{-}} \FunctionTok{get\_basedata}\NormalTok{()}

\FunctionTok{head}\NormalTok{(bases)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 9
  countryname ccode iso3c basename            lat   lon  base lilypad fundedsite
  <chr>       <dbl> <chr> <chr>             <dbl> <dbl> <dbl>   <dbl>      <dbl>
1 Afghanistan   700 AFG   Bagram AB          34.9  69.3     1       0          0
2 Afghanistan   700 AFG   Kandahar Airfield  31.5  65.8     1       0          0
3 Afghanistan   700 AFG   Mazar-e-Sharif     36.7  67.2     1       0          0
4 Afghanistan   700 AFG   Gardez             33.6  69.2     1       0          0
5 Afghanistan   700 AFG   Kabul              34.5  69.2     1       0          0
6 Afghanistan   700 AFG   Herat              34.3  62.2     1       0          0
\end{verbatim}

We will look at the locations of US military bases in: Germany, Japan,
and Australia. The \texttt{troopdata} dataset already has latitude and
longitude of the base. We will use that as our item of interest. The
first step is to define a bounding box for each of country.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggmap)}

\CommentTok{\# Based on: https://data.humdata.org/dataset/bounding{-}boxes{-}for{-}countries}
\NormalTok{bbox\_germany }\OtherTok{\textless{}{-}}
  \FunctionTok{c}\NormalTok{(}
    \AttributeTok{left =} \FloatTok{5.867}\NormalTok{,}
    \AttributeTok{bottom =} \FloatTok{45.967}\NormalTok{,}
    \AttributeTok{right =} \FloatTok{15.033}\NormalTok{,}
    \AttributeTok{top =} \FloatTok{55.133}
\NormalTok{  )}

\NormalTok{bbox\_japan }\OtherTok{\textless{}{-}}
  \FunctionTok{c}\NormalTok{(}
    \AttributeTok{left =} \DecValTok{127}\NormalTok{,}
    \AttributeTok{bottom =} \DecValTok{30}\NormalTok{,}
    \AttributeTok{right =} \DecValTok{146}\NormalTok{,}
    \AttributeTok{top =} \DecValTok{45}
\NormalTok{  )}

\NormalTok{bbox\_australia }\OtherTok{\textless{}{-}}
  \FunctionTok{c}\NormalTok{(}
    \AttributeTok{left =} \FloatTok{112.467}\NormalTok{,}
    \AttributeTok{bottom =} \SpecialCharTok{{-}}\DecValTok{45}\NormalTok{,}
    \AttributeTok{right =} \DecValTok{155}\NormalTok{,}
    \AttributeTok{top =} \SpecialCharTok{{-}}\FloatTok{9.133}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Then we need to get the tiles using \texttt{get\_stamenmap()} from
\texttt{ggmap}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{germany\_stamen\_map }\OtherTok{\textless{}{-}}
  \FunctionTok{get\_stamenmap}\NormalTok{(bbox\_germany, }\AttributeTok{zoom =} \DecValTok{6}\NormalTok{, }\AttributeTok{maptype =} \StringTok{"toner{-}lite"}\NormalTok{)}

\NormalTok{japan\_stamen\_map }\OtherTok{\textless{}{-}}
  \FunctionTok{get\_stamenmap}\NormalTok{(bbox\_japan, }\AttributeTok{zoom =} \DecValTok{6}\NormalTok{, }\AttributeTok{maptype =} \StringTok{"toner{-}lite"}\NormalTok{)}

\NormalTok{australia\_stamen\_map }\OtherTok{\textless{}{-}}
  \FunctionTok{get\_stamenmap}\NormalTok{(bbox\_australia, }\AttributeTok{zoom =} \DecValTok{5}\NormalTok{, }\AttributeTok{maptype =} \StringTok{"toner{-}lite"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And finally, we can bring it all together with maps show US military
bases in Germany (Figure~\ref{fig-mapbasesingermany}), Japan
(Figure~\ref{fig-mapbasesinjapan}), and Australia
(Figure~\ref{fig-mapbasesinaustralia}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggmap}\NormalTok{(germany\_stamen\_map) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ bases,}
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lon,}
                 \AttributeTok{y =}\NormalTok{ lat)}
\NormalTok{             ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Longitude"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Latitude"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-mapbasesingermany-1.pdf}

}

\caption{\label{fig-mapbasesingermany}Map of US military bases in
Germany}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggmap}\NormalTok{(japan\_stamen\_map) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ bases,}
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lon,}
                 \AttributeTok{y =}\NormalTok{ lat)}
\NormalTok{             ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Longitude"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Latitude"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-mapbasesinjapan-1.pdf}

}

\caption{\label{fig-mapbasesinjapan}Map of US military bases in Japan}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggmap}\NormalTok{(australia\_stamen\_map) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ bases,}
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lon,}
                 \AttributeTok{y =}\NormalTok{ lat)}
\NormalTok{             ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Longitude"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Latitude"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-mapbasesinaustralia-1.pdf}

}

\caption{\label{fig-mapbasesinaustralia}Map of US military bases in
Australia}

\end{figure}

\hypertarget{geocoding}{%
\subsection{Geocoding}\label{geocoding}}

To this point we assumed that we already had geocoded data, which means
that we have a latitude and longitude. If we only have place names, such
as `Canberra, Australia', `Ottawa, Canada', `Accra, Ghana', `Quito,
Ecuador' are just names, they do not actually inherently have a
location. To plot them we need to get a latitude and longitude for them.
The process of going from names to coordinates is called geocoding.

There are a range of options to geocode data in R, but
\texttt{tidygeocoder} is especially useful (Cambon and Belanger 2021).
We first need a dataframe of locations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{place\_names }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{city =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Canberra\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Ottawa\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Accra\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Quito\textquotesingle{}}\NormalTok{),}
    \AttributeTok{country =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Canada\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Ghana\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Ecuador\textquotesingle{}}\NormalTok{)}
\NormalTok{  )}

\NormalTok{place\_names}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 2
  city     country  
  <chr>    <chr>    
1 Canberra Australia
2 Ottawa   Canada   
3 Accra    Ghana    
4 Quito    Ecuador  
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidygeocoder)}

\NormalTok{place\_names }\OtherTok{\textless{}{-}}
  \FunctionTok{geo}\NormalTok{(}\AttributeTok{city =}\NormalTok{ place\_names}\SpecialCharTok{$}\NormalTok{city,}
      \AttributeTok{country =}\NormalTok{ place\_names}\SpecialCharTok{$}\NormalTok{country,}
      \AttributeTok{method =} \StringTok{\textquotesingle{}osm\textquotesingle{}}\NormalTok{)}

\NormalTok{place\_names}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 4
  city     country       lat    long
  <chr>    <chr>       <dbl>   <dbl>
1 Canberra Australia -35.3   149.   
2 Ottawa   Canada     45.4   -75.7  
3 Accra    Ghana       5.56   -0.201
4 Quito    Ecuador    -0.220 -78.5  
\end{verbatim}

And we can now plot and label these cities (Figure~\ref{fig-mynicemap}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{world }\OtherTok{\textless{}{-}} \FunctionTok{map\_data}\NormalTok{(}\AttributeTok{map =} \StringTok{"world"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_polygon}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ world,}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ long,}
        \AttributeTok{y =}\NormalTok{ lat,}
        \AttributeTok{group =}\NormalTok{ group),}
    \AttributeTok{fill =} \StringTok{"white"}\NormalTok{,}
    \AttributeTok{colour =} \StringTok{"grey"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{coord\_map}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{47}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{47}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ place\_names}\SpecialCharTok{$}\NormalTok{long,}
                 \AttributeTok{y =}\NormalTok{ place\_names}\SpecialCharTok{$}\NormalTok{lat),}
             \AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ place\_names}\SpecialCharTok{$}\NormalTok{long,}
    \AttributeTok{y =}\NormalTok{ place\_names}\SpecialCharTok{$}\NormalTok{lat,}
    \AttributeTok{label =}\NormalTok{ place\_names}\SpecialCharTok{$}\NormalTok{city}
\NormalTok{  ),}
  \AttributeTok{nudge\_y =} \SpecialCharTok{{-}}\DecValTok{5}\NormalTok{,) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Longitude"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Latitude"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./06-static_communication_files/figure-pdf/fig-mynicemap-1.pdf}

}

\caption{\label{fig-mynicemap}Map of Accra, Canberra, Ottawa, and Quito
after geocoding to obtain their locations}

\end{figure}

\hypertarget{exercises-and-tutorial-5}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-5}}

\hypertarget{exercises-5}{%
\subsection{Exercises}\label{exercises-5}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assume \texttt{tidyverse} and \texttt{datasauRus} are installed and
  loaded. What would be the outcome of the following code?
  \texttt{datasaurus\_dozen\ \textbar{}\textgreater{}\ filter(dataset\ ==\ "v\_lines")\ \textbar{}\textgreater{}\ ggplot(aes(x=x,\ y=y))\ +\ geom\_point()}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Four vertical lines
  \item
    Five vertical lines
  \item
    Three vertical lines
  \item
    Two vertical lines
  \end{enumerate}
\item
  Assume \texttt{tidyverse} and the `beps' dataset have been installed
  and loaded. What change should be made to the following to make the
  bars for the different parties be next to each other rather than on
  top of each other?
  \texttt{beps\ \textbar{}\textgreater{}\ ggplot(mapping\ =\ aes(x\ =\ age,\ fill\ =\ vote))\ +\ geom\_bar()}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{position\ =\ "side\_by\_side"}
  \item
    \texttt{position\ =\ "dodge"}
  \item
    \texttt{position\ =\ "adjacent"}
  \item
    \texttt{position\ =\ "closest"}
  \end{enumerate}
\item
  Which theme should be used to remove the solid lines along the x and y
  axes?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{theme\_minimal()}
  \item
    \texttt{theme\_classic()}
  \item
    \texttt{theme\_bw()}
  \item
    \texttt{theme\_dark()}
  \end{enumerate}
\item
  Assume \texttt{tidyverse} and the `beps' dataset have been installed
  and loaded. What should be added to `labs()' to change the text of the
  legend?
  \texttt{beps\ \textbar{}\textgreater{}\ ggplot(mapping\ =\ aes(x\ =\ age,\ fill\ =\ vote))\ +\ geom\_bar()\ +\ theme\_minimal()\ +\ labs(x\ =\ "Age\ of\ respondent",\ y\ =\ "Number\ of\ respondents")}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{color\ =\ "Voted\ for"}
  \item
    \texttt{legend\ =\ "Voted\ for"}
  \item
    \texttt{scale\ =\ "Voted\ for"}
  \item
    \texttt{fill\ =\ "Voted\ for"}
  \end{enumerate}
\item
  Which palette from \texttt{scale\_colour\_brewer()} is divergent?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `Accent'
  \item
    `RdBu'
  \item
    `GnBu'
  \item
    `Set1'
  \end{enumerate}
\item
  Which geom should be used to make a scatter plot?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{geom\_smooth()}
  \item
    \texttt{geom\_point()}
  \item
    \texttt{geom\_bar()}
  \item
    \texttt{geom\_dotplot()}
  \end{enumerate}
\item
  Which of these would result in the largest number of bins?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{geom\_histogram(binwidth\ =\ 5)}
  \item
    \texttt{geom\_histogram(binwidth\ =\ 2)}
  \end{enumerate}
\item
  If there is a dataset that contains the heights of 100 birds each from
  one of three different species. If we are interested in understanding
  the distribution of these heights, then in a paragraph or two, please
  explain which type of graph should be used and why?
\item
  Assume the dataset and columns exist. Would this code work?
  \texttt{data\ \textbar{}\textgreater{}\ ggplot(aes(x\ =\ col\_one))\ \textbar{}\textgreater{}\ geom\_point()}
  (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Yes
  \item
    No
  \end{enumerate}
\item
  Which geom should be used to plot categorical data (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{geom\_bar()}
  \item
    \texttt{geom\_point()}
  \item
    \texttt{geom\_abline()}
  \item
    \texttt{geom\_boxplot()}
  \end{enumerate}
\item
  Why are boxplots often inappropriate (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    They hide the full distribution of the data.
  \item
    They are hard to make.
  \item
    They are ugly.
  \item
    The mode is clearly displayed.
  \end{enumerate}
\item
  Which of the following, if any, are elements of the layered grammar of
  graphics (Wickham 2010) (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A default dataset and set of mappings from variables to aesthetics.
  \item
    One or more layers, with each layer having one geometric object, one
    statistical transformation, one position adjustment, and optionally,
    one dataset and set of aesthetic mappings.
  \item
    Colors that enable the reader to understand the main point.
  \item
    A coordinate system.
  \item
    The facet specification.
  \item
    One scale for each aesthetic mapping used.
  \end{enumerate}
\item
  Which function from \texttt{modelsummary} is used to create a table of
  descriptive statistics?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{datasummary\_descriptive()}
  \item
    \texttt{datasummary\_skim()}
  \item
    \texttt{datasummary\_crosstab()}
  \item
    \texttt{datasummary\_balance()}
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial-5}{%
\subsection{Tutorial}\label{tutorial-5}}

Using R Markdown, please create a graph using \texttt{ggplot2} and a map
using \texttt{ggmap} and add explanatory text to accompany both. Be sure
to include cross-references and captions, etc. This should take one to
two pages for each of them.

Then, for the graph, please reflect on Vanderplas, Cook, and Hofmann
(2020) and add a few paragraphs about the different options that you
considered that the graph more effective. (If you've not now got at
least two pages about your graph you've likely written too little.)

And finally, for the map, please reflect on the following quote from
Heather Krause: `maps only show people who aren't invisible to the
makers' as well as Chapter 3 from D'Ignazio and Klein (2020) and add a
few paragraphs related to this. (Again, if you've not now got at least
two pages about your map you've likely written too little.)

Please submit a PDF.

\hypertarget{interactive-communication-sec-interactive-communication}{%
\chapter{Interactive communication \{\#sec-interactive
communication\}}\label{interactive-communication-sec-interactive-communication}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{M-F-E-O: postcards + distill}, (Presmanes Hill 2021a).
\item
  Read \emph{Geocomputation with R}, Chapter 2 `Geographic data in R',
  (Lovelace, Nowosad, and Muenchow 2019).
\item
  Read \emph{Mastering Shiny}, Chapter 1 `Your first Shiny app',
  (Wickham 2021a).
\item
  Read \emph{We Still Can't See American Slavery for What It Was},
  (Bouie 2022).
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Building a website within the R environment using: \texttt{postcards}
  (Kross 2021), \texttt{distill} (Allaire et al. 2021), and
  \texttt{blogdown} (Xie, Dervieux, and Hill 2021).
\item
  Adding interaction to maps, using \texttt{leaflet} (Cheng,
  Karambelkar, and Xie 2021) and \texttt{mapdeck} (Cooley 2020).
\item
  Adding interaction to graphs, using \texttt{Shiny} (Chang et al.
  2021).
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{blogdown} (Xie, Dervieux, and Hill 2021)
\item
  \texttt{distill} (Allaire et al. 2021)
\item
  \texttt{leaflet} (Cheng, Karambelkar, and Xie 2021)
\item
  \texttt{mapdeck} (Cooley 2020)
\item
  \texttt{postcards} (Kross 2021)
\item
  \texttt{tidyverse} (Wickham et al. 2019a)
\item
  \texttt{usethis} (Wickham and Bryan 2020)
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{blogdown::serve\_site()}
\item
  \texttt{distill::create\_article()}
\item
  \texttt{leaflet::addCircleMarkers()}
\item
  \texttt{leaflet::addLegend()}
\item
  \texttt{leaflet::addMarkers()}
\item
  \texttt{leaflet::addTiles()}
\item
  \texttt{leaflet::leaflet()}
\item
  \texttt{mapdeck:::add\_scatterplot()}
\item
  \texttt{mapdeck::mapdeck()}
\item
  \texttt{mapdeck::mapdeck\_style()}
\item
  \texttt{shiny::fluidPage()}
\item
  \texttt{shiny::mainPanel()}
\item
  \texttt{shiny::plotOutput()}
\item
  \texttt{shiny::renderPlot()}
\item
  \texttt{shiny::shinyApp()}
\item
  \texttt{usethis::edit\_r\_environ()}
\item
  \texttt{usethis::use\_git()}
\item
  \texttt{usethis::use\_github()}
\end{itemize}

\hypertarget{introduction-4}{%
\section{Introduction}\label{introduction-4}}

Books and papers have been the primary mediums for communication for
thousands of years. But with the rise of computers, and especially the
internet, in recent decades, these static approaches have been
complemented with interactive approaches. Fundamentally, the internet is
about making files available others. If we additionally allow them to do
something with what we make available, then we need to take a variety of
additional aspects into consideration.

In this chapter we begin by covering how to create and publish a
website. This serves as a place to host a portfolio of work. After that
we cover adding interaction to maps and graphs, which are two that
nicely lend themselves to this.

\hypertarget{making-a-website}{%
\section{Making a website}\label{making-a-website}}

A website is a critical part of communication. For instance, it is a
place to make a portfolio of work publicly available. One way to make a
website is to use \texttt{blogdown} (Xie, Dervieux, and Hill 2021).
\texttt{blogdown} is a package that allows you to make websites, not
just blogs, notwithstanding its name, largely within R Studio. It builds
on `Hugo', which is a popular general framework for making websites.
\texttt{blogdown} enables us to freely and quickly get a website
up-and-running. It is easy to add content from time-to-time. And it
integrates with R Markdown, which makes it easy to share work. But
\texttt{blogdown} is brittle. Because it is so dependent on Hugo,
features that work today may not work tomorrow. Also, owners of Hugo
templates can update them at any time, without thought to existing
users. \texttt{blogdown} is a good option if we know what we are doing,
or have a specific use-case, or style, in mind. But two other
alternatives are better starting points.

The first is \texttt{distill} (Allaire et al. 2021). Again, this is an R
package that wraps around another framework, in this case `Distill'. But
in contrast to Hugo, Distill is more focused on common needs in data
science, and is also only maintained by one group, so it may be
considered a more stable choice. That said, the default \texttt{distill}
site is a little plain looking. As such, following Presmanes Hill
(2021a), we will pair it with a third option, \texttt{postcards} (Kross
2021).

The third option, and the one that we will start with, is
\texttt{postcards} (Kross 2021). This is a tailored solution that
creates simple biographical websites that look great. Having set-up
GitHub in R Studio, it is literally possible to have a
\texttt{postcards} website online in five minutes.

\hypertarget{postcards}{%
\subsection{Postcards}\label{postcards}}

Begin by installing \texttt{postcards}, with
\texttt{install.packages(\textquotesingle{}postcards\textquotesingle{})}
and then creating a new project for the website (`File' -\textgreater{}
`New Project' -\textgreater{} `New Directory' -\textgreater{} `Postcards
Website'). We can then pick a name and location for the project, and
select a postcards theme. In this case, we can start with `trestles' but
this can be changed later. Click the option to `Open in new session' and
then create the project.

That will open a new file and we can now build the site by clicking
`Knit'. This will result in a one-page website
(Figure~\ref{fig-trestles}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/trestles.png"}

}

\caption{\label{fig-trestles}Example of a website made with
\texttt{postcards} using the `trestles' theme}

\end{figure}

We can now update the basic content such as name, bio and links, to
match our own (Figure~\ref{fig-trestlesredux}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/trestlesredux.png"}

}

\caption{\label{fig-trestlesredux}Example of Trestles website with
updated details}

\end{figure}

After the details are personalized, we can push it to GitHub which will
act as a host for our website. By default, GitHub would try to build the
site, which we do not want, so we need to first add a hidden file to
turn that off, by running this in the console:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{file.create}\NormalTok{(}\StringTok{\textquotesingle{}.nojekyll\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then, assuming GitHub was set-up in Chapter
@ref(reproducible-workflows), we can use \texttt{usethis} (Wickham and
Bryan 2020) to get our newly created project onto GitHub. We use
\texttt{use\_git()} to initialize a Git repository, and then
\texttt{use\_github()} pushes it to GitHub.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(usethis)}
\FunctionTok{use\_git}\NormalTok{()}
\FunctionTok{use\_github}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The project will then be on GitHub. We can use GitHub pages to host it:
`Settings -\textgreater{} Pages' and then change the source to `main' or
`master', depending on your settings. GitHub will let you know the
address that you can share to visit your site.

\hypertarget{distill}{%
\subsection{Distill}\label{distill}}

We will now use \texttt{distill} (Allaire et al. 2021) to build
additional infrastructure around our \texttt{postcards} site, following
Presmanes Hill (2021a). After that we will explore some of the aspects
of \texttt{distill} that make it a nice choice, and mention some of the
trade-offs. First, we install \texttt{distill} with
\texttt{install.packages(\textquotesingle{}distill\textquotesingle{})},
and again, create a new project for the website (`File' -\textgreater{}
`New Project' -\textgreater{} `New Directory' -\textgreater{} `Distill
Blog').

We can then pick a name and location for the project, and set a title.
Select `Configure for GitHub Pages' and also `Open in a new session'.
These options can be changed \emph{ex post}. It should look something
like Figure~\ref{fig-distillone}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/distill_one.png"}

}

\caption{\label{fig-distillone}Example settings for setting up
\texttt{distill}}

\end{figure}

At this point we can click `Build Website' in the Build tab, and we
should see the default website (Figure~\ref{fig-distilltwo}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/distill_two.png"}

}

\caption{\label{fig-distilltwo}Example of default \texttt{distill}
website}

\end{figure}

Again, now we need to update it to reflect our own details. The default
for a `Distill Blog' is that the blog is the homepage. We can change
that to use a \texttt{postcards} page as the homepage. First we change
the name of `index.Rmd' to `blog.Rmd' and then create a new `trestles'
page:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{postcards}\SpecialCharTok{::}\FunctionTok{create\_postcard}\NormalTok{(}\AttributeTok{file =} \StringTok{"index.Rmd"}\NormalTok{, }\AttributeTok{template =} \StringTok{"trestles"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The trestles page will open, and we need to add the following line in
the yaml file: \texttt{site:\ distill::distill\_website}. In
Figure~\ref{fig-distillthree} it was added at line 16, and then we can
rebuild the website.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/distill_three.png"}

}

\caption{\label{fig-distillthree}Updating the yaml to change the
homepage}

\end{figure}

We can make the same changes to the default content as earlier, for
instance, updating the links, image, and bio. The advantage of using
\texttt{distill} is that we now have additional pages, not just a
one-page website, and we also have a blog. By default, we have an
`about' page, but some other pages that may be useful, depending on the
particular use-case, include: `research', `teaching', `talks',
`projects', `software', and `datasets'. As an example, we will add and
edit a page called `software' using
\texttt{distill::create\_article(file\ =\ \textquotesingle{}software\textquotesingle{})}.

That will create and open an R Markdown document. To add it to the
website, open '\_site.yml' and then add a line to the `navbar'
(Figure~\ref{fig-distillfour}). After this is done then we can rebuild
the website, and the `software' page will have been added.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/distill_four.png"}

}

\caption{\label{fig-distillfour}Adding another page to the website}

\end{figure}

We can continue with this process until we are happy with the website.
For instance, we may want to add a blog. To do this we follow the same
pattern as before, but with `blog' instead of `software'.

When we are happy with our website, we can push it to GitHub and then
use GitHub Pages to host it, in the same way that we did with the
\texttt{postcards} site.

Using \texttt{distill} is a good option when we need a multi-page
website, but still want a fairly controlled environment. There are many
options that can be changed, and Presmanes Hill (2021a) is a good
starting point, in addition to the \texttt{distill} homepage:
https://rstudio.github.io/distill/.

That said, \texttt{distill} is opinionated. While it is a great option,
if we want something a little more flexible then \texttt{blogdown} might
be a better option.

\hypertarget{blogdown}{%
\subsection{Blogdown}\label{blogdown}}

Using \texttt{blogdown} (Xie, Dervieux, and Hill 2021) is more work than
Google sites or Squarespace. It requires a little more knowledge than
using a basic Wordpress site. And if we need to customize absolutely
every aspect of the website, or need everything to be `just so' then
\texttt{blogdown} may not be a good option. But \texttt{blogdown} allows
a variety and level of expression that is not possible with
\texttt{distill}. Presmanes Hill (2021b) and Xie, Thomas, and Presmanes
Hill (2021) are useful for learning more about \texttt{blogdown}.

First we need to install \texttt{blogdown} with
\texttt{install.packages("blogdown")}. And then we create a new project
for the website (`File' -\textgreater{} `New Project' -\textgreater{}
`New Directory' -\textgreater{} `Website using blogdown'). At this point
we can set a name and location, and also select `Open in a new session'
(Figure~\ref{fig-blogdownone}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/blogdown_one.png"}

}

\caption{\label{fig-blogdownone}Example settings for setting up
blogdown}

\end{figure}

We can click `Build Website' from the `Build' pane, but then an extra
step is needed; we need to serve the site with
\texttt{blogdown:::serve\_site()}. After this, the site will show in the
`Viewer' pane (Figure~\ref{fig-blogdowntwo}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/blogdown_two.png"}

}

\caption{\label{fig-blogdowntwo}Serving default blogdown site}

\end{figure}

The default website is now being `served' locally. This means that
changes we make will be reflected in the website that we see in the
Viewer pane. To see the website in a web browser, click `Show in new
window' on the top left of the Viewer. That will open the website using
the address that R Studio also provides.

We now want to update the content, starting with the `About' section. To
do that we go to `content -\textgreater{} about.md' and modify or add
content. One nice aspect of \texttt{blogdown} is that it will
automatically reload the content when we save, and so changes should
appear immediately We could modify other aspects also. For instance, we
could change the logo, by adding a square image to `public/images/' and
then changing the call to `logo.png' in `config.yaml'. When we are happy
with it, we can make our website public in the same way as we did for
\texttt{postcards}.

One advantage of using \texttt{blogdown} is that it allows us to use
Hugo templates. This provides a large number of beautifully crafted
websites. To pick a theme we go to the Hugo themes page:
https://themes.gohugo.io. There are hundreds of different themes. In
general, most of them can be made to work with \texttt{blogdown}, but
sometimes it can be a bit of a hassle.

One nice option is Apéro: https://hugo-apero-docs.netlify.app. We can
specify the use of this theme as part of creating a new site (`File'
-\textgreater{} `New Project' -\textgreater{} `New Directory'
-\textgreater{} `Website using blogdown'). At this point, in addition to
setting the name and location, we can specify a theme. Specifically, in
the `Hugo theme' field, we can specify a GitHub username and repository,
which in this case is `hugo-apero/apero'
(Figure~\ref{fig-blogdownthree}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/blogdown_three.png"}

}

\caption{\label{fig-blogdownthree}Using the Apéro theme}

\end{figure}

\hypertarget{interactive-maps}{%
\section{Interactive maps}\label{interactive-maps}}

The nice thing about interactive maps is that we can let our user decide
what they are interested in. For instance, in the case of a map, some
people will be interested in, say, Toronto, while others will be
interested in Chennai or even Auckland. But it would be difficult to
present a map that focused on all of those, so an interactive map is a
way to allow users to focus on what they want.

That said, it is important to be cognizant of what we are doing when we
build maps, and more broadly, what is being done at scale to enable us
to be able to build our own maps. For instance, with regard to Google,
McQuire (2019) says:

\begin{quote}
Google began life in 1998 as a company famously dedicated to organising
the vast amounts of data on the Internet. But over the last two decades
its ambitions have changed in a crucial way. Extracting data such as
words and numbers from the physical world is now merely a stepping-stone
towards apprehending and organizing the physical world as data. Perhaps
this shift is not surprising at a moment when it has become possible to
comprehend human identity as a form of (genetic) `code'. However,
apprehending and organizing the world as data under current settings is
likely to take us well beyond Heidegger's `standing reserve' in which
modern technology enframed `nature' as productive resource. In the 21st
century, it is the stuff of human life itself---from genetics to bodily
appearances, mobility, gestures, speech, and behaviour---that is being
progressively rendered as productive resource that can not only be
harvested continuously but subject to modulation over time.
\end{quote}

Does this mean that we should not use or build interactive maps? Of
course not. But it is important to be aware of the fact that this is a
frontier, and the boundaries of appropriate use are still being
determined. Indeed, the literal boundaries of the maps themselves are
being consistently determined and updated. The move to digital maps,
compared with physical printed maps, means that it is possible for
different users to be presented with different realities. For instance,
`\ldots Google routinely takes sides in border disputes. Take, for
instance, the representation of the border between Ukraine and Russia.
In Russia, the Crimean Peninsula is represented with a hard-line border
as Russian-controlled, whereas Ukrainians and others see a dotted-line
border. The strategically important peninsula is claimed by both nations
and was violently seized by Russia in 2014, one of many skirmishes over
control' Bensinger (2020).

\hypertarget{leaflet}{%
\subsection{Leaflet}\label{leaflet}}

We can use \texttt{leaflet} (Cheng, Karambelkar, and Xie 2021) to make
interactive maps. The essentials are similar to \texttt{ggmap} (Kahle
and Wickham 2013), but there are many additional aspects beyond that. We
can redo the US military deployments map from Chapter
@ref(static-communication) that used \texttt{troopdata} (Flynn 2021).
The advantage with an interactive map is that we can plot all the bases
and allow the user to focus on which area they want, in comparison with
Chapter @ref(static-communication) where we just picked a few particular
countries.

In the same way as a graph in \texttt{ggplot2} begins with
\texttt{ggplot()}, a map in \texttt{leaflet} begins with
\texttt{leaflet()}. Here we can specify data, and other options such as
width and height. After this, we add `layers' in the same way that we
added them in \texttt{ggplot2}. The first layer that we add is a tile,
using \texttt{addTiles()}. In this case, the default is from
OpenStreeMap. After that we add markers with \texttt{addMarkers()} to
show the location of each base (Figure~\ref{fig-canhasbase}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(leaflet)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(troopdata)}

\NormalTok{bases }\OtherTok{\textless{}{-}} \FunctionTok{get\_basedata}\NormalTok{()}

\CommentTok{\# Some of the bases include unexpected characters which we need to address}
\FunctionTok{Encoding}\NormalTok{(bases}\SpecialCharTok{$}\NormalTok{basename) }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}latin1\textquotesingle{}}

\FunctionTok{leaflet}\NormalTok{(}\AttributeTok{data =}\NormalTok{ bases) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{addTiles}\NormalTok{() }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# Add default OpenStreetMap map tiles}
  \FunctionTok{addMarkers}\NormalTok{(}\AttributeTok{lng =}\NormalTok{ bases}\SpecialCharTok{$}\NormalTok{lon, }
             \AttributeTok{lat =}\NormalTok{ bases}\SpecialCharTok{$}\NormalTok{lat, }
             \AttributeTok{popup =}\NormalTok{ bases}\SpecialCharTok{$}\NormalTok{basename,}
             \AttributeTok{label =}\NormalTok{ bases}\SpecialCharTok{$}\NormalTok{countryname)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./07-interactive_communication_files/figure-pdf/fig-canhasbase-1.pdf}

}

\caption{\label{fig-canhasbase}Interactive map of US bases}

\end{figure}

There are two new arguments, compared with \texttt{ggmap}. The first is
`popup', which is the behavior that occurs when the user clicks on the
marker. In this case, the name of the base is provided. The second is
`label', which is what happens when the user hovers on the marker. In
this case it is the name of the country.

We can try another example, this time of the amount spent building those
bases. We will introduce a different type of marker here, which is
circles. This will allow us to use different colors for the outcomes of
each type. There are four possible outcomes: ``More than \$100,000'',
``More than \$10,000'', ``More than \$1,000'', ``\$1,000 or less''
(Figure~\ref{fig-canhasbaseandmoney}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{build }\OtherTok{\textless{}{-}} 
  \FunctionTok{get\_builddata}\NormalTok{(}\AttributeTok{startyear =} \DecValTok{2008}\NormalTok{, }\AttributeTok{endyear =} \DecValTok{2019}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(lon)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{cost =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      spend\_construction }\SpecialCharTok{\textgreater{}} \DecValTok{100000} \SpecialCharTok{\textasciitilde{}} \StringTok{"More than $100,000"}\NormalTok{,}
\NormalTok{      spend\_construction }\SpecialCharTok{\textgreater{}} \DecValTok{10000} \SpecialCharTok{\textasciitilde{}} \StringTok{"More than $10,000"}\NormalTok{,}
\NormalTok{      spend\_construction }\SpecialCharTok{\textgreater{}} \DecValTok{1000} \SpecialCharTok{\textasciitilde{}} \StringTok{"More than $1,000"}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"$1,000 or less"}
\NormalTok{      )}
\NormalTok{    )}

\NormalTok{pal }\OtherTok{\textless{}{-}}
  \FunctionTok{colorFactor}\NormalTok{(}\StringTok{"Dark2"}\NormalTok{, }\AttributeTok{domain =}\NormalTok{ build}\SpecialCharTok{$}\NormalTok{cost }\SpecialCharTok{|\textgreater{}} \FunctionTok{unique}\NormalTok{())}

\FunctionTok{leaflet}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{addTiles}\NormalTok{() }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# Add default OpenStreetMap map tiles}
  \FunctionTok{addCircleMarkers}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ build,}
    \AttributeTok{lng =}\NormalTok{ build}\SpecialCharTok{$}\NormalTok{lon,}
    \AttributeTok{lat =}\NormalTok{ build}\SpecialCharTok{$}\NormalTok{lat,}
    \AttributeTok{color =} \FunctionTok{pal}\NormalTok{(build}\SpecialCharTok{$}\NormalTok{cost),}
    \AttributeTok{popup =} \FunctionTok{paste}\NormalTok{(}
      \StringTok{"\textless{}b\textgreater{}Location:\textless{}/b\textgreater{}"}\NormalTok{,}
      \FunctionTok{as.character}\NormalTok{(build}\SpecialCharTok{$}\NormalTok{location),}
      \StringTok{"\textless{}br\textgreater{}"}\NormalTok{,}
      \StringTok{"\textless{}b\textgreater{}Amount:\textless{}/b\textgreater{}"}\NormalTok{,}
      \FunctionTok{as.character}\NormalTok{(build}\SpecialCharTok{$}\NormalTok{spend\_construction),}
      \StringTok{"\textless{}br\textgreater{}"}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{addLegend}\NormalTok{(}
    \StringTok{"bottomright"}\NormalTok{,}
    \AttributeTok{pal =}\NormalTok{ pal,}
    \AttributeTok{values =}\NormalTok{ build}\SpecialCharTok{$}\NormalTok{cost }\SpecialCharTok{|\textgreater{}} \FunctionTok{unique}\NormalTok{(),}
    \AttributeTok{title =} \StringTok{"Type"}\NormalTok{,}
    \AttributeTok{opacity =} \DecValTok{1}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./07-interactive_communication_files/figure-pdf/fig-canhasbaseandmoney-1.pdf}

}

\caption{\label{fig-canhasbaseandmoney}Interactive map of US bases with
colored circules to indicate spend}

\end{figure}

\hypertarget{mapdeck}{%
\subsection{Mapdeck}\label{mapdeck}}

\texttt{mapdeck} (Cooley 2020) is based on WebGL. This means the web
browser will do a lot of work for us. This enables us to accomplish
things with \texttt{mapdeck} that \texttt{leaflet} struggles with, such
as larger datasets.

To this point we have used `stamen maps' as our underlying tile, but
\texttt{mapdeck} uses `Mapbox': https://www.mapbox.com/. This requires
registering an account and obtaining a token. This is free and only
needs to be done once. Once we have that token we add it to our R
environment (the details of this process are covered in Chapter
@ref(gather-data) by running \texttt{usethis::edit\_r\_environ()}, which
will open a text file. There we can add our Mapbox secret token.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MAPBOX\_TOKEN }\OtherTok{=} \StringTok{\textquotesingle{}PUT\_YOUR\_MAPBOX\_SECRET\_HERE\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

We then save this `.Renviron' file, and restart R (`Session'
-\textgreater{} `Restart R').

Having obtained a token, we can create a plot of our base spend data
from earlier (Figure~\ref{fig-canhasbaseandmoneymapdeck}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mapdeck)}

\FunctionTok{mapdeck}\NormalTok{(}\AttributeTok{style =} \FunctionTok{mapdeck\_style}\NormalTok{(}\StringTok{\textquotesingle{}dark\textquotesingle{}}\NormalTok{)}
\NormalTok{        ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_scatterplot}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ build, }
    \AttributeTok{lat =} \StringTok{"lat"}\NormalTok{, }
    \AttributeTok{lon =} \StringTok{"lon"}\NormalTok{, }
    \AttributeTok{layer\_id =} \StringTok{\textquotesingle{}scatter\_layer\textquotesingle{}}\NormalTok{,}
    \AttributeTok{radius =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{radius\_min\_pixels =} \DecValTok{5}\NormalTok{,}
    \AttributeTok{radius\_max\_pixels =} \DecValTok{100}\NormalTok{,}
    \AttributeTok{tooltip =} \StringTok{"location"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./07-interactive_communication_files/figure-pdf/fig-canhasbaseandmoneymapdeck-1.pdf}

}

\caption{\label{fig-canhasbaseandmoneymapdeck}Interactive map of US
bases using Mapdeck}

\end{figure}

\hypertarget{shiny}{%
\section{Shiny}\label{shiny}}

\texttt{shiny} (Chang et al. 2021) is a way of making interactive web
applications using R. It is fun, but fiddly. Here we are going to step
through one way to take advantage of \texttt{shiny}. Which is to quickly
add some interactivity to our graphs. We will return to \texttt{shiny}
in more detail in Chapter @ref(deploying-models).

We are going to make an interactive graph based on the `babynames'
dataset from \texttt{babynames} (Wickham 2019b). First, we will build a
static version (Figure~\ref{fig-babynames}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(babynames)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{top\_five\_names\_by\_year }\OtherTok{\textless{}{-}}
\NormalTok{  babynames }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(year, sex) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}

\NormalTok{top\_five\_names\_by\_year }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{fill =}\NormalTok{ sex)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Babies with that name"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Occurances"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Sex"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./07-interactive_communication_files/figure-pdf/fig-babynames-1.pdf}

}

\caption{\label{fig-babynames}Popular baby names}

\end{figure}

We can see the most popular boys names tend to be more clustered,
compared with the most-popular girls names, which may be more spread
out. However, one thing that we might be interested in is how the effect
of the `bins' parameter shapes what we see. We might like to use
interactivity to explore different values.

To get started, create a new \texttt{shiny} app (`File -\textgreater{}
New File -\textgreater{} Shiny Web App'). Give it a name, such as
`not\_my\_first\_shiny' and then leave all the other options as the
default. A new file `app.R' will open and we click `Run app' to see what
it looks like.

Now replace the content in that file, `app.R', with the content below,
and then again click `Run app'

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(shiny)}

\CommentTok{\# Define UI for application that draws a histogram}
\NormalTok{ui }\OtherTok{\textless{}{-}} \FunctionTok{fluidPage}\NormalTok{(}
  \CommentTok{\# Application title}
  \FunctionTok{titlePanel}\NormalTok{(}\StringTok{"Count of names for five most popular names each year."}\NormalTok{),}
  
  \CommentTok{\# Sidebar with a slider input for number of bins}
  \FunctionTok{sidebarLayout}\NormalTok{(}\FunctionTok{sidebarPanel}\NormalTok{(}
    \FunctionTok{sliderInput}\NormalTok{(}
      \AttributeTok{inputId =} \StringTok{"number\_of\_bins"}\NormalTok{,}
      \AttributeTok{label =} \StringTok{"Number of bins:"}\NormalTok{,}
      \AttributeTok{min =} \DecValTok{1}\NormalTok{,}
      \AttributeTok{max =} \DecValTok{50}\NormalTok{,}
      \AttributeTok{value =} \DecValTok{30}
\NormalTok{    )}
\NormalTok{  ),}
  
  \CommentTok{\# Show a plot of the generated distribution}
  \FunctionTok{mainPanel}\NormalTok{(}\FunctionTok{plotOutput}\NormalTok{(}\StringTok{"distPlot"}\NormalTok{)))}
\NormalTok{)}

\CommentTok{\# Define server logic required to draw a histogram}
\NormalTok{server }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(input, output) \{}
\NormalTok{  output}\SpecialCharTok{$}\NormalTok{distPlot }\OtherTok{\textless{}{-}} \FunctionTok{renderPlot}\NormalTok{(\{}
    \CommentTok{\# Draw the histogram with the specified number of bins}
\NormalTok{    top\_five\_names\_by\_year }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{fill =}\NormalTok{ sex)) }\SpecialCharTok{+}
      \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{, }\AttributeTok{bins =}\NormalTok{ input}\SpecialCharTok{$}\NormalTok{number\_of\_bins) }\SpecialCharTok{+}
      \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
      \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Babies with that name"}\NormalTok{,}
           \AttributeTok{y =} \StringTok{"Occurances"}\NormalTok{,}
           \AttributeTok{fill =} \StringTok{"Sex"}\NormalTok{)}
\NormalTok{  \})}
\NormalTok{\}}

\CommentTok{\# Run the application}
\FunctionTok{shinyApp}\NormalTok{(}\AttributeTok{ui =}\NormalTok{ ui, }\AttributeTok{server =}\NormalTok{ server)}
\end{Highlighting}
\end{Shaded}

We have just build an interactive graph where the number of bins can be
changed. It should look like Figure~\ref{fig-shinyone}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/shiny_one.png"}

}

\caption{\label{fig-shinyone}Example of Shiny app where the user
controls the number of bins}

\end{figure}

\hypertarget{exercises-and-tutorial-6}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-6}}

\hypertarget{exercises-6}{%
\subsection{Exercises}\label{exercises-6}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Based on Presmanes Hill (2021a), are posts in \texttt{distill}
  re-built automatically (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    No
  \item
    Yes
  \item
    Depends on settings
  \end{enumerate}
\item
  Based on Lovelace, Nowosad, and Muenchow (2019), please explain in a
  paragraph or two, what is the difference between vector data and
  raster data in the context of geographic data?
\item
  Based on Wickham (2021a), \texttt{shiny} uses:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Object-oriented programming
  \item
    Functional programming
  \item
    Reactive programming
  \end{enumerate}
\item
  In a paragraph or two, why is it important to have a website?
\item
  Which of the following are packages that we could use to make a
  website (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{distill}
  \item
    \texttt{blogdown}
  \item
    \texttt{postcards}
  \item
    \texttt{hugo}
  \end{enumerate}
\item
  Looking at the help file for \texttt{postcards}, which of the
  following are themes that we could use (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{jolla}
  \item
    \texttt{jolla-blue}
  \item
    \texttt{jolla-red}
  \item
    \texttt{trestles}
  \item
    \texttt{mjolnir}
  \item
    \texttt{onofre}
  \item
    \texttt{solana}
  \end{enumerate}
\item
  Which function should we use to stop GitHub itself from trying to
  build our site instead of just serving it (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{file.create(\textquotesingle{}.nojekyll\textquotesingle{})}
  \item
    \texttt{file.remove(\textquotesingle{}.nojekyll\textquotesingle{})}
  \item
    \texttt{file.create(\textquotesingle{}.jekyll\textquotesingle{})}
  \item
    \texttt{file.remove(\textquotesingle{}.jekyll\textquotesingle{})}
  \end{enumerate}
\item
  Which argument to \texttt{addMarkers()} is used to specify the
  behavior that occurs after a marker is clicked (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{layerId}
  \item
    \texttt{icon}
  \item
    \texttt{popup}
  \item
    \texttt{label}
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial-6}{%
\subsection{Tutorial}\label{tutorial-6}}

The catalyst for this tutorial was work by Mauricio Vargas Sepúlveda
(`Pachá') and Andrew Whitby.

Please obtain data on the ethnic origins and number of victims of
Auschwitz. Then use \texttt{shiny} to create an interactive graph and an
interactive table. These should show the number of people murdered by
nationality/category and should allow the user to specify the groups
they are interested in seeing data for. Publish them. Then, based on the
themes brought up in Bouie (2022), discuss your work in at least two
pages. Submit a PDF created using R Markdown, and ensure that it
contains a link to your app and the GitHub repo that contains all code
and data.

\hypertarget{paper-1}{%
\subsection{Paper}\label{paper-1}}

At about this point, Paper Two (Appendix @ref(paper-two)) would be
appropriate.

\part{Acquisition}

\hypertarget{sec-farm-data}{%
\chapter{Farm data}\label{sec-farm-data}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{Atlas of AI}, Chapter 3 `Data', (Crawford 2021).
\item
  Read \emph{Guide to the Census of Population, 2016}, Chapter 10 `Data
  quality assessment', (Statistics Canada 2017).
\item
  Read \emph{Working-Class Households in Reading}, (focus on the method
  and approach, not necessarily the specific results) (Bowley 1913).
\item
  Read \emph{Representative Method: The Method of Stratified Sampling
  and the Method of Purposive Selection}, Parts I `Introduction', III
  `Different Aspects of the Representative Method', V `Conclusion' and
  Bowley's discussion p.~607 - 610, (Neyman 1934).
\item
  Read \emph{Using sex and gender in survey adjustment}, (Kennedy et al.
  2020).
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Why we conduct sampling and the two approaches: probability and
  non-probability.
\item
  Terminology and concepts including: target population, sampling frame,
  sample, simple random sampling, systematic sampling, stratified
  sampling, and cluster sampling.
\item
  Ratio and regression estimators.
\item
  Non-probability sampling including convenience and quota sampling and
  also snowball and respondent-driven sampling.
\item
  Obtain data from censuses and other datasets provided by governments.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{cancensus} (von Bergmann, Shkolnik, and Jacobs 2021)
\item
  \texttt{canlang} (T. Timbers 2020)
\item
  \texttt{tidyverse} (Wickham et al. 2019a)
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{cancensus::get\_census()}
\item
  \texttt{cancensus::list\_census\_datasets()}
\item
  \texttt{cancensus::list\_census\_regions()}
\item
  \texttt{cancensus::list\_census\_vectors()}
\item
  \texttt{cancensus::set\_api\_key()}
\item
  \texttt{canlang::can\_lang}
\item
  \texttt{canlang::region\_lang}
\item
  \texttt{dplyr::left\_join()}
\item
  \texttt{dplyr::mutate()}
\item
  \texttt{dplyr::row\_number()}
\item
  \texttt{dplyr::select()}
\item
  \texttt{dplyr::slice\_max()}
\item
  \texttt{dplyr::slice\_min()}
\item
  \texttt{dplyr::slice\_sample()}
\item
  \texttt{readr::read\_csv()}
\item
  \texttt{set.seed()}
\end{itemize}

\hypertarget{introduction-5}{%
\section{Introduction}\label{introduction-5}}

As we think about our world and telling stories about it, one of the
most difficult aspects is to reduce the beautiful complexity of it into
a dataset that we can use. We need to know what we are giving up when we
do this. Often, we are interested in understanding the implications of
some dataset, making forecasts based on it, or using that dataset to
make claims about the broader world. Regardless of how we turn our world
into data, we will only ever have a sample of the data that we need.
Statistics provides formal approaches that we use to keep these issues
front of mind.

In this chapter we first introduce statistical notions around sampling
to provide a framework that we use to guide our data gathering. We then
discuss censuses.

\hypertarget{sampling-essentials}{%
\section{Sampling essentials}\label{sampling-essentials}}

Statistics is at the heart of telling stories with data. Statisticians
have spent considerable time and effort thinking about the properties
that various samples of data will have and how they enable us to speak
to implications for the broader population.

Let us say that we have some data. For instance, a particular toddler
goes to sleep at 6:00pm every night. We might be interested to know
whether that bedtime is common among all toddlers, or if we have an
unusual toddler. We only have one toddler so our ability to use his
bedtime to speak about all toddlers is limited.

One approach would be to talk to friends who also have toddlers. And
then talk to friends of friends. How many friends, and friends of
friends, do we have to ask because we can begin to feel comfortable
speaking about some underlying truth of toddler bedtime?

Wu and Thompson (2020, 3) describe statistics as `the science of how to
collect and analyze data and draw statements and conclusions about
unknown populations.' Here `population' refers to some infinite group
that we can never know exactly, but that we can use the probability
distributions of random variables to describe the characteristics of.
Another way to say this is that statistics involves getting some data
and trying to say something sensible based on it.

Some of the critical terminology includes:

\begin{itemize}
\tightlist
\item
  `Target population': The collection of all items about which we would
  like to speak.
\item
  `Sampling frame': A list of all the items from the target population
  that we could get data about.
\item
  `Sample': The items from the sampling frame that we get data about.
\end{itemize}

A target population is a finite set of labelled items, of size \(N\).
For instance, we could hypothetically add a label to all the books in
the world: `Book 1', `Book 2', `Book 3', \ldots, `Book \(N\)'. There is
a difference between use of the term population here, and that of
everyday usage. For instance, one sometimes hears those who work with
census data say that they do not need to worry about sampling because
they have the whole population of the country. This is a conflation of
the terms, as what they have is the sample gathered by the census of the
population of a country.

It can be difficult to define a target population. For instance, say we
have been asked to find out about the consumption habits of hipsters.
How can we define that target population? If someone regularly eats
avocado toast, but has never drunk bullet coffee, then are they in the
population? Some aspects that we might be interested in are formally
defined to an extent that is not always commonly realized. For instance,
whether an area is classified as rural is often formally defined by a
country's statistical agency. But other aspects are less clear. For
instance, how do we classify someone as a `smoker'? If a 15-year-old has
had 100 cigarettes over their lifetime, then we need to treat them
differently than if they have had none. But if a 90-year-old has had 100
cigarettes over their lifetime, then are they likely to different to a
90-year-old who has had none? At what age, and number of cigarettes do
these answers change?

Consider if we want to speak to the titles of all the books ever
written. Our target population is all books ever written. But it is
almost impossible for us to imagine that we could get information about
the title of a book that was written in the nineteenth century, but that
the author locked in their desk and never told anyone about. One
sampling frame could be all books in the Library of Congress Online
Catalog, another could be the 25 million that were digitized by Google
(Somers 2017). And then finally, our sample may be the tens of thousands
that are available through Project Gutenberg, and that we can access
using \texttt{gutenbergr} (D. Robinson 2021).

To consider another example, consider wanting to speak of the attitudes
of all Brazilians who live in Germany. The target population is all
Brazilians who live in Germany. One possible source of information would
be Facebook and so in that case, the sampling frame might be all
Brazilians who live in Germany who have Facebook. And then our sample be
might all Brazilians who live in Germany who have Facebook who we can
gather data about. The target population and the sampling frame will be
different because not all Brazilians who live in Germany will have
Facebook. And the sampling frame will be different to the sample because
we will likely not be able to gather data about all Brazilians who live
in Germany and have Facebook.

\hypertarget{sampling-in-dublin-and-reading}{%
\subsection{Sampling in Dublin and
Reading}\label{sampling-in-dublin-and-reading}}

To be clearer, we will consider two examples: a 1798 count of the number
of inhabitants of Dublin, Ireland (Whitelaw 1905), and a 1912 count of
working-class households in Reading, England (Bowley 1913).

In 1798 the Reverend James Whitelaw conducted a survey of Dublin,
Ireland, to count its population. Whitelaw (1905) describes how
population estimates had a wide variation, for instance the estimated
size of London at the time ranged from 128,570 to 300,000. Reverend
Whitelaw expected that the Lord Mayor of Dublin could compel the person
in charge of each house to affix a list of the inhabitants of that house
to the door, and then Reverend Whitelaw could simply use this.

Instead, he found that the lists were `frequently illegible, and
generally short of the actual number by a third, or even one-half'. And
so instead he recruited assistants, and they went door-to-door making
their own counts. The resulting estimates are particularly informative
(Figure~\ref{fig-whitelawsresults}). And the total population of Dublin
in 1798 was estimated at 182,370.

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/whitelaw.png}

}

\caption{\label{fig-whitelawsresults}Extract of the results that
Reverend Whitelaw found in 1798}

\end{figure}

One aspect worth noticing is that Reverend Whitelaw includes information
about class. It is difficult to know how that was determined, but it
played a large role in the data collection. Reverend Whitelaw describes
how the houses of `the middle and upper classes always contained some
individual who was competent to the task {[}of making a list{]}'. But
that `among the lower class, which forms the great mass of the
population of this city, the case was very different'. It is difficult
to know how Reverend Whitelaw could have known that the upper and middle
classes were not representing their number, while the lower class was.
It is also difficult to imagine Reverend Whitelaw going into the houses
of the upper class and counting their number, as he and his assistants
did for the lower classes. As always, the issue of defining the target
population is a difficult one, and it seems that there may have been
slightly different approaches to each class.

A little over one hundred years later, Bowley (1913) was interested in
counting the number of working-class households in Reading, England.
Bowley selects the sample using the following procedure (Bowley 1913,
672):

\begin{quote}
One building in ten was marked throughout the local directory in
alphabetical order of streets, making about 1,950 in all. Of those about
300 were marked as shops, factories, institutions and non-residential
buildings, and about 300 were found to be indexed among Principal
Residents, and were so marked. The remaining 1,350 were working-class
houses, and a number of volunteers set out to visit every one of
these\ldots{} {[}I{]}t was decided to take only one house in 20,
rejecting the incomplete information as to the intermediate tenths. The
visitors were instructed never to substitute another house for that
marked, however difficult it proved to get information, or whatever the
type of house.
\end{quote}

Bowley (1913) continues that they ended up with information about 622
working-class households. And, having judged, based on the census that
there were about 18,000 households in Reading, Bowley (1913) applies
`{[}t{]}he multiplier twenty-one\ldots{} to all the sample data to give
estimates for the whole of Reading.' Bowley (1913) explains that the
reasonableness of the estimates depends `not on its proportion to the
whole, but on its own magnitude, if the conditions of random sampling
are secured, as it is believed they have been in this inquiry'. Bowley
is, for instance, able to furnish information about the rent paid per
week (Figure~\ref{fig-bowleyrents}).

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/bowleyrents.png}

}

\caption{\label{fig-bowleyrents}Extract of the results that Bowley found
about rent paid by the working-class in Reading, England}

\end{figure}

\hypertarget{probabilistic-sampling}{%
\subsection{Probabilistic sampling}\label{probabilistic-sampling}}

Having identified a target population and a sampling frame, we need to
distinguish between probability and non-probability sampling, which
Neyman (1934) describes as `random sampling' and `purposive selection':

\begin{itemize}
\tightlist
\item
  `Probability sampling': Every unit in the sampling frame has some,
  known, chance of being sampled and the specific sample is obtained
  randomly based on these chances. Note that these chances do not
  necessarily need to be same for each unit.
\item
  `Non-probability sampling': Units from the sampling frame are sampled
  based on convenience, quotas, judgement, or other non-random
  processes.
\end{itemize}

Often the difference between probability and non-probability sampling is
one of degree. For instance, we cannot often forcibly obtain data and so
there is almost always an aspect of volunteering. Even when there are
penalties for not providing data, such as the case for completing a
census form in many countries, it is difficult for even a government to
force people to fill it out completely or truthfully. One reason that
the Randomized Control Trial revolution, discussed in Chapter
@ref(hunt-data), was needed was due to a lack of probability sampling.
The most important aspect to be clear about with probability sampling is
the role of uncertainty. This allows us to make claims about the
population, based on our sample, with known amounts of error. The
trade-off is that probability sampling is often expensive and difficult.

To add some more specificity to our discussion, following Lohr (2019,
27) it may help to consider the numbers 1 to 100 and let us define that
as our target population. With simple random sampling, every unit has
the same chance of being included. In this case it is 20 per cent. We
would expect to have around 20 units in our sample, or around 1 in 5
compared with our target population.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{illustrative\_sampling }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{unit =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{,}
         \AttributeTok{simple\_random\_sampling =} 
           \FunctionTok{sample}\NormalTok{(}
             \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Included"}\NormalTok{, }\StringTok{"Not included"}\NormalTok{),}
             \AttributeTok{size =} \DecValTok{100}\NormalTok{,}
             \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{,}
             \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.8}\NormalTok{)}
\NormalTok{             ))}

\NormalTok{illustrative\_sampling}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 2
    unit simple_random_sampling
   <int> <chr>                 
 1     1 Not included          
 2     2 Not included          
 3     3 Not included          
 4     4 Not included          
 5     5 Not included          
 6     6 Not included          
 7     7 Not included          
 8     8 Not included          
 9     9 Not included          
10    10 Not included          
# ... with 90 more rows
\end{verbatim}

With systematic sampling, as was used by Bowley (1913), we proceed by
selecting some value, say 5. We randomly pick a starting point in units
1 to 5, say 3. And we then include every fifth unit. That starting point
is usually randomly selecting.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{starting\_point }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{), }
                         \AttributeTok{size =} \DecValTok{1}\NormalTok{)}

\NormalTok{illustrative\_sampling }\OtherTok{\textless{}{-}}
\NormalTok{  illustrative\_sampling }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{systematic\_sampling =} 
           \FunctionTok{if\_else}\NormalTok{(}\FunctionTok{row\_number}\NormalTok{() }\SpecialCharTok{\%in\%} \FunctionTok{seq.int}\NormalTok{(}\AttributeTok{from =}\NormalTok{ starting\_point, }
                                             \AttributeTok{to =} \DecValTok{100}\NormalTok{, }
                                             \AttributeTok{by =} \DecValTok{5}\NormalTok{), }
                   \StringTok{"Included"}\NormalTok{, }
                   \StringTok{"Not included"}\NormalTok{)}
\NormalTok{         )}

\NormalTok{illustrative\_sampling}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 3
    unit simple_random_sampling systematic_sampling
   <int> <chr>                  <chr>              
 1     1 Not included           Included           
 2     2 Not included           Not included       
 3     3 Not included           Not included       
 4     4 Not included           Not included       
 5     5 Not included           Not included       
 6     6 Not included           Included           
 7     7 Not included           Not included       
 8     8 Not included           Not included       
 9     9 Not included           Not included       
10    10 Not included           Not included       
# ... with 90 more rows
\end{verbatim}

When we consider our population, it will typically have some grouping.
This may be as straight-forward as a country having states, provinces,
counties, or statistical districts; a university having faculties and
departments; and humans having age-groups. A stratified structure is one
in which we can divide the population into mutually exclusive and
collectively exhaustive sub-populations, or strata.

We use stratification to help with the efficiency of sampling or with
the balance of the survey. For instance, the population of the US is
around 335 million, with 40 million being in California, while Wyoming
as around half a million. So even a survey of 10,000 responses would
only expect to have 15 responses from Wyoming, which could make
inference about Wyoming difficult. We could use stratification to ensure
there are 200 responses from each of the 50 US states. We would use
random sampling within each state to select the person about whom data
will be gathered.

In our case, we will stratify our illustration, we will consider that
our strata are the 10s, that is, 1 to 10 is one stratum, 11 to 20 is
another, and so on. We will use simple random sampling within these
strata to select two units.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{selected\_within\_strata }\OtherTok{\textless{}{-}}
\NormalTok{  illustrative\_sampling }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{strata =}\NormalTok{ (}\FunctionTok{row\_number}\NormalTok{() }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%/\%} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(strata) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(unit)}

\NormalTok{illustrative\_sampling }\OtherTok{\textless{}{-}}
\NormalTok{  illustrative\_sampling }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{stratified\_sampling =} \FunctionTok{if\_else}\NormalTok{(}
      \FunctionTok{row\_number}\NormalTok{() }\SpecialCharTok{\%in\%}\NormalTok{ selected\_within\_strata,}
      \StringTok{"Included"}\NormalTok{,}
      \StringTok{"Not included"}
\NormalTok{    )}
\NormalTok{  )}

\NormalTok{illustrative\_sampling}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 4
    unit simple_random_sampling systematic_sampling stratified_sampling
   <int> <chr>                  <chr>               <chr>              
 1     1 Not included           Included            Included           
 2     2 Not included           Not included        Not included       
 3     3 Not included           Not included        Not included       
 4     4 Not included           Not included        Not included       
 5     5 Not included           Not included        Not included       
 6     6 Not included           Included            Not included       
 7     7 Not included           Not included        Not included       
 8     8 Not included           Not included        Not included       
 9     9 Not included           Not included        Included           
10    10 Not included           Not included        Not included       
# ... with 90 more rows
\end{verbatim}

And finally, we can also take advantage of some clusters that may exist
in our dataset. Like strata, clusters are collectively exhaustive and
mutually exclusive. Our examples from earlier, of states, departments,
and age-groups remain valid as clusters. However, it is our intentions
toward these groups that is different. Specific, with cluster sampling,
we do not intend to collect data from every cluster, whereas with
stratified sampling we do. With stratified sampling we look at every
stratum and conduct simple random sampling within each strata to select
the sample. With cluster sampling we conduct simple random sampling to
select clusters of interest. We can then either sample every unit in
those selected clusters or use simple random sampling, within the
selected clusters, to select units. That all said, this difference can
become less clear in practice, especially \emph{ex post}.

In our case, we will cluster our illustration again based on the 10s. We
will use simple random sampling to select two clusters for which we will
use the entire cluster.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{selected\_clusters }\OtherTok{\textless{}{-}} 
  \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{9}\NormalTok{),}
         \AttributeTok{size =} \DecValTok{2}\NormalTok{)}

\NormalTok{illustrative\_sampling }\OtherTok{\textless{}{-}}
\NormalTok{  illustrative\_sampling }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{cluster =}\NormalTok{ (}\FunctionTok{row\_number}\NormalTok{() }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\%/\%} \DecValTok{10}\NormalTok{, }
         \AttributeTok{cluster\_sampling =} \FunctionTok{if\_else}\NormalTok{(}
\NormalTok{           cluster }\SpecialCharTok{\%in\%}\NormalTok{ selected\_clusters,}
           \StringTok{"Included"}\NormalTok{,}
           \StringTok{"Not included"}
\NormalTok{           )}
\NormalTok{         ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{cluster)}

\NormalTok{illustrative\_sampling}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 5
    unit simple_random_sampl~ systematic_samp~ stratified_samp~ cluster_sampling
   <int> <chr>                <chr>            <chr>            <chr>           
 1     1 Not included         Included         Included         Included        
 2     2 Not included         Not included     Not included     Included        
 3     3 Not included         Not included     Not included     Included        
 4     4 Not included         Not included     Not included     Included        
 5     5 Not included         Not included     Not included     Included        
 6     6 Not included         Included         Not included     Included        
 7     7 Not included         Not included     Not included     Included        
 8     8 Not included         Not included     Not included     Included        
 9     9 Not included         Not included     Included         Included        
10    10 Not included         Not included     Not included     Included        
# ... with 90 more rows
\end{verbatim}

At this point we can illustrate the differences between our approaches
(Figure~\ref{fig-samplingexamples}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\AttributeTok{simple\_random\_sampling =} \StringTok{"Simple random sampling"}\NormalTok{, }
                \AttributeTok{systematic\_sampling =} \StringTok{"Systematic sampling"}\NormalTok{,}
                \AttributeTok{stratified\_sampling =} \StringTok{"Stratified sampling"}\NormalTok{,}
                \AttributeTok{cluster\_sampling =} \StringTok{"Cluster sampling"}\NormalTok{)}

\NormalTok{illustrative\_sampling\_long }\OtherTok{\textless{}{-}} 
\NormalTok{  illustrative\_sampling }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_longer}\NormalTok{(}
    \AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}
\NormalTok{      simple\_random\_sampling,}
\NormalTok{      systematic\_sampling,}
\NormalTok{      stratified\_sampling,}
\NormalTok{      cluster\_sampling),}
    \AttributeTok{names\_to =} \StringTok{"sampling\_method"}\NormalTok{,}
    \AttributeTok{values\_to =} \StringTok{"in\_sample"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sampling\_method =} \FunctionTok{factor}\NormalTok{(sampling\_method,}
                                  \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"simple\_random\_sampling"}\NormalTok{,}
                                             \StringTok{"systematic\_sampling"}\NormalTok{,}
                                             \StringTok{"stratified\_sampling"}\NormalTok{,}
                                             \StringTok{"cluster\_sampling"}\NormalTok{))}
\NormalTok{         ) }

\NormalTok{illustrative\_sampling\_long }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(in\_sample }\SpecialCharTok{==} \StringTok{"Included"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ unit, }\AttributeTok{y =}\NormalTok{ in\_sample)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(sampling\_method),}
             \AttributeTok{dir =} \StringTok{"v"}\NormalTok{,}
             \AttributeTok{ncol =} \DecValTok{1}\NormalTok{,}
             \AttributeTok{labeller =} \FunctionTok{labeller}\NormalTok{(}\AttributeTok{sampling\_method =}\NormalTok{ new\_labels)}
\NormalTok{             ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Unit"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Is included in sample"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.y =} \FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./08-farm_files/figure-pdf/fig-samplingexamples-1.pdf}

}

\caption{\label{fig-samplingexamples}Illustrative example of simple
random sampling, systematic sampling, stratified sampling, and cluster
sampling over the numbers from 1 to 100}

\end{figure}

Having established our sample, we typically want to use it to make
claims about the population. Neyman (1934, 561) goes further and says
that `{[}o{]}bviously the problem of the representative method is
\emph{par excellence} the problem of statistical estimation. We are
interested in characteristics of a certain population, such \(\pi\),
which it is either impossible or at least very difficult to study in
detail, and we try to estimate these characteristics basing our judgment
on the sample.'

In particular, we would typically be interested to estimate a population
mean and variance.

Scaling up can be used when we are interested in using a count from our
sample to imply some total count for the population. We saw this in
Bowley (1913) where the ratio of the number of households in the sample,
compared with the number of households known from the census, is 21, and
this information is used to scale up the sample.

To consider an example, perhaps we were interested in the sum of the
numbers from 1 to 100. We know that our samples are of size 20, and so
need to be scaled up five times (Table~\ref{tbl-scaleup}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{illustrative\_sampling\_long }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(in\_sample }\SpecialCharTok{==} \StringTok{"Included"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(sampling\_method) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{sum\_from\_sample =} \FunctionTok{sum}\NormalTok{(unit)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{scaled\_by\_five =}\NormalTok{ sum\_from\_sample }\SpecialCharTok{*} \DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Sampling method"}\NormalTok{, }\StringTok{"Sum of sample"}\NormalTok{, }\StringTok{"Implied population sum"}\NormalTok{),}
    \AttributeTok{format.args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{big.mark =} \StringTok{","}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-scaleup}{}
\begin{longtable}[]{@{}lrr@{}}
\caption{\label{tbl-scaleup}Sum of the numbers in each sample, and
implied sum of population}\tabularnewline
\toprule()
Sampling method & Sum of sample & Implied population sum \\
\midrule()
\endfirsthead
\toprule()
Sampling method & Sum of sample & Implied population sum \\
\midrule()
\endhead
simple\_random\_sampling & 840 & 4,200 \\
systematic\_sampling & 970 & 4,850 \\
stratified\_sampling & 979 & 4,895 \\
cluster\_sampling & 910 & 4,550 \\
\bottomrule()
\end{longtable}

The actual sum of the population is 5,050. We can obtain this using a
trick, attributed to Euler, who noticed that the sum of 1 to any number
can be quickly obtained by finding the middle number and then
multiplying that by one plus the number. So, in this case, it
\texttt{50*101}. Alternatively we can use R: \texttt{sum(1:100)}.

Our estimate of the population sum, based on the scaling, are especially
revealing. The closest is stratified sample, closely followed by
systematic sampling. Cluster sampling is a little over 10 per cent off,
while simple random sampling is a little further away. To get close, it
is important that our sampling method gets as many of the higher values
as possible. And so stratified and systematic sampling, both of which
ensured that we had unit from the larger numbers did particularly well.
The performance of cluster and simple random sampling would depend on
the particular clusters, and units, selected. In this case, stratified
and systematic sampling ensured that our estimate of the sum of the
population, would not be too far away from the actual population sum.

This approach has a long history. For instance, Adolphe Quetelet, the
nineteenth century astronomer, mathematician, statistician, and
sociologist proposed one. Stigler (1986, 163) describes how by 1826
Quetelet had become involved in the statistical bureau, and they were
planning for a census. Quetelet argued that births and deaths were well
known, but migration was not. He proposed an approach based on counts in
specific geographies, which could then be scaled up to the whole
country. The criticism of the plan focused on the difficulty of
selecting appropriate geographies, which we saw also in our example of
cluster sampling. The criticism was reasonable, and even today, some two
hundred years later, something that we should keep front of mind,
(Stigler 1986):

\begin{quote}
He {[}Quetelet{]} was acutely aware of the infinite number of factors
that could affect the quantities he wished to measure, and he lacked the
information that could tell him which were indeed important. He\ldots{}
was reluctant to group together as homogenous, data that he had reason
to believe was not\ldots{} To be aware of a myriad of potentially
important factors, without knowing which are truly important and how
their effect may be felt, is often to fear the worst'\ldots. He
{[}Quetelet{]} could not bring himself to treat large regions as
homogeneous, {[}and so{]} he could not think of a single rate as
applying to a large area
\end{quote}

We are able to do this scaling up when we know the population total, but
if we do not know that, or we have concerns around the precision of that
approach then we may use a ratio estimator.

Ratio estimators also have a long history. For instance, in 1802 they
were used by Pierre-Simon Laplace to estimate the total population of
France, based on the ratio of the number of registered births, which was
known throughout the country, to the number of inhabitants, which was
only know for certain communes. He calculated this ratio for the three
communes, and then scaled it, based on knowing the number of births
across the whole country to produce an estimate of the population of
France (Lohr 2019).

In particular, a ratio estimator of some population parameter is the
ratio of two means. For instance, we may have some information on the
number of hours that a toddler sleeps overnight, \(x\), and the number
of hours their parents sleep overnight \(y\) over a 30 day period.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{sleep }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{toddler\_sleep =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{14}\NormalTok{), }\AttributeTok{size =} \DecValTok{30}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{difference =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{3}\NormalTok{), }\AttributeTok{size =} \DecValTok{30}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{parent\_sleep =}\NormalTok{ toddler\_sleep }\SpecialCharTok{{-}}\NormalTok{ difference}
\NormalTok{  )}

\NormalTok{sleep}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 30 x 3
   toddler_sleep difference parent_sleep
           <int>      <int>        <int>
 1            10          1            9
 2            11          0           11
 3            14          2           12
 4             2          2            0
 5             6          1            5
 6            14          2           12
 7             3          0            3
 8             5          2            3
 9             4          3            1
10             4          1            3
# ... with 20 more rows
\end{verbatim}

And the average of each is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sleep }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{toddler\_sleep\_average =} \FunctionTok{mean}\NormalTok{(toddler\_sleep),}
            \AttributeTok{parent\_sleep\_average =} \FunctionTok{mean}\NormalTok{(parent\_sleep))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  toddler_sleep_average parent_sleep_average
                  <dbl>                <dbl>
1                  6.17                  4.9
\end{verbatim}

Then the ratio estimate of the proportion of sleep that a parent gets
compared with their toddler is:

\[\hat{B} = \frac{\bar{y}}{\bar{x}} = \frac{4.9}{6.16} \approx 0.8\]

While acknowledging that it is a spectrum, much of statistics was
developed based on probability sampling. But a considerable amount of
modern sampling is done using non-probability sampling. A common
approach is to use Facebook and other advertisements to recruit a panel
of respondents in exchange for compensation. This panel is then the
group that is sent various surveys as necessary. But think for a moment
about the implications of this. For instance, what type of people are
likely to respond to such an advertisement? Is the richest person in the
world likely to respond? Are especially young or especially old people
likely to respond? In some cases, it is possible to do a census.
Nation-states typically do one every five to ten years. But there is a
reason that it is only nation states that do them---they are expensive,
time-consuming, and surprisingly, they are sometimes not as accurate as
we may hope because of how general they need to be.

\hypertarget{non-probability-samples}{%
\subsection{Non-probability samples}\label{non-probability-samples}}

Non-probability samples have an important role to play because they are
typically cheaper and quicker to obtain than probability samples.
Further, as we have discussed, the difference between probability and
non-probability samples is sometimes one of degree, rather than
dichotomy. In any case, non-probability samples are legitimate and
appropriate for some tasks provided one is clear about the trade-offs
and ensure transparency (Baker et al. 2013).

Convenience sampling involves gathering data from a sample that is easy
to access. For instance, one often asks one's friends and family to fill
out a survey as a way of testing it before more wide-scale distribution.
If instead we were to analyze such a sample, then we would likely be
using convenience sampling.

The main issue with convenience sampling is that it is unlikely to be
able to speak to much of a broader population than those who filled out
the survey. There are also tricky ethical considerations, and typically
a lack of anonymity which may further bias the results. On the other
hand, it can be useful to cheaply get a quick sense of a situation while
rolling out sampling approaches likely to be more broadly useful.

Quota sampling occurs when we have strata, but we do not use random
sampling within those strata to select the unit. For instance, if we
again stratified the US based on state, but then instead of ensuring
that everyone in Wyoming had the chance to be chosen for that stratum,
just picked people at Jackson Hole. Again, there are some advantages to
this approach, especially in terms of speed and cost, but the resulting
sample is likely biased in various ways.

As the saying goes, birds of a feather flock together. And we can take
advantage of that in our sampling. Although Handcock and Gile (2011)
describe various uses before this, and it is notoriously difficult to
define attribution in multidisciplinary work, snowball sampling is
nicely defined by Goodman (1961). Following Goodman (1961), to conduct
snowball sampling, we first draw a random sample from the sampling
frame. Each of these is asked to name \(k\) others also in the sample
population, but not in that initial draw, and these form the `first
stage'. Each individual in the first stage is then similarly asked to
name \(k\) others who are also in the sample population, but again not
in the random draw or the first stage, and these form the `second
stage'. We need to have specified the number of stages, \(s\), and also
\(k\) ahead of time.

Respondent-driven sampling was developed by Heckathorn (1997) to focus
on hidden populations, which are those for which: 1) there is no
sampling frame and 2) being known to be in the sampling population could
have a negative effect. For instance, we could imagine various countries
in which it would be difficult to sample from the gay population or
those who have had abortions because it is illegal. Respondent-driven
sampling differs from snowball sampling in two ways: 1) In addition to
compensation for their own response, as is the case with snowball
sampling, respondent-driven sampling typically also involves
compensation for recruiting others. 2) Respondents are not asked to
provide information about others to the investigator, but instead
recruit them into the study. Selection into the sample occurs not from
sampling frame, but instead from the networks of those already in the
sample (M. J. Salganik and Heckathorn 2004).

Having established the foundations of sampling, which should remain
front of mind, we turn to describe some approaches to gathering data.
These will largely represent convenience samples.

\hypertarget{censuses}{%
\section{Censuses}\label{censuses}}

There are a variety of sources of data that have been produced for the
purposes of being used as datasets. One thinks here especially of
censuses. Whitby (2020, 30--31) provides an enthralling overview,
describing how the earliest censuses that we have written suggestions of
are from China's Yellow River valley, and that they were used for more
than just purposes of taxation and conscription. Whitby (2020) also
highlights the links between censuses and religion, quoting from Book of
Luke `In those days Caesar Augustus issued a decree that a census should
be taken of the entire Roman world', which led to David and Mary
travelling to Bethlehem. The

Taxation was a substantial motivator for censuses. Jones (1953)
describes how census records survive that `were probably engraved in the
late third or early fourth century A.D., when Diocletian and his
colleagues and successors are known to have been active in carrying out
censuses to serve as the basis of their new system of taxation'. And
detailed records of this sort have been abused. For instance, Luebke and
Milton (1994) say how `(t)he Nazi regime gathered its information with
two relatively conventional tools of modern administration: the national
census and police registration'.

Another source of data deliberately put together to be a dataset include
economic conditions such as unemployment, inflation, and GDP.
Interestingly, Rockoff (2019) describes how these economic statistics
were not actually developed by the federal government, even though
federal governments typically eventually took over that role. Typically,
these sources of data are put together by governments. They have the
powers of the state behind them which enables them to be thorough in a
way that other datasets cannot be, and similarly bring a specific
perspective. That is not to say that census data are unimpeachable, and
common errors include under- and over-enumeration, as well as
misreporting {[}steckel1991quality{]}.

Another, similarly, large and established source of data are from
long-running large surveys. These are conducted on a regular basis, and
while not usually directly conducted by the government, they are usually
funded, one way or another, by the government. For instance, here we
often think of electoral surveys, such as the Canadian Election Study,
which has run in association with every federal election since 1965, and
similarly the British Election Study which has been associated with
every general election since 1964.

Finally, there has been a large push toward open data in government.
While the term has become contentious because of how it has occurred in
practice, the underlying principle---that the government should make
available the data that is has---is undeniable. In this chapter we cover
these datasets, which we term `farmed data'. They are typically fairly
nicely put together and the work of collecting, preparing and cleaning
these datasets has typically been done. They are also, usually,
conducted on a known release cycle. For instance, most developed
countries release unemployment and inflation dataset on a monthly basis,
GDP on a quarterly basis, and a census every five to ten years.

While these datasets have always been useful, they were developed for a
time when much analysis was conducted without the use of scripts and
programming languages. A cottage industry of R package development has
sprung up around making it easier to get these datasets into R. In this
chapter we cover a few that are especially useful.

It is important to recognize that data are not neutral. Thinking clearly
about who is included in the dataset, and who is systematically
excluded, is critical. As Crawford (2021, 121) says:

\begin{quote}
The way data is understood, captured, classified, and named is
fundamentally an act of world-making and containment\ldots. The myth of
data collection as a benevolent practice\ldots{} has obscured its
operations of power, protecting those who profit most while avoiding
responsibility for its consequences.
\end{quote}

At this point, it is worth briefly discussing the role of sex and gender
in survey research, following Kennedy et al. (2020). Sex is based on
biological attributes, while gender is socially constructed. We are
likely interested in the effect of gender on our dependent variable.
Moving away from a non-binary concept of gender, in terms of official
statistics, is only something that has happened recently. As a
researcher one of the problems of insisting on a binary is that, as
Kennedy et al. (2020, 2) say `\ldots when measuring gender with simply
two categories, there is a failure to capture the unique experiences of
those who do not identify as either male or female, or for those whose
gender does not align with their sex classification.'. A researcher has
a variety of ways of proceeding, and Kennedy et al. (2020) discuss these
based on: ethics, accuracy, practicality, and flexibility. However,
`there is no single good solution that can be applied to all situations.
Instead, it is important to recognize that there is a compromise between
ethical concerns, statistical concerns, and the most appropriate
decision will be reflective of this' {[}p.~16{]}. The most important
consideration is to ensure appropriate `respect and consideration for
the survey respondent'.

\hypertarget{canada}{%
\subsection{Canada}\label{canada}}

The first census in Canada was conducted in 1666. There were 3,215
inhabitants that were counted, and the census asked about age, sex,
marital status, and occupation (Statistics Canada 2017). In association
with Confederation, in 1867 a decennial census was required so that
political representatives could be allocated for the new Parliament.
Regular censuses have occurred since then, the most recent in 2021.

We can explore some data on languages spoken in Canada from the 2016
Census using \texttt{canlang} (T. Timbers 2020). This package is not yet
available on CRAN, and so we install it from GitHub, using
\texttt{devtools} (Wickham, Hester, and Chang 2020).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)}
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"ttimbers/canlang"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We will start with the `can\_lang' dataset, which provides the number of
Canadians who use that language for 214 languages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(canlang)}

\NormalTok{can\_lang}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 214 x 6
   category          language mother_tongue most_at_home most_at_work lang_known
   <chr>             <chr>            <dbl>        <dbl>        <dbl>      <dbl>
 1 Aboriginal langu~ Aborigi~           590          235           30        665
 2 Non-Official & N~ Afrikaa~         10260         4785           85      23415
 3 Non-Official & N~ Afro-As~          1150          445           10       2775
 4 Non-Official & N~ Akan (T~         13460         5985           25      22150
 5 Non-Official & N~ Albanian         26895        13135          345      31930
 6 Aboriginal langu~ Algonqu~            45           10            0        120
 7 Aboriginal langu~ Algonqu~          1260          370           40       2480
 8 Non-Official & N~ America~          2685         3020         1145      21930
 9 Non-Official & N~ Amharic          22465        12785          200      33670
10 Non-Official & N~ Arabic          419890       223535         5585     629055
# ... with 204 more rows
\end{verbatim}

We can quickly see the top-10 most common languages to have as mother
tongue.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{can\_lang }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_max}\NormalTok{(mother\_tongue, }\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(language, mother\_tongue)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 2
   language                     mother_tongue
   <chr>                                <dbl>
 1 English                           19460850
 2 French                             7166700
 3 Mandarin                            592040
 4 Cantonese                           565270
 5 Punjabi (Panjabi)                   501680
 6 Spanish                             458850
 7 Tagalog (Pilipino, Filipino)        431385
 8 Arabic                              419890
 9 German                              384040
10 Italian                             375635
\end{verbatim}

We could combine two datasets together `region\_lang' and
`region\_data', to see if the five most-common languages differ between
the largest region, Toronto, and the smallest, Belleville.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{region\_lang }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{left\_join}\NormalTok{(region\_data, }\AttributeTok{by =} \StringTok{"region"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_max}\NormalTok{(}\FunctionTok{c}\NormalTok{(population)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_max}\NormalTok{(mother\_tongue, }\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(region, language, mother\_tongue, population) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop =}\NormalTok{ mother\_tongue }\SpecialCharTok{/}\NormalTok{ population)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 5
  region  language          mother_tongue population   prop
  <chr>   <chr>                     <dbl>      <dbl>  <dbl>
1 Toronto English                 3061820    5928040 0.516 
2 Toronto Cantonese                247710    5928040 0.0418
3 Toronto Mandarin                 227085    5928040 0.0383
4 Toronto Punjabi (Panjabi)        171225    5928040 0.0289
5 Toronto Italian                  151415    5928040 0.0255
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{region\_lang }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{left\_join}\NormalTok{(region\_data, }\AttributeTok{by =} \StringTok{"region"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_min}\NormalTok{(}\FunctionTok{c}\NormalTok{(population)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_max}\NormalTok{(mother\_tongue, }\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(region, language, mother\_tongue, population) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop =}\NormalTok{ mother\_tongue }\SpecialCharTok{/}\NormalTok{ population)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 5
  region     language mother_tongue population    prop
  <chr>      <chr>            <dbl>      <dbl>   <dbl>
1 Belleville English          93655     103472 0.905  
2 Belleville French            2675     103472 0.0259 
3 Belleville German             635     103472 0.00614
4 Belleville Dutch              600     103472 0.00580
5 Belleville Spanish            350     103472 0.00338
\end{verbatim}

We can see a considerable difference between the proportions, with a
little over 50 per cent of those in Toronto having English as their
mother tongue, while that is the case for around 90 per cent of those in
Belleville.

In general, data from Canadian censuses are not as easily available as
in other countries. Statistics Canada, which is the government agency
that is responsible for the census and other official statistics freely
provides a Individuals File from the 2016 census as a Public Use
Microdata File (PUMF), but only in response to a request. And while it
is a 2.7 per cent sample from the 2016 census, this PUMF provides
limited detail.

Another way to access data from the Canadian census is to use
\texttt{cancensus} (von Bergmann, Shkolnik, and Jacobs 2021). This
package can be installed from CRAN. It requires an API key, which can be
requested by creating an
\href{https://censusmapper.ca/users/sign_up}{account} and then going to
`edit profile'. The package has a helper function
\texttt{set\_api\_key("ADD\_YOUR\_API\_KEY\_HERE",\ install\ =\ TRUE)}
that makes it easier to add the API key to an `.Renviron' file, in the
same way that we did in Chapter @ref(gather-data).

We can use \texttt{get\_census()} to get census data. We need to specify
a census of interest, and a variety of other arguments. For instance, we
could get data from the 2016 census about Ontario, which is the largest
Canadian province by population.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(cancensus)}

\NormalTok{ontario\_population }\OtherTok{\textless{}{-}} 
  \FunctionTok{get\_census}\NormalTok{(}\AttributeTok{dataset =} \StringTok{"CA16"}\NormalTok{,}
             \AttributeTok{level =} \StringTok{"Regions"}\NormalTok{,}
             \AttributeTok{vectors =} \StringTok{"v\_CA16\_1"}\NormalTok{, }
             \AttributeTok{regions =} \FunctionTok{list}\NormalTok{(}\AttributeTok{PR=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}35\textquotesingle{}}\NormalTok{)}
\NormalTok{                            )}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Downloading: 120 B     
Downloading: 120 B     
Downloading: 120 B     
Downloading: 120 B     
Downloading: 120 B     
Downloading: 120 B     
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ontario\_population}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 0 x 0
\end{verbatim}

Data from the 1996, 2001, 2006, 2011, and 2016 censuses are available,
and \texttt{list\_census\_datasets()} provides the metadata that we need
to provide to \texttt{get\_census()} to access these. Data are available
based on a variety of regions, and \texttt{list\_census\_regions()}
provides the metadata that we need. And finally,
\texttt{list\_census\_vectors()} provides the metadata about the
variables that available.

\hypertarget{usa}{%
\subsection{USA}\label{usa}}

The requirement for a US Census is included in the US Constitution, and
decent, though clunky, access is provided. But the US is in the envious
situation where there is usually a better approach than going through
the national statistical agency of IPUMS. IPUMS provides access to a
wide range of datasets, including international census microdata. In the
specific case of the US, the American Community Survey (ACS) is a survey
that is comparable to the questions asked on many censuses, but it is
that are available on an annual basis, compared with a census which
could be quite out-of-date by the time the data are available. It ends
up with millions of responses each year. Although the ACS is smaller
than a census, the advantage is that it is available on a more timely
basis. We will access the ACS through IPUMS.

Go to \href{https://ipums.org}{IPUMS}, then `IPUMS USA', and select `get
data' (Figure~\ref{fig-ipumsone}).

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/ipums_one.png}

}

\caption{\label{fig-ipumsone}The IPUMS homepage, with IPUMS USA shown in
the top left box}

\end{figure}

We are interested in a sample, so go to `SELECT SAMPLE', and un-select
`Default sample from each year' and instead select `2019 ACS' and then
`SUBMIT SAMPLE SELECTIONS' (Figure~\ref{fig-ipumstwo}).

\begin{figure}

{\centering \includegraphics[width=0.45\textwidth,height=\textheight]{./figures/ipums_two.png}

}

\caption{\label{fig-ipumstwo}Selecting a sample from IPUMS USA and
specifying interest in the 2019 ACS}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/ipums_three.png}

}

\end{figure}

We might be interested in data based on state. So we would begin by
looking at `HOUSEHOLD' variables and selecting `GEOGRAPHIC'
(Figure~\ref{fig-ipumsfour}).

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/ipums_four.png}

}

\caption{\label{fig-ipumsfour}Specifying that we are interested in the
state}

\end{figure}

We add `STATEICP' to our `cart' by clicking the plus, which will then
turn into a tick (Figure~\ref{fig-ipumssix}).

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/ipums_six.png}

}

\caption{\label{fig-ipumssix}Adding STATEICP to our cart}

\end{figure}

We might then be interested in data on a `PERSON' basis, for instance,
`DEMOGRAPHIC' variables such as `AGE', which we should add to our cart.
Still on a `PERSON' basis, we might be interested in `INCOME', for
instance, `Total personal income' `INCTOT' and we could add that to our
cart (Figure~\ref{fig-ipumsseven}).

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/ipums_seven.png}

}

\caption{\label{fig-ipumsseven}Adding additional demographic variables
that are available on an individual basis}

\end{figure}

When we are done, we can `VIEW CART', and then `CREATE DATA EXTRACT'
(Figure~\ref{fig-ipumseight}). At this point there are two aspects that
we likely want to change:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Change the `DATA FORMAT' from dat to csv (Figure~\ref{fig-ipumsnine}).
\item
  Customize the sample size as we likely do not need three million
  responses, and could just change it to, say, 500,000
  (Figure~\ref{fig-ipumsten}).
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/ipums_eight.png}

}

\caption{\label{fig-ipumseight}Beginning the checkout process}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/ipums_nine.png}

}

\caption{\label{fig-ipumsnine}Specifying that we are interested in CSV
files}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/ipums_ten.png}

}

\caption{\label{fig-ipumsten}Reducing the sample size from three million
responses to half a million}

\end{figure}

Finally, we want to include a descriptive name for the extract, for
instance, `2022-02-06: Income based on state and age', which specifies
the date we made the extract and what is in the extract. After that we
can `SUBMIT EXTRACT'.

We will be asked to log in or create an account, and after doing that
will be able to submit the request. IPUMS will email when the extract is
available, after which we can download it and read it into R in the
usual way. It is critical that we cite this dataset when we use it
(Ruggles et al. 2021).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{ipums\_extract }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"usa\_00010.csv"}\NormalTok{)}

\NormalTok{ipums\_extract}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
   YEAR STATEICP   AGE INCTOT
  <dbl>    <dbl> <dbl>  <dbl>
1  2019       41    39   9000
2  2019       41    35   9300
3  2019       41    39  60000
4  2019       41    32  14400
5  2019       41    21      0
6  2019       41    61  11100
\end{verbatim}

Incredibly, full count, that is the entire census, data are available
through IPUMS for the US censuses conducted on: 1850, 1860, 1870, 1880,
1900, 1910, 1920, 1930, and 1940. Most of the 1890 census records were
destroyed due to a fire in 1921. 1 per cent samples are available for
these years, and through to 1990. And then ACS data are available from
2000.

\hypertarget{exercises-and-tutorial-7}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-7}}

\hypertarget{exercises-7}{%
\subsection{Exercises}\label{exercises-7}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Please identify three other sources of data that you are interested in
  and describe where are they available (please include a link or code)?
\item
  Please focus on one of those sources. What steps do you have to go
  through in order to get a dataset that can be analyzed in R?
\item
  Let us say you take a job at RBC (a Canadian bank) and they already
  have some quantitative data for you to use. What are some questions
  that you should explore when deciding whether that data will be useful
  to you?
\item
  Write three points (you are welcome to use dot points) about why
  government data may be especially useful?
\item
  Please pick a government of interest and find their inflation
  statistics. To what extent do you know about how these data were
  gathered?
\item
  With reference to Chen et al. (2019) and Martinez (2019) to what
  extent do you think we can trust government statistics? Please mention
  at least three governments in your answer.
\item
  The 2021 census in Canada asked, firstly, `What was this person's sex
  at birth? Sex refers to sex assigned at birth. Male/Female', and then
  `What is this person's gender? Refers to current gender which may be
  different from sex assigned at birth and may be different from what is
  indicated on legal documents. Male/Female/Or please specify this
  person's gender (space for a typed or handwritten answer)'. With
  reference to Statistics Canada (2020), please discuss the extent to
  which you think this is an appropriate way for census to have
  proceeded. You are welcome to discuss the case of a different country
  if you are more familiar with that.
\item
  Pretend that we have conducted a survey of everyone in Canada, where
  we asked for age, sex, and gender. Your friend claims that there is no
  need to worry about uncertainty `because we have the whole
  population'. Is your friend right or wrong, and why?
\end{enumerate}

\hypertarget{tutorial-7}{%
\subsection{Tutorial}\label{tutorial-7}}

Use IPUMS to access the ACS. Download some data that are of interest and
write a two-to-three page paper analyzing it.

\hypertarget{sec-gather-data}{%
\chapter{Gather data}\label{sec-gather-data}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{Turning History into Data: Data Collection, Measurement,
  and Inference in HPE}, (Cirone and Spirling 2021).
\item
  Read \emph{Two Regimes of Prison Data Collection}, (K. R. Johnson
  2021).
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Initial usage of APIs, both directly, including dealing with
  semi-structured data, and indirectly through R Packages.
\item
  Use R environments to manage keys.
\item
  Web scraping, especially reasonable use and ethical concerns.
\item
  Cleaning data from unstructured data to structured, tidy, data.
\item
  Extracting data from PDFs, both those that are able to be parsed and
  those that are an image and require OCR.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{babynames} (Wickham 2019b)
\item
  \texttt{httr} (Wickham 2019c)
\item
  \texttt{jsonlite} (Ooms 2014)
\item
  \texttt{lubridate} (Grolemund and Wickham 2011)
\item
  \texttt{pdftools} (Ooms 2018a)
\item
  \texttt{purrr} (Henry and Wickham 2020)
\item
  \texttt{rtweet} (Kearney 2019)
\item
  \texttt{rvest} (Wickham 2019d)
\item
  \texttt{spotifyr} (Thompson et al. 2020)
\item
  \texttt{tesseract} (Ooms 2018b)
\item
  \texttt{tidyverse} (Wickham et al. 2019a)
\item
  \texttt{usethis} (Wickham and Bryan 2020)
\item
  \texttt{xml2} (Wickham, Hester, and Ooms 2021)
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{download.file()}
\item
  \texttt{factor()}
\item
  \texttt{function()}
\item
  \texttt{httr::GET()}
\item
  \texttt{pdftools::pdf\_text()}
\item
  \texttt{purrr::walk2()}
\item
  \texttt{rtweet::get\_favorites()}
\item
  \texttt{rtweet::get\_friends()}
\item
  \texttt{rtweet::get\_timelines()}
\item
  \texttt{rtweet::search\_tweets()}
\item
  \texttt{rvest::html\_nodes()}
\item
  \texttt{rvest::html\_text()}
\item
  \texttt{set.seed()}
\item
  \texttt{spotifyr::get\_artist\_audio\_features()}
\item
  \texttt{sys.sleep()}
\item
  \texttt{tesseract::ocr()}
\item
  \texttt{usethis::edit\_r\_environ()}
\end{itemize}

\hypertarget{introduction-6}{%
\section{Introduction}\label{introduction-6}}

In this chapter we first go through a variety of approaches for
gathering data, including the use of APIs and semi-structured data, such
as JSON and XML, web scraping, converting PDFs, and using optical
character recognition, especially to obtain text data.

\hypertarget{apis}{%
\section{APIs}\label{apis}}

In everyday language, and for our purposes, an Application Programming
Interface (API) is a situation in which someone has set up specific
files on their computer such that we can follow their instructions to
get them. For instance, when we use a gif on Slack, Slack asks Giphy's
server for the appropriate gif, Giphy's server gives that gif to Slack
and then Slack inserts it into your chat. The way in which Slack and
Giphy interact is determined by Giphy's API. More strictly, an API is
just an application that runs on a server that we access using the HTTP
protocol.

We focus on using APIs for gathering data. And so, with that focus, an
API is a website that is set-up for another computer to be able to
access, rather than a person. For instance, we could go to Google Maps:
https://www.google.com/maps. And we could then scroll and click and drag
to center the map on Canberra, Australia. Or we could paste this into
the browser: https://www.google.com/maps/@-35.2812958,149.1248113,16z.
We just used the Google Maps API, and the result should be a map similar
to Figure~\ref{fig-focuson2020}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/googlemaps.png}

}

\caption{\label{fig-focuson2020}Example of Google Maps, as at 29 January
2022}

\end{figure}

The advantage of using an API is that the data provider specifies
exactly the data that they are willing to provide, and the terms under
which they will provide it. These terms may include aspects such as rate
limits (i.e.~how often we can ask for data), and what we can do with the
data, for instance, we might not be allowed to use it for commercial
purposes, or to republish it. Additionally, because the API is being
provided specifically for us to use it, it is less likely to be subject
to unexpected changes or legal issues. Because of this it is ethically
and legally clear that when an API is available we should try to use it
rather than web scraping.

We will now go through a few case studies of using APIs. In the first we
deal directly with an API using \texttt{httr} (Wickham 2019c). In the
second we access data from Twitter using \texttt{rtweet} (Kearney 2019).
And in the third we access data from Spotify using \texttt{spotifyr}
(Thompson et al. 2020). Developing comfort with gathering data through
APIs enables access to exciting datasets. For instance, Wong (2020) use
the Facebook Political Ad API to gather all 218,100 of the Trump 2020
campaign ads to better understand the campaign.

\hypertarget{case-study-gathering-data-from-arxiv-nasa-and-dataverse}{%
\subsection{Case study: Gathering data from arXiv, NASA, and
Dataverse}\label{case-study-gathering-data-from-arxiv-nasa-and-dataverse}}

We use \texttt{GET()} from \texttt{httr} (Wickham 2019c) to obtain data
from an API directly. This will try to get some specific data and the
main argument is `url'. In a way, this is very similar to the earlier
Google Maps example. In that example, the specific information that we
were interested in was a map.

In this case study we will use an API provided by arXiv:
https://arxiv.org. arXiv is an online repository for academic papers
before they go through peer-review, and these are typically referred to
as `pre-prints'. After installing and loading \texttt{httr}, we use
\texttt{GET()} to ask arXiv to obtain some information about the
pre-print of R. Alexander and Alexander (2021).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(httr)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
-- Attaching packages --------------------------------------- tidyverse 1.3.1 --
\end{verbatim}

\begin{verbatim}
v ggplot2 3.3.5     v purrr   0.3.4
v tibble  3.1.6     v dplyr   1.0.8
v tidyr   1.2.0     v stringr 1.4.0
v readr   2.1.2     v forcats 0.5.1
\end{verbatim}

\begin{verbatim}
-- Conflicts ------------------------------------------ tidyverse_conflicts() --
x dplyr::filter() masks stats::filter()
x dplyr::lag()    masks stats::lag()
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(xml2)}

\NormalTok{arxiv }\OtherTok{\textless{}{-}}
  \FunctionTok{GET}\NormalTok{(}\StringTok{"http://export.arxiv.org/api/query?id\_list=2111.09299"}\NormalTok{)}

\FunctionTok{status\_code}\NormalTok{(arxiv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 200
\end{verbatim}

We can use \texttt{status\_code()} to check whether we received an error
from the server. And assuming we received something back from the
server, we can use \texttt{content()} to display the information. In
this case we have received XML formatted data, which we can read using
\texttt{read\_xml()} from \texttt{xml2} (Wickham, Hester, and Ooms
2021). XML is a semi-formatted structure, and it can be useful to start
by having a look at it using \texttt{html\_structure()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{content}\NormalTok{(arxiv) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{read\_xml}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{html\_structure}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<feed [xmlns]>
  <link [href, rel, type]>
  <title [type]>
    {text}
  <id>
    {text}
  <updated>
    {text}
  <totalResults [xmlns:opensearch]>
    {text}
  <startIndex [xmlns:opensearch]>
    {text}
  <itemsPerPage [xmlns:opensearch]>
    {text}
  <entry>
    <id>
      {text}
    <updated>
      {text}
    <published>
      {text}
    <title>
      {text}
    <summary>
      {text}
    <author>
      <name>
        {text}
    <author>
      <name>
        {text}
    <comment [xmlns:arxiv]>
      {text}
    <link [href, rel, type]>
    <link [title, href, rel, type]>
    <primary_category [term, scheme, xmlns:arxiv]>
    <category [term, scheme]>
\end{verbatim}

Or we might be interested to create a dataset based on extracting
various aspects of this XML tree. For instance, we might be interested
to look at the `entry', which is the eighth item, and in particular to
obtain the title and the URL, which are the fourth and ninth items,
respectively, within entry.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_from\_arxiv }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{title =} \FunctionTok{content}\NormalTok{(arxiv) }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{read\_xml}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{xml\_child}\NormalTok{(}\AttributeTok{search =} \DecValTok{8}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{xml\_child}\NormalTok{(}\AttributeTok{search =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{xml\_text}\NormalTok{(),}
    \AttributeTok{link =} \FunctionTok{content}\NormalTok{(arxiv) }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{read\_xml}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{xml\_child}\NormalTok{(}\AttributeTok{search =} \DecValTok{8}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{xml\_child}\NormalTok{(}\AttributeTok{search =} \DecValTok{9}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{xml\_attr}\NormalTok{(}\StringTok{"href"}\NormalTok{)}
\NormalTok{  )}
\NormalTok{data\_from\_arxiv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  title                                                                    link 
  <chr>                                                                    <chr>
1 "The Increased Effect of Elections and Changing Prime Ministers on Topi~ http~
\end{verbatim}

Each day NASA provides the Astronomy Picture of the Day (APOD) through
its APOD API. We can again use \texttt{GET()} to obtain the URL for the
photo on particular dates and then display it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NASA\_APOD\_20211226 }\OtherTok{\textless{}{-}}
  \FunctionTok{GET}\NormalTok{(}\StringTok{"https://api.nasa.gov/planetary/apod?api\_key=DEMO\_KEY\&date=2021{-}12{-}26"}\NormalTok{)}

\NormalTok{NASA\_APOD\_20190719 }\OtherTok{\textless{}{-}}
  \FunctionTok{GET}\NormalTok{(}\StringTok{"https://api.nasa.gov/planetary/apod?api\_key=DEMO\_KEY\&date=2019{-}07{-}19"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Examining the returned data using \texttt{content()}, we can see that we
are provided with various fields, such as date, title, explanation, and
a URL. And we can provide that URL to \texttt{include\_graphics()} from
\texttt{knitr} to display it (Figure~\ref{fig-nasaone})).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{content}\NormalTok{(NASA\_APOD\_20211226)}\SpecialCharTok{$}\NormalTok{date}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "2021-12-26"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{content}\NormalTok{(NASA\_APOD\_20211226)}\SpecialCharTok{$}\NormalTok{title}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "James Webb Space Telescope over Earth"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{content}\NormalTok{(NASA\_APOD\_20211226)}\SpecialCharTok{$}\NormalTok{explanation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "There's a big new telescope in space. This one, the James Webb Space Telescope (JWST), not only has a mirror over five times larger than Hubble's in area, but can see better in infrared light. The featured picture shows JWST high above the Earth just after being released by the upper stage of an Ariane V rocket, launched yesterday from French Guiana. Over the next month, JWST will move out near the Sun-Earth L2 point where it will co-orbit the Sun with the Earth. During this time and for the next five months, JWST will unravel its segmented mirror and an array of sophisticated scientific instruments -- and test them. If all goes well, JWST will start examining galaxies across the universe and planets orbiting stars across our Milky Way Galaxy in the summer of 2022.   APOD Gallery: Webb Space Telescope Launch"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{content}\NormalTok{(NASA\_APOD\_20211226)}\SpecialCharTok{$}\NormalTok{url}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "https://apod.nasa.gov/apod/image/2112/JwstLaunch_Arianespace_1080.jpg"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{content}\NormalTok{(NASA\_APOD\_20190719)}\SpecialCharTok{$}\NormalTok{date}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "2019-07-19"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{content}\NormalTok{(NASA\_APOD\_20190719)}\SpecialCharTok{$}\NormalTok{title}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Tranquility Base Panorama"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{content}\NormalTok{(NASA\_APOD\_20190719)}\SpecialCharTok{$}\NormalTok{explanation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "On July 20, 1969 the Apollo 11 lunar module Eagle safely touched down on the Moon. It landed near the southwestern corner of the Moon's Mare Tranquillitatis at a landing site dubbed Tranquility Base. This panoramic view of Tranquility Base was constructed from the historic photos taken from the lunar surface. On the far left astronaut Neil Armstrong casts a long shadow with Sun is at his back and the Eagle resting about 60 meters away ( AS11-40-5961). He stands near the rim of 30 meter-diameter Little West crater seen here to the right ( AS11-40-5954). Also visible in the foreground is the top of the camera intended for taking stereo close-ups of the lunar surface."
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{content}\NormalTok{(NASA\_APOD\_20190719)}\SpecialCharTok{$}\NormalTok{url}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "https://apod.nasa.gov/apod/image/1907/apollo11TranquilitybasePan600h.jpg"
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{./figures/JwstLaunch_Arianespace_1080.jpg}

}

\caption{\label{fig-nasaone}Photo of the James Webb Space Telescope over
Earth and another of Tranquility Base obtained from the NASA APOD API}

\end{figure}

Finally, another common API response in semi-structured form is JSON. We
can parse JSON with \texttt{jsonlite} (Ooms 2014). A Dataverse is a web
application that makes it easier to share dataset. We can use an API go
query a demonstration dataverse. For instance we might be interested in
datasets related to politics.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(jsonlite)}

\NormalTok{politics\_datasets }\OtherTok{\textless{}{-}} \FunctionTok{fromJSON}\NormalTok{(}\StringTok{"https://demo.dataverse.org/api/search?q=politics"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can also look at the dataset using \texttt{View(politics\_datasets)},
which allows us to expand the tree based on what we are interested in
and even get the code that we need to focus on different aspects by
hovering on the item and then clicking the icon with the green arrow
(Figure~\ref{fig-jsonfirst})).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/jsonlite.png}

}

\caption{\label{fig-jsonfirst}Example of hovering over an JSON element,
`items', where the icon with a green arrow can be clicked on to get the
code that would focus on that element}

\end{figure}

This tells us how to obtain the dataset of interest.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as\_tibble}\NormalTok{(politics\_datasets[[}\StringTok{"data"}\NormalTok{]][[}\StringTok{"items"}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 6
  name                    type      url      identifier description published_at
  <chr>                   <chr>     <chr>    <chr>      <chr>       <chr>       
1 China Archive Dataverse dataverse https:/~ china-arc~ Introducti~ 2016-12-09T~
\end{verbatim}

\hypertarget{case-study-gathering-data-from-twitter}{%
\subsection{Case study: Gathering data from
Twitter}\label{case-study-gathering-data-from-twitter}}

Twitter is a rich source of text and other data. The Twitter API is the
way in which Twitter asks that we gather these data. And \texttt{rtweet}
(Kearney 2019) is built around this API and allows us to interact with
it in ways that are similar to using any other R package. Initially, we
can use the Twitter API with just a regular Twitter account.

Begin by installing and loading \texttt{rtweet} and \texttt{tidyverse}.
We then need to authorize \texttt{rtweet} and we start that process by
calling a function from the package, for instance
\texttt{get\_favorites()} which will return a tibble of a user's
favorites. This will open a browser, and we then log into a regular
Twitter account (Figure~\ref{fig-rtweetlogin})).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rtweet)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_favorites}\NormalTok{(}\AttributeTok{user =} \StringTok{"RohanAlexander"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/rtweet.png}

}

\caption{\label{fig-rtweetlogin}rtweet authorisation page}

\end{figure}

Once the application is authorized, then we can use
\texttt{get\_favorites()} to actually get the favorites of a user and
save them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rohans\_favorites }\OtherTok{\textless{}{-}} \FunctionTok{get\_favorites}\NormalTok{(}\StringTok{"RohanAlexander"}\NormalTok{)}

\FunctionTok{saveRDS}\NormalTok{(rohans\_favorites, }\StringTok{"rohans\_favorites.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We could then look at some recent favorites, keeping in mind that they
may be different depending on when they are being accessed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rohans\_favorites }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(created\_at)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(screen\_name, text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 2
   screen_name text                                                             
   <chr>       <chr>                                                            
 1 EconAndrew  "How much better are the investment opportunities available to t~
 2 simonpcouch "There's a new release of #rstats broom up on CRAN as of last ni~
 3 MineDogucu  "🚨 New manuscript🚨\n📕 Content and Computing Outline of Two Un~
 4 reid_nancy  "Latest issue. From the intro: \"... it has been a great privile~
 5 tjmahr      "bathing is good, folks"                                         
 6 andrewheiss "finished hand washing that load in the bathtub and am now the w~
 7 monkmanmh   "@CMastication https://t.co/3Eh0mLy44v"                          
 8 eplusgg     "Stares from Ontario https://t.co/swzYhaptF9"                    
 9 ryancbriggs "Same. https://t.co/C9pNpXO0F9"                                  
10 flynnpolsci "I’m not great at coming up with assignments for intro courses b~
\end{verbatim}

We can use \texttt{search\_tweets()} to search for tweets about a
particular topic. For instance, we could look at tweets using a hashtag
commonly associated with R: `\#rstats'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rstats\_tweets }\OtherTok{\textless{}{-}} \FunctionTok{search\_tweets}\NormalTok{(}
  \AttributeTok{q =} \StringTok{"\#rstats"}\NormalTok{,}
  \AttributeTok{include\_rts =} \ConstantTok{FALSE}
\NormalTok{)}

\FunctionTok{saveRDS}\NormalTok{(rstats\_tweets, }\StringTok{"rstats\_tweets.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rstats\_tweets }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(screen\_name, text) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 2
  screen_name     text                                                          
  <chr>           <chr>                                                         
1 SuccessAnalytiX "The Science of Success \n\nhttps://t.co/xLM2OrqHBd\n\n#BigDa~
2 babycoin_dev    "BabyCoin (BABY)\n\nGUI wallet v2.05 =&gt; https://t.co/CFNtp~
3 rstatsdata      "#rdata #rstats: Yield of 6 barley varieties at 18 locations ~
4 PDH_SciTechNews "#Coding Arm Puts Security Architecture to the Test With New ~
5 PDH_SciTechNews "#Coding Network Engineer: Skills, Roles &amp; Responsibiliti~
6 PDH_SciTechNews "#Coding CockroachDB Strengthens Change Data Capture - iProgr~
\end{verbatim}

Other useful functions that can be used include \texttt{get\_friends()}
to get all the accounts that a user follows, and
\texttt{get\_timelines()} to get a user's recent tweets. Registering as
a developer enables access to more API functionality.

When using APIs, even when they are wrapped in an R package, in this
case \texttt{rtweet}, it is important to read the terms under which
access is provided. The Twitter API docs are surprisingly readable, and
the developer policy is especially clear:
https://developer.twitter.com/en/developer-terms/policy. To see how easy
it is to violate the terms under which an API provider makes data
available, consider that we saved the tweets that we downloaded. If we
were to push these to GitHub, then it is possible that we may have
accidentally stored sensitive information if there happened to be some
in the tweets. Twitter is also explicit about asking those that use
their API to be especially careful about sensitive information and not
matching Twitter users with off-Twitter folks. Again, the documentation
around these restricted uses is clear and readable:
https://developer.twitter.com/en/developer-terms/more-on-restricted-use-cases.

\hypertarget{case-study-gathering-data-from-spotify}{%
\subsection{Case study: Gathering data from
Spotify}\label{case-study-gathering-data-from-spotify}}

For the final case study, we will use \texttt{spotifyr} (Thompson et al.
2020), which is a wrapper around the Spotify API. Install
\texttt{install.packages(\textquotesingle{}spotifyr\textquotesingle{})}
and load the package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(spotifyr)}
\end{Highlighting}
\end{Shaded}

To access the Spotify API, we need a Spotify Developer Account:
https://developer.spotify.com/dashboard/. This will require logging in
with a Spotify account and then accepting the Developer Terms
(Figure~\ref{fig-spotifyaccept}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/spotify.png}

}

\caption{\label{fig-spotifyaccept}Spotify Developer Account Terms
agreement page}

\end{figure}

Continuing with the registration process, in our case, we `do not know'
what we are building and so Spotify requires us to use a non-commercial
agreement. To use the Spotify API we need a `Client ID' and a `Client
Secret'. These are things that we want to keep to ourselves because
anyone with the details could use our developer account as though they
were us. One way to keep these details secret with a minimum of hassle
is to keep them in our `System Environment'. In this way, when we push
to GitHub they should not be included. (We followed this process without
explanation in Chapter @ref(interactive-communication) when we used
\texttt{mapdeck}.) We will use \texttt{usethis} (Wickham and Bryan 2020)
to modify our System Environment. In particular, there is a file called
`.Renviron' which we will open using \texttt{edit\_r\_environ()} and add
our `Client ID' and `Client Secret' to.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(usethis)}

\FunctionTok{edit\_r\_environ}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

When we run \texttt{edit\_r\_environ()}, our `.Renviron' file will open
and we can add our `Spotify Client ID' and `Client Secret'. It is
important to use the same names, because \texttt{spotifyr} will look in
our environment for keys with those specific names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SPOTIFY\_CLIENT\_ID }\OtherTok{=} \StringTok{\textquotesingle{}PUT\_YOUR\_CLIENT\_ID\_HERE\textquotesingle{}}
\NormalTok{SPOTIFY\_CLIENT\_SECRET }\OtherTok{=} \StringTok{\textquotesingle{}PUT\_YOUR\_SECRET\_HERE\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

Save the `.Renviron' file, and then restart R (`Session' -\textgreater{}
`Restart R'). We can now use our `Spotify Client ID' and `Client Secret'
as needed. And functions that require those details as arguments will
work without them being explicitly specified again. We will get and save
some information about Radiohead, the English rock band, using
\texttt{get\_artist\_audio\_features()}. One of the required arguments
is \texttt{authorization}, but as that is set, by default, to look at
the `.Renviron' file, we do not need to specify it here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{radiohead }\OtherTok{\textless{}{-}} \FunctionTok{get\_artist\_audio\_features}\NormalTok{(}\StringTok{\textquotesingle{}radiohead\textquotesingle{}}\NormalTok{)}
\FunctionTok{saveRDS}\NormalTok{(radiohead, }\StringTok{"radiohead.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{radiohead }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"radiohead.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There is a variety of information available based on songs. We might be
interested to see whether their songs are getting longer over time
(Figure~\ref{fig-readioovertime})).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{radiohead }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(artist\_name, track\_name, album\_name) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  artist_name                    track_name   album_name
1   Radiohead Everything In Its Right Place KID A MNESIA
2   Radiohead                         Kid A KID A MNESIA
3   Radiohead           The National Anthem KID A MNESIA
4   Radiohead   How to Disappear Completely KID A MNESIA
5   Radiohead                   Treefingers KID A MNESIA
6   Radiohead                    Optimistic KID A MNESIA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lubridate)}

\NormalTok{radiohead }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{album\_release\_date =} \FunctionTok{ymd}\NormalTok{(album\_release\_date)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ album\_release\_date, }\AttributeTok{y =}\NormalTok{ duration\_ms)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Album release date"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Duration of song (ms)"}
\NormalTok{       ) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./09-gather_files/figure-pdf/fig-readioovertime-1.pdf}

}

\caption{\label{fig-readioovertime}Length of each Radiohead song, over
time, as gathered from Spotify}

\end{figure}

One interesting variable provided by Spotify about each song is
`valence'. The Spotify documentation describe this as a measure between
0 and 1 that signals the `the musical positiveness' of the track with
higher values being more positive. Further details are available at the
documentation:
https://developer.spotify.com/documentation/web-api/reference/\#/operations/get-audio-features.
We might be interested to compare valence over time between a few
artists, for instance, the American rock band The National, and the
American singer Taylor Swift.

First, we need to gather the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{taylor\_swift }\OtherTok{\textless{}{-}} \FunctionTok{get\_artist\_audio\_features}\NormalTok{(}\StringTok{\textquotesingle{}taylor swift\textquotesingle{}}\NormalTok{)}
\NormalTok{the\_national }\OtherTok{\textless{}{-}} \FunctionTok{get\_artist\_audio\_features}\NormalTok{(}\StringTok{\textquotesingle{}the national\textquotesingle{}}\NormalTok{)}

\FunctionTok{saveRDS}\NormalTok{(taylor\_swift, }\StringTok{"taylor\_swift.rds"}\NormalTok{)}
\FunctionTok{saveRDS}\NormalTok{(the\_national, }\StringTok{"the\_national.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then we can bring them together and make the graph
(Figure~\ref{fig-swiftyvsnationalvsradiohead})). This appears to show
that while Taylor Swift and Radiohead have largely maintained their
level of valence overtime, The National has decreased theirs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{three\_artists }\OtherTok{\textless{}{-}}
  \FunctionTok{rbind}\NormalTok{(taylor\_swift, the\_national, radiohead) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(artist\_name, album\_release\_date, valence) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{album\_release\_date =} \FunctionTok{ymd}\NormalTok{(album\_release\_date))}

\NormalTok{three\_artists }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ album\_release\_date,}
             \AttributeTok{y =}\NormalTok{ valence,}
             \AttributeTok{color =}\NormalTok{ artist\_name)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Album release date"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Valence"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Artist"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./09-gather_files/figure-pdf/fig-swiftyvsnationalvsradiohead-1.pdf}

}

\caption{\label{fig-swiftyvsnationalvsradiohead}Comparing valence, over
time, for Radiohead, Taylor Swift, and The National}

\end{figure}

How amazing that we live in a world that all that information is
available with very little effort or cost. And having gathered the data,
there is a lot that could be done. For instance, Pavlik (2019) uses an
expanded dataset to classify musical genres and The Economist (2022)
looks at how language is associated with music streaming on Spotify. Our
ability to gather such data enables us to answer questions that had to
be considered experimentally in the past, for instance M. J. Salganik,
Dodds, and Watts (2006) had to use experimental data rather than the
real data we are able to access. But at the same time, it is worth
thinking about what valence is purporting to represent. Little
information is available in the Spotify documentation about how this is
being created. And it is doubtful that one number can completely
represent how positive a song is.

\hypertarget{web-scraping}{%
\section{Web scraping}\label{web-scraping}}

Web scraping is a way to get data from websites. Rather than going to a
website using a browser the copy and pasting, we write code that does it
for us. This opens a lot of data to us, but on the other hand, it is not
typically data that is being made available for these purposes. This
means that it is important to be respectful of it. While generally not
illegal, the specifics about the legality of web scraping depend on
jurisdictions and the specifics of what we are doing, and so it is also
important to be mindful of this. While our use would rarely be
commercially competitive, of particular concern is the conflict between
the need for our work to be reproducible with the need to respect terms
of service that may disallow data republishing (Luscombe, Dick, and
Walby 2021). And finally, web scraping imposes a cost on the website
host, and so it is important to reduce this to the extent possible.

That all said, web scraping is an invaluable source of data. But they
are typically datasets that can be created as a by-product of someone
trying to achieve another aim. For instance, a retailer may have a
website with their products and their prices. That has not been created
deliberately as a source of data, but we can scrape it to create a
dataset. As such, the following principles are useful to guide web
scraping.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Avoid it. Try to use an API wherever possible.
\item
  Abide by their desires. Some websites have a `robots.txt' file that
  contains information about what they are comfortable with scrapers
  doing, for instance `https://www.google.com/robots.txt'.
\item
  Reduce the impact.

  \begin{itemize}
  \tightlist
  \item
    Firstly, slow down the scraper, for instance, rather than having it
    visit the website every second, slow it down using
    \texttt{sys.sleep()}. If we only need a few hundred files, then why
    not just have it visit the website a few times a minute, running in
    the background overnight?
  \item
    Secondly, consider the timing of when we run our scraper. For
    instance, if we are scraping a retailer then maybe we should set our
    script to run from 10pm through to the morning, when fewer customers
    are likely using the site. Similarly, if it is a government website
    and they have a big monthly release, then it might be polite to
    avoid that day.
  \end{itemize}
\item
  Take only what is needed. For instance, we do not need to scrape the
  entire of Wikipedia if all we need is the names of the ten largest
  cities in Croatia. This reduces the impact on the website, and allows
  us to more easily justify our actions.
\item
  Only scrape once. This means we should save everything as we go so
  that we do not have to re-collect data. Similarly, once we have the
  data, we should keep that separate and not modify it. Of course, if we
  need data over time then we will need to go back, but this is
  different to needlessly re-scraping a page.
\item
  Do not republish the pages that were scraped. (This contrasts with
  datasets that we create from it.)
\item
  Take ownership and ask permission if possible. At a minimum level all
  scripts should have our contact details in them. Depending on the
  circumstances, it may be worthwhile asking for permission before you
  scrape.
\end{enumerate}

Web scraping is possible by taking advantage of the underlying structure
of a webpage. We use patterns in the HTML/CSS to get the data that we
want. To look at the underlying HTML/CSS we can either:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  open a browser, right-click, and choose something like `Inspect'; or
\item
  save the website and then open it with a text editor rather than a
  browser.
\end{enumerate}

HTML/CSS is a markup language comprised of matching tags. If we want
text to be bold, then we would use something like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}b}\OperatorTok{\textgreater{}}\NormalTok{My bold text\textless{}/b}\OperatorTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

Similarly, if we want a list then we start and end the list, as well as
each item.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}ul}\OperatorTok{\textgreater{}}
\NormalTok{  \textless{}li}\OperatorTok{\textgreater{}}\NormalTok{Learn webscraping\textless{}/li}\OperatorTok{\textgreater{}}
\NormalTok{  \textless{}li}\OperatorTok{\textgreater{}}\NormalTok{Do data science\textless{}/li}\OperatorTok{\textgreater{}}
\NormalTok{  \textless{}li}\OperatorTok{\textgreater{}}\NormalTok{Proft\textless{}/li}\OperatorTok{\textgreater{}}
\NormalTok{\textless{}/ul}\OperatorTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

When scraping we will search for these tags.

To get started, we can pretend that we obtained some HTML from a
website, and that we want to get the name from it. We can see that the
name is in bold, so we want to focus on that feature and extract it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{website\_extract }\OtherTok{\textless{}{-}} \StringTok{"\textless{}p\textgreater{}Hi, I’m \textless{}b\textgreater{}Rohan\textless{}/b\textgreater{} Alexander.\textless{}/p\textgreater{}"}
\CommentTok{\#| echo: true}
\end{Highlighting}
\end{Shaded}

We will use \texttt{read\_html()} from \texttt{rvest} (Wickham 2019d) to
read in the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("rvest")}
\CommentTok{\#| echo: true}
\FunctionTok{library}\NormalTok{(rvest)}

\NormalTok{rohans\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(website\_extract)}

\NormalTok{rohans\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{html_document}
<html>
[1] <body><p>Hi, I’m <b>Rohan</b> Alexander.</p></body>
\end{verbatim}

The language used by \texttt{rvest} to look for tags is `node', so we
focus on bold nodes. By default \texttt{html\_nodes()} returns the tags
as well. We can focus on the text that they contain, with
\texttt{html\_text()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rohans\_data }\SpecialCharTok{|\textgreater{}} 
\CommentTok{\#| echo: true}
  \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"b"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{xml_nodeset (1)}
[1] <b>Rohan</b>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first\_name }\OtherTok{\textless{}{-}} 
\NormalTok{  rohans\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"b"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{html\_text}\NormalTok{()}

\NormalTok{first\_name}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Rohan"
\end{verbatim}

\hypertarget{case-study-web-scraping-book-information}{%
\subsection{Case study: Web scraping book
information}\label{case-study-web-scraping-book-information}}

In this case study we will scrape a list of books from:
https://rohanalexander.com/bookshelf.html. We will then clean the data
and look at the distribution of the first letters of author surnames. It
is slightly more complicated than the example above, but the underlying
approach is the same: download the website, look for the nodes of
interest, extract the information, clean it.

We use \texttt{rvest} (Wickham 2019d) to download a website, and to then
navigate the html to find the aspects that we are interested in. And we
use \texttt{tidyverse} to clean the dataset. We first need to go to the
website and then save a local copy.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rvest)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(xml2)}

\NormalTok{books\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(}\StringTok{"https://rohanalexander.com/bookshelf.html"}\NormalTok{)}

\FunctionTok{write\_html}\NormalTok{(books\_data, }\StringTok{"raw\_data.html"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Now we need to navigate the HTML to get the aspects that we want, and to
then put them into some sensible structure. We will start with trying to
get the data into a tibble as quickly as possible because this will
allow us to more easily use \texttt{dplyr} verbs and \texttt{tidyverse}
functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{books\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(}\StringTok{"inputs/my\_website/raw\_data.html"}\NormalTok{)}
\CommentTok{\#| echo: true}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{books\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{html_document}
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
[1] <head>\n<meta http-equiv="Content-Type" content="text/html; charset=UTF-8 ...
[2] <body>\n\n<!--radix_placeholder_front_matter-->\n\n<script id="distill-fr ...
\end{verbatim}

To get the data into a tibble we first need to identify the data that we
are interested in using html tags. If we look at the website then we
need to focus on list items (Figure~\ref{fig-rohansbooks}). And we can
look at the source, focusing particularly on looking for a list
(Figure~\ref{fig-rohanssourceone}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/rohansbooks.png}

}

\caption{\label{fig-rohansbooks}Books website as displayed}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/sourcetop.png}

}

\caption{\label{fig-rohanssourceone}HTML for the top of the books
website and the list of books}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/sourcelist.png}

}

\end{figure}

The tag for a list item is `li', so we can use that to focus on the
list.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  books\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"li"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{html\_text}\NormalTok{()}

\NormalTok{all\_books }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{books =}\NormalTok{ text\_data)}

\FunctionTok{head}\NormalTok{(all\_books)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 1
  books                     
  <chr>                     
1 Academic                  
2 Non-fiction               
3 Fiction                   
4 Cookbooks                 
5 Want to buy               
6 Best books that I read in:
\end{verbatim}

We now need to clean the data. First we want to separate the title and
the author using \texttt{separate()} and then clean up the author and
title columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_books }\OtherTok{\textless{}{-}}
\NormalTok{  all\_books }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{7}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(all\_books)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{separate}\NormalTok{(books, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"author"}\NormalTok{, }\StringTok{"title"}\NormalTok{), }\AttributeTok{sep =} \StringTok{", ‘"}\NormalTok{)}

\NormalTok{all\_books }\OtherTok{\textless{}{-}}
\NormalTok{  all\_books }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{separate}\NormalTok{(title, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"title"}\NormalTok{, }\StringTok{"debris"}\NormalTok{), }\AttributeTok{sep =} \StringTok{"’."}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{debris) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{author =} \FunctionTok{str\_remove\_all}\NormalTok{(author, }\StringTok{"\^{}, "}\NormalTok{),}
         \AttributeTok{author =} \FunctionTok{str\_squish}\NormalTok{(author),}
         \AttributeTok{title =} \FunctionTok{str\_remove}\NormalTok{(title, }\StringTok{"“"}\NormalTok{),}
         \AttributeTok{title =} \FunctionTok{str\_remove}\NormalTok{(title, }\StringTok{"\^{}{-}"}\NormalTok{)}
\NormalTok{         )}

\FunctionTok{head}\NormalTok{(all\_books)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 2
  author                               title                                    
  <chr>                                <chr>                                    
1 Bryant, John, and Junni L. Zhang     Bayesian Demographic Estimation and Fore~
2 Chan, Ngai Hang                      Time Series                              
3 Clark, Greg                          The Son Also Rises                       
4 Duflo, Esther                        Expérience, science et lutte contre la p~
5 Foster, Ghani, Jarmin, Kreuter, Lane Big Data and Social Science              
6 Francois Chollet with JJ Allaire     Deep Learning with R                     
\end{verbatim}

\#\textbar{} echo: true

There are some at the end that we need to get rid of because they are
from a `best of'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_books }\OtherTok{\textless{}{-}} 
\NormalTok{  all\_books }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{142}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(author }\SpecialCharTok{!=} \StringTok{"‘150 Years of Stats Canada!’."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finally, we could make a table of the distribution of the first letter
of the names (Table~\ref{tbl-lettersofbooks}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_books }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{first\_letter =} \FunctionTok{str\_sub}\NormalTok{(author, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(first\_letter) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"First letter"}\NormalTok{, }\StringTok{"Number of times"}\NormalTok{),}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }
    \AttributeTok{linesep =} \StringTok{""}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-lettersofbooks}{}
\begin{longtable}[]{@{}lr@{}}
\caption{\label{tbl-lettersofbooks}Distribution of first letter of
author names in a collection of books}\tabularnewline
\toprule()
First letter & Number of times \\
\midrule()
\endfirsthead
\toprule()
First letter & Number of times \\
\midrule()
\endhead
⭐ & 12 \\
A & 6 \\
B & 8 \\
C & 13 \\
D & 7 \\
E & 5 \\
F & 6 \\
G & 13 \\
H & 6 \\
I & 3 \\
J & 1 \\
K & 3 \\
l & 1 \\
L & 4 \\
M & 8 \\
N & 2 \\
O & 4 \\
P & 7 \\
R & 3 \\
S & 12 \\
T & 6 \\
W & 9 \\
Y & 1 \\
Z & 1 \\
\bottomrule()
\end{longtable}

\hypertarget{case-study-web-scraping-uk-prime-ministers-from-wikipedia}{%
\subsection{Case study: Web scraping UK Prime Ministers from
Wikipedia}\label{case-study-web-scraping-uk-prime-ministers-from-wikipedia}}

In this case study we are interested in how long UK prime ministers
lived, based on the year that they were born. We will scrape data from
Wikipedia using \texttt{rvest} (Wickham 2019d), clean it, and then make
a graph. Every time we scrape a website things will change. Each scrape
will largely be bespoke, even if we can borrow some code from earlier
projects. It is completely normal to feel frustrated at times. It helps
to begin with an end in mind.

To that end, we can start by generating some simulated data. Ideally, we
want a table that has a row for each prime minister, a column for their
name, and a column each for the birth and death years. If they are still
alive, then that death year can be empty. We know that birth and death
years should be somewhere between 1700 and 1990, and that death year
should be larger than birth year. Finally, we also know that the years
should be integers, and the names should be characters. So, we want
something that looks roughly like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(babynames)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{simulated\_dataset }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{prime\_minister =} \FunctionTok{sample}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ babynames }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(prop }\SpecialCharTok{\textgreater{}} \FloatTok{0.01}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
        \FunctionTok{select}\NormalTok{(name) }\SpecialCharTok{|\textgreater{}} \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \FunctionTok{unlist}\NormalTok{(),}
      \AttributeTok{size =} \DecValTok{10}\NormalTok{,}
      \AttributeTok{replace =} \ConstantTok{FALSE}
\NormalTok{    ),}
    \AttributeTok{birth\_year =} \FunctionTok{sample}\NormalTok{(}
      \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1700}\SpecialCharTok{:}\DecValTok{1990}\NormalTok{),}
      \AttributeTok{size =} \DecValTok{10}\NormalTok{,}
      \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{    ),}
    \AttributeTok{years\_lived =} \FunctionTok{sample}\NormalTok{(}
      \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{50}\SpecialCharTok{:}\DecValTok{100}\NormalTok{),}
      \AttributeTok{size =} \DecValTok{10}\NormalTok{,}
      \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{    ),}
    \AttributeTok{death\_year =}\NormalTok{ birth\_year }\SpecialCharTok{+}\NormalTok{ years\_lived}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(prime\_minister, birth\_year, death\_year, years\_lived) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(birth\_year)}
\end{Highlighting}
\end{Shaded}

One of the advantages of generating a simulated dataset is that if we
are working in groups then one person can start making the graph, using
the simulated dataset, while the other person gathers the data. In terms
of a graph, we are aiming for something like
Figure~\ref{fig-pmsgraphexample}.

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/pms_graph_plan.png}

}

\caption{\label{fig-pmsgraphexample}Sketch of planned graph showing how
long UK prime ministers lived}

\end{figure}

We are starting with a question that is of interest, which how long each
UK prime minister lived. As such, we need to identify a source of data
While there are plenty of data sources that have the births and deaths
of each prime minister, we want one that we can trust, and as we are
going to be scraping, we want one that has some structure to it. The
Wikipedia page about UK prime ministers fits both these criteria:
https://en.wikipedia.org/wiki/List\_of\_prime\_ministers\_of\_the\_United\_Kingdom.
As it is a popular page the information is more likely to be correct,
and the data are available in a table.

We load \texttt{rvest} and then download the page using
\texttt{read\_html()}. Saving it locally provides us with a copy that we
need for reproducibility in case the website changes, and also means
that we do not have to keep visiting the website. But it is likely not
our property, and so this is typically not something that should be
necessarily redistributed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rvest)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw\_data }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_html}\NormalTok{(}\StringTok{"https://en.wikipedia.org/wiki/List\_of\_prime\_ministers\_of\_the\_United\_Kingdom"}\NormalTok{)}
\FunctionTok{write\_html}\NormalTok{(raw\_data, }\StringTok{"pms.html"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As with the earlier case study we are looking for patterns in the HTML
that we can use to help us get closer to the data that we want. This is
an iterative process and requires a lot of trial and error. Even simple
examples will take time.

One tool that may help is the SelectorGadget:
https://rvest.tidyverse.org/articles/articles/selectorgadget.html. This
allows us to pick and choose the elements that we want, and then gives
us the input to give to \texttt{html\_nodes()}
(Figure~\ref{fig-selectorgadget}))

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/uk_pms.png}

}

\caption{\label{fig-selectorgadget}Using the Selector Gadget to identify
the tag, as at 13 March 2020.}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in our saved data}
\NormalTok{raw\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(}\StringTok{"pms.html"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We can parse tags in order}
\NormalTok{parse\_data\_selector\_gadget }\OtherTok{\textless{}{-}} 
\NormalTok{  raw\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"td:nth{-}child(3)"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_text}\NormalTok{()}

\FunctionTok{head}\NormalTok{(parse\_data\_selector\_gadget)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "\nSir Robert Walpole(1676–1745)\n"                       
[2] "\nSpencer Compton1st Earl of Wilmington(1673–1743)\n"    
[3] "\nHenry Pelham(1694–1754)\n"                             
[4] "\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\n"
[5] "\nWilliam Cavendish4th Duke of Devonshire(1720–1764)\n"  
[6] "\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\n"
\end{verbatim}

In this case there is are a few blank lines that we will need to filter
away.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parsed\_data }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{raw\_text =}\NormalTok{ parse\_data\_selector\_gadget) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(raw\_text }\SpecialCharTok{!=} \StringTok{"—}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}
    \SpecialCharTok{!}\NormalTok{raw\_text }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}
      \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{1868}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
      \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{1874}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
      \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{1880}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
      \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{1885}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
      \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{1892}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
      \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{1979}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
      \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{1997}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
      \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{2010}\SpecialCharTok{\textbackslash{}n}\StringTok{"}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}
    \SpecialCharTok{!}\NormalTok{raw\_text }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}
      \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{National Labour}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
      \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{William Pulteney1st Earl of Bath(1684–1764)}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
      \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{James Waldegrave2nd Earl Waldegrave(1715–1763)}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,}
      \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{Edward VII}\SpecialCharTok{\textbackslash{}n\textbackslash{}n\textbackslash{}n}\StringTok{1901–1910}\SpecialCharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}\NormalTok{, }
      \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{George V}\SpecialCharTok{\textbackslash{}n\textbackslash{}n\textbackslash{}n}\StringTok{1910–1936}\SpecialCharTok{\textbackslash{}n\textbackslash{}n}\StringTok{"}
\NormalTok{    )}
\NormalTok{  )}

\FunctionTok{head}\NormalTok{(parsed\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 1
  raw_text                                                  
  <chr>                                                     
1 "\nSir Robert Walpole(1676–1745)\n"                       
2 "\nSpencer Compton1st Earl of Wilmington(1673–1743)\n"    
3 "\nHenry Pelham(1694–1754)\n"                             
4 "\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\n"
5 "\nWilliam Cavendish4th Duke of Devonshire(1720–1764)\n"  
6 "\nThomas Pelham-Holles1st Duke of Newcastle(1693–1768)\n"
\end{verbatim}

Now that we have the parsed data, we need to clean it to match what we
wanted. In particular we want a names column, as well as columns for
birth year and death year. We will use \texttt{separate()} to take
advantage of the fact that it looks like the dates are distinguished by
brackets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial\_clean }\OtherTok{\textless{}{-}} 
\NormalTok{  parsed\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{raw\_text =} \FunctionTok{str\_remove\_all}\NormalTok{(raw\_text, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{separate}\NormalTok{(raw\_text, }
            \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"Name"}\NormalTok{, }\StringTok{"not\_name"}\NormalTok{), }
            \AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{("}\NormalTok{,}
            \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# The remove = FALSE option here means that we }
  \CommentTok{\# keep the original column that we are separating.}
  \FunctionTok{separate}\NormalTok{(not\_name, }
            \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"Date"}\NormalTok{, }\StringTok{"all\_the\_rest"}\NormalTok{), }
            \AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{,}
            \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{)}

\FunctionTok{head}\NormalTok{(initial\_clean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  raw_text                                     Name  not_name Date  all_the_rest
  <chr>                                        <chr> <chr>    <chr> <chr>       
1 Sir Robert Walpole(1676–1745)                Sir ~ 1676–17~ 1676~ ""          
2 Spencer Compton1st Earl of Wilmington(1673–~ Spen~ 1673–17~ 1673~ ""          
3 Henry Pelham(1694–1754)                      Henr~ 1694–17~ 1694~ ""          
4 Thomas Pelham-Holles1st Duke of Newcastle(1~ Thom~ 1693–17~ 1693~ ""          
5 William Cavendish4th Duke of Devonshire(172~ Will~ 1720–17~ 1720~ ""          
6 Thomas Pelham-Holles1st Duke of Newcastle(1~ Thom~ 1693–17~ 1693~ ""          
\end{verbatim}

Finally, we need to clean up the columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial\_clean }\OtherTok{\textless{}{-}} 
\NormalTok{ initial\_clean }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ Name, }
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"Name"}\NormalTok{, }\StringTok{"Title"}\NormalTok{),}
           \AttributeTok{sep =} \StringTok{"[[:digit:]]"}\NormalTok{,}
           \AttributeTok{extra =} \StringTok{"merge"}\NormalTok{,}
           \AttributeTok{fill =} \StringTok{"right"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ Name, }
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"Name"}\NormalTok{, }\StringTok{"Title"}\NormalTok{),}
           \AttributeTok{sep =} \StringTok{"MP for"}\NormalTok{,}
           \AttributeTok{extra =} \StringTok{"merge"}\NormalTok{,}
           \AttributeTok{fill =} \StringTok{"right"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Name =} \FunctionTok{str\_remove}\NormalTok{(Name, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{[b}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{]"}\NormalTok{))}

\FunctionTok{head}\NormalTok{(initial\_clean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 6
  raw_text                               Name  Title not_name Date  all_the_rest
  <chr>                                  <chr> <chr> <chr>    <chr> <chr>       
1 Sir Robert Walpole(1676–1745)          Sir ~ <NA>  1676–17~ 1676~ ""          
2 Spencer Compton1st Earl of Wilmington~ Spen~ <NA>  1673–17~ 1673~ ""          
3 Henry Pelham(1694–1754)                Henr~ <NA>  1694–17~ 1694~ ""          
4 Thomas Pelham-Holles1st Duke of Newca~ Thom~ <NA>  1693–17~ 1693~ ""          
5 William Cavendish4th Duke of Devonshi~ Will~ <NA>  1720–17~ 1720~ ""          
6 Thomas Pelham-Holles1st Duke of Newca~ Thom~ <NA>  1693–17~ 1693~ ""          
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  initial\_clean }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(Name, Date) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{separate}\NormalTok{(Date, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"Birth"}\NormalTok{, }\StringTok{"Died"}\NormalTok{), }\AttributeTok{sep =} \StringTok{"–"}\NormalTok{, }\AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# The }
  \CommentTok{\# PMs who have died have their birth and death years separated by a hyphen, but }
  \CommentTok{\# we need to be careful with the hyphen as it seems to be a slightly odd type of }
  \CommentTok{\# hyphen and we need to copy/paste it.}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Birth =} \FunctionTok{str\_remove\_all}\NormalTok{(Birth, }\StringTok{"born"}\NormalTok{),}
         \AttributeTok{Birth =} \FunctionTok{str\_trim}\NormalTok{(Birth)}
\NormalTok{         ) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Alive PMs have slightly different format}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Date) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Name =} \FunctionTok{str\_remove}\NormalTok{(Name, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Remove some html tags that remain}
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(Birth, Died), }\SpecialCharTok{\textasciitilde{}}\FunctionTok{as.integer}\NormalTok{(.)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Change birth and death to integers}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Age\_at\_Death =}\NormalTok{ Died }\SpecialCharTok{{-}}\NormalTok{ Birth) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# Add column of the number of years they lived}
  \FunctionTok{distinct}\NormalTok{() }\CommentTok{\# Some of the PMs had two goes at it.}

\FunctionTok{head}\NormalTok{(cleaned\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
  Name                 Birth  Died Age_at_Death
  <chr>                <int> <int>        <int>
1 Sir Robert Walpole    1676  1745           69
2 Spencer Compton       1673  1743           70
3 Henry Pelham          1694  1754           60
4 Thomas Pelham-Holles  1693  1768           75
5 William Cavendish     1720  1764           44
6 John Stuart           1713  1792           79
\end{verbatim}

Our dataset looks similar to the one that we said we wanted at the start
(Table~\ref{tbl-canadianpmscleanddata}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_data }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Prime Minister"}\NormalTok{, }\StringTok{"Birth year"}\NormalTok{, }\StringTok{"Death year"}\NormalTok{, }\StringTok{"Age at death"}\NormalTok{),}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }
    \AttributeTok{linesep =} \StringTok{""}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-canadianpmscleanddata}{}
\begin{longtable}[]{@{}lrrr@{}}
\caption{\label{tbl-canadianpmscleanddata}UK Prime Ministers, by how old
they were when they died}\tabularnewline
\toprule()
Prime Minister & Birth year & Death year & Age at death \\
\midrule()
\endfirsthead
\toprule()
Prime Minister & Birth year & Death year & Age at death \\
\midrule()
\endhead
Sir Robert Walpole & 1676 & 1745 & 69 \\
Spencer Compton & 1673 & 1743 & 70 \\
Henry Pelham & 1694 & 1754 & 60 \\
Thomas Pelham-Holles & 1693 & 1768 & 75 \\
William Cavendish & 1720 & 1764 & 44 \\
John Stuart & 1713 & 1792 & 79 \\
George Grenville & 1712 & 1770 & 58 \\
Charles Watson-Wentworth & 1730 & 1782 & 52 \\
William Pitt the Elder & 1708 & 1778 & 70 \\
Augustus FitzRoy & 1735 & 1811 & 76 \\
Frederick NorthLord North & 1732 & 1792 & 60 \\
William Petty & 1737 & 1805 & 68 \\
William Cavendish-Bentinck & 1738 & 1809 & 71 \\
William Pitt the Younger & 1759 & 1806 & 47 \\
Henry Addington & 1757 & 1844 & 87 \\
William Grenville & 1759 & 1834 & 75 \\
Spencer Perceval & 1762 & 1812 & 50 \\
Robert Jenkinson & 1770 & 1828 & 58 \\
George Canning & 1770 & 1827 & 57 \\
F. J. Robinson & 1782 & 1859 & 77 \\
Arthur Wellesley & 1769 & 1852 & 83 \\
Charles Grey & 1764 & 1845 & 81 \\
William Lamb & 1779 & 1848 & 69 \\
Sir Robert Peel & 1788 & 1850 & 62 \\
Lord John Russell & 1792 & 1878 & 86 \\
Edward Smith-Stanley & 1799 & 1869 & 70 \\
George Hamilton-Gordon & 1784 & 1860 & 76 \\
Henry John Temple & 1784 & 1865 & 81 \\
John Russell & 1792 & 1878 & 86 \\
Benjamin Disraeli & 1804 & 1881 & 77 \\
William Ewart Gladstone & 1809 & 1898 & 89 \\
Robert Gascoyne-Cecil & 1830 & 1903 & 73 \\
Archibald Primrose & 1847 & 1929 & 82 \\
Arthur Balfour & 1848 & 1930 & 82 \\
Sir Henry Campbell-Bannerman & 1836 & 1908 & 72 \\
H. H. Asquith & 1852 & 1928 & 76 \\
David Lloyd George & 1863 & 1945 & 82 \\
Bonar Law & 1858 & 1923 & 65 \\
Stanley Baldwin & 1867 & 1947 & 80 \\
Ramsay MacDonald & 1866 & 1937 & 71 \\
Neville Chamberlain & 1869 & 1940 & 71 \\
Winston Churchill & 1874 & 1965 & 91 \\
Clement Attlee & 1883 & 1967 & 84 \\
Sir Winston Churchill & 1874 & 1965 & 91 \\
Sir Anthony Eden & 1897 & 1977 & 80 \\
Harold Macmillan & 1894 & 1986 & 92 \\
Sir Alec Douglas-Home & 1903 & 1995 & 92 \\
Harold Wilson & 1916 & 1995 & 79 \\
Edward Heath & 1916 & 2005 & 89 \\
James Callaghan & 1912 & 2005 & 93 \\
Margaret Thatcher & 1925 & 2013 & 88 \\
John Major & 1943 & NA & NA \\
Tony Blair & 1953 & NA & NA \\
Gordon Brown & 1951 & NA & NA \\
David Cameron & 1966 & NA & NA \\
Theresa May & 1956 & NA & NA \\
Boris Johnson & 1964 & NA & NA \\
\bottomrule()
\end{longtable}

At this point we would like to make a graph that illustrates how long
each prime minister lived. If they are still alive then we would like
them to run to the end, but we would like to color them differently.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{still\_alive =} \FunctionTok{if\_else}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(Died), }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{),}
         \AttributeTok{Died =} \FunctionTok{if\_else}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(Died), }\FunctionTok{as.integer}\NormalTok{(}\DecValTok{2022}\NormalTok{), Died)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Name =} \FunctionTok{as\_factor}\NormalTok{(Name)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Birth, }
             \AttributeTok{xend =}\NormalTok{ Died,}
             \AttributeTok{y =}\NormalTok{ Name,}
             \AttributeTok{yend =}\NormalTok{ Name, }
             \AttributeTok{color =}\NormalTok{ still\_alive)) }\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year of birth"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Prime minister"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"PM is alive"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"How long each UK Prime Minister lived, by year of birth"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./09-gather_files/figure-pdf/unnamed-chunk-100-1.pdf}

}

\end{figure}

\hypertarget{case-study-downloading-multiple-files}{%
\subsection{Case study: Downloading multiple
files}\label{case-study-downloading-multiple-files}}

Considering text as data is exciting and opens up a lot of different
research questions. Many guides assume that we already have a nicely
formatted text dataset, but that is rarely actually the case. In this
case study we will download files from a few different pages. While we
have already seen two examples of web scraping, those were focused on
just one page, whereas we often need many. Here we will focus on this
iteration. We will use \texttt{download.file()} to do the download, and
\texttt{purrr} (Henry and Wickham 2020) to apply this function across
multiple sites.

The Reserve Bank of Australia (RBA) is Australia's central bank and sets
monetary policy. It has responsibility for setting the cash rate, which
is the interest rate used for loans between banks. This interest rate is
an especially important one, and has a large impact on the other
interest rates in the economy. Four times a year -- February, May,
August, and November -- the RBA publishes a statement on monetary
policy, and these are available as PDFs. In this example we will
download the four statements published in 2021.

First we set-up a dataframe that has the information that we need.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{statements\_of\_interest }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{address =} \FunctionTok{c}\NormalTok{(}\StringTok{"https://www.rba.gov.au/publications/smp/2021/nov/pdf/00{-}overview.pdf"}\NormalTok{,}
                \StringTok{"https://www.rba.gov.au/publications/smp/2021/aug/pdf/00{-}overview.pdf"}\NormalTok{,}
                \StringTok{"https://www.rba.gov.au/publications/smp/2021/may/pdf/00{-}overview.pdf"}\NormalTok{,}
                \StringTok{"https://www.rba.gov.au/publications/smp/2021/feb/pdf/00{-}overview.pdf"}
\NormalTok{                ),}
    \AttributeTok{local\_save\_name =} \FunctionTok{c}\NormalTok{(}
      \StringTok{"2021{-}11.pdf"}\NormalTok{,}
      \StringTok{"2021{-}08.pdf"}\NormalTok{,}
      \StringTok{"2021{-}05.pdf"}\NormalTok{,}
      \StringTok{"2021{-}02.pdf"}
\NormalTok{    )}
\NormalTok{  )}

\NormalTok{statements\_of\_interest}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 2
  address                                                        local_save_name
  <chr>                                                          <chr>          
1 https://www.rba.gov.au/publications/smp/2021/nov/pdf/00-overv~ 2021-11.pdf    
2 https://www.rba.gov.au/publications/smp/2021/aug/pdf/00-overv~ 2021-08.pdf    
3 https://www.rba.gov.au/publications/smp/2021/may/pdf/00-overv~ 2021-05.pdf    
4 https://www.rba.gov.au/publications/smp/2021/feb/pdf/00-overv~ 2021-02.pdf    
\end{verbatim}

Then we can apply the function \texttt{download.files()} to these four

Then we can write a function that will download the file, let us know
that it was downloaded, wait a polite amount of time, and then go get
the next file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{visit\_download\_and\_wait }\OtherTok{\textless{}{-}}
  \ControlFlowTok{function}\NormalTok{(the\_address\_to\_visit, where\_to\_save\_it\_locally) \{}
    
    \FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =}\NormalTok{ the\_address\_to\_visit,}
                  \AttributeTok{destfile =}\NormalTok{ where\_to\_save\_it\_locally}
\NormalTok{                  )}

    \FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Done with"}\NormalTok{, the\_address\_to\_visit, }\StringTok{"at"}\NormalTok{, }\FunctionTok{Sys.time}\NormalTok{()))}
    
    \FunctionTok{Sys.sleep}\NormalTok{(}\FunctionTok{sample}\NormalTok{(}\DecValTok{5}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

We now apply that function to our list of URLs.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{walk2}\NormalTok{(statements\_of\_interest}\SpecialCharTok{$}\NormalTok{address,}
\NormalTok{      statements\_of\_interest}\SpecialCharTok{$}\NormalTok{local\_save\_name,}
      \SpecialCharTok{\textasciitilde{}}\FunctionTok{visit\_download\_and\_wait}\NormalTok{(.x, .y))}
\end{Highlighting}
\end{Shaded}

The result is that we have downloaded these four PDFs and saved them to
our computer. In the next section we will build on this to discuss
getting information from these PDFs.

\hypertarget{pdfs}{%
\section{PDFs}\label{pdfs}}

In contrast to an API, a PDF is usually only produced for human rather
than computer consumption. The nice thing about PDFs is that they are
static and constant. And it is nice that they make data available at
all. But the trade-off is that:

\begin{itemize}
\tightlist
\item
  It is not overly useful to do larger-scale statistical analysis.
\item
  We do not know how the PDF was put together so we do not know whether
  we can trust it.
\item
  We cannot manipulate the data to get results that we are interested
  in.
\end{itemize}

Indeed, sometimes governments publish data as PDFs because they do not
actually want us to be able to analyze it. Being able to get data from
PDFs opens up a large number of datasets.

There are two important aspects to keep in mind when approaching a PDF
with a mind to extracting data from it:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Begin with an end in mind. Planning and then literally sketching out
  what we want from a final dataset/graph/paper stops us wasting time
  and keeps us focused.
\item
  Start simple, then iterate. The quickest way to make a complicated
  model is often to first build a simple model and then complicate it.
  Start with just trying to get one page of the PDF working or even just
  one line. Then iterate from there.
\end{enumerate}

We will start by walking through several examples and then go through a
case study where we will gather data on US Total Fertility Rate, by
state.

Figure~\ref{fig-firstpdfexample} is a PDF that consists of just the
first sentence from Jane Eyre taken from Project Gutenberg (Bronte
1847).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./inputs/pdfs/first_example.png}

}

\caption{\label{fig-firstpdfexample}First sentence of Jane Eyre}

\end{figure}

If assume that it was saved as `first\_example.pdf', then we can
\texttt{pdftools} (Ooms 2019a) to get the text from this one-page PDF
into R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pdftools)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{first\_example }\OtherTok{\textless{}{-}} \FunctionTok{pdf\_text}\NormalTok{(}\StringTok{"first\_example.pdf"}\NormalTok{)}

\NormalTok{first\_example}

\FunctionTok{class}\NormalTok{(first\_example)}
\end{Highlighting}
\end{Shaded}

We can see that the PDF has been correctly read in, as a character
vector.

We will now try a slightly more complicated example that consists of the
first few paragraphs of Jane Eyre (Figure~\ref{fig-secondpdfexample}).
Also notice that now we have the chapter heading as well.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./inputs/pdfs/second_example.png}

}

\caption{\label{fig-secondpdfexample}First few paragraphs of Jane Eyre}

\end{figure}

We use the same function as before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{second\_example }\OtherTok{\textless{}{-}}\NormalTok{ pdftools}\SpecialCharTok{::}\FunctionTok{pdf\_text}\NormalTok{(}\StringTok{"second\_example.pdf"}\NormalTok{)}

\NormalTok{second\_example}

\FunctionTok{class}\NormalTok{(second\_example)}
\end{Highlighting}
\end{Shaded}

Again, we have a character vector. The end of each line is signaled by
`\textbackslash n', but other than that it looks pretty good. Finally,
we consider the first two pages.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{third\_example }\OtherTok{\textless{}{-}}\NormalTok{ pdftools}\SpecialCharTok{::}\FunctionTok{pdf\_text}\NormalTok{(}\StringTok{"third\_example.pdf"}\NormalTok{)}

\NormalTok{third\_example}

\FunctionTok{class}\NormalTok{(third\_example)}
\end{Highlighting}
\end{Shaded}

Notice that the first page is the first element of the character vector,
and the second page is the second element. As we are most familiar with
rectangular data, we will try to get it into that format as quickly as
possible. And then we can use our regular \texttt{tidyverse} functions
to deal with it.

First we want to convert the character vector into a tibble. At this
point we may like to add page numbers as well.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{jane\_eyre }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{raw\_text =}\NormalTok{ third\_example,}
                    \AttributeTok{page\_number =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We then want to separate the lines so that each line is an observation.
We can do that by looking for `\textbackslash n' remembering that we
need to escape the backslash as it is a special character.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{jane\_eyre }\OtherTok{\textless{}{-}} 
  \FunctionTok{separate\_rows}\NormalTok{(jane\_eyre, raw\_text, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\AttributeTok{convert =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{jane\_eyre}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 93 x 2
   raw_text                                                          page_number
   <chr>                                                                   <int>
 1 "CHAPTER I"                                                                 1
 2 "There was no possibility of taking a walk that day. We had been~           1
 3 "leafless shrubbery an hour in the morning; but since dinner (Mr~           1
 4 "company, dined early) the cold winter wind had brought with it ~           1
 5 "penetrating, that further out-door exercise was now out of the ~           1
 6 ""                                                                          1
 7 "I was glad of it: I never liked long walks, especially on chill~           1
 8 "coming home in the raw twilight, with nipped fingers and toes, ~           1
 9 "chidings of Bessie, the nurse, and humbled by the consciousness~           1
10 "Eliza, John, and Georgiana Reed."                                          1
# ... with 83 more rows
\end{verbatim}

\hypertarget{case-study-gathering-data-on-the-us-total-fertility-rate}{%
\subsection{Case-study: Gathering data on the US Total Fertility
Rate}\label{case-study-gathering-data-on-the-us-total-fertility-rate}}

The US Department of Health and Human Services Vital Statistics Report
provides information about the total fertility rate (the average number
of births per woman if women experience the current age-specific
fertility rates throughout their reproductive years) for each state for
nineteen years. The US persists in only making this data available in
PDFs, which hinders research. But we can use the approaches above to get
the data into a nice dataset.

For instance, in the case of the year 2000 the table that we are
interested in is on page 40 of a PDF that is available at
https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50\_05.pdf. The column of
interest is labelled: ``Total fertility rate''
(Figure~\ref{fig-dhsexample}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/dhs_example.png}

}

\caption{\label{fig-dhsexample}Example Vital Statistics Report, from
2000}

\end{figure}

The first step when getting data out of a PDF is to sketch out what we
eventually want. A PDF typically contains a lot of information, and so
it is handy to be very clear about what you need. This helps keep you
focused, and prevents scope creep, but it is also helpful when thinking
about data checks. We literally write down on paper what we have in
mind. In this case, what is needed is a table with a column for state,
year and TFR (Figure~\ref{fig-tfrdesired}).

\begin{figure}

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{./figures/tfr_dataset_sketch.png}

}

\caption{\label{fig-tfrdesired}Planned dataset of TFR for each year and
US state}

\end{figure}

There are 19 different PDFs, and we are interested in a particular
column in a particular table in each of them. Unfortunately, there is
nothing magical about what is coming. This first step requires working
out the link for each, and the page and column name that is of interest.
In the end, this looks like this.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gt)}

\NormalTok{summary\_tfr\_dataset }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(year, page, table, column\_name, url) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{gt}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{rrrll}
\toprule
year & page & table & column\_name & url \\ 
\midrule
2000 & 40 & 10 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50\_05.pdf \\ 
2001 & 41 & 10 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr51/nvsr51\_02.pdf \\ 
2002 & 46 & 10 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr52/nvsr52\_10.pdf \\ 
2003 & 45 & 10 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr54/nvsr54\_02.pdf \\ 
2004 & 52 & 11 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr55/nvsr55\_01.pdf \\ 
2005 & 52 & 11 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr56/nvsr56\_06.pdf \\ 
2006 & 49 & 11 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr57/nvsr57\_07.pdf \\ 
2007 & 41 & 11 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr58/nvsr58\_24.pdf \\ 
2008 & 43 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr59/nvsr59\_01.pdf \\ 
2009 & 43 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr60/nvsr60\_01.pdf \\ 
2010 & 42 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr61/nvsr61\_01.pdf \\ 
2011 & 40 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr62/nvsr62\_01.pdf \\ 
2012 & 38 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr62/nvsr62\_09.pdf \\ 
2013 & 37 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64\_01.pdf \\ 
2014 & 38 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64\_12.pdf \\ 
2015 & 42 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66\_01.pdf \\ 
2016 & 29 & 8 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67\_01.pdf \\ 
2016 & 30 & 8 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67\_01.pdf \\ 
2017 & 23 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67\_08-508.pdf \\ 
2017 & 24 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr67/nvsr67\_08-508.pdf \\ 
2018 & 23 & 12 & Total fertility rate & https://www.cdc.gov/nchs/data/nvsr/nvsr68/nvsr68\_13-508.pdf \\ 
 \bottomrule
\end{longtable}

The first step is to get some code that works for one of them. I'll step
through the code in a lot more detail than normal because we're going to
use these pieces a lot.

We will choose the year 2000. We first download and save the PDF using
\texttt{download.file()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =}\NormalTok{ summary\_tfr\_dataset}\SpecialCharTok{$}\NormalTok{url[}\DecValTok{1}\NormalTok{], }
              \AttributeTok{destfile =} \StringTok{"year\_2000.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# INTERNAL}

\FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =}\NormalTok{ summary\_tfr\_dataset}\SpecialCharTok{$}\NormalTok{url[}\DecValTok{1}\NormalTok{], }
              \AttributeTok{destfile =} \StringTok{"inputs/pdfs/dhs/year\_2000.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We then read the PDF in as a character vector using \texttt{pdf\_text()}
from \texttt{pdftools}. And then convert it to a tibble, so that we can
use familiar verbs on it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pdftools)}
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} \FunctionTok{pdf\_text}\NormalTok{(}\StringTok{"year\_2000.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{raw\_data =}\NormalTok{ dhs\_2000)}

\FunctionTok{head}\NormalTok{(dhs\_2000)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 1
  raw_data                                                                      
  <chr>                                                                         
1 "Volume 50, Number 5                                                         ~
2 "2   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\n\n\~
3 "                                                                            ~
4 "4   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\n\n\~
5 "                                                                            ~
6 "6   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\n\n ~
\end{verbatim}

Grab the page that is of interest (remembering that each page is a
element of the character vector, hence a row in the tibble).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} 
\NormalTok{  dhs\_2000 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(summary\_tfr\_dataset}\SpecialCharTok{$}\NormalTok{page[}\DecValTok{1}\NormalTok{])}

\FunctionTok{head}\NormalTok{(dhs\_2000)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  raw_data                                                                      
  <chr>                                                                         
1 "40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022\n~
\end{verbatim}

Now we want to separate the rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} 
\NormalTok{  dhs\_2000 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{separate\_rows}\NormalTok{(raw\_data, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\AttributeTok{convert =} \ConstantTok{FALSE}\NormalTok{)}

\FunctionTok{head}\NormalTok{(dhs\_2000)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 1
  raw_data                                                                      
  <chr>                                                                         
1 "40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022"  
2 ""                                                                            
3 "Table 10. Number of births, birth rates, fertility rates, total fertility ra~
4 "United States, each State and territory, 2000"                               
5 "[By place of residence. Birth rates are live births per 1,000 estimated popu~
6 "estimated in each area; total fertility rates are sums of birth rates for 5-~
\end{verbatim}

Now we are searching for patterns that we can use. Let us look at the
first ten lines of content.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000[}\DecValTok{13}\SpecialCharTok{:}\DecValTok{22}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 1
   raw_data                                                                     
   <chr>                                                                        
 1 "                                  State                                    ~
 2 "                                                                           ~
 3 "                                                                           ~
 4 ""                                                                           
 5 ""                                                                           
 6 "United States 1 ......................................................     ~
 7 ""                                                                           
 8 "Alabama ...............................................................    ~
 9 "Alaska ................................................................... ~
10 "Arizona .................................................................  ~
\end{verbatim}

It does not get much better than this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We have dots separating the states from the data.
\item
  We have a space between each of the columns.
\end{enumerate}

So we can now separate this in to separate columns. First we want to
match on when there is at least two dots (remembering that the dot is a
special character and so needs to be escaped).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} 
\NormalTok{  dhs\_2000 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ raw\_data, }
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"data"}\NormalTok{), }
           \AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.\{2,\}"}\NormalTok{, }
           \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{,}
           \AttributeTok{fill =} \StringTok{"right"}
\NormalTok{           )}

\FunctionTok{head}\NormalTok{(dhs\_2000)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  raw_data                                                           state data 
  <chr>                                                              <chr> <chr>
1 "40 National Vital Statistics Report, Vol. 50, No. 5, Revised May~ "40 ~ <NA> 
2 ""                                                                 ""    <NA> 
3 "Table 10. Number of births, birth rates, fertility rates, total ~ "Tab~ <NA> 
4 "United States, each State and territory, 2000"                    "Uni~ <NA> 
5 "[By place of residence. Birth rates are live births per 1,000 es~ "[By~ <NA> 
6 "estimated in each area; total fertility rates are sums of birth ~ "est~ <NA> 
\end{verbatim}

We get the expected warnings about the top and the bottom as they do not
have multiple dots. (Another option here is to use \texttt{pdf\_data()}
which would allow us to use location rather than delimiters.)

We can now separate the data based on spaces. There is an inconsistent
number of spaces, so we first squish any example of more than one space
into just one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} 
\NormalTok{  dhs\_2000 }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{data =} \FunctionTok{str\_squish}\NormalTok{(data)) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ data, }
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"number\_of\_births"}\NormalTok{, }
                    \StringTok{"birth\_rate"}\NormalTok{, }
                    \StringTok{"fertility\_rate"}\NormalTok{, }
                    \StringTok{"TFR"}\NormalTok{, }
                    \StringTok{"teen\_births\_all"}\NormalTok{, }
                    \StringTok{"teen\_births\_15\_17"}\NormalTok{, }
                    \StringTok{"teen\_births\_18\_19"}\NormalTok{), }
           \AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{, }
           \AttributeTok{remove =} \ConstantTok{FALSE}
\NormalTok{           )}

\FunctionTok{head}\NormalTok{(dhs\_2000)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 10
  raw_data          state data  number_of_births birth_rate fertility_rate TFR  
  <chr>             <chr> <chr> <chr>            <chr>      <chr>          <chr>
1 "40 National Vit~ "40 ~ <NA>  <NA>             <NA>       <NA>           <NA> 
2 ""                ""    <NA>  <NA>             <NA>       <NA>           <NA> 
3 "Table 10. Numbe~ "Tab~ <NA>  <NA>             <NA>       <NA>           <NA> 
4 "United States, ~ "Uni~ <NA>  <NA>             <NA>       <NA>           <NA> 
5 "[By place of re~ "[By~ <NA>  <NA>             <NA>       <NA>           <NA> 
6 "estimated in ea~ "est~ <NA>  <NA>             <NA>       <NA>           <NA> 
# ... with 3 more variables: teen_births_all <chr>, teen_births_15_17 <chr>,
#   teen_births_18_19 <chr>
\end{verbatim}

This is all looking fairly great. The only thing left is to clean up.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}} 
\NormalTok{  dhs\_2000 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(state, TFR) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{13}\SpecialCharTok{:}\DecValTok{69}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year =} \DecValTok{2000}\NormalTok{)}

\NormalTok{dhs\_2000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 57 x 3
   state                                                             TFR    year
   <chr>                                                             <chr> <dbl>
 1 "                                  State                        ~ <NA>   2000
 2 "                                                               ~ <NA>   2000
 3 "                                                               ~ <NA>   2000
 4 ""                                                                <NA>   2000
 5 ""                                                                <NA>   2000
 6 "United States 1 "                                                2,13~  2000
 7 ""                                                                <NA>   2000
 8 "Alabama "                                                        2,02~  2000
 9 "Alaska "                                                         2,43~  2000
10 "Arizona "                                                        2,65~  2000
# ... with 47 more rows
\end{verbatim}

And we're done for that year. Now we want to take these pieces, put them
into a function and then run that function over all 19 years.

The first part is downloading each of the 19 PDFs that we need. We are
going to build on the code that we used before. That code was:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =}\NormalTok{ summary\_tfr\_dataset}\SpecialCharTok{$}\NormalTok{url[}\DecValTok{1}\NormalTok{], }\AttributeTok{destfile =} \StringTok{"year\_2000.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To modify this we need:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To have it iterate through each of the lines in the dataset that
  contains our CSVs (i.e.~where it says 1, we want 1, then 2, then 3,
  etc.).
\item
  Where it has a filename, we need it to iterate through our desired
  filenames (i.e.~year\_2000, then year\_2001, then year\_2002, etc).
\item
  We would like for it to do all of this in a way that is a little
  robust to errors. For instance, if one of the URLs is wrong or the
  internet drops out then we would like it to just move onto the next
  PDF, and then warn us at the end that it missed one, not to stop.
  (This does not really matter because it is only 19 files, but it is
  easy to find oneself doing this for thousands of files).
\end{enumerate}

We will draw on \texttt{purrr} for this (Henry and Wickham 2020).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(purrr)}

\NormalTok{summary\_tfr\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  summary\_tfr\_dataset }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pdf\_name =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"dhs/year\_"}\NormalTok{, year, }\StringTok{".pdf"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{walk2}\NormalTok{(}
\NormalTok{  summary\_tfr\_dataset}\SpecialCharTok{$}\NormalTok{url,}
\NormalTok{  summary\_tfr\_dataset}\SpecialCharTok{$}\NormalTok{pdf\_name,}
  \FunctionTok{safely}\NormalTok{( }\SpecialCharTok{\textasciitilde{}} \FunctionTok{download.file}\NormalTok{(.x , .y))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here we take \texttt{download.file()} and pass it two arguments:
\texttt{.x} and \texttt{.y}. Then \texttt{walk2()} applies that function
to the inputs that we give it, in this case the URLs columns is the
\texttt{.x} and the pdf\_names column is the \texttt{.y}. Finally,
\texttt{safely()} means that if there are any failures then it just
moves onto the next file instead of throwing an error.

We now have each of the PDFs saved and we can move onto getting the data
from them.

Now we need to get the data from the PDFs. As before, we are going to
build on the code that we used before. That code (overly condensed) was:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}}\NormalTok{ pdftools}\SpecialCharTok{::}\FunctionTok{pdf\_text}\NormalTok{(}\StringTok{"year\_2000.pdf"}\NormalTok{)}

\NormalTok{dhs\_2000 }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{raw\_data =}\NormalTok{ dhs\_2000) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice}\NormalTok{(summary\_tfr\_dataset}\SpecialCharTok{$}\NormalTok{page[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{separate\_rows}\NormalTok{(raw\_data, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\AttributeTok{convert =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{separate}\NormalTok{(}
    \AttributeTok{col =}\NormalTok{ raw\_data,}
    \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"data"}\NormalTok{),}
    \AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.\{2,\}"}\NormalTok{,}
    \AttributeTok{remove =} \ConstantTok{FALSE}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{data =} \FunctionTok{str\_squish}\NormalTok{(data)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{separate}\NormalTok{(}
    \AttributeTok{col =}\NormalTok{ data,}
    \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}
      \StringTok{"number\_of\_births"}\NormalTok{,}
      \StringTok{"birth\_rate"}\NormalTok{,}
      \StringTok{"fertility\_rate"}\NormalTok{,}
      \StringTok{"TFR"}\NormalTok{,}
      \StringTok{"teen\_births\_all"}\NormalTok{,}
      \StringTok{"teen\_births\_15\_17"}\NormalTok{,}
      \StringTok{"teen\_births\_18\_19"}
\NormalTok{    ),}
    \AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{,}
    \AttributeTok{remove =} \ConstantTok{FALSE}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(state, TFR) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{13}\SpecialCharTok{:}\DecValTok{69}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year =} \DecValTok{2000}\NormalTok{)}

\NormalTok{dhs\_2000}
\end{Highlighting}
\end{Shaded}

The first thing that we want to iterate is the argument to
\texttt{pdf\_text()}, then the number in in \texttt{slice()} will also
need to change (that is doing the work to get only the page that we are
interested in).

Two aspects are hardcoded, and these may need to be updated. In
particular:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  The separate only works if each of the tables has the same columns in
  the same order; and
\item
  the slice (which restricts the data to just the states) only works in
  this case.
\end{enumerate}

Finally, we add the year only at the end, whereas we would need to bring
that up earlier in the process.

We will start by writing a function that will go through all the files,
grab the data, get the page of interest, and then expand the rows. We
will then use \texttt{pmap\_dfr()} from \texttt{purrr} to apply that
function to all of the PDFs and to output a tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_pdf\_convert\_to\_tibble }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(pdf\_name, page, year)\{}
\CommentTok{\#| echo: true}
  
\NormalTok{  dhs\_table\_of\_interest }\OtherTok{\textless{}{-}} 
    \FunctionTok{tibble}\NormalTok{(}\AttributeTok{raw\_data =}\NormalTok{ pdftools}\SpecialCharTok{::}\FunctionTok{pdf\_text}\NormalTok{(pdf\_name)) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{slice}\NormalTok{(page) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{separate\_rows}\NormalTok{(raw\_data, }\AttributeTok{sep =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\AttributeTok{convert =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ raw\_data, }
             \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"state"}\NormalTok{, }\StringTok{"data"}\NormalTok{), }
             \AttributeTok{sep =} \StringTok{"[�|}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.]}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s+(?=[[:digit:]])"}\NormalTok{, }
             \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{mutate}\NormalTok{(}
      \AttributeTok{data =} \FunctionTok{str\_squish}\NormalTok{(data),}
      \AttributeTok{year\_of\_data =}\NormalTok{ year)}

  \FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Done with"}\NormalTok{, year))}
  
  \FunctionTok{return}\NormalTok{(dhs\_table\_of\_interest)}
\NormalTok{\}}

\NormalTok{raw\_dhs\_data }\OtherTok{\textless{}{-}}\NormalTok{ purrr}\SpecialCharTok{::}\FunctionTok{pmap\_dfr}\NormalTok{(summary\_tfr\_dataset }\SpecialCharTok{|\textgreater{}} \FunctionTok{select}\NormalTok{(pdf\_name, page, year),}
\NormalTok{                                get\_pdf\_convert\_to\_tibble)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Done with 2000"
[1] "Done with 2001"
[1] "Done with 2002"
[1] "Done with 2003"
[1] "Done with 2004"
[1] "Done with 2005"
[1] "Done with 2006"
[1] "Done with 2007"
[1] "Done with 2008"
[1] "Done with 2009"
[1] "Done with 2010"
[1] "Done with 2011"
[1] "Done with 2012"
[1] "Done with 2013"
[1] "Done with 2014"
[1] "Done with 2015"
[1] "Done with 2016"
[1] "Done with 2016"
[1] "Done with 2017"
[1] "Done with 2017"
[1] "Done with 2018"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(raw\_dhs\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
  raw_data                                              state data  year_of_data
  <chr>                                                 <chr> <chr>        <dbl>
1 "40 National Vital Statistics Report, Vol. 50, No. 5~ "40 ~ 50, ~         2000
2 ""                                                    ""    <NA>          2000
3 "Table 10. Number of births, birth rates, fertility ~ "Tab~ <NA>          2000
4 "United States, each State and territory, 2000"       "Uni~ <NA>          2000
5 "[By place of residence. Birth rates are live births~ "[By~ <NA>          2000
6 "estimated in each area; total fertility rates are s~ "est~ <NA>          2000
\end{verbatim}

Now we need to clean up the state names and then filter on them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{states }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Alabama"}\NormalTok{, }\StringTok{"Alaska"}\NormalTok{, }\StringTok{"Arizona"}\NormalTok{, }\StringTok{"Arkansas"}\NormalTok{, }\StringTok{"California"}\NormalTok{, }\StringTok{"Colorado"}\NormalTok{, }
\CommentTok{\#| echo: true}
            \StringTok{"Connecticut"}\NormalTok{, }\StringTok{"Delaware"}\NormalTok{, }\StringTok{"Florida"}\NormalTok{, }\StringTok{"Georgia"}\NormalTok{, }\StringTok{"Hawaii"}\NormalTok{, }\StringTok{"Idaho"}\NormalTok{, }
            \StringTok{"Illinois"}\NormalTok{, }\StringTok{"Indiana"}\NormalTok{, }\StringTok{"Iowa"}\NormalTok{, }\StringTok{"Kansas"}\NormalTok{, }\StringTok{"Kentucky"}\NormalTok{, }\StringTok{"Louisiana"}\NormalTok{, }
            \StringTok{"Maine"}\NormalTok{, }\StringTok{"Maryland"}\NormalTok{, }\StringTok{"Massachusetts"}\NormalTok{, }\StringTok{"Michigan"}\NormalTok{, }\StringTok{"Minnesota"}\NormalTok{, }
            \StringTok{"Mississippi"}\NormalTok{, }\StringTok{"Missouri"}\NormalTok{, }\StringTok{"Montana"}\NormalTok{, }\StringTok{"Nebraska"}\NormalTok{, }\StringTok{"Nevada"}\NormalTok{, }
            \StringTok{"New Hampshire"}\NormalTok{, }\StringTok{"New Jersey"}\NormalTok{, }\StringTok{"New Mexico"}\NormalTok{, }\StringTok{"New York"}\NormalTok{, }\StringTok{"North Carolina"}\NormalTok{, }
            \StringTok{"North Dakota"}\NormalTok{, }\StringTok{"Ohio"}\NormalTok{, }\StringTok{"Oklahoma"}\NormalTok{, }\StringTok{"Oregon"}\NormalTok{, }\StringTok{"Pennsylvania"}\NormalTok{, }
            \StringTok{"Rhode Island"}\NormalTok{, }\StringTok{"South Carolina"}\NormalTok{, }\StringTok{"South Dakota"}\NormalTok{, }\StringTok{"Tennessee"}\NormalTok{, }\StringTok{"Texas"}\NormalTok{, }
            \StringTok{"Utah"}\NormalTok{, }\StringTok{"Vermont"}\NormalTok{, }\StringTok{"Virginia"}\NormalTok{, }\StringTok{"Washington"}\NormalTok{, }\StringTok{"West Virginia"}\NormalTok{, }\StringTok{"Wisconsin"}\NormalTok{, }
            \StringTok{"Wyoming"}\NormalTok{, }\StringTok{"District of Columbia"}\NormalTok{)}

\NormalTok{raw\_dhs\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  raw\_dhs\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{state =} \FunctionTok{str\_remove\_all}\NormalTok{(state, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{."}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_remove\_all}\NormalTok{(state, }\StringTok{"�"}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_remove\_all}\NormalTok{(state, }\StringTok{"\textbackslash{}u0008"}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_replace\_all}\NormalTok{(state, }\StringTok{"United States 1"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_replace\_all}\NormalTok{(state, }\StringTok{"United States1"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_replace\_all}\NormalTok{(state, }\StringTok{"United States 2"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_replace\_all}\NormalTok{(state, }\StringTok{"United States2"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
         \AttributeTok{state =} \FunctionTok{str\_replace\_all}\NormalTok{(state, }\StringTok{"United States²"}\NormalTok{, }\StringTok{"United States"}\NormalTok{),}
\NormalTok{         ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{state =} \FunctionTok{str\_squish}\NormalTok{(state)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(state }\SpecialCharTok{\%in\%}\NormalTok{ states)}

\FunctionTok{head}\NormalTok{(raw\_dhs\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
  raw_data                                              state data  year_of_data
  <chr>                                                 <chr> <chr>        <dbl>
1 Alabama ............................................~ Alab~ 63,2~         2000
2 Alaska .............................................~ Alas~ 9,97~         2000
3 Arizona ............................................~ Ariz~ 85,2~         2000
4 Arkansas ...........................................~ Arka~ 37,7~         2000
5 California .........................................~ Cali~ 531,~         2000
6 Colorado ...........................................~ Colo~ 65,4~         2000
\end{verbatim}

The next step is to separate the data and get the correct column from
it. We are going to separate based on spaces once it is cleaned up.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw\_dhs\_data }\OtherTok{\textless{}{-}} 
\CommentTok{\#| echo: true}
\NormalTok{  raw\_dhs\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{data =} \FunctionTok{str\_remove\_all}\NormalTok{(data, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{*"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{separate}\NormalTok{(data, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"col\_1"}\NormalTok{, }\StringTok{"col\_2"}\NormalTok{, }\StringTok{"col\_3"}\NormalTok{, }\StringTok{"col\_4"}\NormalTok{, }\StringTok{"col\_5"}\NormalTok{, }
                          \StringTok{"col\_6"}\NormalTok{, }\StringTok{"col\_7"}\NormalTok{, }\StringTok{"col\_8"}\NormalTok{, }\StringTok{"col\_9"}\NormalTok{, }\StringTok{"col\_10"}\NormalTok{), }
           \AttributeTok{sep =} \StringTok{" "}\NormalTok{,}
           \AttributeTok{remove =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{head}\NormalTok{(raw\_dhs\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 14
  raw_data     state data  col_1 col_2 col_3 col_4 col_5 col_6 col_7 col_8 col_9
  <chr>        <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>
1 Alabama ...~ Alab~ 63,2~ 63,2~ 14.4  65.0  2,02~ 62.9  37.9  97.3  <NA>  <NA> 
2 Alaska ....~ Alas~ 9,97~ 9,974 16.0  74.6  2,43~ 42.4  23.6  69.4  <NA>  <NA> 
3 Arizona ...~ Ariz~ 85,2~ 85,2~ 17.5  84.4  2,65~ 69.1  41.1  111.3 <NA>  <NA> 
4 Arkansas ..~ Arka~ 37,7~ 37,7~ 14.7  69.1  2,14~ 68.5  36.7  114.1 <NA>  <NA> 
5 California ~ Cali~ 531,~ 531,~ 15.8  70.7  2,18~ 48.5  28.6  75.6  <NA>  <NA> 
6 Colorado ..~ Colo~ 65,4~ 65,4~ 15.8  73.1  2,35~ 49.2  28.6  79.8  <NA>  <NA> 
# ... with 2 more variables: col_10 <chr>, year_of_data <dbl>
\end{verbatim}

We can now grab the correct column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfr\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  raw\_dhs\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{TFR =} \FunctionTok{if\_else}\NormalTok{(year\_of\_data }\SpecialCharTok{\textless{}} \DecValTok{2008}\NormalTok{, col\_4, col\_3)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(state, year\_of\_data, TFR) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{year =}\NormalTok{ year\_of\_data)}

\FunctionTok{head}\NormalTok{(tfr\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  state       year TFR    
  <chr>      <dbl> <chr>  
1 Alabama     2000 2,021.0
2 Alaska      2000 2,437.0
3 Arizona     2000 2,652.5
4 Arkansas    2000 2,140.0
5 California  2000 2,186.0
6 Colorado    2000 2,356.5
\end{verbatim}

Finally, we need to convert the case.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(tfr\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  state       year TFR    
  <chr>      <dbl> <chr>  
1 Alabama     2000 2,021.0
2 Alaska      2000 2,437.0
3 Arizona     2000 2,652.5
4 Arkansas    2000 2,140.0
5 California  2000 2,186.0
6 Colorado    2000 2,356.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfr\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  tfr\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{TFR =} \FunctionTok{str\_remove\_all}\NormalTok{(TFR, }\StringTok{","}\NormalTok{),}
         \AttributeTok{TFR =} \FunctionTok{as.numeric}\NormalTok{(TFR))}

\FunctionTok{head}\NormalTok{(tfr\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  state       year   TFR
  <chr>      <dbl> <dbl>
1 Alabama     2000 2021 
2 Alaska      2000 2437 
3 Arizona     2000 2652.
4 Arkansas    2000 2140 
5 California  2000 2186 
6 Colorado    2000 2356.
\end{verbatim}

And run some checks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfr\_data}\SpecialCharTok{$}\NormalTok{state }\SpecialCharTok{|\textgreater{}} \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \FunctionTok{length}\NormalTok{() }\SpecialCharTok{==} \DecValTok{51}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfr\_data}\SpecialCharTok{$}\NormalTok{year }\SpecialCharTok{|\textgreater{}} \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \FunctionTok{length}\NormalTok{() }\SpecialCharTok{==} \DecValTok{19}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] TRUE
\end{verbatim}

In particular we want for there to be 51 states and for there to be 19
years.

And we are done (Table~\ref{tbl-tfrforthewin})!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tfr\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  knitr}\SpecialCharTok{::} \FunctionTok{kable}\NormalTok{(}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"State"}\NormalTok{, }\StringTok{"Year"}\NormalTok{, }\StringTok{"TFR"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{0}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{, }
    \AttributeTok{linesep =} \StringTok{""}\NormalTok{,}
    \AttributeTok{format.args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{big.mark =} \StringTok{","}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-tfrforthewin}{}
\begin{longtable}[]{@{}lrr@{}}
\caption{\label{tbl-tfrforthewin}First ten rows of a dataset of TFR by
US state, 2000-2019}\tabularnewline
\toprule()
State & Year & TFR \\
\midrule()
\endfirsthead
\toprule()
State & Year & TFR \\
\midrule()
\endhead
Alabama & 2,000 & 2,021 \\
Alaska & 2,000 & 2,437 \\
Arizona & 2,000 & 2,652 \\
Arkansas & 2,000 & 2,140 \\
California & 2,000 & 2,186 \\
Colorado & 2,000 & 2,356 \\
Connecticut & 2,000 & 1,932 \\
Delaware & 2,000 & 2,014 \\
District of Columbia & 2,000 & 1,976 \\
Florida & 2,000 & 2,158 \\
\bottomrule()
\end{longtable}

\hypertarget{optical-character-recognition}{%
\subsection{Optical Character
Recognition}\label{optical-character-recognition}}

All of the above is predicated on having a PDF that is already
`digitized'. But what if it is images? In that case we need to first use
Optical Character Recognition (OCR) using \texttt{tesseract} (Ooms
2019c). This is a R wrapper around the Tesseract open-source OCR engine.

Let us see an example with a scan from the first page of Jane Eyre
(Figure~\ref{fig-janescan}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/jane_scan.png}

}

\caption{\label{fig-janescan}Scan of first page of Jane Eyre}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tesseract)}

\NormalTok{text }\OtherTok{\textless{}{-}}\NormalTok{ tesseract}\SpecialCharTok{::}\FunctionTok{ocr}\NormalTok{(here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{(}\StringTok{"jane\_scan.png"}\NormalTok{), }\AttributeTok{engine =} \FunctionTok{tesseract}\NormalTok{(}\StringTok{"eng"}\NormalTok{))}
\FunctionTok{cat}\NormalTok{(text)}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercises-and-tutorial-8}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-8}}

\hypertarget{exercises-8}{%
\subsection{Exercises}\label{exercises-8}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What are some types of probability sampling, and in what circumstances
  might you want to implement them (write two or three pages)?
\item
  There have been some substantial political polling `misses' in recent
  years (Trump and Brexit come to mind). To what extent do you think
  non-response bias was the cause of this (write a page or two, being
  sure to ground your writing with citations)?
\item
  It seems like a lot of businesses have closed since the pandemic. To
  investigate this, we walk along some blocks downtown and count the
  number of businesses that are closed and open. To decide which blocks
  to walk, we open a map, start at the lake, and then pick every 10th
  street. This type of sampling is (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Cluster sampling.
  \item
    Systematic sampling.
  \item
    Stratified sampling.
  \item
    Convenience sampling.
  \end{enumerate}
\item
  Please name some reasons why you may wish to use cluster sampling
  (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Balance in responses.
  \item
    Administrative convenience.
  \item
    Efficiency in terms of money.
  \item
    Underlying systematic concerns.
  \item
    Estimation of sub-populations.
  \end{enumerate}
\item
  Please consider Beaumont, 2020, `Are probability surveys bound to
  disappear for the production of official statistics?'. With reference
  to that paper, do you think that probability surveys will disappear,
  and why or why not (please write a paragraph or two)?
\item
  In your own words, what is an API (write a paragraph or two)?
\item
  Find two APIs and discuss how you could use them to tell interesting
  stories (write a paragraph or two for each)?
\item
  Find two APIs that have an R packages written around them. How could
  you use these to tell interesting stories (write a paragraph or two)?
\item
  What is the main argument to \texttt{httr::GET()} (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `url'
  \item
    `website'
  \item
    `domain'
  \item
    `location'
  \end{enumerate}
\item
  What are three reasons why we should be respectful when getting
  scraping data from websites (write a paragraph or two)?
\item
  What features of a website do we typically take advantage of when we
  parse the code (pick on)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    HTML/CSS mark-up.
  \item
    Cookies.
  \item
    Facebook beacons.
  \item
    Code comments.
  \end{enumerate}
\item
  What are three advantages and three disadvantages of scraping compared
  with using an API (write a paragraph or two)?
\item
  What are three delimiters that could be useful when trying to bring
  order to the PDF that you read in as a character vector (write a
  paragraph or two)?
\item
  Which of the following, used as part of a regular expression, would
  match a full stop (hint: see the `strings' cheat sheet) (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    `.'
  \item
    `.'
  \item
    `\textbackslash.'
  \item
    `\textbackslash.'
  \end{enumerate}
\item
  Name three reasons for sketching out what you want before starting to
  try to extract data from a PDF (write a paragraph or two for each)?
\item
  What are three checks that we might like to use for demographic data,
  such as the number of births in a country in a particular year (write
  a paragraph or two for check)?
\item
  What are three checks that we might like to use for economic data,
  such as GDP for a particular country in a particular year (write a
  paragraph or two for check)?
\item
  What does the \texttt{purrr} package do (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Enhances R's functional programming toolkit.
  \item
    Makes loops easier to code and read.
  \item
    Checks the consistency of datasets.
  \item
    Identifies issues in data structures and proposes replacements.
  \end{enumerate}
\item
  Which of these are functions from the \texttt{purrr} package (select
  all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{map()}
  \item
    \texttt{walk()}
  \item
    \texttt{run()}
  \item
    \texttt{safely()}
  \end{enumerate}
\item
  What are some principles to follow when scraping (select all that
  apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Avoid it if possible
  \item
    Follow the site's guidance
  \item
    Slow down
  \item
    Use a scalpel not an axe.\\
  \end{enumerate}
\item
  What is a robots.txt file (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The instructions that Frankenstein followed.
  \item
    Notes that web scrapers should follow when scraping.
  \end{enumerate}
\item
  What is the html tag for an item in list (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{li}
  \item
    \texttt{body}
  \item
    \texttt{b}
  \item
    \texttt{em}
  \end{enumerate}
\item
  Which function should we use if we have the following text data:
  `rohan\_alexander' in a column called `names' and want to split it
  into first name and surname based on the underbar (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{separate()}
  \item
    \texttt{slice()}
  \item
    \texttt{spacing()}
  \item
    \texttt{text\_to\_columns()}
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial-8}{%
\subsection{Tutorial}\label{tutorial-8}}

Please redo the web scraping example, but for one of:
\href{https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Australia}{Australia},
\href{https://en.wikipedia.org/wiki/List_of_prime_ministers_of_Canada}{Canada},
\href{https://en.wikipedia.org/wiki/List_of_prime_ministers_of_India}{India},
or
\href{https://en.wikipedia.org/wiki/List_of_prime_ministers_of_New_Zealand}{New
Zealand}.

Plan, gather, and clean the data, and then use it to create a similar
table to the one created above. Write a few paragraphs about your
findings. Then write a few paragraphs about the data source, what you
gathered, and how you went about it. What took longer than you expected?
When did it become fun? What would you do differently next time you do
this? Your submission should be at least two pages and likely more.

Please submit a link to a PDF produced using R Markdown that includes a
link to the GitHub repo.

\hypertarget{sec-hunt-data}{%
\chapter{Hunt data}\label{sec-hunt-data}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{Big tech is testing you}, (Fry 2020).
\item
  Read \emph{Inventing the randomized double-blind trial: the Nuremberg
  salt test of 1835}, (Stolberg 2006).
\item
  Read \emph{Impact evaluation in practice}, Chapters 3 and 4, (Gertler
  et al. 2016).
\item
  Read \emph{Statistics and causal inference}, Parts 1-3, (Holland
  1986).
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Treatment and control groups.
\item
  Internal and external validity.
\item
  Average treatment effect.
\item
  Generating simulated datasets.
\item
  Informed consent and establishing the need for an experiment.
\item
  A/B testing
\item
  Putting together surveys with Google Forms
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{tidyverse} (Wickham et al. 2019a)
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{count()}
\item
  \texttt{filter()}
\item
  \texttt{ggplot()}
\item
  \texttt{group\_by()}
\item
  \texttt{head()}
\item
  \texttt{here()}
\item
  \texttt{if\_else()}
\item
  \texttt{left\_join()}
\item
  \texttt{length()}
\item
  \texttt{mean()}
\item
  \texttt{mutate()}
\item
  \texttt{names()}
\item
  \texttt{nrow()}
\item
  \texttt{pivot\_wider()}
\item
  \texttt{read\_csv()}
\item
  \texttt{ref()}
\item
  \texttt{rename()}
\item
  \texttt{rnorm()}
\item
  \texttt{rowwise()}
\item
  \texttt{sample()}
\item
  \texttt{scale\_fill\_brewer()}
\item
  \texttt{seed()}
\item
  \texttt{select()}
\item
  \texttt{str\_detect()}
\item
  \texttt{sum()}
\item
  \texttt{summarize()}
\item
  \texttt{test()}
\item
  \texttt{theme\_classic()}
\item
  \texttt{theme\_minimal()}
\item
  \texttt{tibble()}
\item
  \texttt{ungroup()}
\end{itemize}

\hypertarget{experiments-and-randomized-controlled-trials}{%
\section{Experiments and randomized controlled
trials}\label{experiments-and-randomized-controlled-trials}}

\hypertarget{introduction-7}{%
\subsection{Introduction}\label{introduction-7}}

Ronald Fisher, the twentieth century statistician, and Francis Galton,
the nineteenth century statistician, are the intellectual grandfathers
of much of the work that we cover in this chapter. In some cases it is
directly their work, in other cases it is work that built on their
contributions. Both men believed in eugenics, amongst other things that
are generally reprehensible. In the same way that art history must
acknowledge, say Caravaggio as a murderer, while also considering his
work and influence, so to must statistics and the data sciences more
generally concern themselves with this past, at the same time as we try
to build a better future.

This chapter is about experiments. This is a situation in which we can
explicitly control and vary that which we are interested in. The
advantage of this is that identification should be clear. There is a
treatment group that is subject to that which we are interested in, and
a control group that is not. These are randomly split before treatment.
And so, if they end up different then it must be because of the
treatment. Unfortunately, life is rarely so smooth. Arguing about how
similar the treatment and control groups were tends to carry on
indefinitely. Our ability to speak to whether we have measured the
effect of the treatment, affects our ability to speak to what effect
that treatment could have.

In this chapter we cover experiments, especially constructing treatment
and control groups, and appropriately considering their results. We
discuss some aspects of ethical behaviour in experiments through
reference to the abhorrent Tuskegee Syphilis Study and ECMO. And we go
through the Oregon Health Insurance Experiment as a case study. We then
turn to A/B testing, which is extensively used in industry, and consider
a case study based on Upworthy data. Finally, we go through actually
implementing a survey using Google Forms.

\hypertarget{motivation-and-notation}{%
\subsection{Motivation and notation}\label{motivation-and-notation}}

Professional sports are a big deal in North America. Consider the
situation of someone who moves to San Francisco in 2014, such that as
soon as they moved the Giants win the World Series and the Golden State
Warriors begin a historic streak of World Championships. They move to
Chicago, and immediately the Cubs win the World Series for the first
time in a hundred years. They move to Massachusetts, where the Patriots
win the Super Bowl again, and again, and again. And finally, they move
to Toronto, where the Raptors immediately win the World Championship.
Should a city pay them to move, or could municipal funds be better spent
elsewhere?

One way to get at the answer would be to run an experiment. Make a list
of the North American cities with major sports teams, and then roll a
dice and send them to live there for a year. With enough lifetimes, we
could work it out. The fundamental issue is that we cannot both live in
a city and not live in a city. This is the fundamental problem of causal
inference: a person cannot be both treated and untreated. Experiments
and randomized controlled trials are circumstances in which we try to
randomly allocate some treatment, to have a belief that everything else
was the same (or at least ignorable). The framework that we use to
formalize the situation is the Neyman-Rubin model (Holland 1986).

A treatment, \(t\), will often be a binary variable that is either 0 or
1. It is 0 if the person, \(i\), is not treated, which is to say they
are in the control group, and 1 if they are treated. We will typically
have some outcome, \(Y_i\), of interest for that person, and that could
be binary, multinomial, or continuous. For instance, it could be vote
choice, in which case we could measure whether the person is:
`Conservative' or `Not Conservative'; which party they support, say:
`Conservative', `Liberal', `Democratic', `Green'; or maybe a probability
of support.

A treatment is causal if \((Y_i|t=0) \neq (Y_i|t=1)\). That is to say,
that the outcome for person \(i\), given they were not treated, is
different to their outcome given they were treated. If we could both
treat and control the one individual at the one time, then we would know
that it was only the treatment that had caused any change in outcome, as
there is no other factor that could explain it. But the fundamental
problem of causal inference is that we cannot both treat and control the
one individual at the one time. So when we want to know the effect of
the treatment, we need to compare it with a counterfactual. The
counterfactual is what would have happened if the individual were not
treated. As it turns out, this means one way to think of causal
inference is as a missing data problem, where we are missing the
counterfactual.

As we cannot compared treatment and control in one individual, we
instead compare the average of two groups---all those treated and all
those not. We are looking to estimate the counterfactual at a group
level because of the impossibility of doing it at an individual level.
Making this trade-off allows us to move forward but comes at the cost of
certainty. We must instead rely on randomization, probabilities, and
expectations.

We usually consider a default of there being no effect and we look for
evidence that would cause us to change our mind. As we are interested in
what is happening in groups, we turn to expectations, and notions of
probability to express ourselves. Hence, we will make claims that talk,
on average. Maybe wearing fun socks really does make you have a lucky
day, but on average, across the group, it is probably not the case.

It is worth pointing out that we do not just have to be interested in
the average effect. We may consider the median, or variance, or
whatever. Nonetheless, if we were interested in the average effect, then
one way to proceed would be to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  divide the dataset into two---treated and not treated---and have a
  binary effect column;
\item
  sum the column, then divide it by the length of the column; and
\item
  then look at the ratio.
\end{enumerate}

This is an estimator, touched on in Chapter @ref(on-writing), which is a
way of putting together a guess of something of interest. The estimand
is the thing of interest, in this case the average effect, and the
estimate is whatever our guess turns out to be. We can simulate data to
illustrate the situation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{treatment\_control }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{binary\_effect =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}

\NormalTok{treatment\_control}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 1
   binary_effect
           <dbl>
 1             0
 2             1
 3             1
 4             0
 5             0
 6             0
 7             0
 8             0
 9             1
10             1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estimate }\OtherTok{\textless{}{-}}
  \FunctionTok{sum}\NormalTok{(treatment\_control}\SpecialCharTok{$}\NormalTok{binary\_effect) }\SpecialCharTok{/} \FunctionTok{length}\NormalTok{(treatment\_control}\SpecialCharTok{$}\NormalTok{binary\_effect)}
\NormalTok{estimate}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.4
\end{verbatim}

More broadly, to tell causal stories we need to bring together both
theory and a detailed knowledge of what we are interested in (Cunningham
2021, 4). In Chapter @ref(gather-data) we discussed gathering data that
we observed about the world. In this chapter we are going to be more
active about turning the world into the data that we need. As the
researcher we will decide what to measure and how, and we will need to
define what we are interested in and what we are not. We will be active
participants in the data generating process. That is, if we want to use
this data, then as researchers we must go out and hunt it, if you like.

\hypertarget{randomization}{%
\subsection{Randomization}\label{randomization}}

Correlation can be enough in some settings, but in order to be able to
make forecasts when things change, and the circumstances are slightly
different we need to understand causation. The key is the
counterfactual: what would have happened in the absence of the
treatment. Ideally, we could keep everything else constant, randomly
divide the world into two groups, and treat one and not the other. Then
we can be pretty confident that any difference between the two groups is
due to that treatment. The reason for this is that if we have some
population and we randomly select two groups from it, then our two
groups (provided they are both big enough) should have the same
characteristics as the population. Randomized controlled trials (RCTs)
and A/B testing attempt to get us as close to this `gold standard' as we
can hope. When we, and others such as Athey and Imbens (2017), use
language like gold standard to refer to these approaches, we do not mean
to imply that they are perfect. Just that they can be better than most
of the other options.

What we hope to be able to do is to find treatment and control groups
that are the same, but for the treatment. This means that establishing
the control group is critical because when we do that, we establish the
counterfactual. We might be worried about, say, underlying trends, which
is one issue with a before-and-after comparison, or selection bias,
which could occur when we allow self-selection. Either of these issues
could result in biased estimators. We use randomization to go some way
to addressing these.

To get started, we simulate a population, and then randomly sample from
it. We will set it up so that half the population likes blue, and the
other half likes white. And further, if someone likes blue then they
almost surely prefer dogs, but if they like white then they almost
surely prefer cats. The approach of heavily using simulation is a
critical part of the workflow advocated in this book. This is because we
know roughly what the outcomes should be from the analysis of simulated
data. Whereas if we go straight to analyzing the real data then we do
not know if unexpected outcomes are due to our own analysis errors, or
actual results. Another good reason it is useful to take this approach
of simulation is that when you are working in teams the analysis can get
started before the data collection and cleaning is completed. That
simulation will also help the collection and cleaning team think about
tests they should run on their data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_people }\OtherTok{\textless{}{-}} \DecValTok{5000}

\NormalTok{population }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{person =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{number\_of\_people),}
    \AttributeTok{favorite\_color =} \FunctionTok{sample}\NormalTok{(}
      \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Blue"}\NormalTok{, }\StringTok{"White"}\NormalTok{),}
      \AttributeTok{size  =}\NormalTok{ number\_of\_people,}
      \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{prefers\_dogs\_to\_cats =}
      \FunctionTok{if\_else}\NormalTok{(favorite\_color }\SpecialCharTok{==} \StringTok{"Blue"}\NormalTok{, }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{),}
    \AttributeTok{noise =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{),}
    \AttributeTok{prefers\_dogs\_to\_cats =}
      \FunctionTok{if\_else}\NormalTok{(}
\NormalTok{        noise }\SpecialCharTok{\textless{}=} \DecValTok{8}\NormalTok{,}
\NormalTok{        prefers\_dogs\_to\_cats,}
        \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{), }
               \AttributeTok{size =} \DecValTok{1}\NormalTok{)}
\NormalTok{        )}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{noise)}


\NormalTok{population}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5,000 x 3
   person favorite_color prefers_dogs_to_cats
    <int> <chr>          <chr>               
 1      1 Blue           Yes                 
 2      2 White          No                  
 3      3 White          No                  
 4      4 Blue           Yes                 
 5      5 Blue           Yes                 
 6      6 Blue           Yes                 
 7      7 Blue           Yes                 
 8      8 Blue           Yes                 
 9      9 White          No                  
10     10 White          No                  
# ... with 4,990 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(favorite\_color) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 2
# Groups:   favorite_color [2]
  favorite_color     n
  <chr>          <int>
1 Blue            2547
2 White           2453
\end{verbatim}

We will now construct a frame, assuming that we have a frame that
contains 80 per cent of the population.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{frame }\OtherTok{\textless{}{-}}
\NormalTok{  population }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{in\_frame =} \FunctionTok{sample}\NormalTok{(}
      \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
      \AttributeTok{size  =}\NormalTok{ number\_of\_people,}
      \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{,}
      \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.8}\NormalTok{)}
\NormalTok{  )) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(in\_frame }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}

\NormalTok{frame }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(favorite\_color) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 2
# Groups:   favorite_color [2]
  favorite_color     n
  <chr>          <int>
1 Blue            2023
2 White           1980
\end{verbatim}

For now, we will set aside dog or cat preferences and focus on creating
treatment and control groups on the basis of favorite color only.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{sample }\OtherTok{\textless{}{-}}
\NormalTok{  frame }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{prefers\_dogs\_to\_cats) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{group =} \FunctionTok{sample}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Treatment"}\NormalTok{, }\StringTok{"Control"}\NormalTok{),}
    \AttributeTok{size  =} \FunctionTok{nrow}\NormalTok{(frame),}
    \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{  ))}
\end{Highlighting}
\end{Shaded}

When we look at the mean for the two groups, we can see that the
proportions that prefer blue or white are very similar to what we
specified (\textbf{?@tbl-bluetowhite}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(group, favorite\_color) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(group) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop =}\NormalTok{ n }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(n)) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Group"}\NormalTok{, }\StringTok{"Preferred color"}\NormalTok{, }\StringTok{"Number"}\NormalTok{, }\StringTok{"Proportion"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{linesep =} \StringTok{""}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-tfrforthewin}{}
\begin{longtable}[]{@{}llrr@{}}
\caption{\label{tbl-tfrforthewin}Proportion of the treatment and control
group that prefer blue or white }\tabularnewline
\toprule()
Group & Preferred color & Number & Proportion \\
\midrule()
\endfirsthead
\toprule()
Group & Preferred color & Number & Proportion \\
\midrule()
\endhead
Control & Blue & 987 & 0.50 \\
Control & White & 997 & 0.50 \\
Treatment & Blue & 1036 & 0.51 \\
Treatment & White & 983 & 0.49 \\
\bottomrule()
\end{longtable}

We randomized based on favorite color. But we should also find that we
took dog or cat preferences along at the same time and will have a
`representative' share of people who prefer dogs to cats. Why should
that happen when we have not randomized on these variables? Let's start
by looking at our dataset (Table~\ref{tbl-dogstocats}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{left\_join}\NormalTok{(frame }\SpecialCharTok{|\textgreater{}} \FunctionTok{select}\NormalTok{(person, prefers\_dogs\_to\_cats), }
            \AttributeTok{by =} \StringTok{"person"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(group, prefers\_dogs\_to\_cats) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(group) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop =}\NormalTok{ n }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(n)) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Group"}\NormalTok{, }\StringTok{"Prefers dogs to cats"}\NormalTok{, }\StringTok{"Number"}\NormalTok{, }\StringTok{"Proportion"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{linesep =} \StringTok{""}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-dogstocats}{}
\begin{longtable}[]{@{}llrr@{}}
\caption{\label{tbl-dogstocats}Proportion of the treatment and control
group that prefer dogs or cats}\tabularnewline
\toprule()
Group & Prefers dogs to cats & Number & Proportion \\
\midrule()
\endfirsthead
\toprule()
Group & Prefers dogs to cats & Number & Proportion \\
\midrule()
\endhead
Control & No & 997 & 0.50 \\
Control & Yes & 987 & 0.50 \\
Treatment & No & 983 & 0.49 \\
Treatment & Yes & 1036 & 0.51 \\
\bottomrule()
\end{longtable}

It is exciting to have a representative share on `unobservables'. In
this case, we do `observe' them---to illustrate the point---but we did
not select on them. We get this because the variables were correlated.
But it will break down in several ways that we will discuss. It also
assumes large enough groups. For instance, if we considered specific dog
breeds, instead of dogs as an entity, we may not find ourselves in this
situation. To check that the two groups are the same we look to see if
we can identify a difference between the two groups based on
observables. In this case we looked at the mean, but we could look at
other aspects as well.

This all brings us to Analysis of Variation (ANOVA). ANOVA was
introduced by Fisher while he was working on statistical problems in
agriculture. This is less unexpected than it may seem as historically
agricultural research has been closely tied to statistical innovation.
We mention ANOVA here because of its importance historically, but it is
a variant of linear regression which we cover in some detail in Chapter
@ref(ijalm). Further, in general, we would usually not use ANOVA
day-to-day. There is nothing wrong with it in the right circumstances.
But it is more than a hundred years old and the number of modern
use-case where it is the best option is small.

In any case, we approach ANOVA with the expectation that the groups are
from the same distribution and could conduct it using \texttt{aov()}. In
this case, we would fail to reject our default hypothesis that the
samples are the same.

\hypertarget{treatment-and-control}{%
\subsection{Treatment and control}\label{treatment-and-control}}

If the treated and control groups are the same in all ways and remain
that way, but for the treatment, then we have internal validity, which
is to say that our control will work as a counterfactual and our results
can speak to a difference between the groups in that study. Internal
validity means that our estimates of the effect of the treatment are
speaking to the treatment and not some other aspect. They mean that we
can use our results to make claims about what happened in the
experiment.

If the group to which we applied our randomization were representative
of the broader population, and the experimental set-up were fairly
similar to outside conditions, then we further could have external
validity. That would mean that the difference we find does not just
apply in our own experiment, but also in the broader population.
External validity means that we can use our experiment to make claims
about what would happen outside the experiment. It is randomization that
has allowed that to happen.

But this means we need randomization twice. Firstly, into the group that
was subject to the experiment, and then secondly, between treatment and
control. How do we think about this randomization, and to what extent
does it matter?

We are interested in the effect of being treated. It may be that we
charge different prices, which would be a continuous treatment variable,
or that we compare different colors on a website, which would be a
discrete treatment variable. Either way, we need to make sure that all
the groups are otherwise the same. How can we be convinced of this? One
way is to ignore the treatment variable and to examine all other
variables, looking for whether we can detect a difference between the
groups based on any other variables. For instance, if we are conducting
an experiment on a website, then are the groups roughly similar in terms
of, say:

\begin{itemize}
\tightlist
\item
  Microsoft and Apple users?
\item
  Safari, Chrome, and Firefox users?
\item
  Mobile and desktop users?
\item
  Users from certain locations?
\end{itemize}

Further, are the groups representative of the broader population? These
are all threats to the validity of our claims.

But if done properly, that is if the treatment is truly independent,
then we can estimate the average treatment effect (ATE). In a binary
treatment variable setting this is:

\[\mbox{ATE} = \mathbb{E}[Y|t=1] - \mathbb{E}[Y|t=0].\]

That is, the difference between the treated group, \(t = 1\), and the
control group, \(t = 0\), when measured by the expected value of the
outcome, \(Y\). The ATE becomes the difference between the two
expectations.

To illustrate this concept, we first simulate some data that shows a
difference of one between the treatment and control groups.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{ate\_example }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{person =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{),}
                      \AttributeTok{was\_treated =} \FunctionTok{sample}\NormalTok{(}
                        \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{),}
                        \AttributeTok{size  =} \DecValTok{1000}\NormalTok{,}
                        \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{                      ))}

\CommentTok{\# Make outcome a bit more likely if treated.}
\NormalTok{ate\_example }\OtherTok{\textless{}{-}}
\NormalTok{  ate\_example }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{outcome =} \FunctionTok{if\_else}\NormalTok{(}
\NormalTok{    was\_treated }\SpecialCharTok{==} \StringTok{"No"}\NormalTok{,}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{6}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{  ))}
\end{Highlighting}
\end{Shaded}

We can see the difference, which we simulated to be one, between the two
groups in Figure (Figure~\ref{fig-exampleatefig}). And we can compute
the average between the groups and then the difference to see also that
we get back the result that we put in (Table~\ref{tbl-exampleatetable}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ate\_example }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ outcome,}
             \AttributeTok{fill =}\NormalTok{ was\_treated)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{,}
                 \AttributeTok{binwidth =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Outcome"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of people"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Person was treated"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./10-hunt_files/figure-pdf/fig-exampleatefig-1.pdf}

}

\caption{\label{fig-exampleatefig}Simulated data showing a difference
between the treatment and control group}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ate\_example }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(was\_treated) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(outcome)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ was\_treated, }\AttributeTok{values\_from =}\NormalTok{ mean) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{difference =}\NormalTok{ Yes }\SpecialCharTok{{-}}\NormalTok{ No) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
    \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Average for treated"}\NormalTok{, }\StringTok{"Average for not treated"}\NormalTok{, }\StringTok{"Difference"}\NormalTok{),}
    \AttributeTok{digits =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{linesep =} \StringTok{""}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-exampleatetable}{}
\begin{longtable}[]{@{}rrr@{}}
\caption{\label{tbl-exampleatetable}Average difference between the
treatment and control groups for data simulated to have an average
difference of one}\tabularnewline
\toprule()
Average for treated & Average for not treated & Difference \\
\midrule()
\endfirsthead
\toprule()
Average for treated & Average for not treated & Difference \\
\midrule()
\endhead
5 & 6.06 & 1.06 \\
\bottomrule()
\end{longtable}

Unfortunately, there is often a difference between simulated data and
reality. For instance, an experiment cannot run for too long otherwise
people may be treated many times, or become inured to the treatment; but
it cannot be too short otherwise we cannot measure longer term outcomes.
We cannot have a `representative' sample across every facet of a
population, but if not, then the treatment and control will be
different. Practical difficulties may make it difficult to follow up
with certain groups and so we end up with a biased collection. Some
questions to explore when working with real experimental data include:

\begin{itemize}
\tightlist
\item
  How are the participants being selected into the frame for
  consideration?
\item
  How are they being selected for treatment? We would hope this is being
  done randomly, but this term is applied to a variety of situations.
  Additionally, early `success' can lead to pressure to treat everyone,
  especially in medical settings.
\item
  How is treatment being assessed?
\item
  To what extent is random allocation ethical and fair? Some argue that
  shortages mean it is reasonable to randomly allocate, but that may
  depend on how linear the benefits are. It may also be difficult to
  establish definitions, and the power imbalance between those making
  these decisions and those being treated should be considered.
\end{itemize}

Bias and other issues are not the end of the world. But we need to think
about them carefully. In a well-known example, Abraham Wald, the
twentieth century Hungarian mathematician, was given data on the planes
that came back to Britain after being shot at in WW2. The question was
where to place the armor. One option was to place it over the bullet
holes. Wald recognized that there is a selection effect here---these are
the planes that made it back to be examined---so those holes did not
necessarily need the armor. Arguably armor would be better placed where
there were no bullet holes.

For instance, how would the results of a survey about the difficulty of
a university course differ if only students who completed the course
were surveyed, and not those who dropped out? While we should work to
try to make our dataset as good as possible, it may be possible to use
the model to control for some of the bias. For instance, if there was a
variable that was correlated with say, attrition, then it could be added
to the model either by-itself, or as an interaction. Similarly, if there
was correlation between the individuals. For instance, if there was some
`hidden variable' that we did not know about that meant some individuals
were correlated, then we could use `wider' standard errors. This needs
to be done carefully and we discuss this further in Chapter
@ref(causality). That said, if such issues can be anticipated, then it
can be better to change the experiment. For instance, perhaps it would
be possibly to stratify by that hidden variable.

\hypertarget{fishers-tea-party}{%
\subsection{Fisher's tea party}\label{fishers-tea-party}}

Fisher introduced an experiment designed to see if a person can
distinguish between a cup of tea where the milk was added first, or
last. We begin by preparing eight cups of tea: four with milk added
first and the other four with milk added last. We then randomize the
order of all eight cups. We tell the taster, whom we will call `Ian',
about the experimental set-up: there are eight cups of tea, four of each
type, he will be given cups of tea in a random order, and the task is to
group them into two groups.

One of the nice aspects of this experiment is that we can do it
ourselves. There are a few things to be careful of in practice,
including that: the quantities of milk and tea and consistent; the
groups are marked in some way that the taster cannot see; and the order
is randomized.

Another nice aspect of this experiment is that we can calculate the
chance that Ian is able to randomly get the groupings correct. To decide
if his groupings were likely to have occurred at random, we need to
calculate the probability this could happen. First, we count the number
of successes out of the four that were chosen. R. Fisher (1935, 14) says
there are: \({8 \choose 4} = \frac{8!}{4!(8-4)!}=70\) possible outcomes.

We are asking Ian to group the cups, not to identify which is which, and
so there are two ways for him to be perfectly correct. He could either
correctly identify all the ones that were milk-first (one outcome out of
70) or correctly identify all the ones that were tea-first (one outcome
out of 70). This means the probability of this event is:
\(\frac{2}{70} \approx 0.028\) or about 3 per cent.

As R. Fisher (1935, 15) makes clear, this now becomes a judgement call.
We need to consider the weight of evidence that we require before we
accept the groupings did not occur by chance and that Ian was well-aware
of what he was doing. We need to decide what evidence it takes for us to
be convinced. If there is no possible evidence that would dissuade us
from the view that we held coming into the experiment, say, that there
is no difference between milk-first and tea-first, then what is the
point of doing an experiment? We would expect that if Ian got it
completely right, then most would accept that he was able to tell the
difference.

What if he is almost perfect? By chance, there are 16 ways for a person
to be `off-by-one'. Either Ian thinks there was one cup that was
milk-first when it was tea-first---there are, \({4 \choose 1} = 4\),
four ways this could happen---or he thinks there was one cup that was
tea-first when it was milk-first---again, there are, \({4 \choose 1}\) =
4, four ways this could happen. These outcomes are independent, so the
probability is \(\frac{4\times 4}{70} \approx 0.228\). And so on. Given
there is an almost 23 per cent chance of being off-by-one just be
randomly grouping the teacups, this outcome probably would not convince
us that Ian can tell the difference between tea-first and milk-first.

What we are looking for, in order to claim something is experimentally
demonstrable is the results of not just it being shown once, but instead
we know the features of an experiment where such a result is reliably
found (R. Fisher 1935, 16). We are looking to thoroughly interrogate our
data and our experiments, and to think precisely about the analysis
methods we are using. Rather than searching for meaning in
constellations of stars, we want to make it as easy as possible for
others to reproduce our work. It is only in that way that our
conclusions stand a chance of holding up in the long-term.

\hypertarget{informed-consent-and-the-need-for-an-experiment}{%
\subsection{Informed consent and the need for an
experiment}\label{informed-consent-and-the-need-for-an-experiment}}

One of the foundations of ethical experimental practice is informed
consent and ensuring that an experiment is actually needed. We will now
detail two cases where human life was potentially lost due to these
issues. One issue with experiments in medical settings is that the
weight of evidence is measured in lost lives. Ethical practice in
experiments develops because of the many people who may have
unnecessarily lost their life due to experiments. Two cases that have
dramatically informed practice are the Tuskegee Syphilis Study and ECMO.

The Tuskegee Syphilis Study is an infamous medical trial that began in
1932. As part of this experiment 400 Black Americans with syphilis, and
a control group without, were not given appropriate treatment, nor even
told they had syphilis (in the case of the treatment group), well after
a standard treatment for syphilis was established and widely available
sometime between the mid-1940s and early 1950s (Brandt 1978; Alsan and
Wanamaker 2018). Like the treatment group, the control group were also
given non-effective drugs. These financially-poor Black Americans in the
US South were identified and offered compensation including `hot meals,
the guise of treatment, and burial payments' (Alsan and Wanamaker 2018).
The men were not actually treated for syphilis (Brandt 1978; Alsan and
Wanamaker 2018). The men were not told they were part of an experiment
(Brandt 1978; Alsan and Wanamaker 2018). Further, extensive work was
undertaken to ensure the men would not receive treatment from anywhere
including writing to local doctors, the local health department, and,
incredibly, after some of the men were drafted and told to immediately
get treatment, the draft board complied with a request to have the men
excluded from treatment (Brandt 1978, 25). By the time the study was
stopped in 1972, more than half of the men were deceased and many of
deaths were from syphilis-related causes (Alsan and Wanamaker 2018).

The effect of the Tuskegee Syphilis Study was felt not just by the men
in the study, but more broadly. Alsan and Wanamaker (2018) found that it
is associated with a decrease in life expectancy at age 45 of up to 1.5
years for Black men. In response the US established requirements for
Institutional Review Boards and President Clinton made a formal apology
in 1997. Brandt (1978, 27) says:

\begin{quote}
In retrospect the Tuskegee Study revealed more about the pathology of
racism than the pathology of syphilis; more about the nature of
scientific inquiry than the nature of the disease process\ldots{}
{[}T{]}he notion that science is a value-free discipline must be
rejected. The need for greater vigilance in assessing the specific ways
in which social values and attitudes affect professional behavior is
clearly indicated.
\end{quote}

Turning to the evaluation of extracorporeal membrane oxygenation (ECMO),
J. H. Ware (1989) describes how they viewed ECMO as a possible treatment
for persistent pulmonary hypertension in newborns (PPHN). They enrolled
19 patients and used conventional medical therapy on ten of them, and
ECMO on nine of them. It was found that six of the ten in the control
group survived while all in the treatment group survived. J. H. Ware
(1989) used randomized consent whereby only the parents of infants
randomly selected to be treated with ECMO were asked to consent.

J. H. Ware (1989) are concerned with `equipoise', by which they refer to
a situation in which there is genuine uncertainty about whether the
treatment is more effective than existing procedures. They further note
that in medical settings even if there is initial equipoise it could be
undermined if the treatment is found to be effective early in the study.
J. H. Ware (1989) describe how after the results of these first 19
patients, randomization stopped and only ECMO was used. The recruiters
and those treating the patients were initially not told that
randomization had stopped. It was decided that this complete allocation
to ECMO would continue `until either the 28th survivor or the 4th death
was observed'. After 19 of 20 additional patients survived ECMO the
trial was terminated. So, the actual result of the experiment was
divided into two phases: in the first there was randomized use of ECMO,
and in the second only ECMO was used.

One approach in these settings is a `randomized play-the-winner' rule
following Wei and Durham (1978). Treatment is still randomized, but the
weight of probability shifts with each successful treatment to make
treatment more likely and there is some stopping rule. Berry (1989)
argues that the stopping rule in the case of J. H. Ware (1989) occurred
before the study started and that there was no need for the study at all
because equipoise never existed. Berry (1989) re-visit the literature
mentioned by J. H. Ware (1989) and find extensive evidence that ECMO was
known to be effective. Berry (1989) points out that there is almost
never complete consensus and so one could almost always argue for the
existence of equipoise even in the face of a substantial weight of
evidence. Berry (1989) further criticizes J. H. Ware (1989) for the use
of randomized consent because of the potential that there may have been
different outcomes for the infants subject to conventional therapy had
their parents known there were other options. And instead, Berry (1989)
argues for the need for comprehensive patient registries, enabling the
analysis of large datasets.

While the Tuskegee Syphilis Study and ECMO may seem quite far from our
present circumstances, Dr Monica Alexander, Assistant Professor,
University of Toronto explains that while it may be illegal to do this
exact research these days, it does not mean that unethical research does
not still happen. We see it all the time in machine learning
applications in health and other areas. While we are not meant to
explicitly discriminate and we are meant to get consent, it does not
mean that we cannot implicitly discriminate without any type of buy-in
at all. For instance, Obermeyer et al. (2019) describes how US health
care systems use algorithms to score the severity of how sick a patient
is. They show that for the same score, `Black patients are considerably
sicker than White patients, as evidenced by signs of uncontrolled
illnesses' and that if Black patients were scored in the same way as
White patients, then they would receive considerably more help than they
do now. They find that the discrimination occurs because the algorithm
is based on health care costs, rather than sickness. But because access
to healthcare is unequally distributed between Black and White patients,
the algorithm, however inadvertently, perpetuates racial bias.

\hypertarget{case-study-the-oregon-health-insurance-experiment}{%
\subsection{Case study: The Oregon Health Insurance
Experiment}\label{case-study-the-oregon-health-insurance-experiment}}

In the US, unlike many developed countries, basic health insurance is
not necessarily available to all residents even those on low incomes.
The Oregon Health Insurance Experiment involved low-income adults in
Oregon, a state in the north-west of the US, from 2008 to 2010
(Finkelstein et al. 2012).

Oregon funded 10,000 places in the state-run Medicaid program, which
provides health insurance for people with low incomes. A lottery was
used to allocate these places and was judged fair because it was
expected, correctly as it turned out, that demand for places would
exceed the supply. People had a month to sign up to enter the draw. Then
a lottery was used to determine which of the 89,824 individuals who
signed up would be allowed to apply for Medicaid.

The draws were conducted over a six-month period and those who were
selected had the opportunity to sign up. 35,169 individuals were
selected (the household of those who actually won the draw was given the
opportunity) but only 30 per cent of them completed the paperwork and
were eligible (typically they earned too much). The insurance lasted
indefinitely. This random allocation of insurance allowed the
researchers to understand the effect of health insurance.

The reason that this random allocation is important is that it is not
usually possible to compare those with and without insurance because the
type of people that sign up to get health insurance differ to those who
do not. That decision is `confounded' with other variables and results
in a selection effect.

As the opportunity to apply for health insurance was randomly allocated,
the researchers were able to evaluate the health and earnings of those
who received health insurance and compare them to those who did not. To
do this they used administrative data, such as hospital discharge data,
credit reports that were matched to 68.5 per cent of lottery
participants, and mortality records, which will be uncommon.
Interestingly this collection of data is fairly restrained and so they
included a survey conducted via mail.

The specifics of this are not important, and we will have more to say in
Chapter @ref(ijalm), but they use a statistical model,
\textbf{?@eq-oregon}, to analyze the results (Finkelstein et al. 2012):

\begin{equation}
y_{ihj} = \beta_0 + \beta_1\mbox{Lottery} + X_{ih}\beta+2 + V_{ih}\beta_3 + \epsilon_{ihj} {#eq-oregon}
\end{equation}

Equation @ref(eq:oregon) explains various \(j\) outcomes (such as
health) for an individual \(i\) in household \(h\) as a function of an
indicator variable as to whether household \(h\) was selected by the
lottery. Hence, `(t)he coefficient on Lottery, \(\beta_1\), is the main
coefficient of interest, and gives the average difference in (adjusted)
means between the treatment group (the lottery winners) and the control
group (those not selected by the lottery).'

To complete the specification of Equation @ref(eq:oregon), \(X_{ih}\) is
a set of variables that are correlated with the probability of being
treated. These adjust for that impact to a certain extent. An example of
that is the number of individuals in a household. And finally,
\(V_{ih}\) is a set of variables that are not correlated with the
lottery. These variables include demographics, hospital discharge and
lottery draw.

As has been found in earlier studies such as Brook et al. (1984),
Finkelstein et al. (2012) found that, the treatment group was 25 per
cent more likely to have insurance than the control group. The treatment
group used more health care including both primary and preventive care
as well as hospitalizations but had lower out-of-pocket medical
expenditures. More generally, the treatment group reported better
physical and mental health.

\hypertarget{ab-testing}{%
\section{A/B testing}\label{ab-testing}}

The past decade has probably seen the most experiments ever run by
several orders of magnitude with the extensive use of A/B testing on
websites. Large tech companies typically have extensive infrastructure
for these experiments, and they term them A/B tests because of the
comparison of two groups: one that gets treatment A and the other that
either gets treatment B or does not see any change (M. Salganik 2018,
185). Every time you are online you are probably subject to tens,
hundreds, or potentially thousands, of different A/B tests. If you use
apps like TikTok then this could run to the tens of thousands. While, at
their heart, they are still just surveys that result in data that need
to be analysed, they have several interesting features that we will
discuss.

For instance, Kohavi, Tang, and Xu (2020, 3) discusses the example of
Microsoft's search engine Bing where they increased the amount of
content displayed by their ads. The change triggered an alert that
usually signaled a bug in the billing. But there was no bug, it was
instead the case that revenue would increase by 12 per cent, or around
\$100 million annually in the US, without any significant trade-off
being measured.

We use the term A/B test to strictly refer to the situation in which we
are primarily implementing an experiment through a technology stack
about something that is primarily of the internet, for instance a change
to a website or similar. While at their heart they are just experiments,
A/B testing has a range of specific concerns. There is something
different about doing tens of thousands of small experiments all the
time, compared with our normal experimental set-up of conducting one
experiment over the course of months. Additionally, tech firms have such
distinct cultures that it can be difficult to shift toward an
experimental set-up. Sometimes it can be easier to experiment by not
delivering, or delaying, a change that has been decided to create a
control group rather than a treatment group (M. Salganik 2018, 188).
Often the most difficult aspect of A/B testing and conducting
experiments more generally, is not the statistics, it's the politics.

The first aspect of concern is the delivery of the A/B test (Kohavi,
Tang, and Xu 2020, 153--61). In the case of an experiment, it is usually
clear how it is being delivered. For instance, we may have the person
come to a doctor's clinic and then inject them with either a drug or a
placebo. But in the case of A/B testing, it is less obvious. For
instance, should it be run `server-side', meaning to make a change to a
website, or `client-side', meaning to change an app. This decision
affects our ability to both conduct the experiment and to gather data
from it.

In the case of the effect of conducting the experiment, it is relatively
easy and normal to update a website all the time. This means that small
changes can be easily implemented if the experiment is conducted
server-side. But in the case of a client-side implementation of an app,
then conducting an experiment becomes a bigger deal. For instance, the
release may need to go through an app store, and this usually does not
happen all the time. Instead, it would need to be part of a regular
release cycle. There is also a selection concern because some users will
not update their app and there is the possibility that they are
different to those that do regularly update the app.

Turning to the effect of the delivery decision on our ability to gather
data from the experiment. Again, server-side is less of a big deal
because we get the data anyway as part of the user interacting with the
website. But in the case of an app, the user may use the app offline or
with limited data upload, which then requires a data transmission
protocol or caching, but this then could affect user experience,
especially as some phones place limits are various aspects.

The effect of all this is that we need to plan. For instance, results
are unlikely to be available the day after a change to an app, whereas
they are likely available the day after a change to a website. Further,
we may need to consider our results in the context of different devices
and platforms, potentially using, say, multilevel regression which will
be covered in Chapter @ref(ijalm).

The second aspect of concern is `instrumentation' or the method of
measurement (Kohavi, Tang, and Xu 2020, 162 - 165). When we conduct a
traditional experiment then we might, for instance, ask respondents to
fill out a survey. But this is usually not done with A/B testing. One
approach is to put a cookie on the user's device, but different users
will clear these at different rates. Another approach is to use a
beacon, such as forcing the user to download a tiny image from our
server, so that we know when they have completed some action. For
instance, this is a commonly used approach to know when a user has
opened an email. There are practical concerns around when the beacon
loads, for instance, if it is before the main content loads then the
user experience may be worse, but if it is after then our sample may be
biased.

The third aspect of concern is what are we randomizing over (Kohavi,
Tang, and Xu 2020, 162 - 165). In the case of traditional experiments,
this is usually clear and it is often a person, but sometimes various
groups of people. But in the case of A/B testing it can be less clear.
For instance, are we randomizing over the page, the session, or the
user?

To think about this, let us consider color. For instance, say we are
interested in whether we should change our logo from red to blue on the
homepage. If we are randomizing at the page level, then if the user goes
to some other page of our website, and then back to the homepage the
logo could be back to red. If we are randomizing at the session level,
then while it could be blue while they use the website this time, if
they close it and come back then it could be red. Finally, if we are
randomizing at a user level then possibly it would always be red for one
used, but always blue for another.

The extent to which this matters depends on a trade-off between
consistency and importance. For instance, if we are A/B testing product
prices then consistency is likely a feature. But if we are A/B testing
background colors then consistency might not be as important. On the
other hand, if we are A/B testing the position of a log-in button then
it might be important that we not move that around too much for the one
user, but between users it might matter less.

Interestingly, in A/B testing, as in traditional experiments, we are
concerned that our treatment and control groups are the same, but for
the treatment. In the case of traditional experiments, we satisfy
ourselves of this by making conducting analysis on the basis of the data
that we have after the experiment is conducted. That is usually all we
can do because it would be weird to treat or control both groups. But in
the case of A/B testing, the pace of experimentation allows us to
randomly create the treatment and control groups, and then check, before
we subject the treatment group to the treatment, that the groups are the
same. For instance, if we were to show each group the same website, then
we would expect the same outcomes across the two groups. If we found
different outcomes then we would know that we may have a randomization
issue (Taddy 2019, 129).

One of the interesting aspects of A/B testing is that we are usually
running them not because we desperately care about the specific outcome,
but because that feeds into some other measure that we care about. For
instance, do we care whether the website is quite-dark-blue or
slightly-dark-blue? Probably not, but we probably care a lot about the
company share price. But then what if picking the best blue comes at a
cost to the share price? That example is a bit contrived, so let us
pretend that we work at a food delivery app and we are concerned with
driver retention. Say we do some A/B tests and find that drivers are
always more likely to be retained when they can deliver food to the
customer faster. Our finding is that faster is better, for driver
retention, always. But one way to achieve faster deliveries, is for them
to not put the food into a hot box that would maintain the food's
temperature. Something like that might save 30 seconds, which is
significant on a 10-15 minute delivery. Unfortunately, although we would
decide to encourage that that on the basis of A/B tests designed to
optimize driver-retention, such a decision would likely make the
customer experience worse. If customers receive cold food, when it is
meant to be hot, then they may stop using the app, which would
ultimately be very bad for the business.

This trade-off may be obvious when we run the driver experiment if we
were to look at customer complaints. It is possible that on a small team
we would be exposed to those tickets, but on a larger team we may not
be. Ensuring that A/B tests are not resulting in false optimization is
especially important. And not something that we typically have to worry
about in normal experiments.

\hypertarget{case-study-upworthy}{%
\subsection{Case study: Upworthy}\label{case-study-upworthy}}

The trouble with much of A/B testing is that it is done by firms and so
we typically do not have datasets that we can use. But Matias et al.
(2019) provide access to a dataset of A/B tests from Upworthy, a
clickbait media website that used A/B testing to optimize their content.
Fitts (2014) provides more background information about Upworthy. And
the datasets of A/B tests are available: https://osf.io/jd64p/.

We can look at what the dataset looks like, and get a sense for it by
looking at the names and an extract.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{upworthy }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://osf.io/vy8mj/download"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{upworthy }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "...1"                 "created_at"           "updated_at"          
 [4] "clickability_test_id" "excerpt"              "headline"            
 [7] "lede"                 "slug"                 "eyecatcher_id"       
[10] "impressions"          "clicks"               "significance"        
[13] "first_place"          "winner"               "share_text"          
[16] "square"               "test_week"           
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{upworthy }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 17
   ...1 created_at          updated_at          clickability_test_id     excerpt
  <dbl> <dttm>              <dttm>              <chr>                    <chr>  
1     0 2014-11-20 06:43:16 2016-04-02 16:33:38 546d88fb84ad38b2ce000024 Things~
2     1 2014-11-20 06:43:44 2016-04-02 16:25:54 546d88fb84ad38b2ce000024 Things~
3     2 2014-11-20 06:44:59 2016-04-02 16:25:54 546d88fb84ad38b2ce000024 Things~
4     3 2014-11-20 06:54:36 2016-04-02 16:25:54 546d902c26714c6c44000039 Things~
5     4 2014-11-20 06:54:57 2016-04-02 16:31:45 546d902c26714c6c44000039 Things~
6     5 2014-11-20 06:55:07 2016-04-02 16:25:54 546d902c26714c6c44000039 Things~
# ... with 12 more variables: headline <chr>, lede <chr>, slug <chr>,
#   eyecatcher_id <chr>, impressions <dbl>, clicks <dbl>, significance <dbl>,
#   first_place <lgl>, winner <lgl>, share_text <chr>, square <chr>,
#   test_week <dbl>
\end{verbatim}

It is also useful to look at the documentation for the dataset. This
describes the structure of the dataset, which is that there are packages
within tests. A package is a collection of headlines and images that
were shown randomly to different visitors to the website, as part of a
test. A test can include many packages. Each row in the dataset is a
package and the test that it is part of is specified by the
`clickability\_test\_id' column.

There are a variety of variables. We will focus on:

\begin{itemize}
\tightlist
\item
  `created\_at',
\item
  `clickability\_test\_id' so that we can create comparison groups,
\item
  `headline',
\item
  `impressions' which is the number of people that saw the package, and
\item
  `clicks' which is the number that clicked on that package.
\end{itemize}

Within each batch of tests, we are interested in the effect of the
varied headlines on impressions and clicks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{upworthy\_restricted }\OtherTok{\textless{}{-}} 
\NormalTok{  upworthy }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(created\_at, clickability\_test\_id, headline, impressions, clicks)}

\FunctionTok{head}\NormalTok{(upworthy\_restricted)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  created_at          clickability_test_id     headline       impressions clicks
  <dttm>              <chr>                    <chr>                <dbl>  <dbl>
1 2014-11-20 06:43:16 546d88fb84ad38b2ce000024 They're Being~        3052    150
2 2014-11-20 06:43:44 546d88fb84ad38b2ce000024 They're Being~        3033    122
3 2014-11-20 06:44:59 546d88fb84ad38b2ce000024 They're Being~        3092    110
4 2014-11-20 06:54:36 546d902c26714c6c44000039 This Is What ~        3526     90
5 2014-11-20 06:54:57 546d902c26714c6c44000039 This Is What ~        3506    120
6 2014-11-20 06:55:07 546d902c26714c6c44000039 This Is What ~        3380     98
\end{verbatim}

We will focus on the text contained in headlines, and look at whether
headlines that asked a question got more clicks than those that did not.
We want to remove the effect of different images and so will focus on
those tests that have the same image. To identify whether a headline
asks a question, we search for a question mark. Although there are more
complicated constructions that we could use, this is enough to get
started.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{upworthy\_restricted }\OtherTok{\textless{}{-}}
\NormalTok{  upworthy\_restricted }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{asks\_question =} \FunctionTok{str\_detect}\NormalTok{(}\AttributeTok{string =}\NormalTok{ headline, }\AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{?"}\NormalTok{))}

\NormalTok{upworthy\_restricted }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(asks\_question)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 2
  asks_question     n
  <lgl>         <int>
1 FALSE         19130
2 TRUE           3536
\end{verbatim}

For every test, and for every picture, we want to know whether asking a
question affected the number of clicks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{to\_question\_or\_not\_to\_question }\OtherTok{\textless{}{-}} 
\NormalTok{  upworthy\_restricted }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(clickability\_test\_id, asks\_question) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{ave\_clicks =} \FunctionTok{mean}\NormalTok{(clicks)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`summarise()` has grouped output by 'clickability_test_id'. You can override
using the `.groups` argument.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{look\_at\_differences }\OtherTok{\textless{}{-}} 
\NormalTok{  to\_question\_or\_not\_to\_question }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{id\_cols =}\NormalTok{ clickability\_test\_id,}
              \AttributeTok{names\_from =}\NormalTok{ asks\_question,}
              \AttributeTok{values\_from =}\NormalTok{ ave\_clicks) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{ave\_clicks\_not\_question =} \StringTok{\textasciigrave{}}\AttributeTok{FALSE}\StringTok{\textasciigrave{}}\NormalTok{,}
         \AttributeTok{ave\_clicks\_is\_question =} \StringTok{\textasciigrave{}}\AttributeTok{TRUE}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(ave\_clicks\_not\_question)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(ave\_clicks\_is\_question)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{difference\_in\_clicks =}\NormalTok{ ave\_clicks\_is\_question }\SpecialCharTok{{-}}\NormalTok{ ave\_clicks\_not\_question)}

\NormalTok{look\_at\_differences}\SpecialCharTok{$}\NormalTok{difference\_in\_clicks }\SpecialCharTok{|\textgreater{}} \FunctionTok{mean}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -4.890435
\end{verbatim}

We find that in general, having a question in the headline may slightly
decrease the number of clicks on a headline, although if there is an
effect it does not appear to be very large (Figure~\ref{fig-upworthy}).

\begin{figure}

{\centering \includegraphics{./10-hunt_files/figure-pdf/fig-upworthy-1.pdf}

}

\caption{\label{fig-upworthy}Comparison of the average number of clicks
when a headline contains a question mark or not}

\end{figure}

\hypertarget{implementing-surveys}{%
\section{Implementing surveys}\label{implementing-surveys}}

There are many ways to implement surveys. For instance, there are
dedicated survey platforms such as Survey Monkey and Qualtrics. In
general, the focus of those platforms is on putting together the survey
form and they expect that we already have contact details for the sample
of interest. Some other platforms, such as Mechanical Turk and Prolific,
focus on providing that audience, and we can then ask that audience to
do more than just take a survey. While that is useful, it usually comes
with higher costs. Finally, platforms such as Facebook also provide the
ability to run a survey. One especially common approach, because it is
free, is to use Google Forms.

To create a survey with Google Forms, sign into your Google Account, go
to Google Drive, and then click `New' then `Google Form'. By default,
the form is largely empty (Figure~\ref{fig-googleformfirst}), and we
should add a title and description.

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/googleformfirst.png}

}

\caption{\label{fig-googleformfirst}The default view when a new Google
Form is created contains many empty fields}

\end{figure}

By default, an multiple-choice question is included, and we can update
the content of this by clicking in the question field. Helpfully, there
are often suggestions that can help provide the options. We can make the
question required by toggling (Figure~\ref{fig-googleformsecond}).

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/googleformsecond.png}

}

\caption{\label{fig-googleformsecond}Updating the multiple-choice
question that is included by default}

\end{figure}

We can add another question, by clicking on the plus with a circle
around it, and select different types of question, for instance, `Short
answer', `Checkboxes', or `Linear scale'
(Figure~\ref{fig-googleformthird}). It can be especially useful to use
`Short answer' for aspect such as name and email address, checkboxes and
linear scale to understand preferences.

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/googleformthird.png}

}

\caption{\label{fig-googleformthird}Different options for questions
include short answer, checkboxes, and linear scale}

\end{figure}

When we are happy with our survey, we make like to preview it ourselves,
by clicking on the icon that looks like an eye. After checking it that
way, we can click on `Send'. Usually it is especially useful to use the
second option, which is to send via a link, and it can be handy to
shorten the URL (Figure~\ref{fig-googleformfourth})).

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/googleformfourth.png}

}

\caption{\label{fig-googleformfourth}There are a variety of ways to
share the survey, and one helpful one is to get a link with a short URL}

\end{figure}

After you share your survey, results will accrue in the `Responses' tab
and it can be especially useful to create a spreadsheet to view these
responses, by clicking on the `Sheets' icon. After you have collected
enough responses then you can turn off `Accepting responds'
(Figure~\ref{fig-googleformfifth}).

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{./figures/googleformfifth.png}

}

\caption{\label{fig-googleformfifth}Responses show up alongside the
survey and it can be helpful to add those to a separate spreadsheet}

\end{figure}

\hypertarget{exercises-and-tutorial-9}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-9}}

\hypertarget{exercises-9}{%
\subsection{Exercises}\label{exercises-9}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In your own words, what is the role of randomization in constructing a
  counterfactual (write two or three paragraphs)?
\item
  What is external validity (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Findings from an experiment hold in that setting.
  \item
    Findings from an experiment hold outside that setting.
  \item
    Findings from an experiment that has been repeated many times.
  \item
    Findings from an experiment for which code and data are available.
  \end{enumerate}
\item
  What is internal validity (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Findings from an experiment hold in that setting.
  \item
    Findings from an experiment hold outside that setting.
  \item
    Findings from an experiment that has been repeated many times.
  \item
    Findings from an experiment for which code and data are available.
  \end{enumerate}
\item
  If we have a dataset named `netflix\_data', with the columns `person'
  and `tv\_show' and `hours', (person is a character class uniqueID for
  every person, tv\_show is a character class name of a tv show, and
  hours is double expressing the number of hours that person watched
  that tv show). Could you please write some code that would randomly
  assign people into one of two groups? The data looks like this:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{netflix\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{person =} \FunctionTok{c}\NormalTok{(}\StringTok{"Rohan"}\NormalTok{, }\StringTok{"Rohan"}\NormalTok{, }\StringTok{"Monica"}\NormalTok{, }\StringTok{"Monica"}\NormalTok{, }\StringTok{"Monica"}\NormalTok{, }
                    \StringTok{"Patricia"}\NormalTok{, }\StringTok{"Patricia"}\NormalTok{, }\StringTok{"Helen"}\NormalTok{),}
         \AttributeTok{tv\_show =} \FunctionTok{c}\NormalTok{(}\StringTok{"Broadchurch"}\NormalTok{, }\StringTok{"Duty{-}Shame"}\NormalTok{, }\StringTok{"Broadchurch"}\NormalTok{, }\StringTok{"Duty{-}Shame"}\NormalTok{, }
                     \StringTok{"Shetland"}\NormalTok{, }\StringTok{"Broadchurch"}\NormalTok{, }\StringTok{"Shetland"}\NormalTok{, }\StringTok{"Duty{-}Shame"}\NormalTok{),}
         \AttributeTok{hours =} \FunctionTok{c}\NormalTok{(}\FloatTok{6.8}\NormalTok{, }\FloatTok{8.0}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{9.2}\NormalTok{, }\FloatTok{3.2}\NormalTok{, }\FloatTok{4.0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{10.2}\NormalTok{)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  In the context of randomization, what does stratification mean to you
  (write a paragraph or two)?
\item
  How could you check that your randomization had been done
  appropriately (write two or three paragraphs)?
\item
  Identify three companies that conduct A/B testing commercially and
  write one paragraph for each of them about how they work and the
  trade-offs involved.
\item
  Pretend that you work as a junior analyst for a large consulting firm.
  Further, pretend that your consulting firm has taken a contract to put
  together a facial recognition model for the Canada Border Services
  Agency's Inland Enforcement branch. Taking a page or two, please
  discuss your thoughts on this matter. What would you do and why?
\item
  What is an estimate (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A rule for calculating an estimate of a given quantity based on
    observed data.
  \item
    The quantity of interest.
  \item
    The result.
  \item
    Unknown numbers that determine a statistical model.
  \end{enumerate}
\item
  What is an estimator (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A rule for calculating an estimate of a given quantity based on
    observed data.
  \item
    The quantity of interest.
  \item
    The result.
  \item
    Unknown numbers that determine a statistical model.
  \end{enumerate}
\item
  What is an estimand (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A rule for calculating an estimate of a given quantity based on
    observed data.
  \item
    The quantity of interest.
  \item
    The result.
  \item
    Unknown numbers that determine a statistical model.
  \end{enumerate}
\item
  What is a parameter (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A rule for calculating an estimate of a given quantity based on
    observed data.
  \item
    The quantity of interest.
  \item
    The result.
  \item
    Unknown numbers that determine a statistical model.
  \end{enumerate}
\item
  J. Ware (1989, 298) mentions `a randomized play the winner design'.
  What is it?
\item
  J. Ware (1989, 299) mentions `adaptive randomization'. What is it, in
  your own words?
\item
  J. Ware (1989, 299) mentions `randomized-consent'. He continues that
  it was `attractive in this setting because a standard approach to
  informed consent would require that parents of infants near death be
  approached to give informed consent for an invasive surgical procedure
  that would then, in some instances, not be administered. Those
  familiar with the agonizing experience of having a child in a neonatal
  intensive care unit can appreciate that the process of obtaining
  informed consent would be both frightening and stressful to parents'.
  To what extent do you agree with this position, especially given, as
  Ware (1989), p.~305, mentions `the need to withhold information about
  the study from parents of infants receiving CMT'?
\item
  J. Ware (1989, 300) mentions `equipoise'. In your own words, please
  define and discuss it, using an example from your own experience.
\end{enumerate}

\hypertarget{tutorial-9}{%
\subsection{Tutorial}\label{tutorial-9}}

Please build a website using \texttt{postcards} (Kross 2021). Add Google
Analytics. Deploy it using Netlify. Change some aspect of the website,
add a different tracker, and push it to a new branch. Then use Netlify
to conduct an A/B test. Write a one-to-two page paper about what you did
and what you found.

\hypertarget{paper-2}{%
\subsection{Paper}\label{paper-2}}

At about this point, Paper Three (Appendix @ref(paper-three)) would be
appropriate.

\part{Preparation}

\hypertarget{sec-clean-and-prepare}{%
\chapter{Clean and prepare}\label{sec-clean-and-prepare}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{Data Feminism}, Chapter 5 `Unicorns, Janitors, Ninjas,
  Wizards, and Rock Stars', (D'Ignazio and Klein 2020).
\item
  Read \emph{R for Data Science}, Chapter 12 `Tidy data', (Wickham and
  Grolemund 2017).
\item
  Read \emph{We Gave Four Good Pollsters the Same Raw Data. They Had
  Four Different Results}, (Cohn 2016).
\item
  Read \emph{Column Names as Contracts}, (Riederer 2020)
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Planning an end-point and simulating the dataset that we would like to
  end up with, are key elements of cleaning and preparing data.
\item
  Begin on a small sample of the dataset, write code to fix that, and
  then iterate and generalize to additional tranches.
\item
  Develop a series of tests and checks that the dataset should pass so
  that the features of the dataset are clear.
\item
  Be especially concerned about the class of variables, having clear
  names, and that the values of each variable are as expected given all
  this.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{convo} (Riederer 2022)
\item
  \texttt{janitor} (Firke 2020)
\item
  \texttt{pointblank} (Iannone and Vargas 2022)
\item
  \texttt{purrr} (Henry and Wickham 2020)
\item
  \texttt{stringr} (Wickham 2019e)
\item
  \texttt{tidyr} (Wickham 2021c)
\item
  \texttt{tidyverse} (Wickham 2017)
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{dplyr::count()}
\item
  \texttt{dplyr::mutate()}
\item
  \texttt{dplyr::select()}
\item
  \texttt{janitor::clean\_names()}
\item
  \texttt{stringr::str\_replace\_all()}
\item
  \texttt{stringr::str\_trim()}
\item
  \texttt{tidyr::pivot\_longer()}
\item
  \texttt{tidyr::separate()}
\item
  \texttt{tidyr::separate\_rows()}
\end{itemize}

\hypertarget{introduction-8}{%
\section{Introduction}\label{introduction-8}}

\begin{quote}
``Well, Lyndon, you may be right and they may be every bit as
intelligent as you say,'' said Rayburn, ``but I'd feel a whole lot
better about them if just one of them had run for sheriff once.''

Sam Rayburn's reaction to Lyndon Johnson's enthusiasm about Kennedy's
incoming cabinet, as quoted in \emph{The Best and the Brightest}
(Halberstam 1972, 41).
\end{quote}

In earlier chapters we have done some data cleaning and preparation, and
in this chapter we will put in place more formal approaches. To a large
extent, the role of data cleaning and preparation is so great that the
only people that we can trust understand their data, are those that who
have cleaned it. And, paradoxically, often those that do the cleaning
and preparation are often those that trust it the least. At some point
in every data science workflow, those doing the modelling should get
their hands dirty with data cleaning. To clean and prepare data is to
make many decisions, some of which may have important effects on our
results.

For a long time, data cleaning and preparation was largely overlooked.
We now realize that was a mistake. It has been difficult to trust
results in disciplines that apply statistics. The reproducibility
crisis, which started in psychology but has now extended to many other
fields in the physical and social sciences, has brought to light issues
such as p-value `hacking', researcher degrees of freedom, file-drawer
issues, and even data and results fabrication (Gelman and Loken 2013).
Steps are now being put in place to address these. But, there has been
relatively little focus on the data gathering, cleaning, and preparation
aspects of applied statistics, despite evidence that decisions made
during these steps greatly affect statistical results (Huntington-Klein
et al. 2020). In this chapter we focus on these issues.

While the statistical practices that underpin data science are
themselves correct and robust when applied to simulated datasets, data
science is typically not conducted with these types of datasets. For
instance, data scientists are interested in `messy, unfiltered, and
possibly unclean data---tainted by heteroskedasticity, complex
dependence and missingness patterns---that until recently were avoided
in polite conversations between more traditional statisticians' (Craiu
2019). Big data does not resolve this issue, and may even exacerbate it,
for instance `without taking data quality into account, population
inferences with Big Data are subject to a Big Data Paradox: the more the
data, the surer we fool ourselves' (Meng 2018). It is important to note
that the issues that are found in much applied statistics research are
not necessarily associated with researcher quality, or their biases
(Silberzahn et al. 2018). Instead, they are a result of the environment
within which data science is conducted. This chapter provides an
approach and tools to explicitly think about this work.

Gelman and Vehtari (2020) writing about the most important statistical
ideas of the past 50 years say that each of them enabled new ways of
thinking about data analysis and they brought into the tent of
statistics, approaches that `had been considered more a matter of taste
or philosophy'. The focus on data cleaning and preparation in this
chapter is analogous, insofar, as it represents a codification, or
bringing inside the tent, of aspects that are typically, incorrectly,
considered those of taste rather than statistics.

The workflow that we advocate is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Save the raw data.
\item
  Begin with an end in mind.
\item
  Execute that plan on a small sample.
\item
  Write tests and documentation.
\item
  Iterate the plan.
\item
  Generalize the execution.
\item
  Update tests and documentation.
\end{enumerate}

We will need a variety of skills to be effective, but this is the very
stuff of statistical sciences. The approach needed is some combination
of dogged and sensible. Perfect is very much the enemy of good enough
when it comes to data cleaning. And to be specific, it is better to have
90 per cent of the data cleaned and prepared, and to start exploring
that, before deciding whether it is worth the effort to clean and
prepare the remaining 10 per cent because that remainder will likely
take an awful lot of time and effort.

All data regardless of whether they were obtained from hunting,
gathering, or farming, will have issues and it is critical that we have
approaches that can deal with a variety of issues, and more importantly,
understand how it might affect our modelling (Van den Broeck et al.
2005). To clean data is to analyze data. This is because the process
forces us to make choices about what we value in our results (Au 2020).

\hypertarget{workflow}{%
\section{Workflow}\label{workflow}}

\hypertarget{save-a-copy-of-the-raw-data}{%
\subsection{Save a copy of the raw
data}\label{save-a-copy-of-the-raw-data}}

The first step is to save the raw data into a separate, local, folder.
It is important to save this raw data, to the extent that is possible,
because it establishes the foundation for reproducibility (Wilson et al.
2017). If we are obtaining our data from a third-party, such as a
government website, then we have no control over whether they will
continue to host that data, whether they will update it, and the address
at which it will be available. We also want to reduce the burden that we
impose on their servers, by saving a local copy.

Having locally saved the raw data we must maintain it in that state, and
not modify it. As we begin to clean and prepare it, we instead create
another dataset. Maintaining the initial, raw, state of the dataset, and
using scripts to create the dataset that we are interested in analyzing,
ensures that our entire workflow is reproducible.

\hypertarget{begin-with-an-end-in-mind}{%
\subsection{Begin with an end in mind}\label{begin-with-an-end-in-mind}}

Planning the end state or forcing yourself to begin with an end in mind
is important for a variety of reasons. As with scraping data, it helps
us to be proactive about scope-creep, but with data cleaning it
additionally forces us to really think about what we want the final
dataset to look like.

The first step is to sketch the dataset that we are interested in. The
key features of the sketch will be aspects such as the names of the
columns, their class, and the possible range of values. For instance, we
might be interested in the populations of US states. In which case our
sketch might look like Figure~\ref{fig-sketchdataplan}.

\begin{figure}

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{./figures/state_population_sketch.png}

}

\caption{\label{fig-sketchdataplan}Planned dataset of US states and
their populations}

\end{figure}

In this case, the sketch forces us to decide whether we want full names
or abbreviations for the state names, and that the population has been
measured in millions. The process of sketching this end-point has forced
us to make decisions early on, and be clear about our desired end state.

We then implement that using code to simulate data. Again, this process
forces us to think about what reasonable values look like in our dataset
because we are literally forced to decide which functions to use.
Thinking carefully about the membership of each column here, for
instance if the column is meant to be `gender' then values such as
`male', `female', `other', and `unknown' may be expected, but a number
such as `1,000' would likely be unexpected. It also forces us to be
explicit about variable names because we have to assign the outputs of
those functions to a variable. For instance, we could simulate some data
for the population data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{simulated\_tfr }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{state =}\NormalTok{ state.name,}
    \AttributeTok{population =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =} \DecValTok{50}\NormalTok{, }\AttributeTok{min =} \DecValTok{0}\NormalTok{, }\AttributeTok{max =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{round}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\NormalTok{  )}

\NormalTok{simulated\_tfr}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 50 x 2
   state       population
   <chr>            <dbl>
 1 Alabama          18.0 
 2 Alaska            6.01
 3 Arizona          24.2 
 4 Arkansas         15.8 
 5 California        1.87
 6 Colorado         20.2 
 7 Connecticut       6.54
 8 Delaware         12.1 
 9 Florida           7.9 
10 Georgia           9.44
# ... with 40 more rows
\end{verbatim}

Our purpose, during data cleaning and preparation, is to then bring our
raw data close to that plan. Ideally, we would plan so that the desired
end-state of our dataset is `tidy data', which was introduced in Chapter
@ref(r-essentials).

\hypertarget{start-small}{%
\subsection{Start small}\label{start-small}}

Having thoroughly planned we can turn to the raw data that we are
dealing with. Usually, regardless of what the raw data look like, we
want to manipulate them into a rectangular dataset as quickly as
possible. This allows us to use our family \texttt{dplyr} verbs and
\texttt{tidyverse} approaches. For instance, let us assume that we are
starting with some \texttt{.txt} file.

The first step is to look for regularities in the dataset. We are
wanting to end up with tabular data, which means that we need some type
of delimiter to distinguish different columns. Ideally this might be
features such as a comma, a semicolon, a tab, a double space, or a line
break.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Alabama, }\DecValTok{5}
\NormalTok{Alaska, }\FloatTok{0.7}
\NormalTok{Arizona, }\DecValTok{7}
\NormalTok{Arkansas, }\DecValTok{3}
\NormalTok{California, }\DecValTok{40}
\end{Highlighting}
\end{Shaded}

In worse cases there may be some regular feature of the dataset that we
can take advantage of. For instance, sometimes various text is repeated.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{State is Alabama and population is }\DecValTok{5}\NormalTok{ million.}
\NormalTok{State is Alaska and population is }\FloatTok{0.7}\NormalTok{ million.}
\NormalTok{State is Arizona and population is }\DecValTok{7}\NormalTok{ million.}
\NormalTok{State is Arkansas and population is }\DecValTok{3}\NormalTok{ million.}
\NormalTok{State is California and population is }\DecValTok{40}\NormalTok{ million.}
\end{Highlighting}
\end{Shaded}

In this case, although we do not have a traditional delimiter we can use
the regularity of `State is' and ' and population is ' to get what we
need. A more difficult case is when we do not have line breaks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Alabama }\DecValTok{5}\NormalTok{ Alaska }\FloatTok{0.7}\NormalTok{ Arizona }\DecValTok{7}\NormalTok{ Arkansas }\DecValTok{3}\NormalTok{ California }\DecValTok{40}
\end{Highlighting}
\end{Shaded}

One way to approach this is to take advantage of the different classes
and values that we are looking for. For instance, in this case, we know
that we are after US states, so there are only 50 possible options, and
we could use the existence of these as a delimiter. We could also use
the fact that population is a number here, and so split based on a space
followed by a number.

We will now go through the process of converting this last example into
tidy data using \texttt{tidyr} (Wickham 2021c).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw\_data }\OtherTok{\textless{}{-}}
  \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Alabama 5 Alaska 0.7 Arizona 7 Arkansas 3 California 40\textquotesingle{}}\NormalTok{)}

\NormalTok{data\_as\_tibble }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{raw =}\NormalTok{ raw\_data)}

\NormalTok{tidy\_data }\OtherTok{\textless{}{-}}
\NormalTok{  data\_as\_tibble }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ raw,}
           \AttributeTok{into =}\NormalTok{ letters[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{],}
           \AttributeTok{sep =} \StringTok{"(?\textless{}=[[:digit:]]) "}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =}\NormalTok{ letters[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{],}
               \AttributeTok{names\_to =} \StringTok{"drop\_me"}\NormalTok{,}
               \AttributeTok{values\_to =} \StringTok{"separate\_me"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ separate\_me,}
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}state\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}population\textquotesingle{}}\NormalTok{),}
           \AttributeTok{sep =} \StringTok{" (?=[[:digit:]])"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{drop\_me)}

\NormalTok{tidy\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 2
  state      population
  <chr>      <chr>     
1 Alabama    5         
2 Alaska     0.7       
3 Arizona    7         
4 Arkansas   3         
5 California 40        
\end{verbatim}

\hypertarget{write-tests-and-documentation}{%
\subsection{Write tests and
documentation}\label{write-tests-and-documentation}}

Having established a rectangular dataset, albeit a messy one, we should
begin to look at the classes that we have. We do not necessarily want to
fix the classes at this point, because that can result in us losing
data. But we look at the class to see what it is, and then compare it to
our simulated dataset to see where it needs to get to. We note the
columns where it is different.

Before changing the class and before going onto more bespoke issues, we
should deal with some of the common issues in each class. Some common
issues are:

\begin{itemize}
\tightlist
\item
  Commas and other punctuation, such as denomination signs in columns
  that should be numeric.
\item
  Inconsistent formatting of dates, such as `December' and `Dec' and
  `12'.
\item
  Unexpected characters, especially in unicode, which may not display
  consistently.
\end{itemize}

Typically, we want to fix anything immediately obvious. For instance,
remove commas that have been used to group digits in currencies.
However, the situation will typically quickly become dire. What we need
to do is to look at the membership of each group, and then triage what
we will fix. We should probably make the decision of how to triage based
on what is likely to have the largest impact. That usually means
starting with the counts, sorting in descending order, and then dealing
with each as they come.

When the tests of membership are passed, then finally we can change the
class, and run all the tests again. We are adapting this idea from the
software development approach of unit testing. Tests are crucial because
the enable us to understand whether software (or in this case data) is
fit for purpose (Wilson 2021).

Let us run through an example with a collection of strings, some of
which are slightly wrong. This type of output is typical of OCR, which
often gets most of the way there, but not quite.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{messy\_string }\OtherTok{\textless{}{-}}
  \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Patricia, Ptricia, PatricIa, Patncia, PatricIa, Patricia, Patricia, Patric1a, Patricia , 8atricia\textquotesingle{}}\NormalTok{)}

\NormalTok{messy\_string}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Patricia, Ptricia, PatricIa, Patncia, PatricIa, Patricia, Patricia, Patric1a, Patricia , 8atricia"
\end{verbatim}

As before, we first want to get this into a rectangular dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{messy\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{names =}\NormalTok{ messy\_string) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{separate\_rows}\NormalTok{(names, }\AttributeTok{sep =} \StringTok{", "}\NormalTok{) }

\NormalTok{messy\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 1
   names      
   <chr>      
 1 "Patricia" 
 2 "Ptricia"  
 3 "PatricIa" 
 4 "Patncia"  
 5 "PatricIa" 
 6 "Patricia" 
 7 "Patricia" 
 8 "Patric1a" 
 9 "Patricia "
10 "8atricia" 
\end{verbatim}

We now need decide which of these errors we are going to fix. To help us
decide which are most important, we will create a count.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{messy\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(names, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 7 x 2
  names           n
  <chr>       <int>
1 "Patricia"      3
2 "PatricIa"      2
3 "8atricia"      1
4 "Patncia"       1
5 "Patric1a"      1
6 "Patricia "     1
7 "Ptricia"       1
\end{verbatim}

The most common element is the correct one, which is great. The next one
- `PatricIa' - looks like the `i' has been incorrectly capitalized, and
the one after that - `8atricia' - is distinguished by an `8' instead of
a `P'. Let us quickly fix these issues and then redo the count.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{messy\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  messy\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{names =} \FunctionTok{str\_replace\_all}\NormalTok{(names, }\StringTok{\textquotesingle{}PatricIa\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Patricia\textquotesingle{}}\NormalTok{),}
         \AttributeTok{names =} \FunctionTok{str\_replace\_all}\NormalTok{(names, }\StringTok{\textquotesingle{}8atricia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Patricia\textquotesingle{}}\NormalTok{)}
\NormalTok{         )}

\NormalTok{messy\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(names, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 2
  names           n
  <chr>       <int>
1 "Patricia"      6
2 "Patncia"       1
3 "Patric1a"      1
4 "Patricia "     1
5 "Ptricia"       1
\end{verbatim}

Already this is much better and 60 per cent of the values are correct,
compared with earlier where it was 30 per cent. There are two more
obvious errors - `Ptricia' and `Patncia' - with the first missing an `a'
and the second having an `n' where the `ri' should be. Again, we can
quickly update and fix those.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{messy\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  messy\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{names =} \FunctionTok{str\_replace\_all}\NormalTok{(names, }\StringTok{\textquotesingle{}Ptricia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Patricia\textquotesingle{}}\NormalTok{),}
         \AttributeTok{names =} \FunctionTok{str\_replace\_all}\NormalTok{(names, }\StringTok{\textquotesingle{}Patncia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Patricia\textquotesingle{}}\NormalTok{)}
\NormalTok{         )}

\NormalTok{messy\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(names, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 2
  names           n
  <chr>       <int>
1 "Patricia"      8
2 "Patric1a"      1
3 "Patricia "     1
\end{verbatim}

We have achieved an 80 per cent fix with not too much effort. The two
remaining issues are more subtle. The first - `Patric1a' - has occurred
because the `i' has been incorrectly coded as an `1'. In some fonts this
will show up, but in others it will be more difficult to see. This is a
common issue, especially with OCR, and something to be aware of. The
second - `Patricia' - is similarly subtle and is occurring because there
is a trailing space. Again, trailing and leading spaces are a common
issue and we can address them with \texttt{str\_trim()}. After we fix
these two remaining issues then we will have all entries corrected.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  messy\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{names =} \FunctionTok{str\_replace\_all}\NormalTok{(names, }\StringTok{\textquotesingle{}Patric1a\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Patricia\textquotesingle{}}\NormalTok{),}
         \AttributeTok{names =} \FunctionTok{str\_trim}\NormalTok{(names, }\AttributeTok{side =} \FunctionTok{c}\NormalTok{(}\StringTok{"right"}\NormalTok{))}
\NormalTok{         )}

\NormalTok{cleaned\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(names, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  names        n
  <chr>    <int>
1 Patricia    10
\end{verbatim}

We have been doing the tests in our head in this example. We know that
we are hoping for `Patricia'. But we can start to document this test as
well. One way is to look to see if values other than `Patricia' exist in
the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{check\_me }\OtherTok{\textless{}{-}} 
\NormalTok{  cleaned\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(names }\SpecialCharTok{!=} \StringTok{"Patricia"}\NormalTok{)}

\ControlFlowTok{if}\NormalTok{ (}\FunctionTok{nrow}\NormalTok{(check\_me) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) \{}
  \FunctionTok{print}\NormalTok{(}\StringTok{"Still have values that are not Patricia!"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We can make things a little more imposing by stopping our code execution
if the condition is not met with \texttt{stopifnot()}. To use that we
define a condition that we would like met. We could implement this type
of check throughout our code. For instance, if we expected there to be a
certain number of rows in the dataset, or for a certain column to have
various properties, such as being an integer, or a factor.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{stopifnot}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(check\_me) }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can use \texttt{stopifnot()} to ensure that our script is working as
expected as it goes through. Another way that is especially used

Another way to write tests for our dataset is to use \texttt{testthat}
(Wickham 2011). Although developed for testing packages, we can use the
same functionality to test our datasets. For instance, we can use
\texttt{expect\_length()} to check the length of a dataset and could use
\texttt{expect\_equal()} to check the content.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(testthat)}

\FunctionTok{expect\_length}\NormalTok{(check\_me, }\DecValTok{1}\NormalTok{)}
\FunctionTok{expect\_equal}\NormalTok{(}\FunctionTok{class}\NormalTok{(cleaned\_data}\SpecialCharTok{$}\NormalTok{names), }\StringTok{"character"}\NormalTok{)}
\FunctionTok{expect\_equal}\NormalTok{(}\FunctionTok{unique}\NormalTok{(cleaned\_data}\SpecialCharTok{$}\NormalTok{names), }\StringTok{"Patricia"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If the tests pass then nothing happens, and if the tests fail then the
script will stop.

\hypertarget{iterate-generalize-and-update}{%
\subsection{Iterate, generalize and
update}\label{iterate-generalize-and-update}}

We could now iterate the plan. In this most recent case, we started with
10 entries. There is no reason that we could not increase this to 100 or
even 1,000. We may need to generalize the cleaning procedures and tests.
But eventually we would start to being the dataset into some sort of
order.

\hypertarget{case-study-kenya-census}{%
\section{Case study: Kenya census}\label{case-study-kenya-census}}

\hypertarget{gather-and-clean-data}{%
\subsection{Gather and clean data}\label{gather-and-clean-data}}

To make this all more clear, let us gather, clean, and prepare some data
from the 2019 Kenyan census. The distribution of population by age, sex,
and administrative unit from the 2019 Kenyan census can be downloaded
\href{https://www.knbs.or.ke/?wpdmpro=2019-kenya-population-and-housing-census-volume-iii-distribution-of-population-by-age-sex-and-administrative-units}{here}.
While this format as a PDF makes it easy to look up a particular result,
it is not overly useful if we want to model the data. In order to be
able to do that, we need to convert a PDF of Kenyan census results of
counts, by age and sex, by county and sub-county, into a tidy dataset
that can be analyzed. We will use \texttt{janitor} (Firke 2020),
\texttt{pdftools} (Ooms 2019b), \texttt{tidyverse} (Wickham et al.
2019b), and \texttt{stringi} (Gagolewski 2020).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(pdftools)}
\FunctionTok{library}\NormalTok{(purrr)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(stringi)}
\end{Highlighting}
\end{Shaded}

And we can then download\footnote{If the Kenyan government link breaks
  then replace their URL with:
  https://www.tellingstorieswithdata.com/inputs/pdfs/2019\_Kenya\_census.pdf.}
and read in the PDF of the 2019 Kenyan census. When we have a PDF and
want to read the content into R, then \texttt{pdf\_text()} from
\texttt{pdftools} (Ooms 2019b) is useful. It works well for many
recently produced PDFs because the content is text which it can extract.
But if the PDF is an image, then \texttt{pdf\_text()} will not work.
Instead, the PDF will first need to go through OCR, which was covered in
Chapter @ref(gather-data).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{download.file}\NormalTok{(}
  \StringTok{"https://www.knbs.or.ke/download/2019{-}kenya{-}population{-}and{-}housing{-}census{-}volume{-}iii{-}distribution{-}of{-}population{-}by{-}age{-}sex{-}and{-}administrative{-}units/?wpdmdl=5729\&refresh=620561f1ce3ad1644519921"}\NormalTok{, }
  \StringTok{"2019\_Kenya\_census.pdf"}\NormalTok{,}
  \AttributeTok{mode=}\StringTok{"wb"}\NormalTok{)}

\NormalTok{all\_content }\OtherTok{\textless{}{-}} \FunctionTok{pdf\_text}\NormalTok{(}\StringTok{"2019\_Kenya\_census.pdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can see an example page of the PDF of the 2019 Kenyan census
(Figure~\ref{fig-examplekenyancensuspage}).

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/2020-04-10-screenshot-of-census.png}

}

\caption{\label{fig-examplekenyancensuspage}Example page from the 2019
Kenyan census}

\end{figure}

The first challenge is to get the dataset into a format that we can more
easily manipulate. We will consider each page of the PDF and extract the
relevant parts. To do this, we first write a function, and then apply it
to each page.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The function is going to take an input of a page}
\NormalTok{get\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(i)\{}
  \CommentTok{\# i = 467}
  \CommentTok{\# Just look at the page of interest}
  \CommentTok{\# Based on Bob Rudis: https://stackoverflow.com/a/47793617}
\NormalTok{  just\_page\_i }\OtherTok{\textless{}{-}} \FunctionTok{stri\_split\_lines}\NormalTok{(all\_content[[i]])[[}\DecValTok{1}\NormalTok{]] }
  
\NormalTok{  just\_page\_i }\OtherTok{\textless{}{-}}\NormalTok{ just\_page\_i[just\_page\_i }\SpecialCharTok{!=} \StringTok{""}\NormalTok{]}
  
  \CommentTok{\# Grab the name of the location}
\NormalTok{  area }\OtherTok{\textless{}{-}}\NormalTok{ just\_page\_i[}\DecValTok{3}\NormalTok{] }\SpecialCharTok{|\textgreater{}} \FunctionTok{str\_squish}\NormalTok{()}
\NormalTok{  area }\OtherTok{\textless{}{-}} \FunctionTok{str\_to\_title}\NormalTok{(area)}
  
  \CommentTok{\# Grab the type of table}
\NormalTok{  type\_of\_table }\OtherTok{\textless{}{-}}\NormalTok{ just\_page\_i[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{|\textgreater{}} \FunctionTok{str\_squish}\NormalTok{()}
  
  \CommentTok{\# Get rid of the top matter}
  \CommentTok{\# Manually for now, but could create some rules if needed}
\NormalTok{  just\_page\_i\_no\_header }\OtherTok{\textless{}{-}}\NormalTok{ just\_page\_i[}\DecValTok{5}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(just\_page\_i)] }
  
  \CommentTok{\# Get rid of the bottom matter}
  \CommentTok{\# Manually for now, but could create some rules if needed}
\NormalTok{  just\_page\_i\_no\_header\_no\_footer }\OtherTok{\textless{}{-}}\NormalTok{ just\_page\_i\_no\_header[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{62}\NormalTok{] }
  
  \CommentTok{\# Convert into a tibble}
\NormalTok{  demography\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{all =}\NormalTok{ just\_page\_i\_no\_header\_no\_footer)}
  
  \CommentTok{\# Split columns}
\NormalTok{  demography\_data }\OtherTok{\textless{}{-}}
\NormalTok{    demography\_data }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{all =} \FunctionTok{str\_squish}\NormalTok{(all)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Any space more than two spaces is reduced}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{all =} \FunctionTok{str\_replace}\NormalTok{(all, }\StringTok{"10 {-}14"}\NormalTok{, }\StringTok{"10{-}14"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# One specific issue}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{all =} \FunctionTok{str\_replace}\NormalTok{(all, }\StringTok{"Not Stated"}\NormalTok{, }\StringTok{"NotStated"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# And another}
    \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ all,}
             \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"male"}\NormalTok{, }\StringTok{"female"}\NormalTok{, }\StringTok{"total"}\NormalTok{, }\StringTok{"age\_2"}\NormalTok{, }\StringTok{"male\_2"}\NormalTok{, }\StringTok{"female\_2"}\NormalTok{, }\StringTok{"total\_2"}\NormalTok{),}
             \AttributeTok{sep =} \StringTok{" "}\NormalTok{, }\CommentTok{\# Works fine because the tables are nicely laid out}
             \AttributeTok{remove =} \ConstantTok{TRUE}\NormalTok{,}
             \AttributeTok{fill =} \StringTok{"right"}\NormalTok{,}
             \AttributeTok{extra =} \StringTok{"drop"}
\NormalTok{    )}
  
  \CommentTok{\# They are side by side at the moment, need to append to bottom}
\NormalTok{  demography\_data\_long }\OtherTok{\textless{}{-}}
    \FunctionTok{rbind}\NormalTok{(demography\_data }\SpecialCharTok{|\textgreater{}} \FunctionTok{select}\NormalTok{(age, male, female, total),}
\NormalTok{          demography\_data }\SpecialCharTok{|\textgreater{}}
            \FunctionTok{select}\NormalTok{(age\_2, male\_2, female\_2, total\_2) }\SpecialCharTok{|\textgreater{}}
            \FunctionTok{rename}\NormalTok{(}\AttributeTok{age =}\NormalTok{ age\_2, }\AttributeTok{male =}\NormalTok{ male\_2, }\AttributeTok{female =}\NormalTok{ female\_2, }\AttributeTok{total =}\NormalTok{ total\_2)}
\NormalTok{    )}
  
  \CommentTok{\# There is one row of NAs, so remove it}
\NormalTok{  demography\_data\_long }\OtherTok{\textless{}{-}} 
\NormalTok{    demography\_data\_long }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{remove\_empty}\NormalTok{(}\AttributeTok{which =} \FunctionTok{c}\NormalTok{(}\StringTok{"rows"}\NormalTok{))}
  
  \CommentTok{\# Add the area and the page}
\NormalTok{  demography\_data\_long}\SpecialCharTok{$}\NormalTok{area }\OtherTok{\textless{}{-}}\NormalTok{ area}
\NormalTok{  demography\_data\_long}\SpecialCharTok{$}\NormalTok{table }\OtherTok{\textless{}{-}}\NormalTok{ type\_of\_table}
\NormalTok{  demography\_data\_long}\SpecialCharTok{$}\NormalTok{page }\OtherTok{\textless{}{-}}\NormalTok{ i}
  
  \FunctionTok{rm}\NormalTok{(just\_page\_i,}
\NormalTok{     i,}
\NormalTok{     area,}
\NormalTok{     type\_of\_table,}
\NormalTok{     just\_page\_i\_no\_header,}
\NormalTok{     just\_page\_i\_no\_header\_no\_footer,}
\NormalTok{     demography\_data)}
  
  \FunctionTok{return}\NormalTok{(demography\_data\_long)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We now have a function that does what we need to each page of the PDF.
We will use \texttt{map\_dfr()} from \texttt{purrr} (Henry and Wickham
2020) to apply that function to each page, and then combine all the
outputs into one tibble.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Run through each relevant page and get the data}
\NormalTok{pages }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{30}\SpecialCharTok{:}\DecValTok{513}\NormalTok{)}
\NormalTok{all\_tables }\OtherTok{\textless{}{-}} \FunctionTok{map\_dfr}\NormalTok{(pages, get\_data)}
\FunctionTok{rm}\NormalTok{(pages, get\_data)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_tables}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 59,532 x 7
   age   male    female  total     area    table                            page
   <chr> <chr>   <chr>   <chr>     <chr>   <chr>                           <int>
 1 Total 610,257 598,046 1,208,303 Mombasa Table 2.3: Distribution of Pop~    30
 2 0     15,111  15,009  30,120    Mombasa Table 2.3: Distribution of Pop~    30
 3 1     15,805  15,308  31,113    Mombasa Table 2.3: Distribution of Pop~    30
 4 2     15,088  14,837  29,925    Mombasa Table 2.3: Distribution of Pop~    30
 5 3     14,660  14,031  28,691    Mombasa Table 2.3: Distribution of Pop~    30
 6 4     14,061  13,993  28,054    Mombasa Table 2.3: Distribution of Pop~    30
 7 0-4   74,725  73,178  147,903   Mombasa Table 2.3: Distribution of Pop~    30
 8 5     13,851  14,023  27,874    Mombasa Table 2.3: Distribution of Pop~    30
 9 6     12,889  13,216  26,105    Mombasa Table 2.3: Distribution of Pop~    30
10 7     13,268  13,203  26,471    Mombasa Table 2.3: Distribution of Pop~    30
# ... with 59,522 more rows
\end{verbatim}

Having got it into a rectangular format, we now need to clean the
dataset to make it useful.

The first step is to make the numbers into actual numbers, rather than
characters. Before we can convert the type, we need to remove anything
that is not a number otherwise that cell will be converted into an NA.
We first identify any values that are not numbers so that we can remove
them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Need to convert male, female, and total to integers}
\CommentTok{\# First find the characters that should not be in there}
\NormalTok{all\_tables }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(male, female, total) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate\_all}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_remove\_all}\NormalTok{(., }\StringTok{"[:digit:]"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate\_all}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_remove\_all}\NormalTok{(., }\StringTok{","}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate\_all}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_remove\_all}\NormalTok{(., }\StringTok{"\_"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate\_all}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_remove\_all}\NormalTok{(., }\StringTok{"{-}"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{distinct}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 3
  male  female total
  <chr> <chr>  <chr>
1 ""    ""     ""   
2 "Aug" ""     ""   
3 "Jun" ""     ""   
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We clearly need to remove ",", "\_", and "{-}". }
\CommentTok{\# This also highlights a few issues on p. 185 that need to be manually adjusted}
\CommentTok{\# https://twitter.com/RohanAlexander/status/1244337583016022018}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{male[all\_tables}\SpecialCharTok{$}\NormalTok{male }\SpecialCharTok{==} \StringTok{"23{-}Jun"}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{4923}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{male[all\_tables}\SpecialCharTok{$}\NormalTok{male }\SpecialCharTok{==} \StringTok{"15{-}Aug"}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{4611}
\end{Highlighting}
\end{Shaded}

While we could use \texttt{janitor} here, it is worthwhile at least
first looking at what is going on because sometimes there is odd stuff
that \texttt{janitor} (and other packages) will deal with, but not in a
way that we want. In this case, the Kenyan government used Excel or
similar, and this has converted two entries into dates. If we just took
the numbers from the column then we would have 23 and 15 here, but by
inspecting the column we can use Excel to reverse the process and enter
the correct values of 4,923 and 4,611, respectively.

Having identified everything that needs to be removed, we can do the
actual removal and convert our character column of numbers to integers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_tables }\OtherTok{\textless{}{-}}
\NormalTok{  all\_tables }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(male, female, total), }\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_remove\_all}\NormalTok{(., }\StringTok{","}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(male, female, total), }\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_replace}\NormalTok{(., }\StringTok{"\_"}\NormalTok{, }\StringTok{"0"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(male, female, total), }\SpecialCharTok{\textasciitilde{}}\FunctionTok{str\_replace}\NormalTok{(., }\StringTok{"{-}"}\NormalTok{, }\StringTok{"0"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(male, female, total), }\SpecialCharTok{\textasciitilde{}}\FunctionTok{as.integer}\NormalTok{(.))}

\NormalTok{all\_tables}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 59,532 x 7
   age     male female   total area    table                                page
   <chr>  <int>  <int>   <int> <chr>   <chr>                               <int>
 1 Total 610257 598046 1208303 Mombasa Table 2.3: Distribution of Populat~    30
 2 0      15111  15009   30120 Mombasa Table 2.3: Distribution of Populat~    30
 3 1      15805  15308   31113 Mombasa Table 2.3: Distribution of Populat~    30
 4 2      15088  14837   29925 Mombasa Table 2.3: Distribution of Populat~    30
 5 3      14660  14031   28691 Mombasa Table 2.3: Distribution of Populat~    30
 6 4      14061  13993   28054 Mombasa Table 2.3: Distribution of Populat~    30
 7 0-4    74725  73178  147903 Mombasa Table 2.3: Distribution of Populat~    30
 8 5      13851  14023   27874 Mombasa Table 2.3: Distribution of Populat~    30
 9 6      12889  13216   26105 Mombasa Table 2.3: Distribution of Populat~    30
10 7      13268  13203   26471 Mombasa Table 2.3: Distribution of Populat~    30
# ... with 59,522 more rows
\end{verbatim}

The next thing to clean is the areas. We know that there are 47 counties
in Kenya, and a large number of sub-counties. The Kenyan government
purports to provide a list on pages 19 to 22 of the PDF (document pages
7 to 10). But this list is not complete, and there are a few minor
issues that we will deal with later. In any case, we first need to fix a
few inconsistencies.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fix some area names}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{area[all\_tables}\SpecialCharTok{$}\NormalTok{area }\SpecialCharTok{==} \StringTok{"Taita/ Taveta"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Taita/Taveta"}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{area[all\_tables}\SpecialCharTok{$}\NormalTok{area }\SpecialCharTok{==} \StringTok{"Elgeyo/ Marakwet"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Elgeyo/Marakwet"}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{area[all\_tables}\SpecialCharTok{$}\NormalTok{area }\SpecialCharTok{==} \StringTok{"Nairobi City"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Nairobi"}
\end{Highlighting}
\end{Shaded}

Kenya has 47 counties, each of which has sub-counties. The PDF has them
arranged as the county data then the sub-counties, without designating
which is which. We can use the names, to a certain extent, but in a
handful of cases, there is a sub-county that has the same name as a
county, so we need to first fix that.

The PDF is made-up of three tables. So we can first get the names of the
counties based on those final two tables and then reconcile them to get
a list of the counties.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{table }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{table}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Table 2.3: Distribution of Population by Age, Sex*, County and Sub- County 
                                                                     48216 
      Table 2.4a: Distribution of Rural Population by Age, Sex* and County 
                                                                      5535 
      Table 2.4b: Distribution of Urban Population by Age, Sex* and County 
                                                                      5781 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{list\_counties }\OtherTok{\textless{}{-}} 
\NormalTok{  all\_tables }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(table }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Table 2.4a: Distribution of Rural Population by Age, Sex* and County"}\NormalTok{,}
                      \StringTok{"Table 2.4b: Distribution of Urban Population by Age, Sex* and County"}\NormalTok{)}
\NormalTok{         ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(area) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{distinct}\NormalTok{()}

\NormalTok{list\_counties}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 47 x 1
   area        
   <chr>       
 1 Kwale       
 2 Kilifi      
 3 Tana River  
 4 Lamu        
 5 Taita/Taveta
 6 Garissa     
 7 Wajir       
 8 Mandera     
 9 Marsabit    
10 Isiolo      
# ... with 37 more rows
\end{verbatim}

As we hoped, there are 47 of them. But before we can add a flag based on
those names, we need to deal with the sub-counties that share their
name. We will do this based on the page, then looking it up and deciding
which is the county page and which is the sub-county page.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_tables }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(table }\SpecialCharTok{==} \StringTok{"Table 2.3: Distribution of Population by Age, Sex*, County and Sub{-} County"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(area }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Busia"}\NormalTok{,}
                     \StringTok{"Garissa"}\NormalTok{,}
                     \StringTok{"Homa Bay"}\NormalTok{,}
                     \StringTok{"Isiolo"}\NormalTok{,}
                     \StringTok{"Kiambu"}\NormalTok{,}
                     \StringTok{"Machakos"}\NormalTok{,}
                     \StringTok{"Makueni"}\NormalTok{,}
                     \StringTok{"Samburu"}\NormalTok{,}
                     \StringTok{"Siaya"}\NormalTok{,}
                     \StringTok{"Tana River"}\NormalTok{,}
                     \StringTok{"Vihiga"}\NormalTok{,}
                     \StringTok{"West Pokot"}\NormalTok{)}
\NormalTok{         ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(area, page) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{distinct}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 24 x 2
   area        page
   <chr>      <int>
 1 Samburu       42
 2 Tana River    53
 3 Tana River    56
 4 Garissa       65
 5 Garissa       69
 6 Isiolo        98
 7 Isiolo       100
 8 Machakos     149
 9 Machakos     154
10 Makueni      159
# ... with 14 more rows
\end{verbatim}

Now we can add the flag for whether the area is a county, and adjust for
the ones that are troublesome,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_tables }\OtherTok{\textless{}{-}} 
\NormalTok{  all\_tables }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{area\_type =} \FunctionTok{if\_else}\NormalTok{(area }\SpecialCharTok{\%in\%}\NormalTok{ list\_counties}\SpecialCharTok{$}\NormalTok{area, }\StringTok{"county"}\NormalTok{, }\StringTok{"sub{-}county"}\NormalTok{))}

\NormalTok{all\_tables }\OtherTok{\textless{}{-}} 
\NormalTok{  all\_tables }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{area\_type =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Samburu"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{42} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Tana River"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{56} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Garissa"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{69} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Isiolo"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{100} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Machakos"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{154} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Makueni"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{164} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Kiambu"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{213} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"West Pokot"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{233} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Vihiga"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{333} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Busia"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{353} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Siaya"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{360} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
\NormalTok{    area }\SpecialCharTok{==} \StringTok{"Homa Bay"} \SpecialCharTok{\&}\NormalTok{ page }\SpecialCharTok{==} \DecValTok{375} \SpecialCharTok{\textasciitilde{}} \StringTok{"sub{-}county"}\NormalTok{,}
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}}\NormalTok{ area\_type}
\NormalTok{    )}
\NormalTok{  )}

\FunctionTok{rm}\NormalTok{(list\_counties)}

\NormalTok{all\_tables}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 59,532 x 8
   age     male female   total area    table                      page area_type
   <chr>  <int>  <int>   <int> <chr>   <chr>                     <int> <chr>    
 1 Total 610257 598046 1208303 Mombasa Table 2.3: Distribution ~    30 county   
 2 0      15111  15009   30120 Mombasa Table 2.3: Distribution ~    30 county   
 3 1      15805  15308   31113 Mombasa Table 2.3: Distribution ~    30 county   
 4 2      15088  14837   29925 Mombasa Table 2.3: Distribution ~    30 county   
 5 3      14660  14031   28691 Mombasa Table 2.3: Distribution ~    30 county   
 6 4      14061  13993   28054 Mombasa Table 2.3: Distribution ~    30 county   
 7 0-4    74725  73178  147903 Mombasa Table 2.3: Distribution ~    30 county   
 8 5      13851  14023   27874 Mombasa Table 2.3: Distribution ~    30 county   
 9 6      12889  13216   26105 Mombasa Table 2.3: Distribution ~    30 county   
10 7      13268  13203   26471 Mombasa Table 2.3: Distribution ~    30 county   
# ... with 59,522 more rows
\end{verbatim}

Having dealt with the areas, we can deal with the ages. First, we need
to fix some clear errors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(all\_tables}\SpecialCharTok{$}\NormalTok{age) }\SpecialCharTok{|\textgreater{}} \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    0   0-4     1    10 10-14 10-19 
  484   484   484   484   482     1 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unique}\NormalTok{(all\_tables}\SpecialCharTok{$}\NormalTok{age) }\SpecialCharTok{|\textgreater{}} \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Total" "0"     "1"     "2"     "3"     "4"    
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Looks like there should be 484, so need to follow up on some:}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age[all\_tables}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{==} \StringTok{"NotStated"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Not Stated"}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age[all\_tables}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{==} \StringTok{"43594"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"5{-}9"}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age[all\_tables}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{==} \StringTok{"43752"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"10{-}14"}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age[all\_tables}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{==} \StringTok{"9{-}14"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"5{-}9"}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age[all\_tables}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{==} \StringTok{"10{-}19"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"10{-}14"}
\end{Highlighting}
\end{Shaded}

The census has done some of the work of putting together age-groups for
us, but we want to make it easy to just focus on the counts by
single-year-age. As such we will add a flag as to the type of age it is:
an age group, such as ``ages 0 to 5'', or a single age, such as ``1''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age\_type }\OtherTok{\textless{}{-}}
  \FunctionTok{if\_else}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(all\_tables}\SpecialCharTok{$}\NormalTok{age, }\FunctionTok{c}\NormalTok{(}\StringTok{"{-}"}\NormalTok{)), }\StringTok{"age{-}group"}\NormalTok{, }\StringTok{"single{-}year"}\NormalTok{)}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age\_type }\OtherTok{\textless{}{-}}
  \FunctionTok{if\_else}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(all\_tables}\SpecialCharTok{$}\NormalTok{age, }\FunctionTok{c}\NormalTok{(}\StringTok{"Total"}\NormalTok{)),}
          \StringTok{"age{-}group"}\NormalTok{,}
\NormalTok{          all\_tables}\SpecialCharTok{$}\NormalTok{age\_type)}
\end{Highlighting}
\end{Shaded}

At the moment, age is a character variable. We have a decision to make
here. We do not want it to be a character variable (because it will not
graph properly), but we do not want it to be numeric, because there is
\texttt{total} and \texttt{100+} in there. For now, we will just make it
into a factor, and at least that will be able to be nicely graphed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{age }\OtherTok{\textless{}{-}} \FunctionTok{as\_factor}\NormalTok{(all\_tables}\SpecialCharTok{$}\NormalTok{age)}
\end{Highlighting}
\end{Shaded}

\hypertarget{check-data}{%
\subsection{Check data}\label{check-data}}

Having gathered and cleaned the data, we would like to run a few checks.
Given the format of the data, we can check that `total' is the sum of
`male' and `female'. (While we would prefer to use different groupings,
this is what the Kenyan government collected and makes available.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{follow\_up }\OtherTok{\textless{}{-}} 
\NormalTok{  all\_tables }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{check\_sum =}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ female,}
         \AttributeTok{totals\_match =} \FunctionTok{if\_else}\NormalTok{(total }\SpecialCharTok{==}\NormalTok{ check\_sum, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{         ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(totals\_match }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And we can adjust the one that looks to be wrong.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# There is just one that looks wrong}
\NormalTok{all\_tables}\SpecialCharTok{$}\NormalTok{male[all\_tables}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{==} \StringTok{"10"} \SpecialCharTok{\&}\NormalTok{ all\_tables}\SpecialCharTok{$}\NormalTok{page }\SpecialCharTok{==} \DecValTok{187}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{as.integer}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\FunctionTok{rm}\NormalTok{(follow\_up)}
\end{Highlighting}
\end{Shaded}

The Kenyan census provides different tables for the total of each county
and sub-county; and then within each county, for the number in an urban
area in that county, and the number in a urban area in that county. Some
counties only have an urban count, but we would like to make sure that
the sum of rural and urban counts equals the total count. This requires
pivoting the data from long to wide.

First, we construct different tables for each of the three.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Table 2.3}
\NormalTok{table\_2\_3 }\OtherTok{\textless{}{-}}\NormalTok{ all\_tables }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(table }\SpecialCharTok{==} \StringTok{"Table 2.3: Distribution of Population by Age, Sex*, County and Sub{-} County"}\NormalTok{)}
\NormalTok{table\_2\_4a }\OtherTok{\textless{}{-}}\NormalTok{ all\_tables }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(table }\SpecialCharTok{==} \StringTok{"Table 2.4a: Distribution of Rural Population by Age, Sex* and County"}\NormalTok{)}
\NormalTok{table\_2\_4b }\OtherTok{\textless{}{-}}\NormalTok{ all\_tables }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(table }\SpecialCharTok{==} \StringTok{"Table 2.4b: Distribution of Urban Population by Age, Sex* and County"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then having constructed the constituent parts, we can join then based on
age, area, and whether it is a county.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{both\_2\_4s }\OtherTok{\textless{}{-}}
\CommentTok{\#| echo: true}
  \FunctionTok{full\_join}\NormalTok{(}
\NormalTok{    table\_2\_4a,}
\NormalTok{    table\_2\_4b,}
    \AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"area"}\NormalTok{, }\StringTok{"area\_type"}\NormalTok{),}
    \AttributeTok{suffix =} \FunctionTok{c}\NormalTok{(}\StringTok{"\_rural"}\NormalTok{, }\StringTok{"\_urban"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{all }\OtherTok{\textless{}{-}}
  \FunctionTok{full\_join}\NormalTok{(}
\NormalTok{    table\_2\_3,}
\NormalTok{    both\_2\_4s,}
    \AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"area"}\NormalTok{, }\StringTok{"area\_type"}\NormalTok{),}
    \AttributeTok{suffix =} \FunctionTok{c}\NormalTok{(}\StringTok{"\_all"}\NormalTok{, }\StringTok{"\_"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{all }\OtherTok{\textless{}{-}}
\NormalTok{  all }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{page =}\NormalTok{ glue}\SpecialCharTok{::}\FunctionTok{glue}\NormalTok{(}
      \StringTok{\textquotesingle{}Total from p. \{page\}, rural from p. \{page\_rural\}, urban from p. \{page\_urban\}\textquotesingle{}}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}
    \SpecialCharTok{{-}}\NormalTok{page,}
    \SpecialCharTok{{-}}\NormalTok{page\_rural,}
    \SpecialCharTok{{-}}\NormalTok{page\_urban,}\SpecialCharTok{{-}}\NormalTok{table,}
    \SpecialCharTok{{-}}\NormalTok{table\_rural,}
    \SpecialCharTok{{-}}\NormalTok{table\_urban,}\SpecialCharTok{{-}}\NormalTok{age\_type\_rural,}
    \SpecialCharTok{{-}}\NormalTok{age\_type\_urban}
\NormalTok{  )}

\FunctionTok{rm}\NormalTok{(both\_2\_4s, table\_2\_3, table\_2\_4a, table\_2\_4b)}
\end{Highlighting}
\end{Shaded}

We can now check that the sum of rural and urban is the same as the
total.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{follow\_up }\OtherTok{\textless{}{-}} 
\NormalTok{  all }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{total\_from\_bits =}\NormalTok{ total\_rural }\SpecialCharTok{+}\NormalTok{ total\_urban,}
         \AttributeTok{check\_total\_is\_rural\_plus\_urban =} \FunctionTok{if\_else}\NormalTok{(total }\SpecialCharTok{==}\NormalTok{ total\_from\_bits, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{),}
\NormalTok{         total\_from\_bits }\SpecialCharTok{{-}}\NormalTok{ total) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(check\_total\_is\_rural\_plus\_urban }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}

\FunctionTok{head}\NormalTok{(follow\_up)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 16
  age        male female  total area  area_type age_type male_rural female_rural
  <fct>     <int>  <int>  <int> <chr> <chr>     <chr>         <int>        <int>
1 Not Sta~     31     10     41 Naku~ county    single-~          8            6
2 Total    434287 441379 875666 Bomet county    age-gro~     420119       427576
3 Not Sta~      3      2      5 Bomet county    single-~          2            1
# ... with 7 more variables: total_rural <int>, male_urban <int>,
#   female_urban <int>, total_urban <int>, total_from_bits <int>,
#   check_total_is_rural_plus_urban <dbl>, `total_from_bits - total` <int>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(follow\_up)}
\end{Highlighting}
\end{Shaded}

There are just a few, but as they only have a difference of 1, we will
just move on.

Finally, we want to check that the single age counts sum to the
age-groups.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{follow\_up }\OtherTok{\textless{}{-}} 
\NormalTok{  all }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{groups =} \FunctionTok{case\_when}\NormalTok{(age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"0"}\NormalTok{, }\StringTok{"1"}\NormalTok{, }\StringTok{"2"}\NormalTok{, }\StringTok{"3"}\NormalTok{, }\StringTok{"4"}\NormalTok{, }\StringTok{"0{-}4"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"0{-}4"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"5"}\NormalTok{, }\StringTok{"6"}\NormalTok{, }\StringTok{"7"}\NormalTok{, }\StringTok{"8"}\NormalTok{, }\StringTok{"9"}\NormalTok{, }\StringTok{"5{-}9"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"5{-}9"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"10"}\NormalTok{, }\StringTok{"11"}\NormalTok{, }\StringTok{"12"}\NormalTok{, }\StringTok{"13"}\NormalTok{, }\StringTok{"14"}\NormalTok{, }\StringTok{"10{-}14"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"10{-}14"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"15"}\NormalTok{, }\StringTok{"16"}\NormalTok{, }\StringTok{"17"}\NormalTok{, }\StringTok{"18"}\NormalTok{, }\StringTok{"19"}\NormalTok{, }\StringTok{"15{-}19"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"15{-}19"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"20"}\NormalTok{, }\StringTok{"21"}\NormalTok{, }\StringTok{"22"}\NormalTok{, }\StringTok{"23"}\NormalTok{, }\StringTok{"24"}\NormalTok{, }\StringTok{"20{-}24"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"20{-}24"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"25"}\NormalTok{, }\StringTok{"26"}\NormalTok{, }\StringTok{"27"}\NormalTok{, }\StringTok{"28"}\NormalTok{, }\StringTok{"29"}\NormalTok{, }\StringTok{"25{-}29"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"25{-}29"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"30"}\NormalTok{, }\StringTok{"31"}\NormalTok{, }\StringTok{"32"}\NormalTok{, }\StringTok{"33"}\NormalTok{, }\StringTok{"34"}\NormalTok{, }\StringTok{"30{-}34"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"30{-}34"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"35"}\NormalTok{, }\StringTok{"36"}\NormalTok{, }\StringTok{"37"}\NormalTok{, }\StringTok{"38"}\NormalTok{, }\StringTok{"39"}\NormalTok{, }\StringTok{"35{-}39"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"35{-}39"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"40"}\NormalTok{, }\StringTok{"41"}\NormalTok{, }\StringTok{"42"}\NormalTok{, }\StringTok{"43"}\NormalTok{, }\StringTok{"44"}\NormalTok{, }\StringTok{"40{-}44"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"40{-}44"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"45"}\NormalTok{, }\StringTok{"46"}\NormalTok{, }\StringTok{"47"}\NormalTok{, }\StringTok{"48"}\NormalTok{, }\StringTok{"49"}\NormalTok{, }\StringTok{"45{-}49"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"45{-}49"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"50"}\NormalTok{, }\StringTok{"51"}\NormalTok{, }\StringTok{"52"}\NormalTok{, }\StringTok{"53"}\NormalTok{, }\StringTok{"54"}\NormalTok{, }\StringTok{"50{-}54"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"50{-}54"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"55"}\NormalTok{, }\StringTok{"56"}\NormalTok{, }\StringTok{"57"}\NormalTok{, }\StringTok{"58"}\NormalTok{, }\StringTok{"59"}\NormalTok{, }\StringTok{"55{-}59"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"55{-}59"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"60"}\NormalTok{, }\StringTok{"61"}\NormalTok{, }\StringTok{"62"}\NormalTok{, }\StringTok{"63"}\NormalTok{, }\StringTok{"64"}\NormalTok{, }\StringTok{"60{-}64"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"60{-}64"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"65"}\NormalTok{, }\StringTok{"66"}\NormalTok{, }\StringTok{"67"}\NormalTok{, }\StringTok{"68"}\NormalTok{, }\StringTok{"69"}\NormalTok{, }\StringTok{"65{-}69"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"65{-}69"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"70"}\NormalTok{, }\StringTok{"71"}\NormalTok{, }\StringTok{"72"}\NormalTok{, }\StringTok{"73"}\NormalTok{, }\StringTok{"74"}\NormalTok{, }\StringTok{"70{-}74"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"70{-}74"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"75"}\NormalTok{, }\StringTok{"76"}\NormalTok{, }\StringTok{"77"}\NormalTok{, }\StringTok{"78"}\NormalTok{, }\StringTok{"79"}\NormalTok{, }\StringTok{"75{-}79"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"75{-}79"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"80"}\NormalTok{, }\StringTok{"81"}\NormalTok{, }\StringTok{"82"}\NormalTok{, }\StringTok{"83"}\NormalTok{, }\StringTok{"84"}\NormalTok{, }\StringTok{"80{-}84"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"80{-}84"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"85"}\NormalTok{, }\StringTok{"86"}\NormalTok{, }\StringTok{"87"}\NormalTok{, }\StringTok{"88"}\NormalTok{, }\StringTok{"89"}\NormalTok{, }\StringTok{"85{-}89"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"85{-}89"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"90"}\NormalTok{, }\StringTok{"91"}\NormalTok{, }\StringTok{"92"}\NormalTok{, }\StringTok{"93"}\NormalTok{, }\StringTok{"94"}\NormalTok{, }\StringTok{"90{-}94"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"90{-}94"}\NormalTok{,}
\NormalTok{                            age }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"95"}\NormalTok{, }\StringTok{"96"}\NormalTok{, }\StringTok{"97"}\NormalTok{, }\StringTok{"98"}\NormalTok{, }\StringTok{"99"}\NormalTok{, }\StringTok{"95{-}99"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"95{-}99"}\NormalTok{,}
                            \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"Other"}\NormalTok{)}
\NormalTok{         ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(area\_type, area, groups) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{group\_sum =} \FunctionTok{sum}\NormalTok{(total, }\AttributeTok{na.rm =} \ConstantTok{FALSE}\NormalTok{),}
         \AttributeTok{group\_sum =}\NormalTok{ group\_sum }\SpecialCharTok{/} \DecValTok{2}\NormalTok{,}
         \AttributeTok{difference =}\NormalTok{ total }\SpecialCharTok{{-}}\NormalTok{ group\_sum) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(age }\SpecialCharTok{==}\NormalTok{ groups) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(total }\SpecialCharTok{!=}\NormalTok{ group\_sum) }

\FunctionTok{head}\NormalTok{(follow\_up)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 16
  age    male female total area       area_type age_type male_rural female_rural
  <fct> <int>  <int> <int> <chr>      <chr>     <chr>         <int>        <int>
1 0-4       1      5     6 Mt. Kenya~ sub-coun~ age-gro~         NA           NA
2 5-9       1      2     3 Mt. Kenya~ sub-coun~ age-gro~         NA           NA
3 10-14     6      0     6 Mt. Kenya~ sub-coun~ age-gro~         NA           NA
4 15-19     9      1    10 Mt. Kenya~ sub-coun~ age-gro~         NA           NA
5 20-24    21      4    25 Mt. Kenya~ sub-coun~ age-gro~         NA           NA
6 25-29    59      9    68 Mt. Kenya~ sub-coun~ age-gro~         NA           NA
# ... with 7 more variables: total_rural <int>, male_urban <int>,
#   female_urban <int>, total_urban <int>, groups <chr>, group_sum <dbl>,
#   difference <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(follow\_up)}
\end{Highlighting}
\end{Shaded}

Mt. Kenya Forest, Aberdare Forest, Kakamega Forest are all slightly
dodgy. It does not seem to be in the documentation, but it looks like
the Kenyan government has apportioned these between various countries.
This is understandable, and unlikely to be a big deal, so, again, we
will just move on.

\hypertarget{tidy-up}{%
\subsection{Tidy-up}\label{tidy-up}}

Now that we are confident that everything is looking good, we can just
convert it to tidy format. This will make it easier to work with.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all }\OtherTok{\textless{}{-}}
\NormalTok{  all }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{male\_total =}\NormalTok{ male,}
         \AttributeTok{female\_total =}\NormalTok{ female,}
         \AttributeTok{total\_total =}\NormalTok{ total) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_longer}\NormalTok{(}
    \AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}
\NormalTok{      male\_total,}
\NormalTok{      female\_total,}
\NormalTok{      total\_total,}
\NormalTok{      male\_rural,}
\NormalTok{      female\_rural,}
\NormalTok{      total\_rural,}
\NormalTok{      male\_urban,}
\NormalTok{      female\_urban,}
\NormalTok{      total\_urban}
\NormalTok{    ),}
    \AttributeTok{names\_to =} \StringTok{"type"}\NormalTok{,}
    \AttributeTok{values\_to =} \StringTok{"number"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{separate}\NormalTok{(}
    \AttributeTok{col =}\NormalTok{ type,}
    \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"gender"}\NormalTok{, }\StringTok{"part\_of\_area"}\NormalTok{),}
    \AttributeTok{sep =} \StringTok{"\_"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(area, area\_type, part\_of\_area, age, age\_type, gender, number)}

\FunctionTok{head}\NormalTok{(all)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 7
  area    area_type part_of_area age   age_type  gender  number
  <chr>   <chr>     <chr>        <fct> <chr>     <chr>    <int>
1 Mombasa county    total        Total age-group male    610257
2 Mombasa county    total        Total age-group female  598046
3 Mombasa county    total        Total age-group total  1208303
4 Mombasa county    rural        Total age-group male        NA
5 Mombasa county    rural        Total age-group female      NA
6 Mombasa county    rural        Total age-group total       NA
\end{verbatim}

The original purpose of cleaning this dataset was to make a table for M.
Alexander and Alkema (2021). Just to bring this all together, we could
make a graph of single-year counts, by gender, for Nairobi
(Figure~\ref{fig-monicasnairobigraph})).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{monicas\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  all }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(area\_type }\SpecialCharTok{==} \StringTok{"county"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(part\_of\_area }\SpecialCharTok{==} \StringTok{"total"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(age\_type }\SpecialCharTok{==} \StringTok{"single{-}year"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(area, age, gender, number)}

\FunctionTok{head}\NormalTok{(monicas\_dataset)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
  area    age   gender number
  <chr>   <fct> <chr>   <int>
1 Mombasa 0     male    15111
2 Mombasa 0     female  15009
3 Mombasa 0     total   30120
4 Mombasa 1     male    15805
5 Mombasa 1     female  15308
6 Mombasa 1     total   31113
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{monicas\_dataset }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(area }\SpecialCharTok{==} \StringTok{"Nairobi"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(gender }\SpecialCharTok{!=} \StringTok{"total"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ number, }\AttributeTok{fill =}\NormalTok{ gender)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ number, }\AttributeTok{fill =}\NormalTok{ gender), }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{comma) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_discrete}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\AttributeTok{from =} \DecValTok{0}\NormalTok{, }\AttributeTok{to =} \DecValTok{99}\NormalTok{, }\AttributeTok{by =} \DecValTok{5}\NormalTok{), }\StringTok{"100+"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Number"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Age"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Gender"}\NormalTok{,}
       \AttributeTok{caption =} \StringTok{"Data source: 2019 Kenya Census"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./11-clean_and_prepare_files/figure-pdf/fig-monicasnairobigraph-1.pdf}

}

\caption{\label{fig-monicasnairobigraph}Distribution of age and gender
in Nairobi in 2019, based on Kenyan census}

\end{figure}

A variety of features are clear from
Figure~\ref{fig-monicasnairobigraph}, including age-heaping, a slight
difference in the ratio of male-female birth, and a substantial
difference between ages 15 and 25.

\hypertarget{checks-and-tests}{%
\section{Checks and tests}\label{checks-and-tests}}

Robert Caro, the biographer of Lyndon Johnson, spent years tracking down
everyone connected to the 36th President of the United States. He went
to far as to live in Texas Hill Country for three years so that he could
better understand where LBJ was from. When he heard a story that LBJ
used to run to the Senate when he was a senator, he ran that route
multiple times himself to try to understand why LBJ was running. Caro
eventually understood it only when he ran the route as the sun was
rising, just as LBJ had done, that the sun hits the Senate Rotunda in a
particularly inspiring way (Caro 2019). This background work enabled him
to uncover aspects that no one else knew. For instance, it turns out
that LBJ almost surely stole his first election win as a Texas Senator
(Caro 2019). We need to understand our data to this same extent. We must
turn every page and go to every extreme.

The idea of negative space is well established in design. It refers to
that which surrounds the subject. Sometimes negative space is used as an
effect, for instance the logo of FedEx, an American logistics company,
has negative space between the E and x that creates an arrow. In a
similar way, we want to be cognizant of the data that we have, and the
data that we do not have. We are worried that the data that we do not
have somehow has meaning, potentially even to the extent of changing our
conclusions. When we are cleaning data, we are looking for anomalies. We
are interested in values that are in there that should not be, but also
the opposite situation---values that are missing that should not be.
There are four tools that we use to identify these situations: graphs,
counts, green/red conditions, targets.

\hypertarget{graphs-1}{%
\subsection{Graphs}\label{graphs-1}}

Graphs are an invaluable tool when cleaning data, because they show each
point in the dataset, in relation to the other points. They are
especially useful for identifying when a value does not belong. For
instance, if a value is expected to be numerical, but it is still a
character then it will not plot and a warning will be displayed.

Graphs will be especially useful for numerical data, but are still
useful for text and categorical data. Let us pretend that we have a
situation where we are interested in a person's age, for some youth
survey. We have the following data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{ages =} \FunctionTok{c}\NormalTok{(}\DecValTok{11}\NormalTok{, }\DecValTok{17}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{150}\NormalTok{))}

\NormalTok{raw\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ ages, }\AttributeTok{x =} \DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./11-clean_and_prepare_files/figure-pdf/unnamed-chunk-81-1.pdf}

}

\end{figure}

The graph clearly shows the unexpected value of 150. The most likely
explanation is that the data were incorrectly entered with a trailing 0,
and should be 15. We can fix that, and document it, and then redo the
graph, so as to see that everything seems more reasonable now.

\hypertarget{counts}{%
\subsection{Counts}\label{counts}}

We want to focus on getting most of the data right. So we are interested
in the counts of unique values. Hopefully a majority of the data are
concentrated in the most common counts. But it can also be useful to
invert it, and see what is especially uncommon. The extent to which we
want to deal with these depends on what we need. Ultimately, each time
we fix one we are getting very few additional observations, potentially
even just one! Counts are especially useful with text or categorical
data, but can be helpful with numerical as well.

Let us see an example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{country =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Australie\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Austrelia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australie\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australie\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Aeustralia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Austraia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australia\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Australia\textquotesingle{}}
\NormalTok{                  )}
\NormalTok{         )}

\NormalTok{raw\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(country, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 2
  country        n
  <chr>      <int>
1 Australia      4
2 Australie      3
3 Aeustralia     1
4 Austraia       1
5 Austrelia      1
\end{verbatim}

The use of this count clearly identifies where we should spend our time
- changing `Australie' to `Australia' would almost double our amount of
usable data.

\hypertarget{gono-go}{%
\subsection{Go/no-go}\label{gono-go}}

Some things are so important that you require that your cleaned dataset
have them. These are go/no-go conditions. They would typically come out
of experience, expert knowledge, or the planning and simulation
exercises. An example may be that there are no negative numbers in an
age column, and no ages above 140.

For these we could specifically require that the condition is met. Other
examples include:

\begin{itemize}
\tightlist
\item
  If doing cross-country analysis, then a list of country names that we
  know should be in our dataset would be useful. Our no-go conditions
  would then be if there were: 1) values not in that list in our
  dataset, or, vice versa; 2) countries that we expected to be in there
  that were not.
\end{itemize}

To have a concrete example, let us consider if we were doing some
analysis about the five largest counties in Kenya: `Nairobi', `Kiambu',
`Nakuru', `Kakamega', `Bungoma'. Let us create that array first.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correct\_counties }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Nairobi\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Kiambu\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Nakuru\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Kakamega\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bungoma\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We begin with the following dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top\_five\_kenya }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{county =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Nairobi\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Nairob1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Nakuru\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Kakamega\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Nakuru\textquotesingle{}}\NormalTok{, }
                      \StringTok{\textquotesingle{}Kiambu\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Kiambru\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Kabamega\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bun8oma\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bungoma\textquotesingle{}}\NormalTok{)}
\NormalTok{  )}

\NormalTok{top\_five\_kenya }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(county, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 9 x 2
  county       n
  <chr>    <int>
1 Nakuru       2
2 Bun8oma      1
3 Bungoma      1
4 Kabamega     1
5 Kakamega     1
6 Kiambru      1
7 Kiambu       1
8 Nairob1      1
9 Nairobi      1
\end{verbatim}

Based on the count we know that we have to fix some of them and there
are two with numbers that are obvious fixes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top\_five\_kenya }\OtherTok{\textless{}{-}} 
\NormalTok{  top\_five\_kenya }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{county =} \FunctionTok{str\_replace\_all}\NormalTok{(county, }\StringTok{\textquotesingle{}Nairob1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Nairobi\textquotesingle{}}\NormalTok{),}
         \AttributeTok{county =} \FunctionTok{str\_replace\_all}\NormalTok{(county, }\StringTok{\textquotesingle{}Bun8oma\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bungoma\textquotesingle{}}\NormalTok{)}
\NormalTok{  )}

\NormalTok{top\_five\_kenya }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(county, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 7 x 2
  county       n
  <chr>    <int>
1 Bungoma      2
2 Nairobi      2
3 Nakuru       2
4 Kabamega     1
5 Kakamega     1
6 Kiambru      1
7 Kiambu       1
\end{verbatim}

At this point we can use our go/no-go conditions to decide whether we
are finished or not.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top\_five\_kenya}\SpecialCharTok{$}\NormalTok{county }\SpecialCharTok{|\textgreater{}} \FunctionTok{unique}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Nairobi"  "Nakuru"   "Kakamega" "Kiambu"   "Kiambru"  "Kabamega" "Bungoma" 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\FunctionTok{all}\NormalTok{(top\_five\_kenya}\SpecialCharTok{$}\NormalTok{county }\SpecialCharTok{|\textgreater{}} \FunctionTok{unique}\NormalTok{() }\SpecialCharTok{==}\NormalTok{ top\_five\_kenya)) \{}
  \StringTok{"Oh no"}
\NormalTok{\}}
\ControlFlowTok{if}\NormalTok{(}\FunctionTok{all}\NormalTok{(top\_five\_kenya}\SpecialCharTok{==}\NormalTok{top\_five\_kenya}\SpecialCharTok{$}\NormalTok{county }\SpecialCharTok{|\textgreater{}} \FunctionTok{unique}\NormalTok{()) ) \{}
  \StringTok{"Oh no"}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

And so it is clear that we still have cleaning to do!

We may also find similar conditions from experts and those with
experience in the particular field.

\hypertarget{class-1}{%
\subsection{Class}\label{class-1}}

It is often said that American society is obsessed with money, while
British society is obsessed with class. In the case of data cleaning and
preparation we need to be British. Explicit checks of the class of
variables are essential. Accidentally assigning the wrong class to a
variable can have a large effect on subsequent analysis. In particular:

\begin{itemize}
\tightlist
\item
  check whether some value should be a number or a factor; and
\item
  check that dates are correctly formatted.
\end{itemize}

To understand why it is important to be clear about whether a value is a
number or a factor, consider the following situation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{some\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{response =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{),}
         \AttributeTok{group =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{group\_as\_integer =} \FunctionTok{as.integer}\NormalTok{(group),}
         \AttributeTok{group\_as\_factor =} \FunctionTok{as.factor}\NormalTok{(group),}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

Let us start with `group' as an integer and look at a logistic
regression.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(response}\SpecialCharTok{\textasciitilde{}}\NormalTok{group\_as\_integer, }\AttributeTok{data =}\NormalTok{ some\_data) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = response ~ group_as_integer, data = some_data)

Residuals:
   Min     1Q Median     3Q    Max 
 -0.68  -0.52   0.32   0.32   0.64 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)
(Intercept)        0.8400     0.4495   1.869    0.104
group_as_integer  -0.1600     0.2313  -0.692    0.511

Residual standard error: 0.5451 on 7 degrees of freedom
Multiple R-squared:  0.064, Adjusted R-squared:  -0.06971 
F-statistic: 0.4786 on 1 and 7 DF,  p-value: 0.5113
\end{verbatim}

Now we can try it as a factor. The interpretation of the variable is
completely different.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(response}\SpecialCharTok{\textasciitilde{}}\NormalTok{group\_as\_factor, }\AttributeTok{data =}\NormalTok{ some\_data) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = response ~ group_as_factor, data = some_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.7500 -0.3333  0.2500  0.2500  0.6667 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)  
(Intercept)        0.7500     0.2826   2.654   0.0378 *
group_as_factor2  -0.4167     0.4317  -0.965   0.3717  
group_as_factor3  -0.2500     0.4895  -0.511   0.6278  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5652 on 6 degrees of freedom
Multiple R-squared:  0.1375,    Adjusted R-squared:  -0.15 
F-statistic: 0.4783 on 2 and 6 DF,  p-value: 0.6416
\end{verbatim}

Another critical aspect is to check the dates. In particular we want to
try to make it into the following format: YYYY-MM-DD. There are of
course differences of opinion as to what is an appropriate date format
in the broader world, and reasonable people can differ on whether 1 July
2010 or July 1, 2020, is better, but YYYY-MM-DD is the format that is
generally most appropriate for data.

\hypertarget{naming-things}{%
\section{Naming things}\label{naming-things}}

\begin{quote}
An improved scanning software we developed identified gene name errors
in 30.9\% (3,436/11,117) of articles with supplementary Excel gene
lists; a figure significantly higher than previously estimated. This is
due to gene names being converted not just to dates and floating-point
numbers, but also to internal date format (five-digit numbers).

Abeysooriya et al. (2021)
\end{quote}

Names matter. The land on which much of this book was written is today
named Toronto, which is within a country named Canada, but for a long
time before was known as Turtle Island. While not common, these days
people will sometimes still refer to themselves as being on Turtle
Island. That tells us something about them, and our use of the name
Canada tells them something about us. There is a big rock in the center
of Australia. For a long time, it was called Uluru, then it was known as
Ayers Rock. Today it has a dual name that combines both, and the choice
of which name you use tells someone something about you. Even the
British Royal Family recognize the power of names. In 1917 they changed
from the House of Saxe-Coburg and Gotha to the House of Windsor, due to
a feeling that the former was too Germanic given World War I was
ongoing. Names matter in everyday life. And they matter in data science
too.

The importance of names, and of ignoring existing claims through
re-naming was clear in those cases, but we see it in data science as
well. We need to be very careful when we name our datasets, our
variables, and our functions. There is a tendency, these days, to call
the variable `gender' even though it may only have male and female,
because we do not want to say the word `sex'. Tukey (1962) essentially
defines what we today call data science, but it was popularized by folks
in computer science in the 2010s who ignored, either deliberately or
through ignorance, what came before them. The past ten years has been
characteristic by the renaming of concepts that were well-established in
the fields that computer science has recently expanded into. For
instance, the use of binary variables in regression, sometimes called
`dummy variables', is called one-hot encoding in computer science. Like
all fashions, this one will pass also. We most recently saw this through
the 1980s through to early 2010s with economics. Economists described
themselves as the `queen of the social sciences' and self-described as
imperialistic (Lazear 2000). We are now recognizing the costs of this
imperialism in social sciences, and in the future we will look back and
count the cost of computer science imperialism in data science. The key
here is that no area of study is ever \emph{terra nullius}, or nobody's
land. It is important to recognize, adopt, and use existing names, and
practices.

Names give places meaning, and by ignoring existing names, we ignore
what has come before us. Kimmerer (2012, 34) describes how `Tahawus is
the Algonquin name for Mount Marcy, the highest peak in the Adirondacks.
It's called Mount March to commemorate a governor who never set foot on
those wild slopes.' She continues that `{[}w{]}hen we call a place by
name it is transformed from wilderness to homeland.' She is talking with
regard to physical places, but the same is true of our function names,
our variable names and our dataset names. When we use gender instead of
sex because we do not want to say sex in front of others, we ignore the
preferences of those that provided data.

In addition to respecting the nature of the data, names need to satisfy
two additional considerations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  they need to be machine readable, and
\item
  they need to be human readable.
\end{enumerate}

Machine readable names is an easier standard to meet, but usually means
avoiding spaces and special characters. A space can be replaced with a
underbar. Usually, special characters should just be removed because
they can be inconsistent between different computers and languages. The
names should also be unique within a dataset, and unique within a
collection of datasets unless that particular column is being
deliberately used as a key to join different datasets.

An especially useful function to use to get closer to machine readable
names is \texttt{janitor::clean\_names()} which is from the
\texttt{janitor} package (Firke 2020). This deals with those issues
mentioned above as well as a few others. We can see an example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bad\_names\_good\_names }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \StringTok{\textquotesingle{}First\textquotesingle{}} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{),}
    \StringTok{\textquotesingle{}second name has spaces\textquotesingle{}} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{),}
    \StringTok{\textquotesingle{}weird\#symbol\textquotesingle{}} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{),}
    \StringTok{\textquotesingle{}InCoNsIsTaNtCaPs\textquotesingle{}} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  )}

\NormalTok{bad\_names\_good\_names}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 4
  First `second name has spaces` `weird#symbol` InCoNsIsTaNtCaPs
  <dbl>                    <dbl>          <dbl>            <dbl>
1     1                        1              1                1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bad\_names\_good\_names }\OtherTok{\textless{}{-}} 
\NormalTok{  bad\_names\_good\_names }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  janitor}\SpecialCharTok{::}\FunctionTok{clean\_names}\NormalTok{()}
  
\NormalTok{bad\_names\_good\_names}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 4
  first second_name_has_spaces weird_number_symbol in_co_ns_is_ta_nt_ca_ps
  <dbl>                  <dbl>               <dbl>                   <dbl>
1     1                      1                   1                       1
\end{verbatim}

Human readable names require an additional layer. We need to consider
other cultures and how they may interpret some of the names that we are
using. We also need to consider different experience levels that
subsequent users of your dataset may have. This is both in terms of
experience with programming and statistics, but also experience with
similar datasets. For instance, a column of `flag' is often used to
signal that a column contains data that needs to be followed up with or
treated carefully in some way. An experienced analyst will know this,
but a beginner will not. Try to use meaningful names wherever possible
(Lin, Ali, and Wilson 2020). It has been found that shorter names may
take longer to comprehend (Hofmeister, Siegmund, and Holt 2017), and so
it is often useful to avoid abbreviations where possible.

One interesting feature of R is that in certain cases partial matching
on names is possible. For instance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{never\_use\_partial\_matching }\OtherTok{\textless{}{-}} 
  \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{my\_first\_name =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
    \AttributeTok{another\_name =} \FunctionTok{c}\NormalTok{(}\StringTok{"wow"}\NormalTok{, }\StringTok{"great"}\NormalTok{)}
\NormalTok{  )}

\NormalTok{never\_use\_partial\_matching}\SpecialCharTok{$}\NormalTok{my\_first\_name}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{never\_use\_partial\_matching}\SpecialCharTok{$}\NormalTok{my}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1 2
\end{verbatim}

This behavior is not possible within the \texttt{tidyverse}, for
instance if \texttt{data.frame} were replaced with \texttt{tibble} in
the above code. Partial matching should almost never be used. It makes
it more difficult to understand code after a break, and for others to
come to it fresh.

Riederer (2020) advises using column names as contracts, through
establishing a controlled vocabulary for column names. In this way, we
would define a set of words that we can use in column names. In the
controlled vocabulary of Riederer (2020) a column could start with an
abbreviation for its class, then something specific to what it pertains
to, and then various details. For instance, in the Kenyan data example
earlier we have the following column names: ``area'', ``age'',
``gender'', and ``number''. If we were to use our column names as
contracts, then these could be: ``chr\_area'', ``fctr\_group\_age'',
``chr\_group\_gender'', and ``int\_group\_count''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{column\_names\_as\_contracts }\OtherTok{\textless{}{-}} 
\NormalTok{  monicas\_dataset }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}
    \StringTok{"chr\_area"} \OtherTok{=} \StringTok{"area"}\NormalTok{,}
    \StringTok{"fctr\_group\_age"} \OtherTok{=} \StringTok{"age"}\NormalTok{,}
    \StringTok{"chr\_group\_gender"} \OtherTok{=} \StringTok{"gender"}\NormalTok{,}
    \StringTok{"int\_group\_count"} \OtherTok{=} \StringTok{"number"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We can then use \texttt{pointblank} (Iannone and Vargas 2022) to set-up
tests for us.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pointblank)}

\NormalTok{agent }\OtherTok{\textless{}{-}}
  \FunctionTok{create\_agent}\NormalTok{(}\AttributeTok{tbl =}\NormalTok{ column\_names\_as\_contracts) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{col\_is\_character}\NormalTok{(}\AttributeTok{columns =} \FunctionTok{vars}\NormalTok{(chr\_area, chr\_group\_gender)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{col\_is\_factor}\NormalTok{(}\AttributeTok{columns =} \FunctionTok{vars}\NormalTok{(fctr\_group\_age)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{col\_is\_integer}\NormalTok{(}\AttributeTok{columns =} \FunctionTok{vars}\NormalTok{(int\_group\_count)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{col\_vals\_in\_set}\NormalTok{(}\AttributeTok{columns =}\NormalTok{ chr\_group\_gender,}
                  \AttributeTok{set =} \FunctionTok{c}\NormalTok{(}\StringTok{"male"}\NormalTok{, }\StringTok{"female"}\NormalTok{, }\StringTok{"total"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{interrogate}\NormalTok{()}

\NormalTok{agent}
\end{Highlighting}
\end{Shaded}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{lrlllccrrrcccc}
\caption*{
{\large Pointblank Validation} \\ 
{\small {[}2022-04-19\textbar{}07:48:59{]}}
} \\ 
\toprule
 &  & STEP & COLUMNS & VALUES & TBL & EVAL & UNITS & PASS & FAIL & W & S & N & EXT \\ 
\midrule
 & 1 &  &  & --- &                                                              & ✓ & <code>$1$</code> & <code>$1$</code><br><code>$1.00$</code> & <code>$0$</code><br><code>$0.00$</code> & --- & --- & --- & --- \\ 
 & 2 &  &  & --- &                                                              & ✓ & <code>$1$</code> & <code>$1$</code><br><code>$1.00$</code> & <code>$0$</code><br><code>$0.00$</code> & --- & --- & --- & --- \\ 
 & 3 &  &  & --- &                                                              & ✓ & <code>$1$</code> & <code>$1$</code><br><code>$1.00$</code> & <code>$0$</code><br><code>$0.00$</code> & --- & --- & --- & --- \\ 
 & 4 &  &  & --- &                                                              & ✓ & <code>$1$</code> & <code>$1$</code><br><code>$1.00$</code> & <code>$0$</code><br><code>$0.00$</code> & --- & --- & --- & --- \\ 
 & 5 &  &  &  &                                                              & ✓ & <code>$14K$</code> & <code>$14K$</code><br><code>$1.00$</code> & <code>$0$</code><br><code>$0.00$</code> & --- & --- & --- & --- \\ 
 \bottomrule
\end{longtable}
\begin{minipage}{\linewidth}
2022-04-19 07:48:59 EDT
\textless{} 1 s
2022-04-19 07:48:59 EDT\\ 
\end{minipage}

\hypertarget{exercises-and-tutorial-10}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-10}}

\hypertarget{exercises-10}{%
\subsection{Exercises}\label{exercises-10}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Is the following an example of tidy data?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tibble}\NormalTok{(}\AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Anne\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Bethany\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Stephen\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}William\textquotesingle{}}\NormalTok{),}
       \AttributeTok{age\_group =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}18{-}29\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}30{-}44\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}45{-}60\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}60+\textquotesingle{}}\NormalTok{),}
\NormalTok{       )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 2
  name    age_group
  <chr>   <chr>    
1 Anne    18-29    
2 Bethany 30-44    
3 Stephen 45-60    
4 William 60+      
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  If I am dealing with ages then what is the most likely class for the
  variable? {[}Select all that apply.{]}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    integer
  \item
    matrix
  \item
    numeric
  \item
    factor
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial-10}{%
\subsection{Tutorial}\label{tutorial-10}}

With regard to Jordan (2019), D'Ignazio and Klein (2020), Chapter 6, Au
(2020), and other relevant work, to what extent do you think we should
let the data speak for themselves? {[}Please write a page or two.{]}

\hypertarget{sec-store-and-share}{%
\chapter{Store and share}\label{sec-store-and-share}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{Datasheets for datasets}, (Gebru et al. 2021).
\item
  Read \emph{Data and its (dis)contents: A survey of dataset development
  and use in machine learning research}, (Paullada et al. 2021).
\item
  Read \emph{Ten simple rules for responsible big data research}, (Zook
  et al. 2017).
\item
  Read \emph{How Data Can Be Used Against People: A Classification of
  Personal Data Misuses}, (Kröger, Miceli, and Müller 2021).
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  FAIR principles for data sharing and management.
\item
  Sharing data using: GitHub, R data packages, and depositing data.
\item
  Data documentation and especially datasheets for datasets.
\item
  Understanding what is personally identifying information, and how this
  depends on context.
\item
  Implementing data simulation to share data.
\item
  Know what differential privacy is and some of its likely effect on
  data science.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{openssl} (Ooms 2021)
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{openssl::md5()}
\item
  \texttt{openssl::sha512()}
\end{itemize}

\hypertarget{introduction-9}{%
\section{Introduction}\label{introduction-9}}

After we have put together a dataset, an important part of being
responsible is storing it appropriately and enabling easy retrieval.
While it is certainly possible to be especially concerned about this,
and entire careers are based on the storage and retrieval of data, to a
certain extent, the baseline is not onerous. If we can get our dataset
off our own computer, then we are much of the way there. Further
confirming that someone else can retrieve it and use it, puts us much
further than most.

That said, the FAIR principles are useful when we come to think more
formally about data sharing and management. These are (M. D. Wilkinson
et al. 2016):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Findable. There is one, unchanging, identifier for the dataset and the
  dataset has high-quality descriptions and explanations.
\item
  Accessible. Standardized approaches can be used to retrieve the data,
  and these are open and free, possibly with authentication, and that
  the metadata persists even if the dataset is removed.
\item
  Interoperable. The dataset and its metadata use a broadly applicable
  language, and vocabulary.
\item
  Reusable. There is plenty of description of the dataset and the usage
  conditions are made clear along with provenance.
\end{enumerate}

It is important to recognize that just because a dataset is FAIR, it is
not necessarily an unbiased representation of the world. FAIR reflects
whether a dataset is appropriately available, not whether it is
appropriate.

One reason for the rise of data science is that humans are at the heart
of it. And often the data that we are interested in directly concern
humans. This means there is a tension between sharing the data to
facilitate reproducibility and maintaining privacy. Medicine has
developed approaches to this over a long time. And out of that we have
seen Health Insurance Portability and Accountability Act (HIPAA) in the
US, and the related General Data Protection Regulation (GDPR) in Europe.
Our concerns in data science tend to be about personally identifying
information (PII). We have a variety of ways to protect especially
private information in our datasets, such as emails, including hashing.
And sometimes we simulate data and distribute that instead of sharing
the actual dataset. More recently, differential privacy is being
implemented. But this is usually an inappropriate choice, for anything
other than the most massive of datasets and ensures a level of privacy,
only at the expense of population minorities.

In this chapter we will consider how we plan and organize our datasets
to meet essential requirements. To a large extent we put these in place
to make our own life easier when we come back to use our dataset later.
We then go through putting our dataset on GitHub, building R packages
for data, and finally depositing it in various archives. Then we
consider documentation, and in particular focus on datasheets.

\hypertarget{plan-3}{%
\section{Plan}\label{plan-3}}

The storage and retrieval of information has a long history. This is
especially connected with libraries which have existed since antiquity
and have established protocols for deciding what information to store
and what to discard, as well as its retrieval. One of the defining
aspects of libraries is that deliberate curation and organization. The
cataloging system ensures that books on similar topics are located close
to each other, and there are typically also deliberate plans for
ensuring the collection is up-to-date.

Vannevar Bush defines a `memex' in 1945 as a device used to store books,
records, and communications, in a way that supplements memory (Bush
1945). And the key is the indexing, or linking together of items. We can
see this concept echoed in Tim Berners-Lee proposal for hypertext
(Berners-Lee 1989), which led to the World Wide Web. This is the way
that resources are identified. They are then transported over the
Internet, using HTTP.

At its most fundamental, this is about storing and retrieving data. For
instance, we make various files on our computer available to others. The
internet is famously brittle, but when we are considering the storage
and retrieval of our datasets we want to consider especially, for how
long it is important that the data are stored and for whom (Michener
2015). For instance, if we want some data to be available for a decade
and widely available then it becomes important to store data in open and
persistent formats, such as CSVs (Hart et al. 2016). But if we are just
using some data as part of an intermediate step, we have the raw data,
and the scripts to create it, then it might be fine to not worry too
much about such considerations.

Storing raw data is important and there are many cases where raw data
have revealed or hinted at fraud (Simonsohn 2013). Shared data also
enhances the credibility of our work, by enabling others to verify it,
and can lead to the generation of new knowledge as others use it to
answer different questions (Christensen, Freese, and Miguel 2019).
Finally, research that shares its data may be more highly cited
(Christensen et al. 2019).

\hypertarget{share-data}{%
\section{Share data}\label{share-data}}

\hypertarget{github-1}{%
\subsection{GitHub}\label{github-1}}

The easiest place to store our datasets will be GitHub because that is
already built into our workflow. For instance, when we push, our dataset
becomes available. One great benefit of this is that, if we have set-up
our workspace appropriately, then we likely store our raw data, and the
tidy data, as well as the scripts that are needed to transform one to
the other.

As an example of how we have stored some data, we can access the
`raw\_data.csv' file from the `starter\_folder'. To get the file that we
pass to \texttt{read\_csv()} we navigate to the file in GitHub, and then
click `Raw'.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{starter\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/RohanAlexander/starter\_folder/main/inputs/data/raw\_data.csv"}\NormalTok{)}

\NormalTok{starter\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
  first_col second_col third_col
  <chr>     <chr>      <chr>    
1 some      raw        data     
\end{verbatim}

One issue with this is that the dataset does not have much
documentation. While we can store and retrieve the dataset easily in
this way, it lacks much explanation, a formal dictionary, and aspects
such as a license that would bring our dataset closer to aligning with
the FAIR principles.

\hypertarget{r-packages-for-data}{%
\subsection{R Packages for data}\label{r-packages-for-data}}

To this point we have largely used R packages for their code, although
we have seen a few that were focused on sharing data, for instance,
Flynn (2021). We can build an R Package for our dataset and then add it
to GitHub. This will make it easy to store and retrieve because we can
obtain the dataset by loading the package. This will be the first R
package that we build. In Chapter @ref(deploying-models), will return to
R packages and use them to deploy models.

To get started, create a new package (`File' -\textgreater{} `New
project' -\textgreater{} `New Directory' -\textgreater{} `R Package').
Give the package a name, such as `favcolordata' and select `Open in new
session'. Create a new folder called `data'. We will simulate a dataset
that we will include.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{color\_data }\OtherTok{\textless{}{-}}
    \FunctionTok{tibble}\NormalTok{(}
        \AttributeTok{name =}
            \FunctionTok{c}\NormalTok{(}\StringTok{"Edward"}\NormalTok{,}
              \StringTok{"Helen"}\NormalTok{,}
              \StringTok{"Hugo"}\NormalTok{,}
              \StringTok{"Ian"}\NormalTok{,}
              \StringTok{"Monica"}\NormalTok{,}
              \StringTok{"Myles"}\NormalTok{,}
              \StringTok{"Patricia"}\NormalTok{,}
              \StringTok{"Roger"}\NormalTok{,}
              \StringTok{"Rohan"}\NormalTok{,}
              \StringTok{"Ruth"}
\NormalTok{            ),}
        \AttributeTok{fav\_color =}
            \FunctionTok{sample}\NormalTok{(}
                \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{, }\StringTok{"White"}\NormalTok{, }\StringTok{"Rainbow"}\NormalTok{),}
                \AttributeTok{size =} \DecValTok{10}\NormalTok{,}
                \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{            )}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

To this point we have largely been trying to use CSV files for our
datasets. To include our data in this R package, we will save our
dataset in a particular format, `rda', using \texttt{save()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{save}\NormalTok{(color\_data, }\AttributeTok{file=}\StringTok{"data/color\_data.rda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then create an R file `data.R' in the `R' folder. This R file will only
contain documentation using \texttt{roxygen2} comments, which start with
\texttt{\#\textquotesingle{}}, and we follow the documentation for
\texttt{troopdata} closely.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\textquotesingle{} Favorite color of various people data}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @description \textbackslash{}code\{favcolordata\} returns a dataframe of the favorite color of various people.}
\CommentTok{\#\textquotesingle{} }
\CommentTok{\#\textquotesingle{} @return Returns a dataframe of the favorite color of various people.}
\CommentTok{\#\textquotesingle{} }
\CommentTok{\#\textquotesingle{} @docType data}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @usage data(color\_data)}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @format An dataframe of individual{-}level observations with the following variables: }
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} \textbackslash{}describe\{}
\CommentTok{\#\textquotesingle{} \textbackslash{}item\{\textbackslash{}code\{name\}\}\{A character vector of individual names.\}}
\CommentTok{\#\textquotesingle{} \textbackslash{}item\{\textbackslash{}code\{fav\_color\}\}\{A character vector of one of: black, white, rainbow.\}}
\CommentTok{\#\textquotesingle{} \}}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @keywords datasets}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @source \textbackslash{}url\{https://www.tellingstorieswithdata.com/storing{-}and{-}retrieving{-}data.html\}}
\CommentTok{\#\textquotesingle{}}
\StringTok{"color\_data"}
\end{Highlighting}
\end{Shaded}

Finally, we should add a README which provides a summary of all of this
for someone coming to the project from the outside.

At this we can go to the `Build' tab and then `Install and Restart'.
After this happens, the package `favcolordata', is loaded and the data
can be accessed using `color\_data'. If we were to push this to GitHub,
then anyone would be able to install the package using \texttt{devtools}
(Wickham, Hester, and Chang 2020) and then use our dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"RohanAlexander/favcolordata"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(favcolordata)}

\NormalTok{color\_data}
\end{Highlighting}
\end{Shaded}

This has addressed many of the issues that we faced earlier. For
instance, we have included a README and a data dictionary of sorts in
terms of the descriptions that we added. But if we were to try to put
this package onto CRAN, then we might face some issues. For instance,
the maximum size of a package is 5MB and we would quickly come up
against that. We have also largely forced users to use R, and while
there are considerable benefits of that, we may like to be more language
agnostic (N. J. Tierney and Ram 2020).

\hypertarget{depositing-data}{%
\subsection{Depositing data}\label{depositing-data}}

While it is possible that a dataset will be cited if it is available
through GitHub or an R package, this becomes more likely if the dataset
is deposited somewhere. There are several reasons for this, but one is
that it seems a bit more formal. Zenodo and Open Science Framework (OSF)
are two that are commonly used. For instance, C. Carleton (2021) use
Zenodo to share the dataset and analysis supporting W. C. Carleton,
Campbell, and Collard (2021). Similarly Michael et al. (2021) use Zenodo
to share the dataset that underpins Geuenich et al. (2021).

Another option is a dataverse, such as the Harvard Dataverse, which is a
common requirement for journal publications. One nice aspect of this is
that we can use \texttt{dataverse} (Kuriwaki, Beasley, and Leeper 2022)
to retrieve the dataset as part of a reproducible workflow.

\hypertarget{documentation}{%
\section{Documentation}\label{documentation}}

Datasheets (Gebru et al. 2021) are an increasingly critical aspect of
data science. Datasheets are basically nutrition labels for datasets.
The process of creating them enables us to think more carefully about
what we will feed our model. More importantly, they enable others to
better understand what we fed our model. One important task is going
back and putting together datasheets for datasets that are widely used.
For instance, researchers went back and wrote a datasheet for one of the
most popular datasets in computer science, and they found that around 30
per cent of the data were duplicated (Bandy and Vincent 2021).

Instead of telling us how unhealthy various foods are, a datasheet tells
us things like:

\begin{itemize}
\tightlist
\item
  Who put the dataset together?
\item
  Who paid for the dataset to be created?
\item
  How complete is the dataset?
\item
  What fields are present, and equally not present, for particular
  observations?
\end{itemize}

Sometimes we have done a lot of work to create a dataset. In that case,
we may like to publish and share it on its own, for instance, Biderman,
Bicheno, and Gao (2022) and Bandy and Vincent (2021). But typically a
datasheet might live in an appendix to the paper, or be included in a
file adjacent to the dataset.

We will put together a datasheet for the dataset that underpins R.
Alexander and Hodgetts (2021). The text of the questions directly comes
from Gebru et al. (2021). When we create datasheets for a dataset,
especially a dataset that we did not put together ourselves, it is
possible that the answer to some questions will simply be ``Unknown''.

\textbf{Motivation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{For what purpose was the dataset created? Was there a specific
  task in mind? Was there a specific gap that needed to be filled?
  Please provide a description.}

  \begin{itemize}
  \tightlist
  \item
    The dataset was created to enable analysis of Australian
    politicians. We were unable to find a publicly available dataset in
    a structured format that had the biographical and political
    information on Australian politicians that was needed for modelling.
  \end{itemize}
\item
  \emph{Who created the dataset (for example, which team, research
  group) and on behalf of which entity (for example, company,
  institution, organization)?}

  \begin{itemize}
  \tightlist
  \item
    Rohan Alexander, while working at the Australian National University
    and the University of Toronto
  \end{itemize}
\item
  \emph{Who funded the creation of the dataset? If there is an
  associated grant, please provide the name of the grantor and the grant
  name and number.}

  \begin{itemize}
  \tightlist
  \item
    No direct funding was received for this project, but Rohan received
    a salary from University of Toronto.
  \end{itemize}
\item
  \emph{Any other comments?}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\end{enumerate}

\textbf{Composition}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{What do the instances that comprise the dataset represent (for
  example, documents, photos, people, countries)? Are there multiple
  types of instances (for example, movies, users, and ratings; people
  and interactions between them; nodes and edges)? Please provide a
  description.}

  \begin{itemize}
  \tightlist
  \item
    Each row of the main dataset is an individual, and these then link
    to other datasets where each row refers to various information about
    that person.
  \end{itemize}
\item
  \emph{How many instances are there in total (of each type, if
  appropriate)?}

  \begin{itemize}
  \tightlist
  \item
    A little more than 1700.
  \end{itemize}
\item
  \emph{Does the dataset contain all possible instances or is it a
  sample (not necessarily random) of instances from a larger set? If the
  dataset is a sample, then what is the larger set? Is the sample
  representative of the larger set (for example, geographic coverage)?
  If so, please describe how this representativeness was
  validated/verified. If it is not representative of the larger set,
  please describe why not (for example, to cover a more diverse range of
  instances, because instances were withheld or unavailable).}

  \begin{itemize}
  \tightlist
  \item
    All individuals elected or appointed to the Australian Federal
    Parliament are in the dataset.
  \end{itemize}
\item
  \emph{What data does each instance consist of? ``Raw'' data (for
  example, unprocessed text or images) or features? In either case,
  please provide a description.}

  \begin{itemize}
  \tightlist
  \item
    Each instance consists of biographical information such as
    birthdate, or political information, such as political party
    membership.
  \end{itemize}
\item
  \emph{Is there a label or target associated with each instance? If so,
  please provide a description.}

  \begin{itemize}
  \tightlist
  \item
    Yes there is a unique key comprising the surname and year of birth,
    with a few individuals needing additional demarcation.
  \end{itemize}
\item
  \emph{Is any information missing from individual instances? If so,
  please provide a description, explaining why this information is
  missing (for example, because it was unavailable). This does not
  include intentionally removed information, but might include, for
  example, redacted text.}

  \begin{itemize}
  \tightlist
  \item
    Birthdate is not available in all cases, especially earlier in the
    dataset.
  \end{itemize}
\item
  \emph{Are relationships between individual instances made explicit
  (for example, users' movie ratings, social network links)? If so,
  please describe how these relationships are made explicit.}

  \begin{itemize}
  \tightlist
  \item
    Yes, through the uniqueID.
  \end{itemize}
\item
  \emph{Are there recommended data splits (for example, training,
  development/validation, testing)? If so, please provide a description
  of these splits, explaining the rationale behind them.}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\item
  \emph{Are there any errors, sources of noise, or redundancies in the
  dataset? If so, please provide a description.}

  \begin{itemize}
  \tightlist
  \item
    There is some uncertainty about cabinet and ministries. For
    instance, different sources differ. There is also a little bit of
    uncertainty about birthdates.
  \end{itemize}
\item
  \emph{Is the dataset self-contained, or does it link to or otherwise
  rely on external resources (for example, websites, tweets, other
  datasets)? If it links to or relies on external resources, a) are
  there guarantees that they will exist, and remain constant, over time;
  b) are there official archival versions of the complete dataset (that
  is, including the external resources as they existed at the time the
  dataset was created); c) are there any restrictions (for example,
  licenses, fees) associated with any of the external resources that
  might apply to a dataset consumer? Please provide descriptions of all
  external resources and any restrictions associated with them, as well
  as links or other access points, as appropriate.}

  \begin{itemize}
  \tightlist
  \item
    Self-contained.
  \end{itemize}
\item
  \emph{Does the dataset contain data that might be considered
  confidential (for example, data that is protected by legal privilege
  or by doctor-patient confidentiality, data that includes the content
  of individuals' non-public communications)? If so, please provide a
  description.}

  \begin{itemize}
  \tightlist
  \item
    No, all data were gathered from public sources.
  \end{itemize}
\item
  \emph{Does the dataset contain data that, if viewed directly, might be
  offensive, insulting, threatening, or might otherwise cause anxiety?
  If so, please describe why.}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\item
  \emph{Does the dataset identify any sub-populations (for example, by
  age, gender)? If so, please describe how these subpopulations are
  identified and provide a description of their respective distributions
  within the dataset.}

  \begin{itemize}
  \tightlist
  \item
    Yes, age and gender.
  \end{itemize}
\item
  \emph{Is it possible to identify individuals (that is, one or more
  natural persons), either directly or indirectly (that is, in
  combination with other data) from the dataset? If so, please describe
  how.}

  \begin{itemize}
  \tightlist
  \item
    Yes, individuals are identified by name.
  \end{itemize}
\item
  \emph{Does the dataset contain data that might be considered sensitive
  in any way (for example, data that reveals race or ethnic origins,
  sexual orientations, religious beliefs, political opinions or union
  memberships, or locations; financial or health data; biometric or
  genetic data; forms of government identification, such as social
  security numbers; criminal history)? If so, please provide a
  description.}

  \begin{itemize}
  \tightlist
  \item
    The dataset contains sensitive information, such as political
    membership, however this is all public knowledge as they are federal
    politicians.
  \end{itemize}
\item
  \emph{Any other comments?}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\end{enumerate}

\textbf{Collection process}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{How was the data associated with each instance acquired? Was the
  data directly observable (for example, raw text, movie ratings),
  reported by subjects (for example, survey responses), or indirectly
  inferred/derived from other data (for example, part-of-speech tags,
  model-based guesses for age or language)? If the data was reported by
  subjects or indirectly inferred/derived from other data, was the data
  validated/verified? If so, please describe how.}

  \begin{itemize}
  \tightlist
  \item
    The data were gathered from the Australian Parliamentary Handbook in
    the first instance, and this was augmented with information from
    other parliaments, especially Victoria and New South Wales, and
    Wikipedia.
  \end{itemize}
\item
  \emph{What mechanisms or procedures were used to collect the data (for
  example, hardware apparatuses or sensors, manual human curation,
  software programs, software APIs)? How were these mechanisms or
  procedures validated?}

  \begin{itemize}
  \tightlist
  \item
    Scraping and parsing using R.
  \end{itemize}
\item
  \emph{If the dataset is a sample from a larger set, what was the
  sampling strategy (for example, deterministic, probabilistic with
  specific sampling probabilities)?}

  \begin{itemize}
  \tightlist
  \item
    The dataset is not a sample.
  \end{itemize}
\item
  \emph{Who was involved in the data collection process (for example,
  students, crowdworkers, contractors) and how were they compensated
  (for example, how much were crowdworkers paid)?}

  \begin{itemize}
  \tightlist
  \item
    Rohan Alexander. Paid as a post-doc and an assistant professor,
    although this was not tied to this specific project.
  \end{itemize}
\item
  \emph{Over what timeframe was the data collected? Does this timeframe
  match the creation timeframe of the data associated with the instances
  (for example, recent crawl of old news articles)? If not, please
  describe the timeframe in which the data associated with the instances
  was created.}

  \begin{itemize}
  \tightlist
  \item
    Three years, and then updated from time to time.
  \end{itemize}
\item
  \emph{Were any ethical review processes conducted (for example, by an
  institutional review board)? If so, please provide a description of
  these review processes, including the outcomes, as well as a link or
  other access point to any supporting documentation.}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\item
  \emph{Did you collect the data from the individuals in question
  directly, or obtain it via third parties or other sources (for
  example, websites)?}

  \begin{itemize}
  \tightlist
  \item
    Third parties in almost all cases.
  \end{itemize}
\item
  \emph{Were the individuals in question notified about the data
  collection? If so, please describe (or show with screenshots or other
  information) how notice was provided, and provide a link or other
  access point to, or otherwise reproduce, the exact language of the
  notification itself.}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\item
  \emph{Did the individuals in question consent to the collection and
  use of their data? If so, please describe (or show with screenshots or
  other information) how consent was requested and provided, and provide
  a link or other access point to, or otherwise reproduce, the exact
  language to which the individuals consented.}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\item
  \emph{If consent was obtained, were the consenting individuals
  provided with a mechanism to revoke their consent in the future or for
  certain uses? If so, please provide a description, as well as a link
  or other access point to the mechanism (if appropriate).}

  \begin{itemize}
  \tightlist
  \item
    Consent was not obtained.
  \end{itemize}
\item
  \emph{Has an analysis of the potential impact of the dataset and its
  use on data subjects (for example, a data protection impact analysis)
  been conducted? If so, please provide a description of this analysis,
  including the outcomes, as well as a link or other access point to any
  supporting documentation.}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\item
  \emph{Any other comments?}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\end{enumerate}

\textbf{Preprocessing/cleaning/labeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Was any preprocessing/cleaning/labeling of the data done (for
  example, discretization or bucketing, tokenization, part-of-speech
  tagging, SIFT feature extraction, removal of instances, processing of
  missing values)? If so, please provide a description. If not, you may
  skip the remaining questions in this section.}

  \begin{itemize}
  \tightlist
  \item
    Yes cleaning of the data was done.
  \end{itemize}
\item
  \emph{Was the ``raw'' data saved in addition to the
  preprocessed/cleaned/labeled data (for example, to support
  unanticipated future uses)? If so, please provide a link or other
  access point to the ``raw'' data.}

  \begin{itemize}
  \tightlist
  \item
    In general, no. The scripts that got the data from the parliamentary
    handbook to CSV are not available. There are scripts that go through
    Wikipedia and check things and these are available.
  \end{itemize}
\item
  \emph{Is the software that was used to preprocess/clean/label the data
  available? If so, please provide a link or other access point.}

  \begin{itemize}
  \tightlist
  \item
    R was used.
  \end{itemize}
\item
  \emph{Any other comments?}

  \begin{itemize}
  \tightlist
  \item
    No
  \end{itemize}
\end{enumerate}

\textbf{Uses}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Has the dataset been used for any tasks already? If so, please
  provide a description.}

  \begin{itemize}
  \tightlist
  \item
    Yes, a few papers about Australian politics, for instance,
    https://arxiv.org/abs/2111.09299.
  \end{itemize}
\item
  \emph{Is there a repository that links to any or all papers or systems
  that use the dataset? If so, please provide a link or other access
  point.}

  \begin{itemize}
  \tightlist
  \item
    No
  \end{itemize}
\item
  \emph{What (other) tasks could the dataset be used for?}

  \begin{itemize}
  \tightlist
  \item
    Linking with elections would be interesting.
  \end{itemize}
\item
  \emph{Is there anything about the composition of the dataset or the
  way it was collected and preprocessed/cleaned/labeled that might
  impact future uses? For example, is there anything that a dataset
  consumer might need to know to avoid uses that could result in unfair
  treatment of individuals or groups (for example, stereotyping, quality
  of service issues) or other risks or harms (for example, legal risks,
  financial harms)? If so, please provide a description. Is there
  anything a dataset consumer could do to mitigate these risks or
  harms?}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\item
  \emph{Are there tasks for which the dataset should not be used? If so,
  please provide a description.}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\item
  \emph{Any other comments?}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\end{enumerate}

\textbf{Distribution}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Will the dataset be distributed to third parties outside of the
  entity (for example, company, institution, organization) on behalf of
  which the dataset was created? If so, please provide a description.}

  \begin{itemize}
  \tightlist
  \item
    The dataset is available through GitHub.
  \end{itemize}
\item
  \emph{How will the dataset be distributed (for example, tarball on
  website, API, GitHub)? Does the dataset have a digital object
  identifier (DOI)?}

  \begin{itemize}
  \tightlist
  \item
    GitHub for now, and eventually a deposit.
  \end{itemize}
\item
  \emph{When will the dataset be distributed?}

  \begin{itemize}
  \tightlist
  \item
    The dataset is available now.
  \end{itemize}
\item
  \emph{Will the dataset be distributed under a copyright or other
  intellectual property (IP) license, and/or under applicable terms of
  use (ToU)? If so, please describe this license and/ or ToU, and
  provide a link or other access point to, or otherwise reproduce, any
  relevant licensing terms or ToU, as well as any fees associated with
  these restrictions.}

  \begin{itemize}
  \tightlist
  \item
    No.~MIT license.
  \end{itemize}
\item
  \emph{Have any third parties imposed IP-based or other restrictions on
  the data associated with the instances? If so, please describe these
  restrictions, and provide a link or other access point to, or
  otherwise reproduce, any relevant licensing terms, as well as any fees
  associated with these restrictions.}

  \begin{itemize}
  \tightlist
  \item
    None that are known.
  \end{itemize}
\item
  \emph{Do any export controls or other regulatory restrictions apply to
  the dataset or to individual instances? If so, please describe these
  restrictions, and provide a link or other access point to, or
  otherwise reproduce, any supporting documentation.}

  \begin{itemize}
  \tightlist
  \item
    None that are known.
  \end{itemize}
\item
  \emph{Any other comments?}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\end{enumerate}

\textbf{Maintenance}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Who will be supporting/hosting/maintaining the dataset?}

  \begin{itemize}
  \tightlist
  \item
    Rohan Alexander
  \end{itemize}
\item
  \emph{How can the owner/curator/manager of the dataset be contacted
  (for example, email address)?}

  \begin{itemize}
  \tightlist
  \item
    rohan.alexander@utoronto
  \end{itemize}
\item
  \emph{Is there an erratum? If so, please provide a link or other
  access point.}

  \begin{itemize}
  \tightlist
  \item
    No, the dataset is just updated.
  \end{itemize}
\item
  \emph{Will the dataset be updated (for example, to correct labeling
  errors, add new instances, delete instances)? If so, please describe
  how often, by whom, and how updates will be communicated to dataset
  consumers (for example, mailing list, GitHub)?}

  \begin{itemize}
  \tightlist
  \item
    Yes, roughly quarterly.
  \end{itemize}
\item
  \emph{If the dataset relates to people, are there applicable limits on
  the retention of the data associated with the instances (for example,
  were the individuals in question told that their data would be
  retained for a fixed period of time and then deleted)? If so, please
  describe these limits and explain how they will be enforced.}

  \begin{itemize}
  \tightlist
  \item
    No.
  \end{itemize}
\item
  \emph{Will older versions of the dataset continue to be
  supported/hosted/maintained? If so, please describe how. If not,
  please describe how its obsolescence will be communicated to dataset
  consumers.}

  \begin{itemize}
  \tightlist
  \item
    No the dataset is just updated. Although a history is available
    through GitHub.
  \end{itemize}
\item
  \emph{If others want to extend/augment/build on/contribute to the
  dataset, is there a mechanism for them to do so? If so, please provide
  a description. Will these contributions be validated/verified? If so,
  please describe how. If not, why not? Is there a process for
  communicating/distributing these contributions to dataset consumers?
  If so, please provide a description.}

  \begin{itemize}
  \tightlist
  \item
    Pull request on GitHub.
  \end{itemize}
\item
  \emph{Any other comments?}

  \begin{itemize}
  \tightlist
  \item
    No
  \end{itemize}
\end{enumerate}

\hypertarget{personally-identifying-information}{%
\section{Personally identifying
information}\label{personally-identifying-information}}

Personally identifying information (PII) is that which enables us to
link a row in our dataset with an actual person. For instance, email
addresses are often PII, as are names and addresses. Interestingly,
sometimes the combination of several variables, none of which are PII in
and of themselves, can be PII. For instance, age is unlikely PII by
itself, but age combined with city, education, and a few other variables
could be. One concern is that this re-identification can occur across
datasets. Another interesting aspect is that again while some variable
may not be PII for almost everyone in the dataset, it can become PII in
the extreme. For instance, if we have age, then there are many people of
most ages, but there are fewer people in ages over 100 and it likely
becomes PII. The same scenario happens with both income and wealth. One
response to this is for data to be censored. For instance, we may record
age between 0 and 100, and then group everyone over that into `101+'.

Zook et al. (2017) recommend considering whether data even need to be
gathered in the first place. For instance, if a phone number is not
absolutely required then it might be better to not gather it in the
first place, rather than need to worry about protecting it before data
dissemination.

GDPR and HIPAA are two legal structures that govern data in Europe and
the US, respectively. Due to the influence of these regions, they have a
significant effect outside those regions also. GDPR concerns data
generally, while HIPAA is focused on healthcare. GDPR applies to all
personal data, which is defined as:

\begin{quote}
`personal data' means any information relating to an identified or
identifiable natural person (`data subject'); an identifiable natural
person is one who can be identified, directly or indirectly, in
particular by reference to an identifier such as a name, an
identification number, location data, an online identifier or to one or
more factors specific to the physical, physiological, genetic, mental,
economic, cultural or social identity of that natural person;

Council of European Union (2016), Article 4, `Definitions'
\end{quote}

HIPAA refers to the privacy of medical records in the US and codify the
idea that the patient should have access to their medical records, and
that only the patient should be able to authorize access to their
medical records (Annas 2003). While it only applies to covered entities,
it sets a standard that informs practice, yet is piecemeal given the
variety of applications. For instance, a person's social media posts
about their health is generally not subject to it, nor is knowledge
about a person's location and how active they are, even though based on
that information we may be able to get some idea of their health (Cohen
and Mello 2018). Such data are hugely valuable (Ross 2022).

There are a variety of ways of protecting PII, while sharing data, that
we will now go through.

\hypertarget{hashing-and-salting}{%
\subsection{Hashing and salting}\label{hashing-and-salting}}

A hash is a one-way transformation of data, such that the same input
always provides the same output, but given the output, it is not
reasonably possible to obtain the input. For instance, a function that
doubled its input always gives the same output, for the same input, but
is also easy to reverse.

Donald E. Knuth (1998, 514) relates an interesting etymology for `hash'
by first defining `to hash' as relating to chop up or make a mess, and
then explaining that hashing relates to scrambling the input and using
this partial information to define the output. A collision is when
different inputs map to the same output, and one feature of a good
hashing algorithm is that collisions are reduced. One simple approach is
to rely on the modulo operator. For instance, if we were interested in
10 different groupings for the integers 1 through to 10. A better
approach would be for the number of groupings to be a prime number, such
as 11 or 853.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{hashing }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{ppi\_data =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{),}
         \AttributeTok{modulo\_ten =}\NormalTok{ ppi\_data }\SpecialCharTok{\%\%} \DecValTok{10}\NormalTok{,}
         \AttributeTok{modulo\_eleven =}\NormalTok{ ppi\_data }\SpecialCharTok{\%\%} \DecValTok{11}\NormalTok{,}
         \AttributeTok{modulo\_eightfivethree =}\NormalTok{ ppi\_data }\SpecialCharTok{\%\%} \DecValTok{853}\NormalTok{)}

\NormalTok{hashing}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 4
   ppi_data modulo_ten modulo_eleven modulo_eightfivethree
      <int>      <dbl>         <dbl>                 <dbl>
 1        1          1             1                     1
 2        2          2             2                     2
 3        3          3             3                     3
 4        4          4             4                     4
 5        5          5             5                     5
 6        6          6             6                     6
 7        7          7             7                     7
 8        8          8             8                     8
 9        9          9             9                     9
10       10          0            10                    10
\end{verbatim}

Rather than worry about things ourselves, we can use various hash
functions from \texttt{openssl} (Ooms 2021) including \texttt{sha512()}
and \texttt{md5()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(openssl)}

\NormalTok{openssl\_hashing }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{names =}
            \FunctionTok{c}\NormalTok{(}\StringTok{"Edward"}\NormalTok{,}
              \StringTok{"Helen"}\NormalTok{,}
              \StringTok{"Hugo"}\NormalTok{,}
              \StringTok{"Ian"}\NormalTok{,}
              \StringTok{"Monica"}\NormalTok{,}
              \StringTok{"Myles"}\NormalTok{,}
              \StringTok{"Patricia"}\NormalTok{,}
              \StringTok{"Roger"}\NormalTok{,}
              \StringTok{"Rohan"}\NormalTok{,}
              \StringTok{"Ruth"}
\NormalTok{            )) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{md5 =} \FunctionTok{md5}\NormalTok{(names),}
         \AttributeTok{sha512 =} \FunctionTok{sha512}\NormalTok{(names)}
\NormalTok{  )}

\NormalTok{openssl\_hashing}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 3
   names    md5                              sha512                             
   <chr>    <hash>                           <hash>                             
 1 Edward   243f63354f4c1cc25d50f6269b844369 5759ada975e7cb87c28fe1b6ea5f6a1d51~
 2 Helen    29e00d3659d1c5e75f99e892f0c1a1f1 6ee4156ca7e8e9a5acc42704363f35e278~
 3 Hugo     1b3840b0b70d91c17e70014c8537dbba b1e441a54866906727d842e6a064ba8fb9~
 4 Ian      245a58a5dc42397caf57bc06c2c0afd2 d3cf9cdaea6ffdfd8b8d143fe609bc35b9~
 5 Monica   09084cc0cda34fd80bfa3cc0ae8fe3dc 84250b971b87728aa4ab24cec405864ed9~
 6 Myles    fafdf519cb5877d4751b4cbe6f3f534a 4eae7c19d5c5d4ffffd8c2f97ed7df69f7~
 7 Patricia 54a7b18f26374fc200ddedde0844f8ec e511593a2db805e51ed0bb74c96d0db652~
 8 Roger    efc5c58b9a85926a31587140cbeb0220 f63ab236a5b0135f71c32ee39517001b21~
 9 Rohan    02df8936eee3d4d2568857ed530671b2 5111e18391d41fbabd9005b85dc2413f2a~
10 Ruth     8e06843ec162b74a7902867dd4bca8c8 d7e7d23b69e37208002cdfa7dc7a0fa6f6~
\end{verbatim}

We could share either of these and be comfortable that, in general, it
would be difficult for someone to use only that information to recover
the names of our respondents. That is not to say that it is impossible.
If we made a mistake, such as accidentally committing the original
dataset to GitHub then they could be recovered. And of course, it is
likely that various governments have the ability to reverse the
cryptographic hashes used here.

One issue that remains is that anyone can take advantage of the key
feature of hashing being that the same input always gets the same
output, to test various options for inputs. For instance, they could,
themselves try to has `Rohan', and then noticing that the hash is the
same as the one that we published in our dataset, know that data relates
to that particular individual. We could try to keep our hashing approach
secret, but that is difficult as there are only a few that was widely
used. One approach is to add a salt that we keep secret. This slightly
changes the input. For instance, we could add the salt '\_is\_a\_person'
to all our names and then hash that, although a large random number
might be a better option. Provided the salt is not shared, then it would
be difficult for most folks to reverse our approach in that way.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{openssl\_hashing\_with\_salt }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{names =}
            \FunctionTok{c}\NormalTok{(}\StringTok{"Edward"}\NormalTok{,}
              \StringTok{"Helen"}\NormalTok{,}
              \StringTok{"Hugo"}\NormalTok{,}
              \StringTok{"Ian"}\NormalTok{,}
              \StringTok{"Monica"}\NormalTok{,}
              \StringTok{"Myles"}\NormalTok{,}
              \StringTok{"Patricia"}\NormalTok{,}
              \StringTok{"Roger"}\NormalTok{,}
              \StringTok{"Rohan"}\NormalTok{,}
              \StringTok{"Ruth"}
\NormalTok{            )}
\NormalTok{         ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{names =} \FunctionTok{paste0}\NormalTok{(names, }\StringTok{"\_is\_a\_person"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{md5 =} \FunctionTok{md5}\NormalTok{(names),}
         \AttributeTok{sha512 =} \FunctionTok{sha512}\NormalTok{(names)}
\NormalTok{         )}

\NormalTok{openssl\_hashing\_with\_salt}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 3
   names                md5                              sha512                 
   <chr>                <hash>                           <hash>                 
 1 Edward_is_a_person   9845500d4070c0cbba7c6b81ed306027 e8ce00d98d5d2f22f7c2bf~
 2 Helen_is_a_person    7e4a77b41fb6e108618f93fb9f47bae3 b066b72ebafcb77c765de7~
 3 Hugo_is_a_person     b9b8c4e9870aca482cf062da4681b232 07c18ebb565cc86b84a523~
 4 Ian_is_a_person      9b1ad8fbbc190c2e3ce74372029f9735 eb9928f62e7004aa0a6ef7~
 5 Monica_is_a_person   50bb9dfffa926c855b830845ac61b659 ef429119529d22b72aedbd~
 6 Myles_is_a_person    3635be5fe758ed1fc0d9c78fc0b26458 795dce5816cf5a070ac87c~
 7 Patricia_is_a_person 4e4a5ed8842fd7caad320c3a92bf07db 1b44649bc13fa3c505a29c~
 8 Roger_is_a_person    ea1d56e89771d8b0a7b5981324424ec2 2f753bc3f871d60889c6d5~
 9 Rohan_is_a_person    3ab064d7f746fde604122d072fd4fa97 cc9c7c3d9da05da52f394e~
10 Ruth_is_a_person     8b83f4285ac30a3efa5ede3636b7d687 6d329fde75a259b7de137e~
\end{verbatim}

\hypertarget{data-simulation}{%
\subsection{Data simulation}\label{data-simulation}}

One common approach to deal with the issue of being unable to share the
actual data that underpins an analysis, is to use data simulation. We
have used data simulation throughout this book toward the start of the
workflow to help us to think more deeply about our dataset before we
turn to it. We can use data simulation again at the end, to ensure that
others cannot think about our actual dataset. The workflow advocated in
this book makes this relatively straight-forward.

The approach is to understand the critical features of the dataset and
the appropriate distribution. For instance, if our data were the ages of
some population, then we may want to use the Poisson distribution and
experiment with different parameters for lambda. Having simulated a
dataset, we conduct our analysis using this simulated dataset and ensure
that the results are broadly similar to when we use the real data. We
can then release the simulated dataset along with our code.

For more nuanced situations, Koenecke and Varian (2020) recommend using
the synthetic data vault (Patki, Wedge, and Veeramachaneni 2016) and
then the use of Generative Adversarial Networks, such as implemented by
Athey et al. (2021).

\hypertarget{differential-privacy}{%
\subsection{Differential privacy}\label{differential-privacy}}

Differential privacy implements a mathematical definition of privacy,
that means that even if datasets are combined, a certain level of
privacy will be maintained. A dataset is differentially private to
different levels, based on how much it changes when one person's results
are removed.

A variant of differential privacy has recently been implemented by the
US census. This has been shown to not universally protect respondent
privacy, and yet it is expected to have a significant effect on
redistricting (Kenny et al. 2021). Suriyakumar et al. (2021) found that
such model will be disproportionately affected by large demographic
groups. The implementation of differential privacy is expected to result
in publicly available data that are unusable in the social sciences
(Ruggles et al. 2019).

\hypertarget{exercises-and-tutorial-11}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-11}}

\hypertarget{exercises-11}{%
\subsection{Exercises}\label{exercises-11}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Following M. D. Wilkinson et al. (2016), which of the following are
  FAIR principles (please select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Findable.
  \item
    Approachable.
  \item
    Interoperable.
  \item
    Reusable.
  \item
    Integrated.
  \item
    Fungible.
  \item
    Reduced.
  \item
    Accessible.
  \end{enumerate}
\item
  Please create an R package for a simulated dataset, push it to GitHub,
  and submit the link.
\item
  Please simulate some data, add it to a GitHub repository and then
  submit the link.
\item
  According to Gebru et al. (2021), a datasheet should document a
  dataset's (please select all that apply):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    composition.
  \item
    recommended uses.
  \item
    motivation.
  \item
    collection process.
  \end{enumerate}
\item
  Do you think that a person's name is PII?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Yes.
\item
  No.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Under what circumstances do you think income is PII (please write a
  paragraph or two)?
\item
  Using \texttt{openssl::md5()} what is the hash of ``Rohan'' (pick
  one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    243f63354f4c1cc25d50f6269b844369
  \item
    09084cc0cda34fd80bfa3cc0ae8fe3dc
  \item
    02df8936eee3d4d2568857ed530671b2
  \item
    1b3840b0b70d91c17e70014c8537dbba
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial-11}{%
\subsection{Tutorial}\label{tutorial-11}}

Please identify a dataset you consider interesting and important, that
does not have a datasheet (Gebru et al. 2021). As a reminder, datasheets
accompany datasets and document `motivation, composition, collection
process, recommended uses,' among other aspects. Please put together a
datasheet for this dataset. You are welcome to use the template
\href{https://github.com/RohanAlexander/starter_folder/blob/main/inputs/data/datasheet_template.Rmd}{here}
as a starting point. The datasheet should be completely contained in its
own GitHub repository. Please submit a PDF.

\hypertarget{paper-3}{%
\subsection{Paper}\label{paper-3}}

At about this point, Paper Four (Appendix @ref(paper-four)) would be
appropriate.

\part{Modelling}

\hypertarget{sec-exploratory-data-analysis}{%
\chapter{Exploratory data
analysis}\label{sec-exploratory-data-analysis}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{The Future of Data Analysis}, Part 1 `General
  Considerations', (Tukey 1962).
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Quickly coming to terms with a new dataset by constructing graphs and
  tables.
\item
  Understanding the issues and features of the dataset and how this may
  affect analysis decisions.
\item
  Thinking about missing values and outliers.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{broom}
\item
  \texttt{ggrepel}
\item
  \texttt{here}
\item
  \texttt{janitor}
\item
  \texttt{lubridate}
\item
  \texttt{opendatatoronto}
\item
  \texttt{tidymodels}
\item
  \texttt{tidyverse}
\item
  \texttt{visdat}
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{augment()}
\item
  \texttt{clean\_names()}
\item
  \texttt{coord\_flip()}
\item
  \texttt{count()}
\item
  \texttt{distinct()}
\item
  \texttt{facet\_grid()}
\item
  \texttt{facet\_wrap()}
\item
  \texttt{geom\_bar()}
\item
  \texttt{geom\_col()}
\item
  \texttt{geom\_density()}
\item
  \texttt{geom\_histogram()}
\item
  \texttt{geom\_line()}
\item
  \texttt{geom\_point()}
\item
  \texttt{geom\_smooth()}
\item
  \texttt{geom\_text\_repel()}
\item
  \texttt{get\_dupes()}
\item
  \texttt{glance()}
\item
  \texttt{if\_else()}
\item
  \texttt{ifelse()}
\item
  \texttt{initial\_split()}
\item
  \texttt{left\_join()}
\item
  \texttt{mutate()}
\item
  \texttt{mutate\_all()}
\item
  \texttt{names()}
\item
  \texttt{ncol()}
\item
  \texttt{nrow()}
\item
  \texttt{pivot\_wider()}
\item
  \texttt{scale\_color\_brewer()}
\item
  \texttt{scale\_fill\_brewer()}
\item
  \texttt{scale\_x\_log10()}
\item
  \texttt{scale\_y\_log10()}
\item
  \texttt{str\_detect()}
\item
  \texttt{str\_extract()}
\item
  \texttt{str\_remove()}
\item
  \texttt{str\_split()}
\item
  \texttt{str\_starts()}
\item
  \texttt{summarise()}
\item
  \texttt{summarise\_all()}
\item
  \texttt{theme\_classic()}
\item
  \texttt{theme\_minimal()}
\item
  \texttt{vis\_dat()}
\item
  \texttt{vis\_miss()}
\end{itemize}

\hypertarget{introduction-10}{%
\section{Introduction}\label{introduction-10}}

\begin{quote}
The future of data analysis can involve great progress, the overcoming
of real difficulties, and the provision of a great service to all fields
of science and technology. Will it? That remains to us, to our
willingness to take up the rocky road of real problems in preference to
the smooth road of unreal assumptions, arbitrary criteria, and abstract
results without real attachments. Who is for the challenge?

Tukey (1962, 64).
\end{quote}

Exploratory data analysis is never finished, you just die. It is the
active process of exploring and becoming familiar with our data. Like a
farmer with their hands in the earth, we need to know every contour and
aspect of our data. We need to know how it changes, what it shows,
hides, and what are its limits. Exploratory data analysis (EDA) is the
unstructured process of doing this.

EDA is a means to an end. While it will inform the entire paper,
especially the data section, it is not typically something that ends up
in the final paper. The way to proceed is to make a separate R Markdown
file, and add code as well as brief notes on-the-go. Do not delete
previous code, just add to it. By the end of it we will have created a
useful notebook that captures our exploration of the dataset. This is a
document that will guide the subsequent analysis and modelling.

EDA draws on a variety of skills and there are a lot of options for EDA
(Staniak and Biecek 2019). Every tool should be considered. Look at the
data and scroll through it. Make tables, plots, summary statistics, even
some models. The key is to iterate, move quickly rather than perfectly,
and come to a thorough understanding of the data.

In this chapter we will go through various examples of EDA including TTC
subway delays, and Airbnb.

\hypertarget{case-study-ttc-subway-delays}{%
\section{Case study: TTC subway
delays}\label{case-study-ttc-subway-delays}}

We can use \texttt{opendatatoronto} (Gelfand 2020) and
\texttt{tidyverse} (Wickham et al. 2019a) to obtain data about the
Toronto subway system, and especially the delays that have occurred. The
idea for this case study comes from Monica Alexander.

\#\textbar{} message: false \#\textbar{} warning: false

\#\textbar{} echo: true

To begin, we download the data on Toronto Transit Commission (TTC)
subway delays in 2020. The data are available as a separate dataset for
each month. We are interested in 2020, so we create a column that of the
year, and then filter the resources to just those months that were in
2020. We download them using \texttt{get\_resource()}, iterating through
each month using \texttt{map\_dfr} from \texttt{purrr} (Henry and
Wickham 2020) which brings each of the twelve datasets together, and
then save them. then save them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(opendatatoronto)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# We know this unique key by looking the \textquotesingle{}id\textquotesingle{} of the interest.}
\NormalTok{ttc\_resources }\OtherTok{\textless{}{-}} 
  \FunctionTok{list\_package\_resources}\NormalTok{(}\StringTok{"996cfe8d{-}fb35{-}40ce{-}b569{-}698d51fc683b"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year =} \FunctionTok{str\_extract}\NormalTok{(name, }\StringTok{"20..?"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}  
  \FunctionTok{filter}\NormalTok{(year }\SpecialCharTok{==} \DecValTok{2020}\NormalTok{)}

\NormalTok{all\_2020\_ttc\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{map\_dfr}\NormalTok{(ttc\_resources}\SpecialCharTok{$}\NormalTok{id, get\_resource)}

\NormalTok{all\_2020\_ttc\_data }\OtherTok{\textless{}{-}} \FunctionTok{clean\_names}\NormalTok{(all\_2020\_ttc\_data)}

\FunctionTok{write\_csv}\NormalTok{(all\_2020\_ttc\_data, }\StringTok{"all\_2020\_ttc\_data.csv"}\NormalTok{)}

\NormalTok{all\_2020\_ttc\_data}
\end{Highlighting}
\end{Shaded}

The dataset has a variety of columns, and we can find out more about
each of them by downloading the codebook. The reason for the delay is
coded, and so we can also download the explanations. One particular
variable of interest appears to be `min\_delay', which gives the extent
of the delay in minutes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Data codebook}
\NormalTok{delay\_data\_codebook }\OtherTok{\textless{}{-}} \FunctionTok{get\_resource}\NormalTok{(}\StringTok{"54247e39{-}5a7d{-}40db{-}a137{-}82b2a9ab0708"}\NormalTok{)}
\NormalTok{delay\_data\_codebook }\OtherTok{\textless{}{-}} \FunctionTok{clean\_names}\NormalTok{(delay\_data\_codebook)}
\FunctionTok{write\_csv}\NormalTok{(delay\_data\_codebook, }\StringTok{"delay\_data\_codebook.csv"}\NormalTok{)}

\CommentTok{\# Explanation for delay codes}
\NormalTok{delay\_codes }\OtherTok{\textless{}{-}} \FunctionTok{get\_resource}\NormalTok{(}\StringTok{"fece136b{-}224a{-}412a{-}b191{-}8d31eb00491e"}\NormalTok{)}
\NormalTok{delay\_codes }\OtherTok{\textless{}{-}} \FunctionTok{clean\_names}\NormalTok{(delay\_codes)}
\FunctionTok{write\_csv}\NormalTok{(delay\_codes, }\StringTok{"delay\_codes.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There is no one way to explore a dataset while conducting EDA, but we
are usually especially interested in:

\begin{itemize}
\tightlist
\item
  What should the variables look like? For instance, what is their type,
  what are the values, and what does the distribution of these look
  like?
\item
  What aspects are surprising, both in terms of data that are there that
  we do not expect, such as outliers, but also in terms of data that we
  may expect here but do not have such as missing data.
\item
  Developing a goal for our analysis. For instance, in this case, it
  might be understanding the factors such as stations and the time of
  day, that are associated with delays.
\end{itemize}

It is important to document all aspects as we go through and make note
of anything surprising. We are looking to create a record of the steps
and assumptions that we made as we were going because these will be
important when we come to modelling.

\hypertarget{checking-data}{%
\subsection{Checking data}\label{checking-data}}

We should check that the variables are what they say they are. If they
are not, then we need to work out what to do, for instance, should we
recode them, or even remove them? It is also important to ensure that
the class of the variables is as we expect, for instance variables that
should be a factor are a factor and those that should be a character are
a character. And also that we do not accidentally have, say, factors as
numbers, or vice versa. One way to do this is to use \texttt{unique()},
and another is to use \texttt{table()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unique}\NormalTok{(all\_2020\_ttc\_data}\SpecialCharTok{$}\NormalTok{day)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Wednesday" "Thursday"  "Friday"    "Saturday"  "Sunday"    "Monday"   
[7] "Tuesday"  
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{unique}\NormalTok{(all\_2020\_ttc\_data}\SpecialCharTok{$}\NormalTok{line)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "SRT"                    "YU"                     "BD"                    
 [4] "SHP"                    "YU/BD"                  "YU / BD"               
 [7] "999"                    NA                       "29 DUFFERIN"           
[10] "95 YORK MILLS"          "35 JANE"                "YU-BD"                 
[13] "BLOOR - DANFORTH"       "YU/BD LINE"             "YUS"                   
[16] "YUS/BD"                 "40 JUNCTION-DUNDAS WES" "71 RUNNYMEDE"          
[19] "BD/YU"                  "102 MARKHAM ROAD"       "YUS/DB"                
[22] "YU & BD"                "SHEP"                  
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(all\_2020\_ttc\_data}\SpecialCharTok{$}\NormalTok{day)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   Friday    Monday  Saturday    Sunday  Thursday   Tuesday Wednesday 
     2174      2222      1867      1647      2353      2190      2329 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(all\_2020\_ttc\_data}\SpecialCharTok{$}\NormalTok{line)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

      102 MARKHAM ROAD            29 DUFFERIN                35 JANE 
                     1                      1                      1 
40 JUNCTION-DUNDAS WES           71 RUNNYMEDE          95 YORK MILLS 
                     1                      1                      1 
                   999                     BD                  BD/YU 
                     2                   5473                      1 
      BLOOR - DANFORTH                   SHEP                    SHP 
                     1                      1                    619 
                   SRT                     YU                YU / BD 
                   644                   7620                     22 
               YU & BD                  YU-BD                  YU/BD 
                     2                      1                    338 
            YU/BD LINE                    YUS                 YUS/BD 
                     1                      2                      1 
                YUS/DB 
                     1 
\end{verbatim}

It is clear that we have likely issues in terms of the lines. Some of
them have a clear re-code, but not all. One option would be to drop
them, but we would need to think about whether these errors might be
correlated with something that is of interest, because if they were then
we may be dropping important information. There is usually no one right
answer, because it will usually depend on what we are using the data
for. We would note the issue, as we continued with EDA and then decide
later about what to do. For now, we will remove all the lines that are
not the ones that we know to be correct.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_2020\_ttc\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(line }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"BD"}\NormalTok{, }\StringTok{"YU"}\NormalTok{, }\StringTok{"SHP"}\NormalTok{, }\StringTok{"SRT"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Exploring missing data could be a course in itself, but the presence, or
lack, of missing values can haunt an analysis. To get started we could
look at known-unknowns, which are the NAs for each variable. And
\texttt{vis\_dat()} and \texttt{vis\_miss()} from \texttt{visdat} (N.
Tierney 2017) can be useful to get a feel for how the missing values are
distributed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(visdat)}

\NormalTok{all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise\_all}\NormalTok{(}\FunctionTok{list}\NormalTok{( }\SpecialCharTok{\textasciitilde{}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(.))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 10
   date  time   day station  code min_delay min_gap bound  line vehicle
  <int> <int> <int>   <int> <int>     <int>   <int> <int> <int>   <int>
1     0     0     0       0     0         0       0  3272     0       0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vis\_dat}\NormalTok{(}\AttributeTok{x =}\NormalTok{ all\_2020\_ttc\_data,}
         \AttributeTok{palette =} \StringTok{"cb\_safe"}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/unnamed-chunk-18-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vis\_miss}\NormalTok{(all\_2020\_ttc\_data)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/unnamed-chunk-18-2.pdf}

}

\end{figure}

In this case we have many missing values in `bound' and two in `line'.
For these known-unknowns, we are interested in whether the they are
missing at random. We want to, ideally, show that data happened to just
drop out. This is unlikely, and so we are usually trying to look at what
is systematic about how our data are missing.

Sometime data happen to be duplicated. If we did not notice this then
our analysis would be wrong in ways that we'd not be able to
consistently expect. There are a variety of ways to look for duplicated
rows, but \texttt{get\_dupes()} from \texttt{janitor} (Firke 2020) is
especially useful.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_dupes}\NormalTok{(all\_2020\_ttc\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
No variable names specified - using all columns.
\end{verbatim}

\begin{verbatim}
# A tibble: 37 x 11
   date                time   day    station code  min_delay min_gap bound line 
   <dttm>              <time> <chr>  <chr>   <chr>     <dbl>   <dbl> <chr> <chr>
 1 2020-02-10 00:00:00 06:00  Monday TORONT~ MRO           0       0 <NA>  SRT  
 2 2020-02-10 00:00:00 06:00  Monday TORONT~ MRO           0       0 <NA>  SRT  
 3 2020-02-10 00:00:00 06:00  Monday TORONT~ MUO           0       0 <NA>  SHP  
 4 2020-02-10 00:00:00 06:00  Monday TORONT~ MUO           0       0 <NA>  SHP  
 5 2020-03-10 00:00:00 23:00  Tuesd~ YORK M~ MUO           0       0 <NA>  YU   
 6 2020-03-10 00:00:00 23:00  Tuesd~ YORK M~ MUO           0       0 <NA>  YU   
 7 2020-03-26 00:00:00 13:20  Thurs~ VAUGHA~ MUNOA         3       6 S     YU   
 8 2020-03-26 00:00:00 13:20  Thurs~ VAUGHA~ MUNOA         3       6 S     YU   
 9 2020-03-26 00:00:00 18:32  Thurs~ VAUGHA~ MUNOA         3       6 S     YU   
10 2020-03-26 00:00:00 18:32  Thurs~ VAUGHA~ MUNOA         3       6 S     YU   
# ... with 27 more rows, and 2 more variables: vehicle <dbl>, dupe_count <int>
\end{verbatim}

This dataset has many duplicates. Again, we are interested in whether
there is something systematic going on. Remembering that during EDA we
are trying to quickly come to terms with a dataset, one way forward is
to flag this as an issue to come back to and explore later, and to just
remove duplicates for now using \texttt{distinct()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_2020\_ttc\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{distinct}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The station names are a mess. We could try to quickly bring a little
order to the chaos by just taking just the first word (or, the first two
if it starts with `ST').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_2020\_ttc\_data }\OtherTok{\textless{}{-}}
\NormalTok{  all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{station\_clean =} \FunctionTok{if\_else}\NormalTok{(}\FunctionTok{str\_starts}\NormalTok{(station, }\StringTok{"ST"}\NormalTok{), }
                                 \FunctionTok{word}\NormalTok{(station, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }
                                 \FunctionTok{word}\NormalTok{(station, }\DecValTok{1}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualizing-data}{%
\subsection{Visualizing data}\label{visualizing-data}}

We need to see the data in its original state to understand it and we
use bar charts, scatterplots, line plots and histograms extensively for
this. During EDA we are not as concerned with whether the graph is
aesthetically pleasing, but are instead trying to acquire a sense of the
data as quickly as possible. We can start by looking at the distribution
of `min\_delay', which is one outcome of interest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ min\_delay)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/unnamed-chunk-26-1.pdf}

}

\end{figure}

The largely empty graph suggests the presence of outliers. There are a
variety of ways to try to understand what could be going on, but one
quick way to proceed it to use a log, remembering that we would expect
values of 0 to drop away.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ min\_delay)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/unnamed-chunk-28-1.pdf}

}

\end{figure}

This initial exploration further hints at an issue that we might like to
explore further. We will join this dataset with `delay\_codes' to
understand what is going on.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_2020\_ttc\_data }\OtherTok{\textless{}{-}}
\NormalTok{  all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{left\_join}\NormalTok{(}
\NormalTok{    delay\_codes }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{rename}\NormalTok{(}\AttributeTok{code =}\NormalTok{ sub\_rmenu\_code, }
             \AttributeTok{code\_desc =}\NormalTok{ code\_description\_3) }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{select}\NormalTok{(code, code\_desc),}
    \AttributeTok{by =} \StringTok{"code"}
\NormalTok{  )}

\NormalTok{all\_2020\_ttc\_data }\OtherTok{\textless{}{-}}
\NormalTok{  all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{code\_srt =} \FunctionTok{ifelse}\NormalTok{(line }\SpecialCharTok{==} \StringTok{"SRT"}\NormalTok{, code, }\StringTok{"NA"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{left\_join}\NormalTok{(}
\NormalTok{    delay\_codes }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{rename}\NormalTok{(}\AttributeTok{code\_srt =}\NormalTok{ sub\_rmenu\_code, }\AttributeTok{code\_desc\_srt =}\NormalTok{ code\_description\_7) }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{select}\NormalTok{(code\_srt, code\_desc\_srt),}
    \AttributeTok{by =} \StringTok{"code\_srt"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{code =} \FunctionTok{ifelse}\NormalTok{(code\_srt }\SpecialCharTok{==} \StringTok{"NA"}\NormalTok{, code, code\_srt),}
    \AttributeTok{code\_desc =} \FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(code\_desc\_srt), code\_desc, code\_desc\_srt)}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{code\_srt,}\SpecialCharTok{{-}}\NormalTok{code\_desc\_srt)}
\end{Highlighting}
\end{Shaded}

And so we can see that the 450 minute delay was due to `Transit Control
Related Problems', the 446 minute delay was due to `Miscellaneous
Other', they seem to be outliers, even among the outliers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{left\_join}\NormalTok{(delay\_codes }\SpecialCharTok{|\textgreater{}} 
              \FunctionTok{rename}\NormalTok{(}\AttributeTok{code =}\NormalTok{ sub\_rmenu\_code, }\AttributeTok{code\_desc =}\NormalTok{ code\_description\_3) }\SpecialCharTok{|\textgreater{}}
              \FunctionTok{select}\NormalTok{(code, code\_desc)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{min\_delay) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(date, time, station, line, min\_delay, code, code\_desc)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining, by = c("code", "code_desc")
\end{verbatim}

\begin{verbatim}
# A tibble: 14,335 x 7
   date                time   station            line  min_delay code  code_desc
   <dttm>              <time> <chr>              <chr>     <dbl> <chr> <chr>    
 1 2020-02-13 00:00:00 05:30  ST GEORGE YUS STA~ YU          450 TUCC  Transit ~
 2 2020-05-08 00:00:00 16:16  ST CLAIR STATION   YU          446 MUO   Miscella~
 3 2020-01-22 00:00:00 05:57  KEELE STATION      BD          258 EUTR  Trucks   
 4 2020-03-19 00:00:00 11:26  ROYAL YORK STATION BD          221 MUPR1 Priority~
 5 2020-11-12 00:00:00 23:10  SHEPPARD STATION   YU          197 PUCSC Signal C~
 6 2020-12-13 00:00:00 21:37  MCCOWAN STATION    SRT         167 PRSP  <NA>     
 7 2020-12-04 00:00:00 16:23  MCCOWAN STATION    SRT         165 PRSP  <NA>     
 8 2020-01-18 00:00:00 05:48  SCARBOROUGH RAPID~ SRT         162 PRSL  <NA>     
 9 2020-02-22 00:00:00 05:16  SPADINA TO OSGOODE YU          159 PUSWZ Work Zon~
10 2020-09-03 00:00:00 14:35  CASTLE FRANK STAT~ BD          150 MUPR1 Priority~
# ... with 14,325 more rows
\end{verbatim}

\hypertarget{groups-of-small-counts}{%
\subsection{Groups of small counts}\label{groups-of-small-counts}}

Another thing that we are looking for is various groupings of the data,
especially where sub-groups may end up with only a small numbers of
observations in them. This is because any analysis could be especially
influenced by them. One quick way to do this is to group the data by a
variable that is of interest, for instance `line', using color.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ all\_2020\_ttc\_data) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ min\_delay, }\AttributeTok{y =}\NormalTok{ ..density.., }\AttributeTok{fill =}\NormalTok{ line), }
                 \AttributeTok{position =} \StringTok{\textquotesingle{}dodge\textquotesingle{}}\NormalTok{, }
                 \AttributeTok{bins =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-delaydensity-1.pdf}

}

\caption{\label{fig-delaydensity}Density of the distribution of delay,
in minutes}

\end{figure}

Figure~\ref{fig-delaydensity} uses density so that we can look at the
the distributions more comparably, but we should also be aware of
differences in frequency (Figure~\ref{fig-delayfreq})). In this case, we
will see that `SHP' and `SRT' have much smaller counts.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ all\_2020\_ttc\_data) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ min\_delay, }\AttributeTok{fill =}\NormalTok{ line), }
                 \AttributeTok{position =} \StringTok{\textquotesingle{}dodge\textquotesingle{}}\NormalTok{, }
                 \AttributeTok{bins =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-delayfreq-1.pdf}

}

\caption{\label{fig-delayfreq}Frequency of the distribution of delay, in
minutes}

\end{figure}

To group by another variable we can add facets
(Figure~\ref{fig-delayfreqfacet}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ all\_2020\_ttc\_data) }\SpecialCharTok{+} 
  \FunctionTok{geom\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ min\_delay, }\AttributeTok{color =}\NormalTok{ line), }
               \AttributeTok{bw =}\NormalTok{ .}\DecValTok{08}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(day))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-delayfreqfacet-1.pdf}

}

\caption{\label{fig-delayfreqfacet}Frequency of the distribution of
delay, in minutes, by day}

\end{figure}

We can now plot the top five stations by mean delay.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(line, station\_clean) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_delay =} \FunctionTok{mean}\NormalTok{(min\_delay), }\AttributeTok{n\_obs =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(n\_obs}\SpecialCharTok{\textgreater{}}\DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(line, }\SpecialCharTok{{-}}\NormalTok{mean\_delay) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(station\_clean, mean\_delay)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(line), }
               \AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`summarise()` has grouped output by 'line'. You can override using the
`.groups` argument.
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/unnamed-chunk-37-1.pdf}

}

\end{figure}

\hypertarget{dates}{%
\subsection{Dates}\label{dates}}

Dates are often difficult to work with because they are so prone to
having issues. For this reason, it is especially important to consider
them during EDA. We could create a graph by week, to see if there is any
seasonality. When using dates, \texttt{lubridate} (Grolemund and Wickham
2011) is especially useful. For instance, we can look at the average
delay, of those that were delayed, by week drawing on \texttt{week()} to
construct the weeks (Figure~\ref{fig-delaybyweek}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lubridate)}

\NormalTok{all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(min\_delay }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{week =} \FunctionTok{week}\NormalTok{(date)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(week, line) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_delay =} \FunctionTok{mean}\NormalTok{(min\_delay)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(week, mean\_delay, }\AttributeTok{color =}\NormalTok{ line)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(line),}
              \AttributeTok{scales =} \StringTok{"free\_y"}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-delaybyweek-1.pdf}

}

\caption{\label{fig-delaybyweek}Average delay, in minutes, by week, for
the Toronto subway}

\end{figure}

Now let us look at the proportion of delays that were greater than 10
minutes (Figure~\ref{fig-longdelaybyweek}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{week =} \FunctionTok{week}\NormalTok{(date)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(week, line) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{prop\_delay =} \FunctionTok{sum}\NormalTok{(min\_delay}\SpecialCharTok{\textgreater{}}\DecValTok{10}\NormalTok{)}\SpecialCharTok{/}\FunctionTok{n}\NormalTok{()) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(week, prop\_delay, }\AttributeTok{color =}\NormalTok{ line)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+} 
      \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(line),}
              \AttributeTok{scales =} \StringTok{"free\_y"}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-longdelaybyweek-1.pdf}

}

\caption{\label{fig-longdelaybyweek}Delays longer than ten minutes, by
week, for the Toronto subway}

\end{figure}

These figures, tables, and analysis have no place in a final paper.
Instead, they allow us to become comfortable with the data. We note
aspects about each that stand out, as well as the warnings and any
implications or aspects to return to.

\hypertarget{relationships}{%
\subsection{Relationships}\label{relationships}}

We are also interested in looking at the relationship between two
variables. Scatter plots are especially useful here for continuous
variables, and are a good precursor to modeling. For instance, we may be
interested in the relationship between the delay and the gap
(Figure~\ref{fig-delayvsgap}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ min\_delay, }\AttributeTok{y =}\NormalTok{ min\_gap)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-delayvsgap-1.pdf}

}

\caption{\label{fig-delayvsgap}Relationship between delay and gap for
the Toronto subway in 2020}

\end{figure}

The relationship between categorical variables takes more work, but we
could also, for instance, look at the top five reasons for delay by
station. In particular, we may be interested in whether they differ, and
how any difference could be modeled (Figure~\ref{fig-categorical}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(line, code\_desc) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_delay =} \FunctionTok{mean}\NormalTok{(min\_delay)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{mean\_delay) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ code\_desc,}
             \AttributeTok{y =}\NormalTok{ mean\_delay)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(line), }
             \AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{,}
             \AttributeTok{nrow =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-categorical-1.pdf}

}

\caption{\label{fig-categorical}Relationship between categorical
variables for the Toronto subway in 2020}

\end{figure}

Principal components analysis (PCA) is another powerful exploratory
tool. It allows us to pick up potential clusters and outliers that can
help to inform modeling. To see this, we can look at the types of delay
by station. The delay categories are messy and there a lot of them, but
as we are trying to come to terms with the dataset, we will just take
the first word.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all\_2020\_ttc\_data }\OtherTok{\textless{}{-}}
\NormalTok{  all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{code\_red =} \FunctionTok{case\_when}\NormalTok{(}
    \FunctionTok{str\_starts}\NormalTok{(code\_desc, }\StringTok{"No"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{word}\NormalTok{(code\_desc, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
    \FunctionTok{str\_starts}\NormalTok{(code\_desc, }\StringTok{"Operator"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{word}\NormalTok{(code\_desc, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),}
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \FunctionTok{word}\NormalTok{(code\_desc, }\DecValTok{1}\NormalTok{)}
\NormalTok{  ))}
\end{Highlighting}
\end{Shaded}

Let us also just restrict the analysis to causes that happen at least 50
times over 2019. To do the PCA, the dataframe also needs to be switched
to wide format.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dwide }\OtherTok{\textless{}{-}}
\NormalTok{  all\_2020\_ttc\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(line, station\_clean) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{n\_obs =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(n\_obs }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(code\_red) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tot\_delay =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(tot\_delay) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(tot\_delay }\SpecialCharTok{\textgreater{}} \DecValTok{50}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(line, station\_clean, code\_red) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n\_delay =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ code\_red, }\AttributeTok{values\_from =}\NormalTok{ n\_delay) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate\_all}\NormalTok{(}\AttributeTok{.funs =} \FunctionTok{funs}\NormalTok{(}\FunctionTok{ifelse}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(.), }\DecValTok{0}\NormalTok{, .)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`summarise()` has grouped output by 'line', 'station_clean'. You can override
using the `.groups` argument.
`mutate_all()` ignored the following grouping variables:
\end{verbatim}

\begin{verbatim}
Warning: `funs()` was deprecated in dplyr 0.8.0.
Please use a list of either functions or lambdas: 

  # Simple named list: 
  list(mean = mean, median = median)

  # Auto named with `tibble::lst()`: 
  tibble::lst(mean, median)

  # Using lambdas
  list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))
This warning is displayed once every 8 hours.
Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.
\end{verbatim}

Now we can quickly do some PCA.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delay\_pca }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(dwide[,}\DecValTok{3}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(dwide)])}

\NormalTok{df\_out }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(delay\_pca}\SpecialCharTok{$}\NormalTok{x)}
\NormalTok{df\_out }\OtherTok{\textless{}{-}} \FunctionTok{bind\_cols}\NormalTok{(dwide }\SpecialCharTok{|\textgreater{}} \FunctionTok{select}\NormalTok{(line, station\_clean), df\_out)}
\FunctionTok{head}\NormalTok{(df\_out)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 32
# Groups:   line, station_clean [6]
  line  station_clean    PC1    PC2     PC3   PC4    PC5   PC6    PC7   PC8
  <chr> <chr>          <dbl>  <dbl>   <dbl> <dbl>  <dbl> <dbl>  <dbl> <dbl>
1 BD    BATHURST       10.9   17.2   2.04   12.9   4.60  -2.98  2.31   5.02
2 BD    BAY            14.6    6.54  4.76   14.4   0.406  3.09 -0.144  4.68
3 BD    BLOOR          22.8  -18.6  19.7    -7.37 -1.54  -8.60 -1.36   1.08
4 BD    BLOOR-DANFORTH 23.4  -20.2  20.4    -4.85 -0.429 -6.77 -0.562  1.19
5 BD    BROADVIEW       9.29  22.0  -0.0365  6.72  4.31   1.73  0.304  4.71
6 BD    CASTLE         15.1    5.21  7.62   11.6  -1.17   2.77 -1.71   4.93
# ... with 22 more variables: PC9 <dbl>, PC10 <dbl>, PC11 <dbl>, PC12 <dbl>,
#   PC13 <dbl>, PC14 <dbl>, PC15 <dbl>, PC16 <dbl>, PC17 <dbl>, PC18 <dbl>,
#   PC19 <dbl>, PC20 <dbl>, PC21 <dbl>, PC22 <dbl>, PC23 <dbl>, PC24 <dbl>,
#   PC25 <dbl>, PC26 <dbl>, PC27 <dbl>, PC28 <dbl>, PC29 <dbl>, PC30 <dbl>
\end{verbatim}

We can plot the first two principal components, and add labels to some
of the outlying stations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggrepel)}
\FunctionTok{ggplot}\NormalTok{(df\_out,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{PC1,}\AttributeTok{y=}\NormalTok{PC2,}\AttributeTok{color=}\NormalTok{line )) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_text\_repel}\NormalTok{(}\AttributeTok{data =}\NormalTok{ df\_out }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(PC2}\SpecialCharTok{\textgreater{}}\DecValTok{100}\SpecialCharTok{|}\NormalTok{PC1}\SpecialCharTok{\textless{}}\DecValTok{100}\SpecialCharTok{*{-}}\DecValTok{1}\NormalTok{), }
                  \FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ station\_clean)}
\NormalTok{                  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/unnamed-chunk-49-1.pdf}

}

\end{figure}

We could also plot the factor loadings. We see some evidence that
perhaps one is to do with the public, compared with another to do with
the operator.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_out\_r }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(delay\_pca}\SpecialCharTok{$}\NormalTok{rotation)}
\NormalTok{df\_out\_r}\SpecialCharTok{$}\NormalTok{feature }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(dwide[,}\DecValTok{3}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(dwide)])}

\NormalTok{df\_out\_r}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 30 x 31
        PC1    PC2      PC3      PC4      PC5     PC6      PC7      PC8     PC9
      <dbl>  <dbl>    <dbl>    <dbl>    <dbl>   <dbl>    <dbl>    <dbl>   <dbl>
 1 -0.0279  0.125  -0.0576   0.0679  -0.0133   0.0307 -0.00449  0.134   -0.0472
 2 -0.108   0.343  -0.150    0.205   -0.100    0.317  -0.116    0.252   -0.407 
 3 -0.0196  0.0604 -0.0207   0.0195   0.0189   0.0574 -0.0803  -0.0467  -0.146 
 4 -0.0244  0.0752 -0.0325  -0.0237   0.00121 -0.0263 -0.0137   0.0251   0.104 
 5 -0.0128  0.0113 -0.00340 -0.00977 -0.0255   0.0186 -0.0645  -0.0552  -0.0302
 6 -0.231   0.650  -0.309    0.245    0.222   -0.309   0.172   -0.0711   0.363 
 7 -0.0871  0.233  -0.0904  -0.692   -0.311   -0.414  -0.216    0.00703 -0.116 
 8 -0.00377 0.0193 -0.00201 -0.0140  -0.0424   0.0751 -0.146   -0.0712  -0.0203
 9 -0.0167  0.120  -0.0367  -0.578    0.336    0.563   0.207    0.226    0.289 
10 -0.0708  0.276  -0.118    0.116   -0.368    0.435  -0.178    0.0583  -0.0173
# ... with 20 more rows, and 22 more variables: PC10 <dbl>, PC11 <dbl>,
#   PC12 <dbl>, PC13 <dbl>, PC14 <dbl>, PC15 <dbl>, PC16 <dbl>, PC17 <dbl>,
#   PC18 <dbl>, PC19 <dbl>, PC20 <dbl>, PC21 <dbl>, PC22 <dbl>, PC23 <dbl>,
#   PC24 <dbl>, PC25 <dbl>, PC26 <dbl>, PC27 <dbl>, PC28 <dbl>, PC29 <dbl>,
#   PC30 <dbl>, feature <chr>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(df\_out\_r,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{PC1,}\AttributeTok{y=}\NormalTok{PC2,}\AttributeTok{label=}\NormalTok{feature )) }\SpecialCharTok{+} \FunctionTok{geom\_text\_repel}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: ggrepel: 22 unlabeled data points (too many overlaps). Consider
increasing max.overlaps
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/unnamed-chunk-51-1.pdf}

}

\end{figure}

\hypertarget{case-study-airbnb-listing-in-toronto}{%
\section{Case study: Airbnb listing in
Toronto}\label{case-study-airbnb-listing-in-toronto}}

In this case study we look at Airbnb listings in Toronto. The dataset is
from Inside Airbnb (M. Cox 2021) and we will read it from their website,
and then save a local copy. We can give \texttt{read\_csv()} a link to
where the dataset is and it will download it. This helps with
reproducibility because the source is clear. But, as that link could
change at any time, longer-term reproducibility, as well as wanting to
minimize the effect on the Inside Airbnb servers, suggest that we should
also save a local copy of the data and then use that. As the original
data is not ours, we should not make that public without first getting
written permission. The `guess\_max' option in read\_csv helps us avoid
having to specify the column types. Usually \texttt{read\_csv()} takes a
best guess at the column types based on the first few rows. But
sometimes those first ones are misleading and so `guess\_max' forces it
to look at a larger number of rows to try to work out what is going on.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{airbnb\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"http://data.insideairbnb.com/canada/on/toronto/2021{-}01{-}02/data/listings.csv.gz"}\NormalTok{, }
           \AttributeTok{guess\_max =} \DecValTok{20000}\NormalTok{)}

\FunctionTok{write\_csv}\NormalTok{(airbnb\_data, }\StringTok{"airbnb\_data.csv"}\NormalTok{)}

\NormalTok{airbnb\_data}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore-individual-variables}{%
\subsection{Explore individual
variables}\label{explore-individual-variables}}

There are a large number of columns, so we will just select a few.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(airbnb\_data) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{length}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 74
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(host\_id, }
\NormalTok{         host\_since, }
\NormalTok{         host\_response\_time, }
\NormalTok{         host\_is\_superhost, }
\NormalTok{         host\_listings\_count,}
\NormalTok{         host\_total\_listings\_count,}
\NormalTok{         host\_neighbourhood, }
\NormalTok{         host\_listings\_count, }
\NormalTok{         neighbourhood\_cleansed, }
\NormalTok{         room\_type, }
\NormalTok{         bathrooms, }
\NormalTok{         bedrooms, }
\NormalTok{         price, }
\NormalTok{         number\_of\_reviews, }
\NormalTok{         has\_availability, }
\NormalTok{         review\_scores\_rating, }
\NormalTok{         review\_scores\_accuracy, }
\NormalTok{         review\_scores\_cleanliness, }
\NormalTok{         review\_scores\_checkin, }
\NormalTok{         review\_scores\_communication, }
\NormalTok{         review\_scores\_location, }
\NormalTok{         review\_scores\_value}
\NormalTok{         )}

\NormalTok{airbnb\_data\_selected}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 18,265 x 21
   host_id host_since host_response_time host_is_superhost host_listings_count
     <dbl> <date>     <chr>              <lgl>                           <dbl>
 1    1565 2008-08-08 N/A                FALSE                               1
 2   22795 2009-06-22 N/A                FALSE                               2
 3   48239 2009-10-25 N/A                FALSE                               1
 4   93825 2010-03-15 N/A                FALSE                               2
 5  118124 2010-05-04 within a day       FALSE                               1
 6   22795 2009-06-22 N/A                FALSE                               2
 7  174063 2010-07-20 within an hour     TRUE                                3
 8  183071 2010-07-28 within an hour     TRUE                                2
 9  187320 2010-08-01 within a few hours TRUE                               13
10  192364 2010-08-05 N/A                FALSE                               1
# ... with 18,255 more rows, and 16 more variables:
#   host_total_listings_count <dbl>, host_neighbourhood <chr>,
#   neighbourhood_cleansed <chr>, room_type <chr>, bathrooms <lgl>,
#   bedrooms <dbl>, price <chr>, number_of_reviews <dbl>,
#   has_availability <lgl>, review_scores_rating <dbl>,
#   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>,
#   review_scores_checkin <dbl>, review_scores_communication <dbl>, ...
\end{verbatim}

First we might be interested in price. It is a character at the moment
and so we need to convert it to a numeric. This is a common problem, and
we need to be a little careful that it does not all just convert to NAs.
In our case if we just force the price data to be a numeric then it will
go to NA because there are a lot of characters where it is unclear what
the numeric equivalent is, such as `\$'. So we need to remove those
characters first.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected}\SpecialCharTok{$}\NormalTok{price }\SpecialCharTok{|\textgreater{}} \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "$469.00" "$96.00"  "$64.00"  "$70.00"  "$45.00"  "$127.00"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected}\SpecialCharTok{$}\NormalTok{price }\SpecialCharTok{|\textgreater{}} \FunctionTok{str\_split}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{unlist}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \FunctionTok{unique}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "$" "4" "6" "9" "." "0" "7" "5" "1" "2" "3" "8" ","
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(price) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(price, }\StringTok{","}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 145 x 1
   price    
   <chr>    
 1 $1,724.00
 2 $1,000.00
 3 $1,100.00
 4 $1,450.00
 5 $1,019.00
 6 $1,000.00
 7 $1,300.00
 8 $2,142.00
 9 $2,000.00
10 $1,200.00
# ... with 135 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{price =} \FunctionTok{str\_remove}\NormalTok{(price, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{$"}\NormalTok{),}
         \AttributeTok{price =} \FunctionTok{str\_remove}\NormalTok{(price, }\StringTok{","}\NormalTok{),}
         \AttributeTok{price =} \FunctionTok{as.integer}\NormalTok{(price)}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

Now we can have a look at the distribution of prices
(Figure~\ref{fig-airbnbpricesfirst}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-airbnbpricesfirst-1.pdf}

}

\caption{\label{fig-airbnbpricesfirst}Distribution of prices of Toronto
Airbnb rentals in January 2021}

\end{figure}

It is clear that there are outliers, so again we might like to consider
it on the log scale (Figure~\ref{fig-airbnbpriceslog}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(price }\SpecialCharTok{\textgreater{}} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of properties"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-airbnbpriceslog-1.pdf}

}

\caption{\label{fig-airbnbpriceslog}Distribution of log prices of
Toronto Airbnb rentals in January 2021}

\end{figure}

If we focus on prices that are less than \$1,000 then we see that the
majority of properties have a nightly price less than \$250.
Interestingly it looks like there is some bunching of prices, possible
around numbers ending in zero or nine. Let us just zoom in on prices
between \$90 and \$210, out of interest, but change the bins to be
smaller (\textbf{?@fig-airbnbpricesbunch}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(price }\SpecialCharTok{\textless{}} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}

\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(price }\SpecialCharTok{\textgreater{}} \DecValTok{90}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(price }\SpecialCharTok{\textless{}} \DecValTok{210}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.49\textwidth,height=\textheight]{./13-eda_files/figure-pdf/fig-airbnbpricesbunch-1.pdf}

}

\caption{\label{fig-airbnbpricesbunch-1}Distribution of prices less than
\$1000 for Toronto Airbnb rentals in January 2021 shows bunching}

\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=0.49\textwidth,height=\textheight]{./13-eda_files/figure-pdf/fig-airbnbpricesbunch-2.pdf}

}

\caption{\label{fig-airbnbpricesbunch-2}Distribution of prices less than
\$1000 for Toronto Airbnb rentals in January 2021 shows bunching}

\end{figure}

For now, we will just remove all prices that are more than \$999.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(price }\SpecialCharTok{\textless{}} \DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Superhosts are especially experienced Airbnb hosts, and we might be
interested to learn more about them. For instance, a host either is or
is not a superhost, and so we would not expect any NAs. But we can see
that there are. It might be that the host removed a listing or similar.
For now, we will remove them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(host\_is\_superhost))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 11 x 21
     host_id host_since host_response_time host_is_superhost host_listings_count
       <dbl> <date>     <chr>              <lgl>                           <dbl>
 1  23472830 NA         <NA>               NA                                 NA
 2  31675651 NA         <NA>               NA                                 NA
 3  75779190 NA         <NA>               NA                                 NA
 4  47614482 NA         <NA>               NA                                 NA
 5 201103629 NA         <NA>               NA                                 NA
 6 111820893 NA         <NA>               NA                                 NA
 7  23472830 NA         <NA>               NA                                 NA
 8 196269219 NA         <NA>               NA                                 NA
 9  23472830 NA         <NA>               NA                                 NA
10 266594170 NA         <NA>               NA                                 NA
11 118516038 NA         <NA>               NA                                 NA
# ... with 16 more variables: host_total_listings_count <dbl>,
#   host_neighbourhood <chr>, neighbourhood_cleansed <chr>, room_type <chr>,
#   bathrooms <lgl>, bedrooms <dbl>, price <int>, number_of_reviews <dbl>,
#   has_availability <lgl>, review_scores_rating <dbl>,
#   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>,
#   review_scores_checkin <dbl>, review_scores_communication <dbl>,
#   review_scores_location <dbl>, review_scores_value <dbl>
\end{verbatim}

We will also want to create a binary variable from this. It is
true/false at the moment, which is fine for the modelling, but there are
a handful of situations where it will be easier if we have a 0/1. And
for now we will just remove anyone with an NA for whether they are a
super host.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(host\_is\_superhost)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{host\_is\_superhost\_binary =} \FunctionTok{if\_else}\NormalTok{(}
\NormalTok{    host\_is\_superhost }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

On Airbnb, guests can give 1-5 star ratings across a variety of
different aspects, including cleanliness, accuracy, value, and others.
But when we look at the reviews in our dataset, it is less clear how
this is being constructed, because it appears to be a rating between 20
and 100 and there are also NA values (Figure~\ref{fig-airbnbreviews}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ review\_scores\_rating)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Reviews scores rating"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-airbnbreviews-1.pdf}

}

\caption{\label{fig-airbnbreviews}Distribution of review scores rating
for Toronto Airbnb rentals in January 2021}

\end{figure}

We would like to deal with the NAs in `review\_scores\_rating', but this
is more complicated as there are a lot of them. It may be that this is
just because they do not have any reviews.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(review\_scores\_rating)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{nrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4308
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(review\_scores\_rating)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(number\_of\_reviews) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{table}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   0    1    2    3    4 
4046  226   23   10    3 
\end{verbatim}

So these properties do not have a review rating yet because they do not
have enough reviews. It is a large proportion of the total, at almost a
fifth of them so we might like to look at this in more detail using
\texttt{vis\_miss()} from \texttt{visdat} (N. Tierney 2017). We are
interested to see whether there is something systematic happening with
these properties. For instance, if the NAs were being driven by, say,
some requirement of a minimum number of reviews, then we would expect
they would all be missing.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(visdat)}
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(review\_scores\_rating, }
\NormalTok{         review\_scores\_accuracy, }
\NormalTok{         review\_scores\_cleanliness, }
\NormalTok{         review\_scores\_checkin, }
\NormalTok{         review\_scores\_communication, }
\NormalTok{         review\_scores\_location, }
\NormalTok{         review\_scores\_value) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{vis\_miss}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/unnamed-chunk-73-1.pdf}

}

\end{figure}

Given it looks convincing that in almost all cases, the different types
of reviews are missing for the same observation. One approach would be
to just focus on those that are not missing and the main review score.
It is clear that almost all the reviews are more than 80. Let us just
zoom in on that 60 to 80 range to check what the distribution looks like
in that range (Figure~\ref{fig-airbnbreviewsselected}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(review\_scores\_rating)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(review\_scores\_rating }\SpecialCharTok{\textgreater{}} \DecValTok{60}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(review\_scores\_rating }\SpecialCharTok{\textless{}} \DecValTok{80}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ review\_scores\_rating)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Average review score"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-airbnbreviewsselected-1.pdf}

}

\caption{\label{fig-airbnbreviewsselected}Distribution of review scores,
between 60 and 80, for Toronto Airbnb rentals in January 2021}

\end{figure}

For now, we will remove anyone with an NA in their main review score,
even though this will remove roughly 20 per cent of observations. And we
will also focus only on those hosts with a main review score of at least
70. If we ended up using this dataset for actual analysis, then we would
want to justify this decision in an appendix or similar.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(review\_scores\_rating)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(review\_scores\_rating }\SpecialCharTok{\textgreater{}=} \DecValTok{70}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Another important factor is how quickly a host responds to an enquiry.
Airbnb allows hosts up to 24 hours to respond, but encourages responses
within an hour.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(host\_response\_time)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 2
  host_response_time     n
  <chr>              <int>
1 a few days or more   494
2 N/A                 5708
3 within a day         952
4 within a few hours  1649
5 within an hour      4668
\end{verbatim}

It is unclear a host could have a response time of NA. It may be that
they are related to some other variable. Interestingly it seems like
what looks like `NAs' in `host\_response\_time' variable are not coded
as proper NAs, but are instead being treated as another category. We
will recode them to be actual NAs and also change the variable to be a
factor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{host\_response\_time =} \FunctionTok{if\_else}\NormalTok{(host\_response\_time }\SpecialCharTok{==} \StringTok{"N/A"}\NormalTok{, }\ConstantTok{NA\_character\_}\NormalTok{, host\_response\_time),}
         \AttributeTok{host\_response\_time =} \FunctionTok{factor}\NormalTok{(host\_response\_time)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

There is clearly an issue with NAs as there are a lot of them. For
instance, we might be interested to see if there is a relationship with
the review score (Figure~\ref{fig-airbnbreviewsselectednasresponse}).
There are a lot that have an overall review of 100.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(host\_response\_time)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ review\_scores\_rating)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Average review score"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of properties"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-airbnbreviewsselectednasresponse-1.pdf}

}

\caption{\label{fig-airbnbreviewsselectednasresponse}Distribution of
review scores for properties with NA response time, for Toronto Airbnb
rentals in January 2021}

\end{figure}

For now, we will remove anyone with a NA in their response time. This
will again removes roughly another 20 per cent of the observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(host\_response\_time))}
\end{Highlighting}
\end{Shaded}

There are two versions of a variable that suggest how many properties a
host has on Airbnb. We might be interested to know whether there is a
difference between them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{listings\_count\_is\_same =} \FunctionTok{if\_else}\NormalTok{(host\_listings\_count }\SpecialCharTok{==}\NormalTok{ host\_total\_listings\_count, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(listings\_count\_is\_same }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 0 x 23
# ... with 23 variables: host_id <dbl>, host_since <date>,
#   host_response_time <fct>, host_is_superhost <lgl>,
#   host_listings_count <dbl>, host_total_listings_count <dbl>,
#   host_neighbourhood <chr>, neighbourhood_cleansed <chr>, room_type <chr>,
#   bathrooms <lgl>, bedrooms <dbl>, price <int>, number_of_reviews <dbl>,
#   has_availability <lgl>, review_scores_rating <dbl>,
#   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>, ...
\end{verbatim}

As there are no differences in this dataset, we can just remove one
variable for now and have a look at the other one
(Figure~\ref{fig-airbnbhostlisting}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{host\_listings\_count)}

\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ host\_total\_listings\_count)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-airbnbhostlisting-1.pdf}

}

\caption{\label{fig-airbnbhostlisting}Distribution of the number of
properties a host has on Airbnb, for Toronto Airbnb rentals in January
2021}

\end{figure}

There are a large number who have somewhere in the 2-10 properties
range, but the usual long tail. The number with 0 listings is unexpected
and worth following up on. And there are a bunch with NA that we will
need to deal with.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(host\_total\_listings\_count }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 21
  host_id host_since host_response_time host_is_superhost host_total_listings_c~
    <dbl> <date>     <fct>              <lgl>                              <dbl>
1 3783106 2012-10-06 within an hour     FALSE                                  0
2 3814089 2012-10-09 within an hour     FALSE                                  0
3 3827668 2012-10-10 within a day       FALSE                                  0
4 2499198 2012-05-30 within a day       FALSE                                  0
5 3268493 2012-08-15 within a day       FALSE                                  0
6 8675040 2013-09-06 within an hour     TRUE                                   0
# ... with 16 more variables: host_neighbourhood <chr>,
#   neighbourhood_cleansed <chr>, room_type <chr>, bathrooms <lgl>,
#   bedrooms <dbl>, price <int>, number_of_reviews <dbl>,
#   has_availability <lgl>, review_scores_rating <dbl>,
#   review_scores_accuracy <dbl>, review_scores_cleanliness <dbl>,
#   review_scores_checkin <dbl>, review_scores_communication <dbl>,
#   review_scores_location <dbl>, review_scores_value <dbl>, ...
\end{verbatim}

There is nothing that immediately jumps out as odd about the people with
zero listings, but there must be something going on. For now, we will
focus on only those with one property.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\OtherTok{\textless{}{-}} 
\NormalTok{  airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{add\_count}\NormalTok{(host\_id) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{n)}
\end{Highlighting}
\end{Shaded}

\hypertarget{explore-relationships-between-variables}{%
\subsection{Explore relationships between
variables}\label{explore-relationships-between-variables}}

We might like to make some graphs to see if there are any relationships
that become clear. Some aspects that come to mind is looking at prices
and reviews and super hosts, and number of properties and neighborhood.

Look at the relationship between price and reviews, and whether they are
a super-host (Figure~\ref{fig-priceandreview}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price, }\AttributeTok{y =}\NormalTok{ review\_scores\_rating, }\AttributeTok{color =}\NormalTok{ host\_is\_superhost)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Price per night"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Average review score"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Super host"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./13-eda_files/figure-pdf/fig-priceandreview-1.pdf}

}

\caption{\label{fig-priceandreview}Relationship between price and review
and whether a host is a super host, for Toronto Airbnb rentals in
January 2021}

\end{figure}

One of the aspects that may make someone a super host is how quickly
they respond to inquiries. One could imagine that being a superhost
involves quickly saying yes or no to inquiries. Let us look at the data.
First, we want to look at the possible values of superhost by their
response times.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(host\_is\_superhost) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{proportion =}\NormalTok{ n }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(n),}
         \AttributeTok{proportion =} \FunctionTok{round}\NormalTok{(proportion, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 3
  host_is_superhost     n proportion
  <lgl>             <int>      <dbl>
1 FALSE              1677       0.58
2 TRUE               1234       0.42
\end{verbatim}

Fortunately, it looks like when we removed the reviews rows we removed
any NAs from whether they were a super host, but if we go back and look
into that we may need to check again. We could build a table that looks
at a hosts response time by whether they are a superhost using
\texttt{tabyl()} from \texttt{janitor} (Firke 2020). It is clear that is
a host does not respond within an hour then it is unlikely that they are
a super host.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{tabyl}\NormalTok{(host\_response\_time, host\_is\_superhost) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{adorn\_percentages}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{(}\AttributeTok{digits =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_ns}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{adorn\_title}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                    host_is_superhost          
 host_response_time             FALSE      TRUE
 a few days or more         90% (223) 10%  (26)
       within a day         70% (348) 30% (149)
 within a few hours         55% (354) 45% (284)
     within an hour         49% (752) 51% (775)
\end{verbatim}

Finally, we could look at neighborhood. The data provider has attempted
to clean the neighborhood variable for us, so will just use that
variable for now. Although if we ended up using this variable for our
actual analysis we would want to check they had not made any errors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{tabyl}\NormalTok{(neighbourhood\_cleansed) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{adorn\_totals}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{nrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 140
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb\_data\_selected }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{tabyl}\NormalTok{(neighbourhood\_cleansed) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{n) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}} \DecValTok{100}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{adorn\_totals}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            neighbourhood_cleansed   n percent
 Waterfront Communities-The Island 488   16.8%
                           Niagara 129    4.4%
                             Annex 102    3.5%
                             Total 719       -
\end{verbatim}

\hypertarget{explore-multiple-relationships-with-a-model}{%
\subsection{Explore multiple relationships with a
model}\label{explore-multiple-relationships-with-a-model}}

We will now run some models on our dataset. We will cover modeling in
more detail in Chapter @ref(ijalm), but we can use models during EDA to
help get a better sense of relationships that may exist between multiple
variables in a dataset. For instance, we may like to see whether we can
forecast whether someone is a super host, and the factors that go into
explaining that. As the dependent variable is binary, this is a good
opportunity to use logistic regression. We expect that better reviews
will be associated with faster responses and higher reviews.
Specifically, the model that we estimate is:

\[\mbox{Prob(Is super host} = 1) = \beta_0 + \beta_1 \mbox{Response time} + \beta_2 \mbox{Reviews} + \epsilon\]

We estimate the model using \texttt{glm} in the R language (R Core Team
2021).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logistic\_reg\_superhost\_response\_review }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(host\_is\_superhost }\SpecialCharTok{\textasciitilde{}} 
\NormalTok{                                                host\_response\_time }\SpecialCharTok{+} 
\NormalTok{                                                review\_scores\_rating,}
                                              \AttributeTok{data =}\NormalTok{ airbnb\_data\_selected,}
                                              \AttributeTok{family =}\NormalTok{ binomial}
\NormalTok{                                              )}
\end{Highlighting}
\end{Shaded}

We can have a quick look at the results using \texttt{modelsummary()}
from \texttt{modelsummary} (Arel-Bundock 2021a)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(modelsummary)}
\FunctionTok{modelsummary}\NormalTok{(logistic\_reg\_superhost\_response\_review)}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lc}
\toprule
  & Model 1\\
\midrule
(Intercept) & \num{-18.931}\\
 & (\num{1.273})\\
host\_response\_timewithin a day & \num{1.334}\\
 & (\num{0.235})\\
host\_response\_timewithin a few hours & \num{1.932}\\
 & (\num{0.227})\\
host\_response\_timewithin an hour & \num{2.213}\\
 & (\num{0.219})\\
review\_scores\_rating & \num{0.173}\\
 & (\num{0.013})\\
\midrule
Num.Obs. & \num{2911}\\
AIC & \num{3521.4}\\
BIC & \num{3551.3}\\
Log.Lik. & \num{-1755.698}\\
F & \num{75.649}\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{exercises-and-tutorial-12}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-12}}

\hypertarget{exercises-12}{%
\subsection{Exercises}\label{exercises-12}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In your own words what is exploratory data analysis (this will be
  difficult, but please write only one nuanced paragraph)?
\item
  In Tukey's words, what is exploratory data analysis (please write one
  paragraph)?
\item
  Who was Tukey (please write a paragraph or two)?
\item
  If you have a dataset called `my\_data', which has two columns:
  `first\_col' and `second\_col', then could you please write some rough
  R code that would generate a graph (the type of graph does not
  matter).
\item
  Consider a dataset that has 500 rows and 3 columns, so there are 1,500
  cells. If 100 of the cells are missing data for at least one of the
  columns, then would you remove the whole row your dataset or try to
  run your analysis on the data as is, or some other procedure? What if
  your dataset had 10,000 rows instead, but the same number of missing
  cells?
\item
  Please note three ways of identifying unusual values.
\item
  What is the difference between a categorical and continuous variable?
\item
  What is the difference between a factor and an integer variable?
\item
  How can we think about who is systematically excluded from a dataset?
\item
  Using the \texttt{opendatatoronto} package, download the data on
  mayoral campaign contributions for 2014. (note: the 2014 file you will
  get from \texttt{get\_resource}, so just keep the sheet that relates
  to the Mayor election).

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Clean up the data format (fixing the parsing issue and standardizing
    the column names using \texttt{janitor})
  \item
    Summarize the variables in the dataset. Are there missing values,
    and if so, should we be worried about them? Is every variable in the
    format it should be? If not, create new variable(s) that are in the
    right format.
  \item
    Visually explore the distribution of values of the contributions.
    What contributions are notable outliers? Do they share a similar
    characteristic(s)? It may be useful to plot the distribution of
    contributions without these outliers to get a better sense of the
    majority of the data.
  \item
    List the top five candidates in each of these categories: 1) total
    contributions; 2) mean contribution; and 3) number of contributions.
  \item
    Repeat that process, but without contributions from the candidates
    themselves.
  \item
    How many contributors gave money to more than one candidate?
  \end{enumerate}
\item
  Name three geoms that produce graphs that have bars on them
  \texttt{ggplot()}.
\item
  Consider a dataset 10,000 observations and 27 variables. For each
  observation, there is at least one missing variable. Please discuss,
  in a paragraph or two, the steps that you would take to understand
  what is going on.
\item
  Known missing data, are those that leave holes in your dataset. But
  what about data that were never collected? Please look at McClelland
  (2019) and Luscombe and McClelland (2020). Look into how they gathered
  their dataset and what it took to put this together. What is in the
  dataset and why? What is missing and why? How could this affect the
  results? How might similar biases enter into other datasets that you
  have used or read about?
\end{enumerate}

\hypertarget{tutorial-12}{%
\subsection{Tutorial}\label{tutorial-12}}

Redo the Airbnb EDA but for Paris. Please submit a PDF.

\hypertarget{sec-its-just-a-linear-model}{%
\chapter{It's Just A Linear Model}\label{sec-its-just-a-linear-model}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{An Introduction to Statistical Learning with Applications
  in R}, Chapters 3 `Linear Regression', and 4 `Classification', (James
  et al. 2017)
\item
  Read \emph{Data Analysis Using Regression and Multilevel/Hierarchical
  Models}, Chapters 3 `Linear regression: the basics', 4 `Linear
  regression: before and after fitting the model', 5 `Logistic
  regression', and 6 `Generalized linear models', (Gelman and Hill 2007)
\item
  Read \emph{Why most published research findings are false}, (Ioannidis
  2005)
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Simple and multiple linear regression.
\item
  Logistic and Poisson regression.
\item
  The key role of uncertainty.
\item
  Threats to validity of inference.
\item
  Overfitting.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{broom} (D. Robinson, Hayes, and Couch 2021)
\item
  \texttt{modelsummary} (Arel-Bundock 2021a)
\item
  \texttt{rstanarm} (Goodrich et al. 2020)
\item
  \texttt{tidymodels} (Kuhn and Wickham 2020)
\item
  \texttt{tidyverse} (Wickham et al. 2019a)
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{broom::augment()}
\item
  \texttt{broom::glance()}
\item
  \texttt{broom::tidy()}
\item
  \texttt{glm()}
\item
  \texttt{lm()}
\item
  \texttt{modelsummary::modelsummary()}
\item
  \texttt{parsnip::fit()}
\item
  \texttt{parsnip::linear\_reg()}
\item
  \texttt{parsnip::logistic\_reg()}
\item
  \texttt{parsnip::set\_engine()}
\item
  \texttt{poissonreg::poisson\_reg()}
\item
  \texttt{rnorm()}
\item
  \texttt{rpois()}
\item
  \texttt{rsample::initial\_split()}
\item
  \texttt{rsample::testing()}
\item
  \texttt{rsample::training()}
\item
  \texttt{sample()}
\item
  \texttt{set.seed()}
\item
  \texttt{summary()}
\end{itemize}

\hypertarget{introduction-11}{%
\section{Introduction}\label{introduction-11}}

Linear models have been around for a long time. For instance, speaking
about the development of least squares, which is one way to fit linear
models, in the 1700s, Stigler (1986, 16) describes how it was associated
with foundational problems in astronomy, such as determining the motion
of the moon and reconciling the non-periodic motion of Jupiter and
Saturn. The fundamental issue at the time with least squares was that of
hesitancy to combine different observations. Astronomers were early to
develop a comfort with doing this because they had typically gathered
their observations themselves and knew that the conditions of the data
gathering were similar, even though the value of the observation was
different. It took longer for social scientists to become comfortable
with linear models, possibly because they were hesitant to group
together data they worried was not alike. In this sense, astronomers had
an advantage because they could compare their predictions with what
actually happened whereas this was more difficult for social scientists
(Stigler 1986, 163).

Galton and others of his generation, some of whom were eugenicists, used
linear regression in earnest in the late 1800s and early 1900s. Binary
outcomes quickly became of interest and needed special treatment,
leading to the development and wide adaption of logistic regression and
similar methods in the mid-1900s (Cramer 2002). The generalized linear
model framework came into being, in a formal sense, in the 1970s with
Nelder and Wedderburn (1972) who brought this all together. Generalized
linear models (GLM) broaden the types of outcomes that are allowed. We
still model outcomes as a linear function, but we are not constrained to
an outcome that is normally distributed. The outcome can be anything in
the exponential family, and popular choices include the logistic
distribution, and the Poisson distribution. A further generalization of
GLMs is generalized additive models where we broaden the structure of
the explanatory side. We still explain the dependent variable as an
additive function of various bits and pieces, but those bits and pieces
can be functions. This framework, in this way, came about in the 1990s,
with Hastie and Tibshirani (1990).

It is important to recognize that when we build models, we are not
discovering `the truth'. We are using the model to help us explore and
understand the data that we have. There is no one best model, there are
just useful models that help us learn something about the data that we
have and hence, hopefully, something about the world from which the data
were generated. When we use models, we are trying to understand the
world, but there are enormous constraints on the perspective we bring to
this. It is silly to expect one model to be universal. Further, we
should not just blindly throw data into a regression model and hope that
it will sort it out. `Regression will not sort it out. Regression is
indeed an oracle, but a cruel one. It speaks in riddles and delights in
punishing us for asking bad questions' (McElreath 2020, 162).

We use models to understand the world. We poke, push, and test them. We
build them and rejoice in their beauty, and then seek to understand
their limits and ultimately destroy them. It is this process that is
important, it is this process that allows us to better understand the
world; not the outcome. When we build models, we need to keep in mind
both the world of the model and the broader world that we want to be
able to speak about. To what extent does a model trained on the
experiences of straight, cis, men, speak to the world as it is? It is
not worthless, but it is also not unimpeachable. To what extent does the
model teach us about the data that we have? To what extent do the data
that we have reflect the world about which we would like to draw
conclusions? We need to keep such questions front of mind.

Much of statistics was developed without concern for broader
implications. And that was reasonable because it was developed for
situations such as astronomy and agriculture. Folks were literally able
to randomize the order of fields and planting because they literally
worked at agricultural stations. But many of the subsequent applications
in the twentieth and twenty-first centuries, do not have those
properties. Statistical science is often taught as though it proceeds
through some idealized process where a hypothesis appears, is tested,
and is either confirmed or not. But that is not what happens. We react
to incentives. We dabble, guess, and test, and then follow our
intuition, backfilling as we need. All of this is fine. But it is not a
world in which a traditional null hypothesis holds completely, which
means concepts such as p-values and power lose some of their meaning.
While we need to understand the `old world', we also need to be
sophisticated enough to know when we need to move away from it. We can
appreciate the beauty and ingenuity of a Ford Model T, at the same time
recognizing we could not use it to win Le Mans.

In this chapter we begin with simple linear regression, and then move to
multiple linear regression, the difference being the number of
explanatory variables that we allow. We then consider logistic and
Poisson regression. We consider three approaches for each of these: base
R, which is useful when we want to quickly use the models in EDA;
\texttt{tidymodels} (Kuhn and Wickham 2020) which is useful when we are
interested in forecasting; and \texttt{rstanarm} (Goodrich et al. 2020)
when we are interested in understanding. Regardless of the approach we
use, the important thing to remember is that modelling in this way is
just fancy averaging. The chapter is named for a quote by Daniela
Witten, Professor, University of Washington, who identifies how far we
can get with linear models and the huge extent to which they underpin
statistics.

\hypertarget{simple-linear-regression}{%
\section{Simple linear regression}\label{simple-linear-regression}}

When we are interested in the relationship between two continuous
variables, say \(y\) and \(x\), we can use simple linear regression.
This is based on the Normal, also `Gaussian', distribution. The shape of
the Normal distribution is determined by two parameters, the mean
\(\mu\) and the standard deviation, \(\sigma\):

\[y = \frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{1}{2}z^2},\] where
\(z = (x - \mu)/\sigma\) is the difference between the mean, \(\mu\),
and \(x\) in terms of the standard deviation (Pitman 1993, 94).

As discussed in Chapter @ref(r-essentials), we use \texttt{rnorm()} to
simulate data from the Normal distribution.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{twenty\_draws\_from\_normal\_distribution }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{draws =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{20}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{))}
  
\NormalTok{twenty\_draws\_from\_normal\_distribution}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 20 x 1
     draws
     <dbl>
 1 -0.360 
 2 -0.0406
 3 -1.78  
 4 -1.12  
 5 -1.00  
 6  1.78  
 7 -1.39  
 8 -0.497 
 9 -0.558 
10 -0.824 
11  1.67  
12 -0.682 
13  0.0652
14 -0.260 
15  0.329 
16 -0.437 
17 -0.323 
18  0.115 
19  0.842 
20  0.342 
\end{verbatim}

Here we specified \(n=20\) draws from a Normal distribution with mean of
0 and standard deviation of 1. When we deal with real data, we will not
know these parameters and we want to use our data to estimate them. We
can estimate the mean, \(\bar{x}\), and standard deviation,
\(\hat{\sigma}_x\), with the following estimators:

\[
\begin{aligned}
 \bar{x} &= \frac{1}{n} \times \sum_{i = 1}^{n}x_i\\
 \hat{\sigma}_{x} &= \sqrt{\frac{1}{n} \times \sum_{i = 1}^{n}\left(x_i - \bar{x}\right)^2}
\end{aligned}
\]

If \(\hat{\sigma}_x\) is the standard deviation, then a standard error
of an estimate, say, \(\bar{x}\) is:
\[\mbox{SE}(\bar{x})^2 = \frac{\sigma^2}{n}\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estimated\_mean }\OtherTok{\textless{}{-}}
  \FunctionTok{sum}\NormalTok{(twenty\_draws\_from\_normal\_distribution}\SpecialCharTok{$}\NormalTok{draws) }\SpecialCharTok{/} \FunctionTok{nrow}\NormalTok{(twenty\_draws\_from\_normal\_distribution)}

\NormalTok{estimated\_mean}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.2069253
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estimated\_mean }\OtherTok{\textless{}{-}}
\NormalTok{  twenty\_draws\_from\_normal\_distribution }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{draws\_diff\_square =}\NormalTok{ (draws }\SpecialCharTok{{-}}\NormalTok{ estimated\_mean)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}

\NormalTok{estimated\_standard\_deviation }\OtherTok{\textless{}{-}} 
  \FunctionTok{sqrt}\NormalTok{(}
    \FunctionTok{sum}\NormalTok{(estimated\_mean}\SpecialCharTok{$}\NormalTok{draws\_diff\_square) }\SpecialCharTok{/} \FunctionTok{nrow}\NormalTok{(twenty\_draws\_from\_normal\_distribution)}
\NormalTok{  )}

\NormalTok{estimated\_standard\_deviation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.8832841
\end{verbatim}

We should not be worried that our estimates are slightly off. It will
typically take a larger number of draws before we get the expected
shape, and our estimated parameters get close to the actual parameters,
but it will happen (Figure~\ref{fig-normaldistributiontakingshape}).
Wasserman (2005, 76) describes our certainty of this, which is due to
the Law of Large Numbers, as `a crowning achievement in probability'.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{number\_of\_draws =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"2 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{2}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"5 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{5}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"10 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{10}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"50 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{50}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"100 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{100}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"500 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{500}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"1,000 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{1000}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"10,000 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{10000}\NormalTok{),}
    \FunctionTok{rep.int}\NormalTok{(}\AttributeTok{x =} \StringTok{"100,000 draws"}\NormalTok{, }\AttributeTok{times =} \DecValTok{100000}\NormalTok{)}
\NormalTok{  ),}
  \AttributeTok{draws =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{2}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{50}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{500}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{10000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{100000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{  )}
\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{number\_of\_draws =} \FunctionTok{as\_factor}\NormalTok{(number\_of\_draws)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ draws)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(number\_of\_draws),}
             \AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}Draw\textquotesingle{}}\NormalTok{,}
       \AttributeTok{y =} \StringTok{\textquotesingle{}Density\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./14-ijalm_files/figure-pdf/fig-normaldistributiontakingshape-1.pdf}

}

\caption{\label{fig-normaldistributiontakingshape}The Normal
distribution takes its familiar shape as the number of draws increases}

\end{figure}

When we use simple linear regression, we assume that our relationship is
characterized by the variables and the parameters. If we have two
variables, \(Y\) and \(X\), then we could characterize a linear
relationship between these as: \[Y \approx \beta_0 + \beta_1 X.\]

There are two coefficients, also `parameters': the `intercept',
\(\beta_0\), and the `slope', \(\beta_1\). We are saying that \(Y\) will
have some value, \(\beta_0\), even when \(X\) is 0, and that \(Y\) will
change by \(\beta_1\) units for every one unit change in \(X\). The
language that we use is that `X is being regressed on Y'. We may then
take this relationship to the data that we have, in order to estimate
these coefficients, for the particular data that we have.

To make this example concrete, we will simulate some data and then
discuss it in that context. For instance, we could consider the time it
takes someone to run five kilometers, compared with the time it takes
them to run a marathon (Figure~\ref{fig-fivekmvsmarathon}). We impute a
relationship of 8.4, as that is roughly the ratio between the distance
of a marathon and a five-kilometer race.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_observations }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{expected\_relationship }\OtherTok{\textless{}{-}} \FloatTok{8.4}

\NormalTok{simulated\_running\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{five\_km\_time =} 
           \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_observations, }
                 \AttributeTok{min =} \DecValTok{15}\NormalTok{, }
                 \AttributeTok{max =} \DecValTok{30}\NormalTok{),}
         \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(number\_of\_observations, }\DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{),}
         \AttributeTok{marathon\_time =}\NormalTok{ five\_km\_time }\SpecialCharTok{*}\NormalTok{ expected\_relationship }\SpecialCharTok{+}\NormalTok{ noise}
\NormalTok{         ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{five\_km\_time =} \FunctionTok{round}\NormalTok{(five\_km\_time, }\AttributeTok{digits =} \DecValTok{1}\NormalTok{),}
         \AttributeTok{marathon\_time =} \FunctionTok{round}\NormalTok{(marathon\_time, }\AttributeTok{digits =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{noise)}

\NormalTok{simulated\_running\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 2
   five_km_time marathon_time
          <dbl>         <dbl>
 1         20.4          152.
 2         16.8          134.
 3         22.3          198.
 4         19.7          166.
 5         15.6          163.
 6         21.1          168.
 7         17            131.
 8         18.6          150.
 9         17.4          158.
10         17.8          147.
# ... with 90 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_running\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ five\_km\_time, }\AttributeTok{y =}\NormalTok{ marathon\_time)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Five{-}kilometer time (minutes)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Marathon time (minutes)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./14-ijalm_files/figure-pdf/fig-fivekmvsmarathon-1.pdf}

}

\caption{\label{fig-fivekmvsmarathon}Simulated data of the relationship
between the time to run five kilometers and a marathon}

\end{figure}

In this simulated example, we know what \(\beta_0\) and \(\beta_1\) are.
But our challenge is to see if we can use only the data, and simple
linear regression, to recover them. That is, can we use \(x\), which is
the five-kilometer time, to produce estimates of \(y\), which is the
marathon time, and which we will put a hat on to denote our estimate:

\[\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x.\] The hats are used to
indicate that these are estimated values.

This involves estimating values for \(\beta_0\) and \(\beta_1\), and
again, our estimates will be denoted by a hat on them. But how should we
estimate these coefficients? Even if we impose a linear relationship
there are many options, because a large number of straight lines could
be drawn. But some of those lines would fit the data better than others.

One way we may define a line as being `better' than another, is if it is
as close as possible to each of the \(x\) and \(y\) combinations that we
know. There are a lot of candidates for how we define `as close as
possible', but one is to minimize the residual sum of squares. To do
this we produce estimates for \(\hat{y}\) based on some guesses of
\(\hat{\beta}_0\) and \(\hat{\beta}_1\), given the \(x\). We then work
out how `wrong', for every point \(i\), we were:
\[e_i = y_i - \hat{y}_i.\]

To compute the residual sum of squares (RSS), we sum across all the
points, taking the square to account for negative differences:
\[\mbox{RSS} = e^2_1+ e^2_2 +\dots + e^2_n.\] This results in one
`linear best-fit' line (Figure~\ref{fig-fivekmvsmarathonwithbestfit}),
but it is worth thinking about all the assumptions and decisions that it
took to get us to this point.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_running\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ five\_km\_time, }\AttributeTok{y =}\NormalTok{ marathon\_time)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }
              \AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }
              \AttributeTok{color =} \StringTok{"black"}\NormalTok{, }
              \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
              \AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Five{-}kilometer time (minutes)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Marathon time (minutes)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./14-ijalm_files/figure-pdf/fig-fivekmvsmarathonwithbestfit-1.pdf}

}

\caption{\label{fig-fivekmvsmarathonwithbestfit}Simulated data of the
relationship between the time to run five kilometers and a marathon}

\end{figure}

Underpinning our use of simple linear regression is a belief that there
is some `true' relationship between \(X\) and \(Y\), that is:

\[Y = f(X) + \epsilon.\]

We are going to say that function, \(f()\), is linear, and so for simple
linear regression:

\[\hat{Y} = \beta_0 + \beta_1 X + \epsilon.\]

There is some `true' relationship between \(X\) and \(Y\), but we do not
know what it is. All we can do is use our sample of data to estimate it.
But because our understanding depends on that sample, for every possible
sample, we would get a slightly different relationship, as measured by
the coefficients.

That \(\epsilon\) is a measure of our error---what does the model not
know? There is going to be plenty that the model does not know, but we
hope the error does not depend on \(X\), and that the error is normally
distributed.

We can conduct simple linear regression with \texttt{lm()} from base R.
We specify the dependent variable first, then
\texttt{\textasciitilde{}}, followed by the independent variables.
Finally, we specify the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_running\_data\_first\_model }\OtherTok{\textless{}{-}} 
  \FunctionTok{lm}\NormalTok{(marathon\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ five\_km\_time, }
     \AttributeTok{data =}\NormalTok{ simulated\_running\_data)}
\end{Highlighting}
\end{Shaded}

To see the result of the regression, we can use \texttt{summary()} from
base R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(simulated\_running\_data\_first\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = marathon_time ~ five_km_time, data = simulated_running_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.654  -9.278   0.781  12.606  56.898 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)    8.2393     8.9550    0.92     0.36    
five_km_time   7.9407     0.4072   19.50   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 16.96 on 98 degrees of freedom
Multiple R-squared:  0.7951,    Adjusted R-squared:  0.793 
F-statistic: 380.3 on 1 and 98 DF,  p-value: < 2.2e-16
\end{verbatim}

The first part of the result tells us the regression that we specified,
then information about the residuals, and our estimated coefficients.
And then finally some useful diagnostics.

The intercept is the marathon time that we would expect with a
five-kilometer time of 0 minutes. Hopefully this example illustrates the
need to carefully interpret the intercept coefficient! The coefficient
on five-kilometer run time shows how we expect the marathon time to
change if the five-kilometer run time changed by one unit. In this case
it is about 8.4, which makes sense seeing as a marathon is roughly that
many times longer than a five-kilometer run.

We use \texttt{augment()} from \texttt{broom} (D. Robinson, Hayes, and
Couch 2021) to add the fitted values and residuals to our original
dataset. This allows us to plot the residuals
(\textbf{?@fig-fivekmvsmarathonresids}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}

\NormalTok{simulated\_running\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{augment}\NormalTok{(simulated\_running\_data\_first\_model,}
          \AttributeTok{data =}\NormalTok{ simulated\_running\_data)}

\NormalTok{simulated\_running\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 8
   five_km_time marathon_time .fitted .resid   .hat .sigma   .cooksd .std.resid
          <dbl>         <dbl>   <dbl>  <dbl>  <dbl>  <dbl>     <dbl>      <dbl>
 1         20.4          152.    170. -17.8  0.0108   17.0 0.00611       -1.06 
 2         16.8          134.    142.  -7.84 0.0232   17.0 0.00260       -0.468
 3         22.3          198.    185.  13.1  0.0103   17.0 0.00312        0.775
 4         19.7          166.    165.   1.83 0.0121   17.1 0.0000718      0.108
 5         15.6          163.    132.  31.3  0.0307   16.7 0.0556         1.87 
 6         21.1          168.    176.  -8.09 0.0101   17.0 0.00118       -0.479
 7         17            131.    143. -11.8  0.0222   17.0 0.00564       -0.705
 8         18.6          150.    156.  -6.04 0.0152   17.0 0.000990      -0.359
 9         17.4          158.    146.  11.1  0.0201   17.0 0.00448        0.661
10         17.8          147.    150.  -2.68 0.0183   17.0 0.000238      -0.160
# ... with 90 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(simulated\_running\_data, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ .resid)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Number of occurrences"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Residuals"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(simulated\_running\_data, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ five\_km\_time, }\AttributeTok{y =}\NormalTok{ .resid)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{, }\AttributeTok{color =} \StringTok{"grey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Residuals"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Five{-}kilometer time (minutes)"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(simulated\_running\_data, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ marathon\_time, .fitted)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Estimated marathon time"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Actual marathon time"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{./14-ijalm_files/figure-pdf/fig-fivekmvsmarathonresids-1.pdf}

}

\caption{\label{fig-fivekmvsmarathonresids-1}Residuals from the simple
linear regression with simulated data on the time someone takes to run
five kilometers and a marathon}

}

\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{./14-ijalm_files/figure-pdf/fig-fivekmvsmarathonresids-2.pdf}

}

\caption{\label{fig-fivekmvsmarathonresids-2}Residuals from the simple
linear regression with simulated data on the time someone takes to run
five kilometers and a marathon}

}

\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{./14-ijalm_files/figure-pdf/fig-fivekmvsmarathonresids-3.pdf}

}

\caption{\label{fig-fivekmvsmarathonresids-3}Residuals from the simple
linear regression with simulated data on the time someone takes to run
five kilometers and a marathon}

}

\end{minipage}%

\end{figure}

We want our estimate to be unbiased. When we say our estimate is
unbiased, we are trying to say that even though with some sample our
estimate might be too high, and with another sample our estimate might
be too low, eventually if we have a lot of data then our estimate would
be the same as the population. An estimator is unbiased if it does not
systematically over- or under-estimate (James et al. 2017, 65).

But we want to try to speak to the `true' relationship, so we need to
try to capture how much we think our understanding depends on the
particular sample that we have to analyze. And this is where standard
error comes in. It tells us how off our estimate is compared with the
actual (Figure~\ref{fig-fivekmvsmarathonses}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_running\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ five\_km\_time, }\AttributeTok{y =}\NormalTok{ marathon\_time)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }
              \AttributeTok{se =} \ConstantTok{TRUE}\NormalTok{, }
              \AttributeTok{color =} \StringTok{"black"}\NormalTok{, }
              \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
              \AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Five{-}kilometer time (minutes)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Marathon time (minutes)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./14-ijalm_files/figure-pdf/fig-fivekmvsmarathonses-1.pdf}

}

\caption{\label{fig-fivekmvsmarathonses}Simple linear regression with
simulated data on the time someone takes to run five kilometers and a
marathon, along with standard errors}

\end{figure}

From standard errors, we can compute a confidence interval. A 95 per
cent confidence interval is a range, such that there is roughly a 0.95
probability that the interval happens to contain the population
parameter, which is typically unknown. The lower end of this range is:
\(\hat{\beta_1} - 2 \times \mbox{SE}\left(\hat{\beta_1}\right)\) and the
upper end of this range is:
\(\hat{\beta_1} + 2 \times \mbox{SE}\left(\hat{\beta_1}\right)\).

Now that we have a range, for which we can say there is a roughly 95 per
cent probability that range contains the true population parameter, we
could test claims. For instance, we could claim that there is no
relationship between \(X\) and \(Y\), i.e.~\(\beta_1 = 0\), as an
alternative to a claim that there is some relationship between \(X\) and
\(Y\), i.e.~\(\beta_1 \neq 0\).

In the same way that in Chapter~\ref{sec-hunt-data} we needed to decide
how much evidence it would take to convince us that our tea taster could
distinguish whether milk or tea had been added first, we need to decide
whether our estimate of \(\beta_1\), which is \(\hat{\beta}_1\), is `far
enough' away from zero for us to be comfortable claiming that
\(\beta_1 \neq 0\). How far is `far enough'? If we were very confident
in our estimate of \(\beta_1\) then it would not have to be far, but if
we were not then it would have to be substantial. The standard error of
\(\hat{\beta}_1\) does an awful lot of work here in accounting for a
variety of factors, only some of which it can actually account for.

We compare this standard error with \(\hat{\beta}_1\) to get the
t-statistic: \[t = \frac{\hat{\beta}_1 - 0}{\mbox{SE}(\hat{\beta}_1)}.\]
And we then compare our t-statistic to the t-distribution to compute the
probability of getting this absolute t-statistic or a larger one, if it
was actually the case that \(\beta_1 = 0\). This probability is the
p-value. A smaller p-value means it is less likely that we would observe
our data due to chance if there was not a relationship.

\begin{quote}
Words! Mere words! How terrible they were! How clear, and vivid, and
cruel! One could not escape from them. And yet what a subtle magic there
was in them! They seemed to be able to give a plastic form to formless
things, and to have a music of their own as sweet as that of viol or of
lute. Mere words! Was there anything so real as words?

\emph{The Picture of Dorian Gray} (Wilde 1891).
\end{quote}

We will not make much use of p-values in this book because they are a
specific and subtle concept. They are difficult to understand and easy
to abuse. The main issue is that they embody, and assume correct, every
assumption of the model, including everything that went into gathering
and cleaning the data. While smaller p-values do imply the data are more
unusual if all the assumptions were correct; when we consider the full
data science workflow there are usually an awful lot of assumptions. And
we do not get guidance from p-values about the reasonableness of
specific assumptions (Greenland et al. 2016, 339). A p-value may reject
a null hypothesis because the null hypothesis is actually false, but it
may also be that some data were incorrectly gathered or prepared. We can
only be sure that the p-value speaks to the hypothesis we are interested
in testing, if all the other assumptions are correct. There is nothing
inherently wrong about using p-values, but it is important to use them
in sophisticated and thoughtful ways. D. Cox (2018) provides a lovely
discussion of what this requires.

One application where it is easy to see abuse of p-values is in power
analysis. Power, in a statistical sense, refers to probability of
rejecting a null hypothesis that is actually false. As power relates to
hypothesis testing, it also related to sample size. There is often a
worry that a study is `under-powered', meaning there was not a large
enough sample, but rarely a worry that, say, the data were
inappropriately cleaned, even though we cannot distinguish between these
based only on a p-value.

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Dr Nancy Reid is University Professor in the Department of Statistical
Sciences at the University of Toronto. After obtaining a PhD in
Statistics from Stanford University in 1979, she took a position as a
postdoctoral fellow at Imperial College London. She was then appointed
as assistant professor at the University of British Columbia in 1980,
and then moved to the University of Toronto in 1986, where she was
promoted to full professor in 1988 and served as department chair
between 1997 and 2002. Her research focuses on obtaining accurate
inference in small-sample regimes and developing inferential procedures
for complex models featuring intractable likelihoods. D. R. Cox and Reid
(1987) examines how re-parameterizing models can simplify inference,
Varin, Reid, and Firth (2011) surveys methods for approximating
intractable likelihoods, and Reid (2003) overviews inferential
procedures in the small-sample regime. Dr Reid was awarded the 1992
COPSS Presidents' Award, the 2016 Guy Medal in Silver, and the 2022 Guy
Medal in Gold from the Royal Statistical Society.
\end{tcolorbox}

\hypertarget{multiple-linear-regression}{%
\section{Multiple linear regression}\label{multiple-linear-regression}}

To this point we have just considered one explanatory variable. But we
will usually have more than one. One approach would be to run separate
regressions for each explanatory variable. But compared with separate
linear regressions for each, adding more explanatory variables allows us
to have a better understanding of the intercept and accounts for
interaction. Often the results will be quite different.

We may also like to consider explanatory variables that do not have an
inherent ordering. For instance: pregnant or not; day or night. When
there are only two options then we can use a binary variable, which is
considered either 0 or 1. If we have a column of character values that
only has two values, such as:
\texttt{c("Myles",\ "Ruth",\ "Ruth",\ "Myles",\ "Myles",\ "Ruth")}, then
using this as an explanatory variable in the usual regression set up,
would mean that it is treated as a binary variable. If there are more
than two levels then we can use a combination of binary variables, where
the `missing' outcome (baseline) gets pushed into the intercept.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_running\_data }\OtherTok{\textless{}{-}}
\NormalTok{  simulated\_running\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{was\_raining =} \FunctionTok{sample}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(}\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{),}
    \AttributeTok{size =}\NormalTok{ number\_of\_observations,}
    \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.8}\NormalTok{)}
\NormalTok{  )) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(five\_km\_time, marathon\_time, was\_raining)}

\NormalTok{simulated\_running\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 3
   five_km_time marathon_time was_raining
          <dbl>         <dbl> <chr>      
 1         20.4          152. No         
 2         16.8          134. No         
 3         22.3          198. No         
 4         19.7          166. No         
 5         15.6          163. No         
 6         21.1          168. No         
 7         17            131. No         
 8         18.6          150. No         
 9         17.4          158. No         
10         17.8          147. No         
# ... with 90 more rows
\end{verbatim}

We can add additional explanatory variables to \texttt{lm()} with
\texttt{+}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_running\_data\_rain\_model }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(marathon\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ five\_km\_time }\SpecialCharTok{+}\NormalTok{ was\_raining,}
     \AttributeTok{data =}\NormalTok{ simulated\_running\_data)}

\FunctionTok{summary}\NormalTok{(simulated\_running\_data\_rain\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = marathon_time ~ five_km_time + was_raining, data = simulated_running_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.150  -8.828   0.968  10.522  58.224 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)      9.1030     9.0101   1.010    0.315    
five_km_time     7.8660     0.4154  18.934   <2e-16 ***
was_rainingYes   4.1673     4.5048   0.925    0.357    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 16.98 on 97 degrees of freedom
Multiple R-squared:  0.7969,    Adjusted R-squared:  0.7927 
F-statistic: 190.3 on 2 and 97 DF,  p-value: < 2.2e-16
\end{verbatim}

The result probably is not too surprising if we look at a plot of the
data (Figure~\ref{fig-fivekmvsmarathonbinary}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_running\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ five\_km\_time, }\AttributeTok{y =}\NormalTok{ marathon\_time, }\AttributeTok{color =}\NormalTok{ was\_raining)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
              \AttributeTok{color =} \StringTok{"black"}\NormalTok{, }
              \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{,}
              \AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Five{-}kilometer time (minutes)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Marathon time (minutes)"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Was raining"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./14-ijalm_files/figure-pdf/fig-fivekmvsmarathonbinary-1.pdf}

}

\caption{\label{fig-fivekmvsmarathonbinary}Simple linear regression with
simulated data on the time someone takes to run five kilometers and a
marathon, with a binary variable for whether it was raining}

\end{figure}

In addition to wanting to include additional explanatory variables, we
may think that they are related with each another. For instance, if we
were wanting to explain the amount of snowfall, then we may be
interested in the humidity and the temperature, but those two variables
may also interact. We can do this by using \texttt{*} instead of
\texttt{+} when we specify the model. When we interact variables in this
way, then we almost always need to include the individual variables as
well and \texttt{lm()} will do this by default.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_running\_data }\OtherTok{\textless{}{-}}
\NormalTok{  simulated\_running\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{humidity =} \FunctionTok{sample}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(}\StringTok{"High"}\NormalTok{, }\StringTok{"Low"}\NormalTok{),}
    \AttributeTok{size =}\NormalTok{ number\_of\_observations,}
    \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.8}\NormalTok{)}
\NormalTok{  ))}

\NormalTok{simulated\_running\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 4
   five_km_time marathon_time was_raining humidity
          <dbl>         <dbl> <chr>       <chr>   
 1         20.4          152. No          Low     
 2         16.8          134. No          Low     
 3         22.3          198. No          Low     
 4         19.7          166. No          Low     
 5         15.6          163. No          Low     
 6         21.1          168. No          Low     
 7         17            131. No          Low     
 8         18.6          150. No          Low     
 9         17.4          158. No          High    
10         17.8          147. No          Low     
# ... with 90 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_running\_data\_rain\_and\_humidity\_model }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(marathon\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ five\_km\_time }\SpecialCharTok{+}\NormalTok{ was\_raining}\SpecialCharTok{*}\NormalTok{humidity,}
     \AttributeTok{data =}\NormalTok{ simulated\_running\_data)}

\FunctionTok{summary}\NormalTok{(simulated\_running\_data\_rain\_and\_humidity\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = marathon_time ~ five_km_time + was_raining * humidity, 
    data = simulated_running_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-48.904  -8.523   0.404  10.130  59.951 

Coefficients:
                           Estimate Std. Error t value Pr(>|t|)    
(Intercept)                 15.0595    10.3144   1.460    0.148    
five_km_time                 7.7313     0.4167  18.552   <2e-16 ***
was_rainingYes              15.6008     9.6199   1.622    0.108    
humidityLow                 -3.7380     4.9569  -0.754    0.453    
was_rainingYes:humidityLow -14.5825    10.7410  -1.358    0.178    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 16.79 on 95 degrees of freedom
Multiple R-squared:  0.8054,    Adjusted R-squared:  0.7972 
F-statistic: 98.31 on 4 and 95 DF,  p-value: < 2.2e-16
\end{verbatim}

There are a variety of threats to the validity of linear regression
estimates, and aspects to think about. We need to address these when we
use it, and usually four graphs and associated text are sufficient to
assuage most of these. Aspects of concern include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linearity of explanatory variables. We are concerned with whether the
  independent variables enter in a linear way. Sometimes if we are
  worried that there might be a multiplicative relationship between the
  explanatory variables, rather than an additive one, then we may
  consider a logarithmic transform. We can usually be convinced there is
  enough linearity in our explanatory variables for our purposes by
  using graphs of the variables.
\item
  Independence of errors. We are concerned that the errors are not
  correlated. For instance, if we are interested in weather-related
  measurement such as average daily temperature, then we may find a
  pattern because the temperature on one day is likely similar to the
  temperature on another. We can be convinced that we have satisfied
  this condition by making graphs of the errors, such as
  \textbf{?@fig-fivekmvsmarathonresids}).
\item
  Homoscedasticity of errors. We are concerned that the errors are not
  becoming systematically larger or smaller throughout the sample. If
  that is happening, then we term it heteroscedasticity. Again, graphs
  of errors, such as \textbf{?@fig-fivekmvsmarathonresids}) are used to
  convince us of this.
\item
  Normality of errors. We are concerned that our errors are normally
  distributed when we are interested in making individual-level
  predictions.
\item
  Outliers and other high-impact observations. Finally, we might be
  worried that our results are being driven by a handful of
  observations. For instance, thinking back to Chapter
  @ref(static-communication) and Anscombe's Quartet, we notice that
  linear regression estimates would be heavily influenced by the
  inclusion of one or two particular points. We can become comfortable
  with this by considering our analysis on various sub-sets
\end{enumerate}

Those aspects are statistical concerns and relate to whether the model
is working. The most important threat to validity and hence the aspect
that must be addressed at some length, is speaking to the fact that this
model is appropriate to the circumstances and addresses the research
question at hand.

To this point, we have just had a quick look at the regression results
using \texttt{summary()}. A better approach is to use
\texttt{modelsummary()} from \texttt{modelsummary} (Arel-Bundock 2021a)
(Table~\ref{tbl-modelsummaryruntimes}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(modelsummary)}

\FunctionTok{modelsummary}\NormalTok{(}\FunctionTok{list}\NormalTok{(simulated\_running\_data\_first\_model,}
\NormalTok{                  simulated\_running\_data\_rain\_model, }
\NormalTok{                  simulated\_running\_data\_rain\_and\_humidity\_model),}
             \AttributeTok{fmt =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-modelsummaryruntimes}{}
\begin{table}
\caption{\label{tbl-modelsummaryruntimes}Explaining marathon time based on five-kilometer run times and weather features }

\centering
\begin{tabular}[t]{lccc}
\toprule
  & Model 1 & Model 2 & Model 3\\
\midrule
(Intercept) & \num{8.24} & \num{9.10} & \num{15.06}\\
 & (\num{8.96}) & (\num{9.01}) & (\num{10.31})\\
five\_km\_time & \num{7.94} & \num{7.87} & \num{7.73}\\
 & (\num{0.41}) & (\num{0.42}) & (\num{0.42})\\
was\_rainingYes &  & \num{4.17} & \num{15.60}\\
 &  & (\num{4.50}) & (\num{9.62})\\
humidityLow &  &  & \num{-3.74}\\
 &  &  & (\num{4.96})\\
was\_rainingYes × humidityLow &  &  & \num{-14.58}\\
 &  &  & (\num{10.74})\\
\midrule
Num.Obs. & \num{100} & \num{100} & \num{100}\\
R2 & \num{0.795} & \num{0.797} & \num{0.805}\\
R2 Adj. & \num{0.793} & \num{0.793} & \num{0.797}\\
AIC & \num{854.0} & \num{855.1} & \num{854.8}\\
BIC & \num{861.8} & \num{865.5} & \num{870.4}\\
Log.Lik. & \num{-423.993} & \num{-423.554} & \num{-421.405}\\
F & \num{380.262} & \num{190.279} & \num{98.314}\\
\bottomrule
\end{tabular}
\end{table}

When we are focused on prediction, we will often want to fit many
models. One way to do this is to copy and paste code many times. There
is nothing wrong with that. And that is the way that most people get
started. But we need an approach that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  scales more easily;
\item
  enables us to think carefully about over-fitting; and
\item
  adds model evaluation.
\end{enumerate}

The use of \texttt{tidymodels} (Kuhn and Wickham 2020) satisfies these
criteria by providing a coherent grammar that allows us to easily fit a
variety of models. Like \texttt{tidyverse}, it is a package of packages.

As we are focused on prediction, we are worried about over-fitting our
data, which would limit our ability to make claims about other datasets.
One way to partially address this is to split our dataset into training
and test datasets using \texttt{initial\_split()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidymodels)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{simulated\_running\_data\_split }\OtherTok{\textless{}{-}} 
  \FunctionTok{initial\_split}\NormalTok{(}\AttributeTok{data =}\NormalTok{ simulated\_running\_data, }
                \AttributeTok{prop =} \FloatTok{0.80}\NormalTok{)}

\NormalTok{simulated\_running\_data\_split}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<Analysis/Assess/Total>
<80/20/100>
\end{verbatim}

Having split the data, we then create the training and test datasets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_running\_data\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(simulated\_running\_data\_split)}

\NormalTok{simulated\_running\_data\_train}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 80 x 4
   five_km_time marathon_time was_raining humidity
          <dbl>         <dbl> <chr>       <chr>   
 1         17.4          158. No          High    
 2         23.8          205. No          High    
 3         23.4          198. Yes         Low     
 4         22.3          175  No          Low     
 5         19.3          158  No          Low     
 6         24.4          204. No          High    
 7         17            120  No          High    
 8         19.1          178. No          Low     
 9         22.3          198. No          Low     
10         20.6          166. No          Low     
# ... with 70 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_running\_data\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(simulated\_running\_data\_split)}

\NormalTok{simulated\_running\_data\_test}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 20 x 4
   five_km_time marathon_time was_raining humidity
          <dbl>         <dbl> <chr>       <chr>   
 1         17            131. No          Low     
 2         16.6          118. No          Low     
 3         19.6          164. No          Low     
 4         21.1          164. Yes         Low     
 5         21            180. No          Low     
 6         27.9          246. No          Low     
 7         23.7          198. No          High    
 8         16            143  No          Low     
 9         24.9          202. No          Low     
10         15.2          140. Yes         Low     
11         28.9          238. No          Low     
12         19.2          132  Yes         Low     
13         22            200. No          Low     
14         26.5          229  Yes         High    
15         25.3          222  Yes         High    
16         25.9          208. Yes         Low     
17         15.5          120  No          Low     
18         18            144. No          Low     
19         27.2          227. No          High    
20         20.8          201. No          Low     
\end{verbatim}

When we look at the training and test datasets, we can see that we have
placed most of our dataset into the training dataset. We will use that
to estimate the parameters of our model. We have kept a small amount of
it back, and we will use that to evaluate our model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_running\_data\_first\_model\_tidymodels }\OtherTok{\textless{}{-}} 
  \FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_engine}\NormalTok{(}\AttributeTok{engine =} \StringTok{"lm"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{fit}\NormalTok{(marathon\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ five\_km\_time }\SpecialCharTok{+}\NormalTok{ was\_raining, }
      \AttributeTok{data =}\NormalTok{ simulated\_running\_data\_train}
\NormalTok{      )}

\NormalTok{simulated\_running\_data\_first\_model\_tidymodels}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
parsnip model object

Fit time:  2ms 

Call:
stats::lm(formula = marathon_time ~ five_km_time + was_raining, 
    data = data)

Coefficients:
   (Intercept)    five_km_time  was_rainingYes  
        16.601           7.490           8.244  
\end{verbatim}

We will use \texttt{tidymodels} for forecasting. But when we are focused
on inference, instead, we will use Bayesian approaches. To do this we
use the probabilistic programming language `Stan', and interface with it
using \texttt{rstanarm} (Goodrich et al. 2020). We keep these separate,
rather than adapting Bayesian approaches within \texttt{tidymodels},
because to this point the ecosystems have developed separately, and so
the best books to go onto next are also separate.

In order to use Bayesian approaches we will need to specify a starting
point, or prior. This is another reason for the workflow advocated in
this book; the simulate stage leads directly to priors. We will also
more thoroughly specify the model that we are interested in:

\[
\begin{aligned}
y_i &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \beta_0 +\beta_1x_i\\
\beta_0 &\sim \mbox{Normal}(0, 3) \\
\beta_1 &\sim \mbox{Normal}(0, 3) \\
\sigma &\sim \mbox{Normal}(0, 3) \\
\end{aligned}
\]

On a practical note, one aspect that different between Bayesian
approaches and the way we have been doing modelling to this point, is
that Bayesian models will usually take longer to run. Because of this,
it can be useful to run the model, either within the R Markdown document
or in a separate R script, and then save it with \texttt{saveRDS()}.
With sensible R Markdown chunk options, the model can then be read into
the R Markdown document with \texttt{readRDS()}. In this way, the model,
and hence delay, is only imposed once for a given model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rstanarm)}

\NormalTok{simulated\_running\_data\_first\_model\_rstanarm }\OtherTok{\textless{}{-}}
  \FunctionTok{stan\_lm}\NormalTok{(}
\NormalTok{    marathon\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ five\_km\_time }\SpecialCharTok{+}\NormalTok{ was\_raining, }
    \AttributeTok{data =}\NormalTok{ simulated\_running\_data,}
    \AttributeTok{prior =} \ConstantTok{NULL}\NormalTok{,}
    \AttributeTok{seed =} \DecValTok{853}
\NormalTok{  )}

\CommentTok{\# simulated\_running\_data\_first\_model\_rstanarm \textless{}{-}}
\CommentTok{\#   stan\_lm(}
\CommentTok{\#     formula = marathon\_time \textasciitilde{} five\_km\_time,}
\CommentTok{\#     data = simulated\_running\_data,}
\CommentTok{\#     prior = normal(0, 3),}
\CommentTok{\#     prior\_intercept = normal(0, 3),}
\CommentTok{\#     prior\_aux = normal(0, 3),}
\CommentTok{\#     seed = 853}
\CommentTok{\#     )}

\FunctionTok{saveRDS}\NormalTok{(simulated\_running\_data\_first\_model\_rstanarm,}
        \AttributeTok{file =} \StringTok{"simulated\_running\_data\_first\_model\_rstanarm.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_running\_data\_first\_model\_rstanarm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
stan_lm
 family:       gaussian [identity]
 formula:      marathon_time ~ five_km_time + was_raining
 observations: 100
 predictors:   3
------
               Median MAD_SD
(Intercept)    9.2    8.7   
five_km_time   7.9    0.4   
was_rainingYes 4.6    4.5   

Auxiliary parameter(s):
              Median MAD_SD
R2             0.8    0.0  
log-fit_ratio  0.0    0.0  
sigma         17.1    1.3  

------
* For help interpreting the printed output see ?print.stanreg
* For info on the priors used see ?prior_summary.stanreg
\end{verbatim}

\hypertarget{logistic-regression}{%
\section{Logistic regression}\label{logistic-regression}}

Linear regression is a nice way to come to understand better our data.
But it assumes a continuous outcome variable which can take any number
on the real line. We would like some way to use this same machinery when
we cannot satisfy this condition. We turn to logistic and Poisson
regression for binary and count outcome variables, respectively.

Logistic regression and its close variants are useful in a variety of
settings, from elections (W. Wang et al. 2015) through to horse racing
(Chellel 2018; Bolton and Chapman 1986). We use logistic regression when
the dependent variable is a binary outcome, such as 0 or 1. Although the
presence of a binary outcome variable may sound limiting, there are a
lot of circumstances in which the outcome either naturally falls into
this situation, or can be adjusted into it.

The reason that we use logistic regression is that we will be modelling
a probability and so it will be bounded between 0 and 1. Whereas with
linear regression we may end up with values outside this. This all said,
logistic regression, as Daniella Witten teaches us, is just a linear
model. The foundation of logistic regression is the logit function:

\[
\mbox{logit}(x) = \log\left(\frac{x}{1-x}\right),
\] which will transpose values between 0 and 1, onto the real line. For
instance, \texttt{logit(0.1)\ =\ -2.2}, \texttt{logit(0.5)\ =\ 0}, and
\texttt{logit(0.9)\ =\ 2.2}.

We will simulate data on whether it is day or night, based on the number
of cars that we can see.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{day\_or\_night }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{number\_of\_cars =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{min =} \DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{),}
    \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{),}
    \AttributeTok{is\_night =} \FunctionTok{if\_else}\NormalTok{(number\_of\_cars }\SpecialCharTok{+}\NormalTok{ noise }\SpecialCharTok{\textgreater{}} \DecValTok{50}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{number\_of\_cars =} \FunctionTok{round}\NormalTok{(number\_of\_cars)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{noise)}
  
\NormalTok{day\_or\_night}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 2
   number_of_cars is_night
            <dbl>    <dbl>
 1             36        0
 2             12        0
 3             48        0
 4             32        0
 5              4        0
 6             40        0
 7             13        0
 8             24        0
 9             16        0
10             19        0
# ... with 990 more rows
\end{verbatim}

As with linear regression, logistic regression with can use
\texttt{glm()} from base to put together a quick model and
\texttt{summary()} to look at it. In this case we will try to work out
whether it is day or night, based on the number of cars we can see. We
are interested in estimating Equation @ref(eq:logisticexample): \[
\mbox{Pr}(y_i=1) = \mbox{logit}^{-1}\left(\beta_0+\beta_1 x_i\right). (\#eq:logisticexample)
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{day\_or\_night\_model }\OtherTok{\textless{}{-}} 
  \FunctionTok{glm}\NormalTok{(is\_night }\SpecialCharTok{\textasciitilde{}}\NormalTok{ number\_of\_cars,}
      \AttributeTok{data =}\NormalTok{ day\_or\_night,}
      \AttributeTok{family =} \StringTok{\textquotesingle{}binomial\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(day\_or\_night\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
glm(formula = is_night ~ number_of_cars, family = "binomial", 
    data = day_or_night)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.39419  -0.00002   0.00000   0.00002   2.33776  

Coefficients:
               Estimate Std. Error z value Pr(>|z|)    
(Intercept)    -45.5353     7.3389  -6.205 5.48e-10 ***
number_of_cars   0.9121     0.1470   6.205 5.47e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1386.194  on 999  degrees of freedom
Residual deviance:   77.243  on 998  degrees of freedom
AIC: 81.243

Number of Fisher Scoring iterations: 11
\end{verbatim}

One reason that logistic regression can be a bit tricky initially, is
because the coefficients take a bit of work to interpret. In particular,
our estimate on likelihood of it being night is 0.91 This is the odds.
So, the odds that it is night, increase by 0.91 as the number of cars
that we saw increases. We can translate the result into probabilities
using \texttt{augment()} from \texttt{broom} (D. Robinson, Hayes, and
Couch 2021) and this allows us to graph the results
(Figure~\ref{fig-dayornightprobs})).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}

\NormalTok{day\_or\_night }\OtherTok{\textless{}{-}}
  \FunctionTok{augment}\NormalTok{(day\_or\_night\_model,}
          \AttributeTok{data =}\NormalTok{ day\_or\_night,}
          \AttributeTok{type.predict =} \StringTok{"response"}\NormalTok{)}

\NormalTok{day\_or\_night}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 8
   number_of_cars is_night  .fitted   .resid .std.resid     .hat .sigma  .cooksd
            <dbl>    <dbl>    <dbl>    <dbl>      <dbl>    <dbl>  <dbl>    <dbl>
 1             36        0 3.06e- 6 -2.47e-3   -2.47e-3 1.30e- 5  0.278 1.98e-11
 2             12        0 2.22e-16 -2.11e-8   -2.11e-8 6.91e-15  0.278 7.67e-31
 3             48        0 1.48e- 1 -5.65e-1   -5.71e-1 2.04e- 2  0.278 1.84e- 3
 4             32        0 7.95e- 8 -3.99e-4   -3.99e-4 5.57e- 7  0.278 2.21e-14
 5              4        0 2.22e-16 -2.11e-8   -2.11e-8 1.01e-14  0.278 1.12e-30
 6             40        0 1.17e- 4 -1.53e-2   -1.53e-2 2.58e- 4  0.278 1.51e- 8
 7             13        0 2.22e-16 -2.11e-8   -2.11e-8 6.55e-15  0.278 7.27e-31
 8             24        0 5.39e-11 -1.04e-5   -1.04e-5 7.85e-10  0.278 2.11e-20
 9             16        0 2.22e-16 -2.11e-8   -2.11e-8 5.53e-15  0.278 6.14e-31
10             19        0 5.63e-13 -1.06e-6   -1.06e-6 1.17e-11  0.278 3.29e-24
# ... with 990 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{day\_or\_night }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{is\_night =} \FunctionTok{factor}\NormalTok{(is\_night)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ number\_of\_cars,}
             \AttributeTok{y =}\NormalTok{ .fitted,}
             \AttributeTok{color =}\NormalTok{ is\_night)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{height =} \FloatTok{0.01}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of cars that were seen"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Estimated probability it is night"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Was actually night"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./14-ijalm_files/figure-pdf/fig-dayornightprobs-1.pdf}

}

\caption{\label{fig-dayornightprobs}Logistic regression probability
results with simulated data of whether it is day or night based on the
number of cars that are around}

\end{figure}

We can use \texttt{tidymodels} to run this if we wanted. In order to do
that, we first need to change the class of our dependent variable into a
factor.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{day\_or\_night }\OtherTok{\textless{}{-}}
\NormalTok{  day\_or\_night }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{is\_night =} \FunctionTok{as\_factor}\NormalTok{(is\_night))}

\NormalTok{day\_or\_night\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(day\_or\_night, }\AttributeTok{prop =} \FloatTok{0.80}\NormalTok{)}
\NormalTok{day\_or\_night\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(day\_or\_night\_split)}
\NormalTok{day\_or\_night\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(day\_or\_night\_split)}

\NormalTok{day\_or\_night\_tidymodels }\OtherTok{\textless{}{-}}
  \FunctionTok{logistic\_reg}\NormalTok{(}\AttributeTok{mode =} \StringTok{"classification"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glm"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit}\NormalTok{(is\_night }\SpecialCharTok{\textasciitilde{}}\NormalTok{ number\_of\_cars,}
      \AttributeTok{data =}\NormalTok{ day\_or\_night\_train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{day\_or\_night\_tidymodels}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
parsnip model object

Fit time:  4ms 

Call:  stats::glm(formula = is_night ~ number_of_cars, family = stats::binomial, 
    data = data)

Coefficients:
   (Intercept)  number_of_cars  
      -44.4817          0.8937  

Degrees of Freedom: 799 Total (i.e. Null);  798 Residual
Null Deviance:      1109 
Residual Deviance: 62.5     AIC: 66.5
\end{verbatim}

As before, we can make a graph of the actual results compared with our
estimates. But one nice aspect of this is that we could use our test
dataset to more thoroughly evaluate our model's forecasting ability, for
instance through a confusion matrix. We find that the model does well on
the held-out dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{day\_or\_night\_tidymodels }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{predict}\NormalTok{(}\AttributeTok{new\_data =}\NormalTok{ day\_or\_night\_test) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{cbind}\NormalTok{(day\_or\_night\_test) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{conf\_mat}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ is\_night, }\AttributeTok{estimate =}\NormalTok{ .pred\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          Truth
Prediction   0   1
         0  95   0
         1   3 102
\end{verbatim}

Finally, we might be interested in inference, and so want to build a
Bayesian model using \texttt{rstanarm}. Again, we will more fully
specify our model:

Finally, we can build a Bayesian model and estimate it with
\texttt{rstanarm}.

\[
\begin{aligned}
\mbox{Pr}(y_i=1) & = \mbox{logit}^{-1}\left(\beta_0+\beta_1 x_i\right)\\
\beta_0 & \sim \mbox{Normal}(0, 3)\\
\beta_1 & \sim \mbox{Normal}(0, 3)
\end{aligned}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{day\_or\_night\_rstanarm }\OtherTok{\textless{}{-}}
  \FunctionTok{stan\_glm}\NormalTok{(}
\NormalTok{    is\_night }\SpecialCharTok{\textasciitilde{}}\NormalTok{ number\_of\_cars,}
    \AttributeTok{data =}\NormalTok{ day\_or\_night,}
    \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
    \AttributeTok{prior =} \FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{),}
    \AttributeTok{prior\_intercept =} \FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{),}
    \AttributeTok{seed =} \DecValTok{853}
\NormalTok{  )}

\FunctionTok{saveRDS}\NormalTok{(day\_or\_night\_rstanarm,}
        \AttributeTok{file =} \StringTok{"day\_or\_night\_rstanarm.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{day\_or\_night\_rstanarm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
stan_glm
 family:       binomial [logit]
 formula:      is_night ~ number_of_cars
 observations: 1000
 predictors:   2
------
               Median MAD_SD
(Intercept)    -47.3    8.0 
number_of_cars   0.9    0.2 

------
* For help interpreting the printed output see ?print.stanreg
* For info on the priors used see ?prior_summary.stanreg
\end{verbatim}

\hypertarget{poisson-regression}{%
\section{Poisson regression}\label{poisson-regression}}

When we have count data, we should initially think to use Poisson
distribution. The Poisson distribution has the interesting feature that
the mean is also the variance, and so as the mean increases, so does the
variance. As such, the Poisson distribution is governed by the
parameter, \(\lambda\) and it distributes probabilities over the
non-negative integers. The Poisson distribution is (Pitman 1993, 121):

\[P_{\lambda}(k) = e^{-\lambda}\mu^k/k!\mbox{, for }k=0,1,2,...\] We can
simulate \(n=20\) draws from the Poisson distribution with
\texttt{rpois()}, where \(\lambda\) is both the mean and the variance.
The \(\lambda\) parameter governs the shape of the distribution
(\textbf{?@fig-poissondistributiontakingshape}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =} \DecValTok{20}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 1 5 5 4 5 2 1 4 5 4 6 2 3 4 4 6 5 1 3 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_each }\OtherTok{\textless{}{-}} \DecValTok{1000}

\FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{lambda =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, number\_of\_each),}
    \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, number\_of\_each),}
    \FunctionTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{, number\_of\_each),}
    \FunctionTok{rep}\NormalTok{(}\DecValTok{4}\NormalTok{, number\_of\_each),}
    \FunctionTok{rep}\NormalTok{(}\DecValTok{7}\NormalTok{, number\_of\_each),}
    \FunctionTok{rep}\NormalTok{(}\DecValTok{10}\NormalTok{, number\_of\_each),}
    \FunctionTok{rep}\NormalTok{(}\DecValTok{15}\NormalTok{, number\_of\_each),}
    \FunctionTok{rep}\NormalTok{(}\DecValTok{50}\NormalTok{, number\_of\_each),}
    \FunctionTok{rep}\NormalTok{(}\DecValTok{100}\NormalTok{, number\_of\_each)}
\NormalTok{  ),}
  \AttributeTok{draw =} \FunctionTok{c}\NormalTok{(}
    \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{0}\NormalTok{),}
    \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{1}\NormalTok{),}
    \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{2}\NormalTok{),}
    \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{4}\NormalTok{),}
    \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{7}\NormalTok{),}
    \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{10}\NormalTok{),}
    \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{15}\NormalTok{),}
    \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{50}\NormalTok{),}
    \FunctionTok{rpois}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_each, }\AttributeTok{lambda =} \DecValTok{100}\NormalTok{)}
\NormalTok{  )}
\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ draw)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(lambda),}
             \AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}Integer\textquotesingle{}}\NormalTok{,}
       \AttributeTok{y =} \StringTok{\textquotesingle{}Density\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./14-ijalm_files/figure-pdf/unnamed-chunk-63-1.pdf}

}

\caption{The Poisson distribution is governed by the value of the mean,
which is the same as its variance}

\end{figure}

For instance, if we look at the number of A+ grades that are awarded in
each university course in a given term then for each course we would
have a count.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{count\_of\_A\_plus }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \CommentTok{\# Thanks to Chris DuBois: https://stackoverflow.com/a/1439843}
    \AttributeTok{department =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep.int}\NormalTok{(}\StringTok{"1"}\NormalTok{, }\DecValTok{26}\NormalTok{), }\FunctionTok{rep.int}\NormalTok{(}\StringTok{"2"}\NormalTok{, }\DecValTok{26}\NormalTok{), }\FunctionTok{rep.int}\NormalTok{(}\StringTok{"3"}\NormalTok{, }\DecValTok{26}\NormalTok{)),}
    \AttributeTok{course =} \FunctionTok{c}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"DEP\_1\_"}\NormalTok{, letters), }\FunctionTok{paste0}\NormalTok{(}\StringTok{"DEP\_2\_"}\NormalTok{, letters), }\FunctionTok{paste0}\NormalTok{(}\StringTok{"DEP\_3\_"}\NormalTok{, letters)),}
    \AttributeTok{number\_of\_A\_plus =} \FunctionTok{c}\NormalTok{(}
      \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{),}
             \AttributeTok{size =} \DecValTok{26}\NormalTok{,}
             \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{),}
      \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{),}
             \AttributeTok{size =} \DecValTok{26}\NormalTok{,}
             \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{),}
      \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{25}\NormalTok{),}
             \AttributeTok{size =} \DecValTok{26}\NormalTok{,}
             \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    )}
\NormalTok{  )}

\NormalTok{count\_of\_A\_plus}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 78 x 3
   department course  number_of_A_plus
   <chr>      <chr>              <int>
 1 1          DEP_1_a                9
 2 1          DEP_1_b               10
 3 1          DEP_1_c                1
 4 1          DEP_1_d                5
 5 1          DEP_1_e                2
 6 1          DEP_1_f                4
 7 1          DEP_1_g                3
 8 1          DEP_1_h                3
 9 1          DEP_1_i                1
10 1          DEP_1_j                3
# ... with 68 more rows
\end{verbatim}

Our simulated dataset has the number of A+ grades awarded by courses,
which are structured within departments. We can use \texttt{glm()} and
\texttt{summary()} from base to quickly get a sense of the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grades\_model\_base }\OtherTok{\textless{}{-}} 
  \FunctionTok{glm}\NormalTok{(number\_of\_A\_plus }\SpecialCharTok{\textasciitilde{}}\NormalTok{ department, }
    \AttributeTok{data =}\NormalTok{ count\_of\_A\_plus, }
    \AttributeTok{family =} \StringTok{\textquotesingle{}poisson\textquotesingle{}}\NormalTok{)}

\FunctionTok{summary}\NormalTok{(grades\_model\_base)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
glm(formula = number_of_A_plus ~ department, family = "poisson", 
    data = count_of_A_plus)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-6.739  -1.210  -0.171   1.424   3.952  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  1.44238    0.09535  15.128   <2e-16 ***
department2  1.85345    0.10254  18.075   <2e-16 ***
department3  1.00663    0.11141   9.035   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 952.12  on 77  degrees of freedom
Residual deviance: 450.08  on 75  degrees of freedom
AIC: 768.21

Number of Fisher Scoring iterations: 5
\end{verbatim}

The interpretation of the coefficient on `department2' is that it is the
log of the expected difference between departments. So we expect
\(\exp(1.85345) \approx 6.3\) and \(\exp(1.00663) \approx 2.7\)
additional A+ grades in departments 2 and 3, compared with department 1.

We can use \texttt{tidymodels} to estimate Poisson regression models
with \texttt{poissonreg} (Kuhn 2021).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(poissonreg)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{count\_of\_A\_plus\_split }\OtherTok{\textless{}{-}}
\NormalTok{  rsample}\SpecialCharTok{::}\FunctionTok{initial\_split}\NormalTok{(count\_of\_A\_plus, }\AttributeTok{prop =} \FloatTok{0.80}\NormalTok{)}
\NormalTok{count\_of\_A\_plus\_train }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{training}\NormalTok{(count\_of\_A\_plus\_split)}
\NormalTok{count\_of\_A\_plus\_test }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{testing}\NormalTok{(count\_of\_A\_plus\_split)}

\NormalTok{a\_plus\_model\_tidymodels }\OtherTok{\textless{}{-}}
  \FunctionTok{poisson\_reg}\NormalTok{(}\AttributeTok{mode =} \StringTok{"regression"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glm"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit}\NormalTok{(number\_of\_A\_plus }\SpecialCharTok{\textasciitilde{}}\NormalTok{ department,}
      \AttributeTok{data =}\NormalTok{ count\_of\_A\_plus\_train)}

\NormalTok{a\_plus\_model\_tidymodels}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
parsnip model object

Fit time:  4ms 

Call:  stats::glm(formula = number_of_A_plus ~ department, family = stats::poisson, 
    data = data)

Coefficients:
(Intercept)  department2  department3  
      1.470        1.925        1.011  

Degrees of Freedom: 61 Total (i.e. Null);  59 Residual
Null Deviance:      758 
Residual Deviance: 276.8    AIC: 534.8
\end{verbatim}

And finally, we can build a Bayesian model and estimate it with
\texttt{rstanarm}. We put a tight prior on the coefficients because of
the propensity for the Poisson distribution to expand them
substantially.

\[
\begin{aligned}
y_i &\sim \mbox{Poisson}(\lambda_i)\\
\log(\lambda_i) & = \beta_0 + \beta_1 x_1 + \beta_2 x_2\\
\beta_0 & \sim \mbox{Normal}(0, 0.5)\\
\beta_1 & \sim \mbox{Normal}(0, 0.5)\\
\beta_2 & \sim \mbox{Normal}(0, 0.5)
\end{aligned}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{count\_of\_A\_plus\_rstanarm }\OtherTok{\textless{}{-}}
  \FunctionTok{stan\_glm}\NormalTok{(}
\NormalTok{    number\_of\_A\_plus }\SpecialCharTok{\textasciitilde{}}\NormalTok{ department,}
    \AttributeTok{data =}\NormalTok{ count\_of\_A\_plus,}
    \AttributeTok{family =} \FunctionTok{poisson}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{),}
    \AttributeTok{prior =} \FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
    \AttributeTok{prior\_intercept =} \FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{),}
    \AttributeTok{seed =} \DecValTok{853}
\NormalTok{  )}

\FunctionTok{saveRDS}\NormalTok{(count\_of\_A\_plus\_rstanarm,}
        \AttributeTok{file =} \StringTok{"count\_of\_A\_plus\_rstanarm.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{count\_of\_A\_plus\_rstanarm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
stan_glm
 family:       poisson [log]
 formula:      number_of_A_plus ~ department
 observations: 78
 predictors:   3
------
            Median MAD_SD
(Intercept) 1.5    0.1   
department2 1.8    0.1   
department3 0.9    0.1   

------
* For help interpreting the printed output see ?print.stanreg
* For info on the priors used see ?prior_summary.stanreg
\end{verbatim}

\hypertarget{exercises-and-tutorial-13}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-13}}

\hypertarget{exercises-13}{%
\subsection{Exercises}\label{exercises-13}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Please write a linear relationship between some response variable, Y,
  and some predictor, X. What is the intercept term? What is the slope
  term? What would adding a hat to these indicate?
\item
  What is the least squares criterion? Similarly, what is RSS and what
  are we trying to do when we run least squares regression?
\item
  What is statistical bias?
\item
  If there were three variables: Snow, Temperature, and Wind, please
  write R code that would fit a simple linear regression to explain Snow
  as a function of Temperature and Wind. What do you think about another
  explanatory variable - daily stock market returns - to your model?
\item
  According to Greenland et al. (2016), p-values test (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    All the assumptions about how the data were generated (the entire
    model), not just the targeted hypothesis it is supposed to test
    (such as a null hypothesis).
  \item
    Whether the hypothesis targeted for testing is true or not.
  \item
    A dichotomy whereby results can be declared `statistically
    significant'.
  \end{enumerate}
\item
  According to Greenland et al. (2016), a p-value may be small because
  (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The targeted hypothesis is false.
  \item
    The study protocols were violated.
  \item
    It was selected for presentation based on its small size.
  \end{enumerate}
\item
  According to Obermeyer et al. (2019), why does racial bias occur in an
  algorithm used to guide health decisions in the US (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The algorithm uses health costs as a proxy for health needs.
  \item
    The algorithm was trained on Reddit data.
  \end{enumerate}
\item
  When should we use logistic regression (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Continuous dependent variable.
  \item
    Binary dependent variable.
  \item
    Count dependent variable.
  \end{enumerate}
\item
  We are interested in studying how voting intentions in the recent US
  presidential election vary by an individual's income. We set up a
  logistic regression model to study this relationship. In this study,
  one possible dependent variable would be (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Whether the respondent is a US citizen (yes/no)
  \item
    The respondent's personal income (high/low)
  \item
    Whether the respondent is going to vote for Trump (yes/no)
  \item
    Who the respondent voted for in 2016 (Trump/Clinton)
  \end{enumerate}
\item
  We are interested in studying how voting intentions in the recent US
  presidential election vary by an individual's income. We set up a
  logistic regression model to study this relationship. In this study,
  one possible dependent variable would be (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The race of the respondent (white/not white)
  \item
    The respondent's marital status (married/not)
  \item
    Whether the respondent is registered to vote (yes/no)
  \item
    Whether the respondent is going to vote for Biden (yes/no)
  \end{enumerate}
\item
  Please explain what a p-value is, using only the term itself
  (i.e.~`p-value') and words that are amongst the 1,000 most common in
  the English language according to the XKCD Simple Writer -
  https://xkcd.com/simplewriter/. (Please write one or two paragraphs.)
\item
  The mean of a Poisson distribution is equal to its?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Median.
  \item
    Standard deviation.
  \item
    Variance.
  \end{enumerate}
\item
  What is power (in a statistical context)?
\item
  According to McElreath (2020, 162) `Regression will not sort it out.
  Regression is indeed an oracle, but a cruel one. It speaks in riddles
  and delights in punishing us for\ldots{}' (please select one answer)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    overcomplicating models.
  \item
    asking bad questions.
  \item
    using bad data.
  \end{enumerate}
\item
  Is a model that fits the small or large world more important to you,
  and why?
\end{enumerate}

\hypertarget{tutorial-13}{%
\subsection{Tutorial}\label{tutorial-13}}

Simulate some data that are similar to those discussed by Gould (2013).
Then build a regression model. Discuss your results

\hypertarget{sec-causality-from-observational-data}{%
\chapter{Causality from observational
data}\label{sec-causality-from-observational-data}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{BNT162b2 mRNA Covid-19 Vaccine in a Nationwide Mass
  Vaccination Setting}, (Dagan et al. 2021)
\item
  Read \emph{The Effect: An Introduction to Research Design and
  Causality}, Chapters 18 `Difference-in-Differences', 19 `Instrumental
  Variables', and 20 `Regression Discontinuity', (Huntington-Klein 2021)
\item
  Read \emph{Understanding regression discontinuity designs as
  observational studies}, (Sekhon and Titiunik 2017)
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Being able to put together DAGs.
\item
  Essential matching methods and the weaknesses of matching.
\item
  Implementing difference in differences.
\item
  Identifying opportunities for instrumental variables and implementing
  it.
\item
  Challenges to the validity of instrumental variables.
\item
  Reading in foreign data.
\item
  Understanding regression discontinuity and implementing it both
  manually and using packages.
\item
  Appreciating the threats to the validity of regression discontinuity.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{broom} (D. Robinson, Hayes, and Couch 2021)
\item
  \texttt{DiagrammeR} (Iannone 2020)
\item
  \texttt{estimatr} (G. Blair et al. 2021)
\item
  \texttt{haven} (Wickham and Miller 2020)
\item
  \texttt{MatchIt} (Ho et al. 2011)
\item
  \texttt{modelsummary} (Arel-Bundock 2021a)
\item
  \texttt{rdrobust} (Calonico et al. 2021)
\item
  \texttt{scales} (Wickham and Seidel 2020)
\item
  \texttt{tidyverse} (Wickham 2017)
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{DiagrammeR::grViz()}
\item
  \texttt{estimatr::iv\_robust()}
\item
  \texttt{haven::read\_dta()}
\item
  \texttt{MatchIt::matchit()}
\item
  \texttt{modelsummary::datasummary\_skim()}
\item
  \texttt{modelsummary::modelsummary()}
\item
  \texttt{poly()}
\item
  \texttt{rdrobust::rdrobust()}
\item
  \texttt{scales::dollar\_format()}
\end{itemize}

\hypertarget{introduction-12}{%
\section{Introduction}\label{introduction-12}}

Life is grand when we can conduct experiments to be able to speak to
causality. But there are circumstances in which we cannot run an
experiment, but nonetheless want to be able to make causal claims. And
data from outside experiments have value that experiments do not have.
In this chapter we discuss the circumstances and methods that allow us
to speak to causality using observational data. We use relatively simple
methods, in sophisticated ways, drawing from statistics, but also a
variety of social sciences, including economics, and political science,
as well as epidemiology.

For instance, Dagan et al. (2021) use observational data to confirm the
effectiveness of the Pfizer-BioNTech vaccine. They discuss how one
concern with using observational data in this way is confounding, which
is where we are concerned that there is some variable that affects both
the explanatory and dependent variables and can lead to spurious
relationships. Dagan et al. (2021) adjust for this by first making a
list of potential confounders, such as age, sex, geographic location,
healthcare usage and then adjusting for each of them, by matching,
one-to-one between people that were vaccinated and those that were not.
The experimental data guided the use of observational data, and the
larger size of the later enabled a focus on specific age-groups and
extent of disease.

Using observational data in sophisticated ways is what this chapter is
about. How we can nonetheless be comfortable making causal statements,
even when we cannot run A/B tests or RCTs. Indeed, in what circumstances
may we prefer to not run those or to run observational-based approaches
in addition to them. We cover three of the major methods: difference in
differences; regression discontinuity; and instrumental variables.

\hypertarget{directed-acyclic-graphs}{%
\section{Directed acyclic graphs}\label{directed-acyclic-graphs}}

When we are discussing causality, it can help to be very specific about
what we mean. It is easy to get caught up in observational data and
trick ourselves. It is important to think hard, and to use all the tools
available to us. For instance, in that earlier example, Dagan et al.
(2021) were able to use experimental data as a guide. Most of the time,
we will not be so lucky as to have both experimental data and
observational data available to us. But one framework that can help with
thinking hard about our data is the use of directed acyclic graph (DAG).
DAGs are a fancy name for a flow diagram and involves drawing arrows and
lines between the variables to indicate the relationship between them.
Following Igelström (2020) we use \texttt{DiagrammeR} (Iannone 2020) to
build them here, because we can use the same skills outside of just DAGs
and \texttt{DiagrammeR} provides quite a lot of control
(Figure~\ref{fig-firstdag})). But \texttt{ggdag} is also useful (Barrett
2021b).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DiagrammeR)}

\NormalTok{DiagrammeR}\SpecialCharTok{::}\FunctionTok{grViz}\NormalTok{(}\StringTok{"}
\StringTok{digraph \{}
\StringTok{  graph [ranksep = 0.2]}
\StringTok{  node [shape = plaintext, fontsize = 10, fontname = Helvetica]}
\StringTok{    x}
\StringTok{    y}
\StringTok{  edge [minlen = 2, arrowhead = vee]}
\StringTok{    x{-}\textgreater{}y}
\StringTok{  \{ rank = same; x; y \}}
\StringTok{\}}
\StringTok{"}\NormalTok{, }\AttributeTok{height =} \DecValTok{200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{./15-causality_from_obs_files/figure-pdf/fig-firstdag-1.pdf}

}

\caption{\label{fig-firstdag}Using a DAG to illustrate perceived
relationships}

\end{figure}

In Figure~\ref{fig-firstdag}, we think that \emph{x} causes \emph{y}. We
could build another DAG where the situation is less clear. To make the
examples a little easier to follow, we will switch to fruits
(Figure~\ref{fig-carrotasconfounder}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DiagrammeR}\SpecialCharTok{::}\FunctionTok{grViz}\NormalTok{(}\StringTok{"}
\StringTok{digraph \{}
\StringTok{  graph [ranksep = 0.2]}
\StringTok{  node [shape = plaintext, fontsize = 10, fontname = Helvetica]}
\StringTok{    Apple}
\StringTok{    Banana}
\StringTok{    Carrot}
\StringTok{  edge [minlen = 2, arrowhead = vee]}
\StringTok{    Apple{-}\textgreater{}Banana}
\StringTok{    Carrot{-}\textgreater{}Apple}
\StringTok{    Carrot{-}\textgreater{}Banana}
\StringTok{  \{ rank = same; Apple; Banana \}}
\StringTok{\}}
\StringTok{"}\NormalTok{, }\AttributeTok{height =} \DecValTok{300}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{./15-causality_from_obs_files/figure-pdf/fig-carrotasconfounder-1.pdf}

}

\caption{\label{fig-carrotasconfounder}A DAG showing Carrot as a
confounder}

\end{figure}

In Figure~\ref{fig-carrotasconfounder}, we think \emph{Apple} causes
\emph{Banana}. But we also think that \emph{Carrot} causes
\emph{Banana}, and that \emph{Carrot} also causes \emph{Apple}. That
relationship is a `backdoor path', and would create spurious correlation
in our analysis. We may think that changes in \emph{Apple} are causing
changes in \emph{Banana}, but it could be that \emph{Carrot} is changing
them both. That variable, in this case, \emph{Carrot}, is called a
`confounder'.

Hernan and Robins (2020, 83) discuss an interesting case where a
researcher was interested in whether one person looking up at the sky
makes others look up at the sky also. There was a clear relationship
between the responses of both people. But it was also the case that
there was noise in the sky. So, it was unclear whether the second person
looked up because the first person looked up, or they both looked up
because of the noise. When using experimental data, randomization allows
us to avoid this concern, but with observational data we cannot rely on
that. It is also not the case that bigger data necessarily get around
this problem for us. Instead, it is important to think carefully about
the situation.

If there are confounders, but we are still interested in causal effects,
then we need to adjust for them. One way is to include them in the
regression. But the validity of this requires several assumptions. In
particular, Gelman and Hill (2007, 169) warn that our estimate will only
correspond to the average causal effect in the sample if we include all
of the confounders and have the right model. Putting the second
requirement to one side, and focusing only on the first, if we do not
think about and observe a confounder, then it can be difficult to adjust
for it. And this is an area where both domain expertise and theory can
bring a lot to an analysis.

In Figure~\ref{fig-carrotasmediator} we have a similar situation where
again, we may think that \emph{Apple} causes \emph{Banana}. But in
Figure~\ref{fig-carrotasmediator} \emph{Apple} also causes
\emph{Carrot}, which itself causes \emph{Banana}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DiagrammeR}\SpecialCharTok{::}\FunctionTok{grViz}\NormalTok{(}\StringTok{"}
\StringTok{digraph \{}
\StringTok{  graph [ranksep = 0.2]}
\StringTok{  node [shape = plaintext, fontsize = 10, fontname = Helvetica]}
\StringTok{    Apple}
\StringTok{    Banana}
\StringTok{    Carrot}
\StringTok{  edge [minlen = 2, arrowhead = vee]}
\StringTok{    Apple{-}\textgreater{}Banana}
\StringTok{    Apple{-}\textgreater{}Carrot}
\StringTok{    Carrot{-}\textgreater{}Banana}
\StringTok{  \{ rank = same; Apple; Banana \}}
\StringTok{\}}
\StringTok{"}\NormalTok{, }\AttributeTok{height =} \DecValTok{300}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{./15-causality_from_obs_files/figure-pdf/fig-carrotasmediator-1.pdf}

}

\caption{\label{fig-carrotasmediator}A DAG showing Carrot as a mediator}

\end{figure}

In Figure~\ref{fig-carrotasmediator}, \emph{Carrot} is called a
`mediator' and we would not adjust for it if we were interested in the
effect of \emph{Apple} on \emph{Banana}. If we were to adjust for it,
then some of what we are attributing to \emph{Apple}, would be due to
\emph{Carrot}.

Finally, in Figure~\ref{fig-carrotascollider} we have yet another
similar situation, where we again, think that \emph{Apple} causes
\emph{Banana}. But this time both \emph{Apple} and \emph{Banana} also
cause \emph{Carrot}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DiagrammeR}\SpecialCharTok{::}\FunctionTok{grViz}\NormalTok{(}\StringTok{"}
\StringTok{digraph \{}
\StringTok{  graph [ranksep = 0.2]}
\StringTok{  node [shape = plaintext, fontsize = 10, fontname = Helvetica]}
\StringTok{    Apple}
\StringTok{    Banana}
\StringTok{    Carrot}
\StringTok{  edge [minlen = 2, arrowhead = vee]}
\StringTok{    Apple{-}\textgreater{}Banana}
\StringTok{    Apple{-}\textgreater{}Carrot}
\StringTok{    Banana{-}\textgreater{}Carrot}
\StringTok{  \{ rank = same; Apple; Banana \}}
\StringTok{\}}
\StringTok{"}\NormalTok{, }\AttributeTok{height =} \DecValTok{300}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{./15-causality_from_obs_files/figure-pdf/fig-carrotascollider-1.pdf}

}

\caption{\label{fig-carrotascollider}A DAG showing Carrot as a collider}

\end{figure}

In this case, \emph{Carrot} is called a `collider' and if we were to
condition on it, then we would create a misleading relationship.

It is important to be clear about this: we must create the DAG
ourselves, in the same way that we must put together the model
ourselves. There is nothing that will create it for us. This means that
we need to think carefully about the situation. Because it is one thing
to see something in the DAG and then do something about it. But it is
another to not even know that it is there. McElreath (2020, 180)
describes these as haunted DAGs. DAGs are helpful, but they are just a
tool to help us think deeply about our situation.

\hypertarget{two-common-paradoxes}{%
\section{Two common paradoxes}\label{two-common-paradoxes}}

\hypertarget{simpsons-paradox}{%
\subsection{Simpson's paradox}\label{simpsons-paradox}}

There are two situations where data can trick us that are so common that
we will explicitly go through them. These are: 1) Simpson's paradox, and
2) Berkson's paradox. It is important to keep these situations in mind,
and the use of DAGs can help identify them.

Simpson's paradox occurs when we estimate some relationship for subsets
of our data, but a different relationship when we consider the entire
dataset (Simpson 1951). It is a particular case of the ecological
fallacy, which is when we try to make claims about individuals, based on
their group. For instance, it may be that there is a positive
relationship between undergraduate grades and performance in graduate
school in two departments when considering each department individually.
But if undergraduate grades tended to be higher in one department than
another while graduate school performance tended to be opposite, we may
find a negative relationship between undergraduate grades and
performance in graduate school. We can simulate some data to show this
more clearly (Figure~\ref{fig-simpsonsparadox}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_in\_each }\OtherTok{\textless{}{-}} \DecValTok{1000}

\NormalTok{department\_one }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{undergrad =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_in\_each, }\AttributeTok{min =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{max =} \FloatTok{0.9}\NormalTok{),}
    \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_in\_each, }\DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.1}\NormalTok{),}
    \AttributeTok{grad =}\NormalTok{ undergrad }\SpecialCharTok{+}\NormalTok{ noise,}
    \AttributeTok{type =} \StringTok{"Department 1"}
\NormalTok{  )}

\NormalTok{department\_two }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{undergrad =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_in\_each, }\AttributeTok{min =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{max =} \FloatTok{0.8}\NormalTok{),}
    \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_in\_each, }\DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.1}\NormalTok{),}
    \AttributeTok{grad =}\NormalTok{ undergrad }\SpecialCharTok{+}\NormalTok{ noise }\SpecialCharTok{+} \FloatTok{0.3}\NormalTok{,}
    \AttributeTok{type =} \StringTok{"Department 2"}
\NormalTok{  )}

\NormalTok{both\_departments }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(department\_one, department\_two)}

\NormalTok{both\_departments}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2,000 x 4
   undergrad   noise  grad type        
       <dbl>   <dbl> <dbl> <chr>       
 1     0.772 -0.0566 0.715 Department 1
 2     0.724 -0.0312 0.693 Department 1
 3     0.797  0.0770 0.874 Department 1
 4     0.763 -0.0664 0.697 Department 1
 5     0.707  0.0717 0.779 Department 1
 6     0.781 -0.0165 0.764 Department 1
 7     0.726 -0.104  0.623 Department 1
 8     0.749  0.0527 0.801 Department 1
 9     0.732 -0.0471 0.684 Department 1
10     0.738  0.0552 0.793 Department 1
# ... with 1,990 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{both\_departments }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ undergrad, }\AttributeTok{y =}\NormalTok{ grad)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ type), }\AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ type), }\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{,}
              \AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{,}
              \AttributeTok{color =} \StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Undergraduate results"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Graduate results"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Department"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./15-causality_from_obs_files/figure-pdf/fig-simpsonsparadox-1.pdf}

}

\caption{\label{fig-simpsonsparadox}Illustration of simulated data that
shows Simpson's paradox}

\end{figure}

Simpson's paradox is often illustrated using real-world data from
University of California, Berkeley, on graduate admissions (Bickel,
Hammel, and O'Connell 1975). Bickel, Hammel, and O'Connell (1975)
include what might be one of the greatest sub-titles ever published:
`Measuring bias is harder than is usually assumed, and the evidence is
sometimes contrary to expectation'. More recently, as shown in its
documentation, the `penguins' dataset from \texttt{parlmerpenguins}
(Horst, Hill, and Gorman 2020) provides an example of Simpson's paradox,
using real-world data (Figure~\ref{fig-simpsonsparadoxinpenguins}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(palmerpenguins)}

\NormalTok{penguins }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ body\_mass\_g, }\AttributeTok{y =}\NormalTok{ bill\_depth\_mm)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ species), }\AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ species), }\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{,}
              \AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{,}
              \AttributeTok{color =} \StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Body mass (grams)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Bill depth (millimeters)"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Species"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./15-causality_from_obs_files/figure-pdf/fig-simpsonsparadoxinpenguins-1.pdf}

}

\caption{\label{fig-simpsonsparadoxinpenguins}Illustration of Simpson's
paradox in a dataset of penguin bill depth compared with their body
mass}

\end{figure}

\hypertarget{berksons-paradox}{%
\subsection{Berkson's paradox}\label{berksons-paradox}}

Berkson's paradox occurs when we estimate some relationship based on the
dataset that we have. But because the dataset is so selected, the
relationship is different in a more general dataset (Berkson 1946). For
instance, if we have a dataset of professional cyclists then we might
find there is no relationship between their VO2 max and their chance of
winning a bike race. But if we had a dataset of the general population
then we might find a relationship between these two variables. The
professional dataset has just been so selected that the relationship
disappears; one cannot become a professional cyclist unless one has a
good-enough VO2 max. Again, we can simulate some data to show this more
clearly (Figure~\ref{fig-berksonsparadox}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_pros }\OtherTok{\textless{}{-}} \DecValTok{100}

\NormalTok{number\_of\_public }\OtherTok{\textless{}{-}} \DecValTok{1000}

\NormalTok{professionals }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{VO2 =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_pros, }\AttributeTok{min =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{max =} \FloatTok{0.9}\NormalTok{),}
    \AttributeTok{chance\_of\_winning =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_pros, }\AttributeTok{min =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{max =} \FloatTok{0.9}\NormalTok{),}
    \AttributeTok{type =} \StringTok{"Professionals"}
\NormalTok{  )}

\NormalTok{general\_public }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{VO2 =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_public, }\AttributeTok{min =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{max =} \FloatTok{0.8}\NormalTok{),}
    \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_public, }\DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.03}\NormalTok{),}
    \AttributeTok{chance\_of\_winning =}\NormalTok{ VO2 }\SpecialCharTok{+}\NormalTok{ noise }\SpecialCharTok{+} \FloatTok{0.1}\NormalTok{,}
    \AttributeTok{type =} \StringTok{"Public"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{noise)}

\NormalTok{professionals\_and\_public }\OtherTok{=} \FunctionTok{rbind}\NormalTok{(professionals, general\_public)}

\NormalTok{professionals\_and\_public}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,100 x 3
     VO2 chance_of_winning type         
   <dbl>             <dbl> <chr>        
 1 0.772             0.734 Professionals
 2 0.724             0.773 Professionals
 3 0.797             0.772 Professionals
 4 0.763             0.754 Professionals
 5 0.707             0.843 Professionals
 6 0.781             0.740 Professionals
 7 0.726             0.803 Professionals
 8 0.749             0.750 Professionals
 9 0.732             0.890 Professionals
10 0.738             0.821 Professionals
# ... with 1,090 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{professionals\_and\_public }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ VO2, }\AttributeTok{y =}\NormalTok{ chance\_of\_winning)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ type), }\AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ type), }\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{, }\AttributeTok{color =} \StringTok{\textquotesingle{}black\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"VO2 max"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Chance of winning a bike race"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Type"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./15-causality_from_obs_files/figure-pdf/fig-berksonsparadox-1.pdf}

}

\caption{\label{fig-berksonsparadox}Illustration of simulated data that
shows Berkson's paradox}

\end{figure}

\hypertarget{difference-in-differences}{%
\section{Difference in differences}\label{difference-in-differences}}

\hypertarget{overview}{%
\subsection{Overview}\label{overview}}

The ideal situation of being able to conduct an experiment is rarely
possible. Can we reasonably expect that Netflix would allow us to change
prices? And even if they did once, would they let us do it again, and
again, and again? Further, rarely can we explicitly create treatment and
control groups. Finally, experiments can be expensive or unethical.
Instead, we need to make do with what we have. Rather than our
counterfactual coming to us through randomization, and hence us knowing
that the two are the same but for the treatment, we try to identify
groups that were similar but for the treatment, and hence any
differences can be attributed to the treatment.

With observational data, sometimes there are differences between our two
groups before we treat. Provided those pre-treatment differences satisfy
assumptions that essentially amount to the differences being both
consistent, and that we expect that consistency to continue in the
absence of the treatment---the `parallel trends' assumption---then we
can look to any difference in the differences as the effect of the
treatment. One of the aspects of difference in differences analysis is
that we can do it using relatively straight-forward methods. Linear
regression with a binary variable is enough to get started and do a
convincing job.

Consider wanting to know the effect of a new tennis racket on serve
speed. One way to test this would be to measure the difference between,
say, Roger Federer's serve speed without the tennis racket and the serve
speed of an enthusiastic amateur, let us call them Ville, with the
tennis racket. Yes, we would find a difference, but would we know how
much to attribute to the tennis racket? Another way would be to consider
the difference between Ville's serve speed without the new tennis racket
and Ville's serve speed with the new tennis racket. But what if serves
were just getting faster naturally over time? Instead, we combine the
two approaches to look at the difference in the differences.

We begin by measuring Federer's serve speed and compare it to Ville's
serve speed, both without the new racket. We then measure Federer's
serve speed again, and measure Ville's serve speed with the new racket.
That difference in the differences would then be the estimate of the
effect of the new racket. There are a few key assumptions that we need
to make for this analysis to be appropriate:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Is there something else that may have affected only Ville, and not
  Federer that could affect Ville's serve speed?
\item
  Is it likely that Federer and Ville have the same trajectory of serve
  speed improvement? This is the `parallel trends' assumption, and it
  dominates many discussions of difference in differences analysis.
\item
  Finally, is it likely that the variance of our serve speeds of Federer
  and Ville are the same?
\end{enumerate}

Despite these requirements, difference in differences is a powerful
approach because we do not need the treatment and control group to be
the same before the treatment. We just need to have a good idea of how
they differed.

\hypertarget{simulated-example}{%
\subsection{Simulated example}\label{simulated-example}}

To be more specific about the situation, we simulate data. We will
simulate a situation in which there is initially a difference of one
between the serve speeds of the different people, and then after a new
tennis racket, there is a difference of six. We can use a graph to
illustrate the situation (Figure~\ref{fig-diffindifftennisracket}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{simulated\_difference\_in\_differences }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{person =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{), }\AttributeTok{times =} \DecValTok{2}\NormalTok{),}
    \AttributeTok{time =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{times =} \DecValTok{1000}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{times =} \DecValTok{1000}\NormalTok{)),}
    \AttributeTok{treatment\_group =} \FunctionTok{rep}\NormalTok{(}
      \FunctionTok{sample}\NormalTok{(}
        \AttributeTok{x =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{,}
        \AttributeTok{size  =} \DecValTok{1000}\NormalTok{,}
        \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{        ), }
      \AttributeTok{times =} \DecValTok{2}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{treatment\_group =} \FunctionTok{as.factor}\NormalTok{(treatment\_group),}
         \AttributeTok{time =} \FunctionTok{as.factor}\NormalTok{(time)}
\NormalTok{  )}


\NormalTok{simulated\_difference\_in\_differences }\OtherTok{\textless{}{-}}
\NormalTok{  simulated\_difference\_in\_differences }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{serve\_speed =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      time }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ treatment\_group }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{5}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
\NormalTok{      time }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ treatment\_group }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{6}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
\NormalTok{      time }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ treatment\_group }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{8}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
\NormalTok{      time }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ treatment\_group }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{14}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{    )}
\NormalTok{  )}

\NormalTok{simulated\_difference\_in\_differences}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2,000 x 4
# Rowwise: 
   person time  treatment_group serve_speed
    <int> <fct> <fct>                 <dbl>
 1      1 0     0                      4.43
 2      2 0     1                      6.96
 3      3 0     1                      7.77
 4      4 0     0                      5.31
 5      5 0     0                      4.09
 6      6 0     0                      4.85
 7      7 0     0                      6.43
 8      8 0     0                      5.77
 9      9 0     1                      6.13
10     10 0     1                      7.32
# ... with 1,990 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated\_difference\_in\_differences }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ time,}
             \AttributeTok{y =}\NormalTok{ serve\_speed,}
             \AttributeTok{color =}\NormalTok{ treatment\_group)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{group =}\NormalTok{ person), }\AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Time period"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Serve speed"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Person got a new racket"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./15-causality_from_obs_files/figure-pdf/fig-diffindifftennisracket-1.pdf}

}

\caption{\label{fig-diffindifftennisracket}Illustration of simulated
data that shows a difference before and after getting a new tennis
racket}

\end{figure}

As it is a straight-forward example, we can obtain our estimate
manually, by looking at the average difference of the differences. When
we do that, we find that we estimate the effect of the new tennis racket
to be 5.06, which is similar to what we simulated.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{average\_differences }\OtherTok{\textless{}{-}}
\NormalTok{  simulated\_difference\_in\_differences }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ time,}
              \AttributeTok{values\_from =}\NormalTok{ serve\_speed,}
              \AttributeTok{names\_prefix =} \StringTok{"time\_"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{difference =}\NormalTok{ time\_1 }\SpecialCharTok{{-}}\NormalTok{ time\_0) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(treatment\_group) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{average\_difference =} \FunctionTok{mean}\NormalTok{(difference))}

\NormalTok{average\_differences}\SpecialCharTok{$}\NormalTok{average\_difference[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ average\_differences}\SpecialCharTok{$}\NormalTok{average\_difference[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5.058414
\end{verbatim}

And we can use linear regression to get the same result. The equation
that we are interested in is:
\[Y_{i,t} = \beta_0 + \beta_1\mbox{Treatment binary}_i + \beta_2\mbox{Time binary}_t + \beta_3(\mbox{Treatment binary} \times\mbox{Time binary})_{i,t} + \epsilon_{i,t}\]

While we should include the separate aspects as well, it is the estimate
of the interaction that we are interested in. In this case it is
\(\beta_3\). And we find that our estimated effect is 5.06.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diff\_in\_diff\_example\_regression }\OtherTok{\textless{}{-}} 
  \FunctionTok{lm}\NormalTok{(serve\_speed }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment\_group}\SpecialCharTok{*}\NormalTok{time, }
     \AttributeTok{data =}\NormalTok{ simulated\_difference\_in\_differences)}

\FunctionTok{summary}\NormalTok{(diff\_in\_diff\_example\_regression)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = serve_speed ~ treatment_group * time, data = simulated_difference_in_differences)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.1415 -0.6638 -0.0039  0.6708  3.2664 

Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
(Intercept)             4.97131    0.04281  116.12   <2e-16 ***
treatment_group1        3.03350    0.06225   48.73   <2e-16 ***
time1                   1.00680    0.06055   16.63   <2e-16 ***
treatment_group1:time1  5.05841    0.08803   57.46   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9828 on 1996 degrees of freedom
Multiple R-squared:  0.9268,    Adjusted R-squared:  0.9266 
F-statistic:  8418 on 3 and 1996 DF,  p-value: < 2.2e-16
\end{verbatim}

\hypertarget{assumptions}{%
\subsection{Assumptions}\label{assumptions}}

If we want to use difference in differences, then we need to satisfy the
assumptions. There were three that were touched on earlier, but here we
will focus on one: the `parallel trends' assumption. The parallel trends
assumption haunts everything to do with difference in differences
analysis because we can never prove it, we can just be convinced of it,
and try to convince others.

To see why we can never prove it, consider an example in which we want
to know the effect of a new stadium on a professional sports team's
wins/loses. To do this we consider two teams: the Golden State Warriors
and the Toronto Raptors. The Warriors changed stadiums at the start of
the 2019-20 season, while the Raptors did not, so we will consider four
time periods: the 2016-17 season, 2017-18 season, 2018-19 season, and
finally we will compare the performance with the one after they moved,
so the 2019-20 season. The Raptors here act as our counterfactual. This
means that we assume the relationship between the Warriors and the
Raptors, in the absence of a new stadium, would have continued to change
in a consistent way. But the fundamental problem of causal inference
means that we can never know that for certain. We must present
sufficient evidence to assuage any concerns that a reader may have.

There are four main threats to validity when we use difference in
differences, and we need to address all of them (Cunningham 2021,
272--77):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Non-parallel trends. The treatment and control groups may be based on
  differences. As such it can be difficult to convincingly argue for
  parallel trends. In this case, maybe try to find another factor to
  consider in your model that may adjust for some of that. This may
  require difference in difference in differences (in the earlier
  example, we could perhaps add the San Francisco 49ers as they are in
  the same broad geographic area as the Warriors). Or maybe re-think the
  analysis to see if we can make a different control group. Adding
  additional earlier time periods may help but may introduce more
  issues, which we touch on in the third point.
\item
  Compositional differences. This is a concern when working with
  repeated cross-sections. What if the composition of those
  cross-sections change? For instance, if we are working at an app that
  is rapidly growing, and we want to look at the effect of some change.
  In our initial cross-section, we may have mostly young people, but in
  a subsequent cross-section, we may have more older people as the
  demographics of the app usage change. Hence our results may just be an
  age-effect, not an effect of the change that we are interested in.
\item
  Long-term effects compared with reliability. As we discussed in
  Chapter~\ref{sec-hunt-data}, there is a trade-off between the length
  of the analysis that we run. As we run the analysis for longer there
  is more opportunity for other factors to affect the results. There is
  also increased chance for someone who was not treated to be treated.
  But, on the other hand, it can be difficult to convincingly argue that
  short-term results will continue in the long-term.
\item
  Functional form dependence. This is less of an issue when the outcomes
  are similar, but if they are different then functional form may be
  responsible for some aspects of the results.
\end{enumerate}

\hypertarget{case-study-french-newspaper-prices-between-1960-and-1974}{%
\subsection{Case study: French newspaper prices between 1960 and
1974}\label{case-study-french-newspaper-prices-between-1960-and-1974}}

In this case study we introduce Angelucci and Cagé (2019), and replicate
its main findings.

The business model of newspapers was challenged by the internet and many
local newspapers have closed. And this issue is not new. When television
was introduced, there were similar concerns. Angelucci and Cagé (2019)
use the introduction of television advertising in France, announced in
1967, to examine the effect of decreased advertising revenue on
newspapers. They create a dataset of French newspapers from 1960 to 1974
and then use difference in differences to examine the effect of the
reduction in advertising revenues on newspapers' content and prices. The
change that they focus on is the introduction of television advertising,
which they argue affected national newspapers more than local
newspapers. They find that this change results in both less
journalism-content in the newspapers and lower newspaper prices.
Focusing on this change, and analyzing it using difference in
differences, is important because it allows us to disentangle a few
competing effects. For instance, did newspapers become redundant because
they could no longer charge high prices for their advertisements, or
because consumers preferred to get their news from the television?

We can get free access to the
\href{https://www.openicpsr.org/openicpsr/project/116438/version/V1/view}{data}
that underpins Angelucci and Cagé (2019) after registration. The dataset
is in the Stata data format, `dta', which we can read with
\texttt{read\_dta()} from \texttt{haven} (Wickham and Miller 2020). The
file that we are interested in is
`Angelucci\_Cage\_AEJMicro\_dataset.dta', which is the `dta' folder.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)}

\NormalTok{newspapers }\OtherTok{\textless{}{-}} \FunctionTok{read\_dta}\NormalTok{(}\StringTok{"Angelucci\_Cage\_AEJMicro\_dataset.dta"}\NormalTok{)}

\NormalTok{newspapers}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,196 x 52
    year id_news local national after_national   Had po_cst ps_cst etotal_cst
   <dbl>   <dbl> <dbl>    <dbl>          <dbl> <dbl>  <dbl>  <dbl>      <dbl>
 1  1960       1     1        0              0     0   2.60   2.29  109455728
 2  1961       1     1        0              0     0   2.51   2.20  118256840
 3  1962       1     1        0              0     0   2.39   2.13  108848768
 4  1963       1     1        0              0     0   2.74   2.43  160962528
 5  1964       1     1        0              0     0   2.65   2.35  173738208
 6  1965       1     1        0              0     0   2.59   2.29  177229600
 7  1966       1     1        0              0     0   2.52   2.31  211282496
 8  1967       1     1        0              0     0   3.27   2.88  212639760
 9  1968       1     1        0              0     0   3.91   3.45  209852864
10  1969       1     1        0              0     0   3.67   3.28  239392080
# ... with 1,186 more rows, and 43 more variables: ra_cst <dbl>, ra_s <dbl>,
#   rs_cst <dbl>, rtotal_cst <dbl>, profit_cst <dbl>, nb_journ <dbl>,
#   qs_s <dbl>, qtotal <dbl>, pages <dbl>, ads_q <dbl>, ads_s <dbl>,
#   news_hole <dbl>, share_Hard <dbl>, ads_p4_cst <dbl>,
#   R_sh_edu_primaire_ipo <dbl>, R_sh_edu_secondaire_ipo <dbl>,
#   R_sh_edu_no_ipo <dbl>, R_sh_pcs_agri_ipo <dbl>, R_sh_pcs_patron_ipo <dbl>,
#   R_sh_pcs_cadre_ipo <dbl>, R_sh_pcs_employes_ipo <dbl>, ...
\end{verbatim}

There are 1,196 observations in the dataset and 52 variables. Angelucci
and Cagé (2019) are interested in the 1960-1974 time-period which has
around 100 newspapers. There are 14 national newspapers at the beginning
of the period and 12 at the end. The key period is 1967, when the French
government announced it would allow advertising on television. Angelucci
and Cagé (2019) argue that national newspapers were affected by this
chance, but local newspapers were not. So, the national newspapers are
the treatment group and the local newspapers are the control group.

We focus just on the headline difference in differences result and
construct summary statistics
(Table~\ref{tbl-frenchnewspaperssummarystatistics}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newspapers }\OtherTok{\textless{}{-}} 
\NormalTok{  newspapers }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(year, id\_news, after\_national, local, national, }\CommentTok{\# Diff in diff variables}
\NormalTok{         ra\_cst, ads\_p4\_cst, ads\_s, }\CommentTok{\# Advertising side dependents}
\NormalTok{         ps\_cst, po\_cst, qtotal, qs\_s, rs\_cst) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Reader side dependents}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ra\_cst\_div\_qtotal =}\NormalTok{ ra\_cst }\SpecialCharTok{/}\NormalTok{ qtotal) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# An advertising side dependent needs to be built}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(id\_news, after\_national, local, national), as.factor)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year =} \FunctionTok{as.integer}\NormalTok{(year))}

\NormalTok{newspapers}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,196 x 14
    year id_news after_national local national    ra_cst ads_p4_cst ads_s ps_cst
   <int> <fct>   <fct>          <fct> <fct>        <dbl>      <dbl> <dbl>  <dbl>
 1  1960 1       0              1     0         52890272       NA    30.6   2.29
 2  1961 1       0              1     0         56601060       NA    38.4   2.20
 3  1962 1       0              1     0         64840752       24.0  31.6   2.13
 4  1963 1       0              1     0         70582944       50.3  27.2   2.43
 5  1964 1       0              1     0         74977888       48.6  31.1   2.35
 6  1965 1       0              1     0         74438248       47.5  47.8   2.29
 7  1966 1       0              1     0         81383000       46.2  29.7   2.31
 8  1967 1       0              1     0         80263152       87.9  49.4   2.88
 9  1968 1       0              1     0         87165704       84.1  26.9   3.45
10  1969 1       0              1     0        102596384       79.0  31.7   3.28
# ... with 1,186 more rows, and 5 more variables: po_cst <dbl>, qtotal <dbl>,
#   qs_s <dbl>, rs_cst <dbl>, ra_cst_div_qtotal <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(modelsummary)}

\FunctionTok{datasummary\_skim}\NormalTok{(newspapers)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-frenchnewspaperssummarystatistics}{}
\begin{table}
\caption{\label{tbl-frenchnewspaperssummarystatistics}Summary statistics for French newspapers dataset (1960-1974) }

\centering
\begin{tabular}[t]{lrrrrrrr>{}r}
\toprule
  & Unique (\#) & Missing (\%) & Mean & SD & Min & Median & Max &   \\
\midrule
year & 15 & 0 & \num{1967.0} & \num{4.3} & \num{1960.0} & \num{1967.0} & \num{1974.0} & \includegraphics[width=0.67in, height=0.17in]{15-causality_from_obs_files/figure-latex//hist_f9495a9d1757.pdf}\\
ra\_cst & 1053 & 12 & \num{91531796.9} & \num{137207312.4} & \num{549717.2} & \num{35994710.0} & \num{864369088.0} & \includegraphics[width=0.67in, height=0.17in]{15-causality_from_obs_files/figure-latex//hist_f94963d1fa2e.pdf}\\
ads\_p4\_cst & 558 & 32 & \num{86.4} & \num{75.3} & \num{3.8} & \num{69.0} & \num{327.2} & \includegraphics[width=0.67in, height=0.17in]{15-causality_from_obs_files/figure-latex//hist_f9491af04199.pdf}\\
ads\_s & 988 & 13 & \num{18.7} & \num{9.7} & \num{1.6} & \num{16.9} & \num{59.6} & \includegraphics[width=0.67in, height=0.17in]{15-causality_from_obs_files/figure-latex//hist_f949347b8951.pdf}\\
ps\_cst & 665 & 13 & \num{2.8} & \num{0.7} & \num{0.7} & \num{2.8} & \num{5.6} & \includegraphics[width=0.67in, height=0.17in]{15-causality_from_obs_files/figure-latex//hist_f9494605e71f.pdf}\\
po\_cst & 146 & 11 & \num{3.2} & \num{0.9} & \num{0.8} & \num{3.3} & \num{9.3} & \includegraphics[width=0.67in, height=0.17in]{15-causality_from_obs_files/figure-latex//hist_f9496f0b892f.pdf}\\
qtotal & 1052 & 11 & \num{130817.5} & \num{172954.3} & \num{1480.0} & \num{56775.2} & \num{1143676.0} & \includegraphics[width=0.67in, height=0.17in]{15-causality_from_obs_files/figure-latex//hist_f94952d616d0.pdf}\\
qs\_s & 914 & 10 & \num{27.2} & \num{22.7} & \num{0.7} & \num{22.5} & \num{100.1} & \includegraphics[width=0.67in, height=0.17in]{15-causality_from_obs_files/figure-latex//hist_f949197c00bd.pdf}\\
rs\_cst & 1047 & 13 & \num{97666503.6} & \num{125257120.3} & \num{255760.1} & \num{40736368.0} & \num{750715008.0} & \includegraphics[width=0.67in, height=0.17in]{15-causality_from_obs_files/figure-latex//hist_f94973fab12.pdf}\\
ra\_cst\_div\_qtotal & 1049 & 12 & \num{661.2} & \num{352.7} & \num{61.3} & \num{596.6} & \num{3048.4} & \includegraphics[width=0.67in, height=0.17in]{15-causality_from_obs_files/figure-latex//hist_f94961b2371c.pdf}\\
\bottomrule
\end{tabular}
\end{table}

We are interested in what happened from 1967 onward, especially in terms
of advertising revenue, and whether that was different for national,
compared with local newspapers
(Figure~\ref{fig-frenchnewspapersrevenue}). We use \texttt{scales} to
adjust the y axis (Wickham and Seidel 2020).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(scales)}
\NormalTok{newspapers }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \FunctionTok{if\_else}\NormalTok{(local }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\StringTok{"Local"}\NormalTok{, }\StringTok{"National"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ ra\_cst)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{dollar\_format}\NormalTok{(}\AttributeTok{prefix=}\StringTok{"$"}\NormalTok{, }\AttributeTok{suffix =} \StringTok{"M"}\NormalTok{, }\AttributeTok{scale =} \FloatTok{0.000001}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Advertising revenue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type),}
               \AttributeTok{nrow =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FloatTok{1966.5}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./15-causality_from_obs_files/figure-pdf/fig-frenchnewspapersrevenue-1.pdf}

}

\caption{\label{fig-frenchnewspapersrevenue}Revenue of French newspapers
(1960-1974), by whether they were local or national}

\end{figure}

The model that we are interested in estimating is:
\[\mbox{ln}(y_{n,t}) = \beta_0 + \beta_1(\mbox{National binary}\times\mbox{1967 onward binary}) + \lambda_n + \gamma_y + \epsilon.\]
It is the \(\beta_1\) coefficient that we are especially interested in.
We use \(\lambda_n\) as fixed effect for each newspaper, and the
\(\gamma_y\) as a fixed effect for each year. We estimate the models
using \texttt{lm()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Advertising side}
\NormalTok{ad\_revenue }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(ra\_cst) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{ad\_revenue\_div\_circulation }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(ra\_cst\_div\_qtotal) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{ad\_price }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(ads\_p4\_cst) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{ad\_space }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(ads\_s) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}

\CommentTok{\# Consumer side}
\NormalTok{subscription\_price }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(ps\_cst) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{unit\_price }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(po\_cst) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{circulation }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(qtotal) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{share\_of\_sub }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(qs\_s) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\NormalTok{revenue\_from\_sales }\OtherTok{\textless{}{-}}
  \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(rs\_cst) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ after\_national }\SpecialCharTok{+}\NormalTok{ id\_news }\SpecialCharTok{+}\NormalTok{ year, }\AttributeTok{data =}\NormalTok{ newspapers)}
\end{Highlighting}
\end{Shaded}

Looking at the advertising-side variables
(Table~\ref{tbl-frenchnewspapersadvertising}) we find consistently
negative coefficients for everything apart from advertising space.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selected\_variables }\OtherTok{\textless{}{-}} 
  \FunctionTok{c}\NormalTok{(}\StringTok{"year"} \OtherTok{=} \StringTok{"Year"}\NormalTok{,}
  \StringTok{"after\_national1"} \OtherTok{=} \StringTok{"Is after advertising change"}\NormalTok{)}

\NormalTok{advertising\_models }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \StringTok{"Ad revenue"} \OtherTok{=}\NormalTok{ ad\_revenue,}
  \StringTok{"Ad revenue over circulation"} \OtherTok{=}\NormalTok{ ad\_revenue\_div\_circulation,}
  \StringTok{"Ad prices"} \OtherTok{=}\NormalTok{ ad\_price,}
  \StringTok{"Ad space"} \OtherTok{=}\NormalTok{ ad\_space}
\NormalTok{)}

\FunctionTok{modelsummary}\NormalTok{(}
\NormalTok{  advertising\_models,}
  \AttributeTok{fmt =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{coef\_map =}\NormalTok{ selected\_variables}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-frenchnewspapersadvertising}{}
\begin{table}
\caption{\label{tbl-frenchnewspapersadvertising}Effect of changed television advertising laws on revenue of French newspapers (1960-1974) }

\centering
\begin{tabular}[t]{lcccc}
\toprule
  & Ad revenue & Ad revenue over circulation & Ad prices & Ad space\\
\midrule
Year & \num{0.05} & \num{0.04} & \num{0.04} & \num{0.02}\\
 & (\num{0.00}) & (\num{0.00}) & (\num{0.00}) & (\num{0.00})\\
Is after advertising change & \num{-0.23} & \num{-0.15} & \num{-0.31} & \num{0.01}\\
 & (\num{0.03}) & (\num{0.03}) & (\num{0.07}) & (\num{0.05})\\
\midrule
Num.Obs. & \num{1052} & \num{1048} & \num{809} & \num{1046}\\
R2 & \num{0.985} & \num{0.903} & \num{0.892} & \num{0.720}\\
R2 Adj. & \num{0.984} & \num{0.895} & \num{0.882} & \num{0.699}\\
AIC & \num{-526.7} & \num{-735.0} & \num{705.4} & \num{478.0}\\
BIC & \num{-120.1} & \num{-328.8} & \num{1057.6} & \num{849.5}\\
Log.Lik. & \num{345.341} & \num{449.524} & \num{-277.714} & \num{-164.012}\\
F & \num{814.664} & \num{112.259} & \num{83.464} & \num{34.285}\\
\bottomrule
\end{tabular}
\end{table}

And looking at the advertising-side variables
(Table~\ref{tbl-frenchnewspapersconsumers}) we again, find consistently
negative coefficients for everything apart from the share of
subscriptions and unit price.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{consumer\_models }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \StringTok{"Subscription price"} \OtherTok{=}\NormalTok{ subscription\_price,}
  \StringTok{"Circulation"} \OtherTok{=}\NormalTok{ circulation,}
  \StringTok{"Share of subscriptions"} \OtherTok{=}\NormalTok{ share\_of\_sub,}
  \StringTok{"Revenue from sales"} \OtherTok{=}\NormalTok{ revenue\_from\_sales,}
  \StringTok{"Unit price"} \OtherTok{=}\NormalTok{ unit\_price}
\NormalTok{)}

\FunctionTok{modelsummary}\NormalTok{(}
\NormalTok{  consumer\_models,}
  \AttributeTok{fmt =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{coef\_map =}\NormalTok{ selected\_variables}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-frenchnewspapersconsumers}{}
\begin{table}
\caption{\label{tbl-frenchnewspapersconsumers}Effect of changed television advertising laws on consumers of French newspapers (1960-1974) }

\centering
\begin{tabular}[t]{lccccc}
\toprule
  & Subscription price & Circulation & Share of subscriptions & Revenue from sales & Unit price\\
\midrule
Year & \num{0.05} & \num{0.01} & \num{-0.01} & \num{0.05} & \num{0.05}\\
 & (\num{0.00}) & (\num{0.00}) & (\num{0.00}) & (\num{0.00}) & (\num{0.00})\\
Is after advertising change & \num{-0.04} & \num{-0.06} & \num{0.19} & \num{-0.06} & \num{0.06}\\
 & (\num{0.02}) & (\num{0.02}) & (\num{0.03}) & (\num{0.03}) & (\num{0.02})\\
\midrule
Num.Obs. & \num{1044} & \num{1070} & \num{1072} & \num{1046} & \num{1063}\\
R2 & \num{0.876} & \num{0.991} & \num{0.972} & \num{0.988} & \num{0.867}\\
R2 Adj. & \num{0.865} & \num{0.990} & \num{0.970} & \num{0.987} & \num{0.856}\\
AIC & \num{-1600.3} & \num{-1355.1} & \num{-477.8} & \num{-738.2} & \num{-1650.6}\\
BIC & \num{-1194.3} & \num{-947.2} & \num{-64.7} & \num{-332.1} & \num{-1243.1}\\
Log.Lik. & \num{882.140} & \num{759.573} & \num{321.907} & \num{451.112} & \num{907.285}\\
F & \num{84.659} & \num{1392.863} & \num{421.297} & \num{1030.303} & \num{79.888}\\
\bottomrule
\end{tabular}
\end{table}

In general, we are able to replicate the main results of Angelucci and
Cagé (2019) and find that in many cases there appears to be a difference
from 1967 onward. Our results are similar to Angelucci and Cagé (2019).

\hypertarget{propensity-score-matching}{%
\section{Propensity score matching}\label{propensity-score-matching}}

Difference in differences is a powerful analysis framework. But it can
be tough to identify appropriate treatment and control groups. R.
Alexander and Ward (2018) compare migrant brothers, where one brother
had most of their education in a different country, and the other
brother had most of their education in the US. Given the data that are
available, this match provides a reasonable treatment and control group.
But other matches could have given different results, for instance
friends or cousins.

We can match based on observable variables. For instance, age-group or
education. At two different times we compare smoking rates in
18-year-olds in one city with smoking rates in 18-year-olds in another
city. This would be a coarse match because we know that there are many
differences between 18-year-olds, even in terms of the variables that we
commonly observe, say gender and education. One way to deal with this
would be to create sub-groups: 18-year-old males with a high school
education, etc. But then the sample sizes quickly become small. We also
have the issue of how to deal with continuous variables. And, are an
18-year-old and a 19-year-old really so different? Why not also compare
with them?

One way to proceed is to consider a nearest neighbor approach. But there
can be limited concern for uncertainty with this approach. There can
also be an issue with having many variables because we end up with a
high-dimension graph. This leads to propensity score matching. Here we
will explain the process of propensity score matching, but it is not
something that should be widely used (G. King and Nielsen 2019), and we
will then go through why that is the case.

Propensity score matching involves assigning some probability to each
observation. We construct that probability based on the observation's
values for the independent variables, at their values before the
treatment. That probability is our best guess at the probability of the
observation being treated, regardless of whether it was treated or not.
For instance, if 18-year-old males were treated but 19-year-old males
were not, then as there is not much difference between 18-year-old males
and 19-year-old males our assigned probability would be similar. We can
then compare the outcomes of observations with similar propensity
scores.

One advantage of propensity score matching is that is allows us to
easily consider many independent variables at once, and it can be
constructed using logistic regression.

To be more specific we can simulate some data. We will pretend that we
work for a large online retailer. We are going to treat some individuals
with free shipping to see what happens to their average purchase.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{sample\_size }\OtherTok{\textless{}{-}} \DecValTok{10000}

\NormalTok{purchase\_data }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{unique\_person\_id =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{sample\_size),}
    \AttributeTok{age =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ sample\_size,}
                \AttributeTok{min =} \DecValTok{18}\NormalTok{,}
                \AttributeTok{max =} \DecValTok{100}\NormalTok{),}
    \AttributeTok{city =} \FunctionTok{sample}\NormalTok{(}
      \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Toronto"}\NormalTok{, }\StringTok{"Montreal"}\NormalTok{, }\StringTok{"Calgary"}\NormalTok{),}
      \AttributeTok{size =}\NormalTok{ sample\_size,}
      \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{    ),}
    \AttributeTok{gender =} \FunctionTok{sample}\NormalTok{(}
      \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Female"}\NormalTok{, }\StringTok{"Male"}\NormalTok{, }\StringTok{"Other/decline"}\NormalTok{),}
      \AttributeTok{size =}\NormalTok{ sample\_size,}
      \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{,}
      \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.49}\NormalTok{, }\FloatTok{0.47}\NormalTok{, }\FloatTok{0.02}\NormalTok{)}
\NormalTok{    ),}
    \AttributeTok{income =} \FunctionTok{rlnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ sample\_size,}
                    \AttributeTok{meanlog =} \FloatTok{0.5}\NormalTok{,}
                    \AttributeTok{sdlog =} \DecValTok{1}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Then we need to add some probability of being treated with free
shipping. We will say that it depends on our variables and that younger,
higher-income, male and Toronto-based individuals make this treatment
slightly more likely.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{purchase\_data }\OtherTok{\textless{}{-}}
\NormalTok{  purchase\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{age\_num =} \FunctionTok{case\_when}\NormalTok{(age }\SpecialCharTok{\textless{}} \DecValTok{30} \SpecialCharTok{\textasciitilde{}} \DecValTok{3}\NormalTok{,}
\NormalTok{                        age }\SpecialCharTok{\textless{}} \DecValTok{50} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{,}
\NormalTok{                        age }\SpecialCharTok{\textless{}} \DecValTok{70} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
                        \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{),}
    \AttributeTok{city\_num =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      city }\SpecialCharTok{==} \StringTok{"Toronto"} \SpecialCharTok{\textasciitilde{}} \DecValTok{3}\NormalTok{,}
\NormalTok{      city }\SpecialCharTok{==} \StringTok{"Montreal"} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{,}
\NormalTok{      city }\SpecialCharTok{==} \StringTok{"Calgary"} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}
\NormalTok{    ),}
    \AttributeTok{gender\_num =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      gender }\SpecialCharTok{==} \StringTok{"Male"} \SpecialCharTok{\textasciitilde{}} \DecValTok{3}\NormalTok{,}
\NormalTok{      gender }\SpecialCharTok{==} \StringTok{"Female"} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{,}
\NormalTok{      gender }\SpecialCharTok{==} \StringTok{"Other/decline"} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}
\NormalTok{    ),}
    \AttributeTok{income\_num =} \FunctionTok{case\_when}\NormalTok{(income }\SpecialCharTok{\textgreater{}} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \DecValTok{3}\NormalTok{,}
\NormalTok{                           income }\SpecialCharTok{\textgreater{}} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{,}
\NormalTok{                           income }\SpecialCharTok{\textgreater{}} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
                           \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{0}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{sum\_num =} \FunctionTok{sum}\NormalTok{(age\_num, city\_num, gender\_num, income\_num),}
    \AttributeTok{softmax\_prob =} \FunctionTok{exp}\NormalTok{(sum\_num) }\SpecialCharTok{/} \FunctionTok{exp}\NormalTok{(}\DecValTok{12}\NormalTok{),}
    \AttributeTok{free\_shipping =} \FunctionTok{sample}\NormalTok{(}
      \AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{),}
      \AttributeTok{size =} \DecValTok{1}\NormalTok{,}
      \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{,}
      \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ softmax\_prob, softmax\_prob)}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}

\NormalTok{purchase\_data }\OtherTok{\textless{}{-}}
\NormalTok{  purchase\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{age\_num,}\SpecialCharTok{{-}}\NormalTok{city\_num,}\SpecialCharTok{{-}}\NormalTok{gender\_num,}\SpecialCharTok{{-}}\NormalTok{income\_num,}\SpecialCharTok{{-}}\NormalTok{sum\_num,}\SpecialCharTok{{-}}\NormalTok{softmax\_prob)}
\end{Highlighting}
\end{Shaded}

Finally, we need to have some measure of a person's average spend. We
want those with free shipping to be slightly higher than those without.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{purchase\_data }\OtherTok{\textless{}{-}}
\NormalTok{  purchase\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{mean\_spend =} \FunctionTok{if\_else}\NormalTok{(free\_shipping }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{50}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{average\_spend =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, mean\_spend, }\AttributeTok{sd =} \DecValTok{5}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{mean\_spend) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(city, gender, free\_shipping), as.factor))}

\NormalTok{purchase\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10,000 x 7
   unique_person_id   age city     gender income free_shipping average_spend
              <int> <dbl> <fct>    <fct>   <dbl> <fct>                 <dbl>
 1                1  47.5 Calgary  Female  1.72  0                      41.1
 2                2  27.8 Montreal Male    1.54  0                      55.7
 3                3  57.7 Toronto  Female  3.16  0                      56.5
 4                4  43.9 Toronto  Male    0.636 0                      50.5
 5                5  21.1 Toronto  Female  1.43  0                      44.7
 6                6  51.1 Calgary  Male    1.18  0                      48.8
 7                7  28.7 Toronto  Female  1.49  0                      52.8
 8                8  37.9 Toronto  Female  0.414 0                      52.4
 9                9  31.0 Calgary  Male    0.384 0                      47.6
10               10  33.5 Montreal Female  1.11  0                      49.2
# ... with 9,990 more rows
\end{verbatim}

We use \texttt{matchit()} from \texttt{MatchIt} (Ho et al. 2011) to
implement logistic regression and create matched groups. We then use
\texttt{match.data()} to get the data of matches containing both all 371
people who were actually treated with free shipping and the untreated
person who is considered as similar to them, based on propensity score,
as possible. The result is a dataset of 742 observations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MatchIt)}

\NormalTok{matched\_groups }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(free\_shipping }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income, }
                  \AttributeTok{data =}\NormalTok{ purchase\_data,}
                  \AttributeTok{method =} \StringTok{"nearest"}\NormalTok{, }\AttributeTok{distance =} \StringTok{"glm"}\NormalTok{)}

\NormalTok{matched\_groups}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
A matchit object
 - method: 1:1 nearest neighbor matching without replacement
 - distance: Propensity score
             - estimated with logistic regression
 - number of obs.: 10000 (original), 742 (matched)
 - target estimand: ATT
 - covariates: age, city, gender, income
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matched\_dataset }\OtherTok{\textless{}{-}} \FunctionTok{match.data}\NormalTok{(matched\_groups)}

\NormalTok{matched\_dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 742 x 10
   unique_person_id   age city     gender income free_shipping average_spend
              <int> <dbl> <fct>    <fct>   <dbl> <fct>                 <dbl>
 1                5  21.1 Toronto  Female  1.43  0                      44.7
 2               20  30.0 Montreal Male    8.65  0                      49.0
 3               22  22.8 Toronto  Male    0.898 0                      50.1
 4               38  41.3 Toronto  Female  6.01  1                      61.5
 5               43  24.7 Toronto  Male    1.59  1                      59.6
 6               76  56.4 Toronto  Male   15.0   0                      51.8
 7              102  48.1 Toronto  Male    3.48  1                      59.8
 8              105  76.7 Toronto  Male    2.84  0                      45.1
 9              118  26.7 Toronto  Female  0.315 0                      56.4
10              143  36.3 Toronto  Male   10.6   0                      49.4
# ... with 732 more rows, and 3 more variables: distance <dbl>, weights <dbl>,
#   subclass <fct>
\end{verbatim}

Finally, we can estimate the effect of being treated on average spend
using linear regression. We are particularly interested in the
coefficient associated with the treatment variable, in this case free
shipping.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{propensity\_score\_regression }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(average\_spend }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ city }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ income }\SpecialCharTok{+}\NormalTok{ free\_shipping, }
                                  \AttributeTok{data =}\NormalTok{ matched\_dataset)}

\NormalTok{propensity\_score\_regression}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = average_spend ~ age + city + gender + income + free_shipping, 
    data = matched_dataset)

Coefficients:
        (Intercept)                  age         cityMontreal  
           49.56747              0.00735              0.12787  
        cityToronto           genderMale  genderOther/decline  
            0.58628             -1.09978             -1.99861  
             income       free_shipping1  
            0.01903             10.60550  
\end{verbatim}

We cover propensity score matching because it is widely used. But there
are many issues with propensity score matching that mean that propensity
scores should not be used for matching (G. King and Nielsen 2019). These
include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Matching. Propensity score matching cannot match on unobserved
  variables. This may be fine in a classroom setting, but in more
  realistic settings it will likely cause issues.
\item
  Modelling. The results tend to be specific to the model that is used.
\item
  Statistically. We are using the data twice.
\end{enumerate}

\hypertarget{regression-discontinuity-design}{%
\section{Regression discontinuity
design}\label{regression-discontinuity-design}}

\hypertarget{overview-1}{%
\subsection{Overview}\label{overview-1}}

Regression discontinuity design (RDD) was established by Thistlethwaite
and Campbell (1960) and is a popular way to get causality when there is
a continuous variable with cut-offs that determine treatment. Is there a
difference between a student who gets 79 per cent and a student who gets
80 per cent? Probably not much, but one may get an A-, while the other
may get a B+, and seeing that on a transcript could affect who gets a
job which could affect income. In this case the percentage is a `forcing
variable' and the cut-off for an A- is a `threshold'. As the treatment
is determined by the forcing variable we need to control for that
variable. And, these seemingly arbitrary cut-offs can be seen all the
time. Hence, there has been a great deal of work using RDD.

There is sometimes slightly different terminology used when it comes to
RDD. For instance, Cunningham (2021) refers to the forcing function as a
running variable. The exact terminology that is used does not matter
provided we use it consistently.

\hypertarget{simulated-example-1}{%
\subsection{Simulated example}\label{simulated-example-1}}

To be more specific about the situation, we simulate data. We will
consider the relationship between income and grades, and simulate there
to be a change if a student gets at least 80
(Figure~\ref{fig-rddmarks}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_observation }\OtherTok{\textless{}{-}} \DecValTok{1000}

\NormalTok{rdd\_example\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{person =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{number\_of\_observation),}
  \AttributeTok{mark =} \FunctionTok{runif}\NormalTok{(number\_of\_observation, }\AttributeTok{min =} \DecValTok{78}\NormalTok{, }\AttributeTok{max =} \DecValTok{82}\NormalTok{),}
  \AttributeTok{income =} \FunctionTok{rnorm}\NormalTok{(number\_of\_observation, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{)}

\DocumentationTok{\#\# Make income more likely to be higher if they have a mark at least 80}
\NormalTok{rdd\_example\_data }\OtherTok{\textless{}{-}}
\NormalTok{  rdd\_example\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_observation, }\AttributeTok{mean =} \DecValTok{2}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
    \AttributeTok{income =} \FunctionTok{if\_else}\NormalTok{(mark }\SpecialCharTok{\textgreater{}=} \DecValTok{80}\NormalTok{, income }\SpecialCharTok{+}\NormalTok{ noise, income)}
\NormalTok{  )}

\NormalTok{rdd\_example\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 4
   person  mark income noise
    <int> <dbl>  <dbl> <dbl>
 1      1  79.4   9.43 1.87 
 2      2  78.5   9.69 2.26 
 3      3  79.9  10.8  1.14 
 4      4  79.3   9.34 2.50 
 5      5  78.1  10.7  2.21 
 6      6  79.6   9.83 2.47 
 7      7  78.5   8.96 4.22 
 8      8  79.0  10.5  3.11 
 9      9  78.6   9.53 0.671
10     10  78.8  10.6  2.46 
# ... with 990 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rdd\_example\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mark,}
             \AttributeTok{y =}\NormalTok{ income)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{data =}\NormalTok{ rdd\_example\_data }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(mark }\SpecialCharTok{\textless{}} \DecValTok{80}\NormalTok{), }
              \AttributeTok{method=}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{,}
              \AttributeTok{color =} \StringTok{"black"}\NormalTok{,}
              \AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{data =}\NormalTok{ rdd\_example\_data }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(mark }\SpecialCharTok{\textgreater{}=} \DecValTok{80}\NormalTok{), }
              \AttributeTok{method=}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{,}
              \AttributeTok{color =} \StringTok{"black"}\NormalTok{,}
              \AttributeTok{formula =} \StringTok{\textquotesingle{}y \textasciitilde{} x\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Mark"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Income ($)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./15-causality_from_obs_files/figure-pdf/fig-rddmarks-1.pdf}

}

\caption{\label{fig-rddmarks}Illustration of simulated data that shows
an effect on income from getting a mark that is 80, compared with 79}

\end{figure}

We can use a binary variable with linear regression to estimate the
effect. We expect the coefficient to be around two, which is what we
simulated.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rdd\_example\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  rdd\_example\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{mark\_80\_and\_over =} \FunctionTok{if\_else}\NormalTok{(mark }\SpecialCharTok{\textless{}} \DecValTok{80}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }

\FunctionTok{lm}\NormalTok{(income }\SpecialCharTok{\textasciitilde{}}\NormalTok{ mark }\SpecialCharTok{+}\NormalTok{ mark\_80\_and\_over, }\AttributeTok{data =}\NormalTok{ rdd\_example\_data) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = income ~ mark + mark_80_and_over, data = rdd_example_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.3418 -0.8218 -0.0043  0.7740  6.1209 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)       5.30130    5.16331   1.027    0.305    
mark              0.06025    0.06535   0.922    0.357    
mark_80_and_over  1.89221    0.14921  12.682   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.189 on 997 degrees of freedom
Multiple R-squared:  0.4178,    Adjusted R-squared:  0.4166 
F-statistic: 357.7 on 2 and 997 DF,  p-value: < 2.2e-16
\end{verbatim}

There are various caveats to this estimate that we will discuss, but the
essentials of RDD are here. Given an appropriate set-up, and model, an
RDD can compare favorably to randomized trials (Bloom, Bell, and Reiman
2020).

We could also implement RDD using \texttt{rdrobust} (Calonico et al.
2021). The advantage of this approach is that many extensions are easily
available.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rdrobust)}
\FunctionTok{rdrobust}\NormalTok{(}\AttributeTok{y =}\NormalTok{ rdd\_example\_data}\SpecialCharTok{$}\NormalTok{income, }
         \AttributeTok{x =}\NormalTok{ rdd\_example\_data}\SpecialCharTok{$}\NormalTok{mark, }
         \AttributeTok{c =} \DecValTok{80}\NormalTok{, }\AttributeTok{h =} \DecValTok{2}\NormalTok{, }\AttributeTok{all =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call: rdrobust

Number of Obs.                 1000
BW type                      Manual
Kernel                   Triangular
VCE method                       NN

Number of Obs.                  497          503
Eff. Number of Obs.             497          503
Order est. (p)                    1            1
Order bias  (q)                   2            2
BW est. (h)                   2.000        2.000
BW bias (b)                   2.000        2.000
rho (h/b)                     1.000        1.000
Unique Obs.                     497          503

=============================================================================
        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       
=============================================================================
  Conventional     1.913     0.161    11.876     0.000     [1.597 , 2.229]     
Bias-Corrected     1.966     0.161    12.207     0.000     [1.650 , 2.282]     
        Robust     1.966     0.232     8.461     0.000     [1.511 , 2.422]     
=============================================================================
\end{verbatim}

\hypertarget{assumptions-1}{%
\subsection{Assumptions}\label{assumptions-1}}

The key assumptions of RDD are (Cunningham 2021, 163):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The cut-off is specific, fixed, and known to all.
\item
  The forcing function is continuous.
\end{enumerate}

The first assumption is largely about being unable to manipulate the
cut-off, and ensures that the cut-off has meaning. The second assumption
enables us to be confident that folks on either side of the threshold
are similar, apart from just happening to just fall on either side of
the threshold.

When we discussed randomized control trials and A/B testing in Chapter
@ref(hunt-data) the randomized assignment of the treatment meant that
the control and treatment groups were the same, but for the treatment.
Then we moved to difference in differences, and we assumed that there
was a common trend between the treated and control groups. We allowed
that the groups could be different, but that we could `difference out'
their differences. Finally, we considered matching, and we said that
even if we the control and treatment groups seemed quite different, we
were able to match, to some extent, those who were treated with a group
that were similar to them in all ways, apart from the fact that they
were not treated.

In regression discontinuity we consider a slightly different setting.
The two groups are completely different in terms of the forcing
variable. They are on either side of the threshold. So there is no
overlap at all. But we know the threshold and believe that those on
either side are essentially matched. Let us consider the 2019 NBA
Eastern Conference Semifinals - Toronto and the Philadelphia. Game 1:
Raptors win 108-95; Game 2: 76ers win 94-89; Game 3: 76ers win 116-95;
Game 4: Raptors win 101-96; Game 5: Raptors win 125-89; Game 6: 76ers
win 112-101; and finally, Game 7: Raptors win 92-90, because of a ball
that win in after bouncing on the rim four times. Was there really that
much difference between the teams?

The continuity assumption is important, but we cannot test this as it is
based on a counterfactual. Instead, we need to convince people of it.
Ways to do this include:

\begin{itemize}
\tightlist
\item
  Using a test/train set-up.
\item
  Trying different specifications. We are especially concerned if
  results do not broadly persist with just linear or quadratic
  functions.
\item
  Considering different subsets of the data.
\item
  Considering different windows.
\item
  Be clear about uncertainty intervals, especially in graphs.
\item
  Discussing and assuage concerns about the possibility of omitted
  variables.
\end{itemize}

The threshold is also important. For instance, is there an actual shift
or is there a non-linear relationship?

There are a variety of weaknesses of RDD including:

\begin{itemize}
\tightlist
\item
  External validity may be difficult. For instance, when we think about
  the A-/B+ example, it is hard to see those generalizing to also B-/C+
  students.
\item
  The important responses are those that are close to the cut-off. This
  means that even if we have many A and B students, they do not help
  much. Hence, we need a lot of data or we may have concerns about our
  ability to support our claims (D. P. Green et al. 2009).
\item
  As the researcher, we have a lot of freedom to implement different
  options. This means that open science best practice becomes vital.
\end{itemize}

To this point we have considered `sharp' RDD. That is, the threshold is
strict. But, in reality, often the boundary is a little less strict. For
instance, consider the drinking age. There is a legal drinking age, say
18. If we looked at the number of people who had drunk, then it is
likely to increase in the few years leading up to that age.

In a sharp RDD setting, if we know the value of the forcing function
then we know the outcome. For instance, if a student gets a mark of 80
then we know that they got an A-, but if they got a mark of 79 then we
know that they got a B+. But with fuzzy RDD it is only known with some
probability. We can say that a Canadian 19-year-old is more likely to
have drunk alcohol than a Canadian 18-year-old, but the number of
Canadian 18-year-olds who have drunk alcohol is not zero.

It may be possible to deal with fuzzy RDD settings with appropriate
choice of model or data. It may also be possible to deal with them using
instrumental variables.

We want as `sharp' an effect as possible, but if the thresholds are
known, then they will be gamed. For instance, there is a lot of evidence
that people run for certain marathon times, and we know that people aim
for certain grades. Similarly, from the other side, it is a lot easier
for an instructor to just give out As than it is to have to justify Bs.
One way to look at this is to consider how `balanced' the sample is on
either side of the threshold. We can do this using histograms with
appropriate bins. For instance, think of the age-heaping that we found
in the cleaned Kenyan census data in Chapter @ref(gather-data)

Another key factor for RDD is the possible effect of the decision around
the choice of model. To see this, consider the difference between a
linear and polynomial.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{some\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{outcome =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
         \AttributeTok{running\_variable =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{),}
         \AttributeTok{location =} \StringTok{"before"}\NormalTok{)}

\NormalTok{some\_more\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{outcome =} \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{2}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
         \AttributeTok{running\_variable =} \FunctionTok{c}\NormalTok{(}\DecValTok{101}\SpecialCharTok{:}\DecValTok{200}\NormalTok{),}
         \AttributeTok{location =} \StringTok{"after"}\NormalTok{)}

\NormalTok{both }\OtherTok{\textless{}{-}} 
  \FunctionTok{rbind}\NormalTok{(some\_data, some\_more\_data)}

\NormalTok{both }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ running\_variable, }\AttributeTok{y =}\NormalTok{ outcome, }\AttributeTok{color =}\NormalTok{ location)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x, }\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./15-causality_from_obs_files/figure-pdf/unnamed-chunk-49-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{both }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ running\_variable, }\AttributeTok{y =}\NormalTok{ outcome, }\AttributeTok{color =}\NormalTok{ location)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(x, }\DecValTok{3}\NormalTok{), }\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./15-causality_from_obs_files/figure-pdf/unnamed-chunk-49-2.pdf}

}

\end{figure}

The result is that our estimate is dependent on the choice of model. We
see this issue occur often in RDD (Gelman 2019).

\hypertarget{instrumental-variables}{%
\section{Instrumental variables}\label{instrumental-variables}}

\hypertarget{overview-2}{%
\subsection{Overview}\label{overview-2}}

Instrumental variables (IV) is an approach that can be handy when we
have some type of treatment and control going on, but we have a lot of
correlation with other variables and we possibly do not have a variable
that actually measures what we are interested in. So adjusting for
observables will not be enough to create a good estimate. Instead we
find some variable---the eponymous instrumental variable---that is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  correlated with the treatment variable, but
\item
  not correlated with the outcome.
\end{enumerate}

This solves our problem because the only way the instrumental variable
can have an effect is through the treatment variable, and so we can
adjust our understanding of the effect of the treatment variable
appropriately. The trade-off is that instrumental variables must satisfy
a bunch of different assumptions, and that, frankly, they are difficult
to identify \emph{ex ante}. Nonetheless, when we are able to use them,
they are a powerful tool for speaking about causality.

The canonical instrumental variables example is smoking. These days we
know that smoking causes cancer. But because smoking is correlated with
a lot of other variables, for instance, education, it could be that it
was actually education that causes cancer. RCTs may be possible, but
they are likely to be troublesome in terms of speed and ethics, and so
instead we look for some other variable that is correlated with smoking,
but not, in and of itself, with lung cancer. In this case, we look to
tax rates, and other policy responses, on cigarettes. As the tax rates
on cigarettes are correlated with the number of cigarettes that are
smoked, but not correlated with lung cancer, other than through their
impact on cigarette smoking, through them we can assess the effect of
cigarettes smoked on lung cancer.

To implement instrumental variables we first regress tax rates on
cigarette smoking to get some coefficient on the instrumental variable,
and then (in a separate regression) regress tax rates on lung cancer to
again, get some coefficient on the instrumental variable. Our estimate
is then the ratio of these coefficients, which is described as a `Wald
estimate' (Gelman and Hill 2007, 219).

Following the language of (Gelman and Hill 2007, 216) when we use
instrumental variables we make a variety of assumptions including:

\begin{itemize}
\tightlist
\item
  Ignorability of the instrument.
\item
  Correlation between the instrumental variable and the treatment
  variable.
\item
  Monotonicity.
\item
  Exclusion restriction.
\end{itemize}

The history of instrumental variables is intriguing, and Stock and
Trebbi (2003) provide a brief overview. The method was first published
in Wright (1928). This is a book about the effect of tariffs on animal
and vegetable oil. So why might instrumental variables be important in a
book about tariffs on animal and vegetable oil? The fundamental problem
is that the effect of tariffs depends on both supply and demand. But we
only know prices and quantities, so we do not know what is driving the
effect. We can use instrumental variables to pin down causality. The
intriguing aspect is that the instrumental variables discussion is only
in Appendix B. It would seem odd to relegate a major statistical
break-through to an appendix. Further, Philip G. Wright, the book's
author, had a son Sewall Wright, who had considerable expertise in
statistics and the specific method used in Appendix B. Hence the mystery
of Appendix B: did Philip or Sewall write it? Both Cunningham (2021) and
Stock and Trebbi (2003) go into more detail, but on balance feel that it
is likely that Philip did actually author the work.

\hypertarget{simulated-example-2}{%
\subsection{Simulated example}\label{simulated-example-2}}

Let us generate some data. We will explore a simulation related to the
canonical example of health status, smoking, and tax rates. So we are
looking to explain how healthy someone is based on the amount they
smoke, via the tax rate on smoking. We are going to generate different
tax rates by provinces. My understanding is that the tax rate on
cigarettes is now pretty much the same in each of the provinces, but
that this is fairly recent. So we will pretend that Alberta had a low
tax, and Nova Scotia had a high tax.

As a reminder, we are simulating data for illustrative purposes, so we
need to impose the answer that we want. When you actually use
instrumental variables you will be reversing the process.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_observation }\OtherTok{\textless{}{-}} \DecValTok{10000}

\NormalTok{iv\_example\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{person =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{number\_of\_observation),}
                          \AttributeTok{smoker =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{),}
                                          \AttributeTok{size =}\NormalTok{ number\_of\_observation, }
                                          \AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{                          )}
\end{Highlighting}
\end{Shaded}

Now we need to relate the number of cigarettes that someone smoked to
their health. We will model health status as a draw from the normal
distribution, with either a high or low mean depending on whether the
person smokes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iv\_example\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  iv\_example\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{health =} \FunctionTok{if\_else}\NormalTok{(smoker }\SpecialCharTok{==} \DecValTok{0}\NormalTok{,}
                          \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{(), }\AttributeTok{mean =} \DecValTok{1}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{),}
                          \FunctionTok{rnorm}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{(), }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{                          )}
\NormalTok{         )}
\DocumentationTok{\#\# So health will be one standard deviation higher for people who do not or barely smoke.}
\end{Highlighting}
\end{Shaded}

Now we need a relationship between cigarettes and the province (because
in this illustration, the provinces have different tax rates).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iv\_example\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  iv\_example\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{province =} 
           \FunctionTok{case\_when}\NormalTok{(smoker }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Nova Scotia"}\NormalTok{, }\StringTok{"Alberta"}\NormalTok{),}
                                          \AttributeTok{size =} \DecValTok{1}\NormalTok{, }
                                          \AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{, }
                                          \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)),}
\NormalTok{                     smoker }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\StringTok{"Nova Scotia"}\NormalTok{, }\StringTok{"Alberta"}\NormalTok{),}
                                          \AttributeTok{size =} \DecValTok{1}\NormalTok{, }
                                          \AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{, }
                                          \AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\DecValTok{4}\NormalTok{, }\DecValTok{3}\SpecialCharTok{/}\DecValTok{4}\NormalTok{)))) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{()}

\NormalTok{iv\_example\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  iv\_example\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tax =} \FunctionTok{case\_when}\NormalTok{(province }\SpecialCharTok{==} \StringTok{"Alberta"} \SpecialCharTok{\textasciitilde{}} \FloatTok{0.3}\NormalTok{,}
\NormalTok{                         province }\SpecialCharTok{==} \StringTok{"Nova Scotia"} \SpecialCharTok{\textasciitilde{}} \FloatTok{0.5}\NormalTok{,}
                         \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \DecValTok{9999999}
\NormalTok{  )}
\NormalTok{  )}

\NormalTok{iv\_example\_data}\SpecialCharTok{$}\NormalTok{tax }\SpecialCharTok{|\textgreater{}} \FunctionTok{table}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

 0.3  0.5 
6206 3794 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(iv\_example\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  person smoker  health province      tax
   <int>  <int>   <dbl> <chr>       <dbl>
1      1      0  1.11   Alberta       0.3
2      2      1 -0.0831 Alberta       0.3
3      3      1 -0.0363 Alberta       0.3
4      4      0  2.48   Alberta       0.3
5      5      0  0.617  Alberta       0.3
6      6      0  0.748  Nova Scotia   0.5
\end{verbatim}

Now we can look at our data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iv\_example\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{smoker =} \FunctionTok{as\_factor}\NormalTok{(smoker)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ health, }\AttributeTok{fill =}\NormalTok{ smoker)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{, }\AttributeTok{binwidth =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Health rating"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Number of people"}\NormalTok{,}
       \AttributeTok{fill =} \StringTok{"Smoker"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(province))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./15-causality_from_obs_files/figure-pdf/unnamed-chunk-57-1.pdf}

}

\end{figure}

Finally, we can use the tax rate as an instrumental variable to estimate
the effect of smoking on health.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{health\_on\_tax }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(health }\SpecialCharTok{\textasciitilde{}}\NormalTok{ tax, }\AttributeTok{data =}\NormalTok{ iv\_example\_data)}
\NormalTok{smoker\_on\_tax }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(smoker }\SpecialCharTok{\textasciitilde{}}\NormalTok{ tax, }\AttributeTok{data =}\NormalTok{ iv\_example\_data)}

\FunctionTok{coef}\NormalTok{(health\_on\_tax)[}\StringTok{"tax"}\NormalTok{] }\SpecialCharTok{/} \FunctionTok{coef}\NormalTok{(smoker\_on\_tax)[}\StringTok{"tax"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       tax 
-0.8554502 
\end{verbatim}

So we find, luckily, that if you smoke then your health is likely to be
worse than if you do not smoke.

Equivalently, we can think of instrumental variables in a two-stage
regression context.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first\_stage }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(smoker }\SpecialCharTok{\textasciitilde{}}\NormalTok{ tax, }\AttributeTok{data =}\NormalTok{ iv\_example\_data)}
\NormalTok{health\_hat }\OtherTok{\textless{}{-}}\NormalTok{ first\_stage}\SpecialCharTok{$}\NormalTok{fitted.values}
\NormalTok{second\_stage }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(health }\SpecialCharTok{\textasciitilde{}}\NormalTok{ health\_hat, }\AttributeTok{data =}\NormalTok{ iv\_example\_data)}

\FunctionTok{summary}\NormalTok{(second\_stage)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = health ~ health_hat, data = iv_example_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.9867 -0.7600  0.0068  0.7709  4.3293 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.91632    0.04479   20.46   <2e-16 ***
health_hat  -0.85545    0.08911   -9.60   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.112 on 9998 degrees of freedom
Multiple R-squared:  0.009134,  Adjusted R-squared:  0.009034 
F-statistic: 92.16 on 1 and 9998 DF,  p-value: < 2.2e-16
\end{verbatim}

We can use \texttt{iv\_robust()} from \texttt{estimatr} (G. Blair et al.
2021) to estimate IV. One nice reason for doing this is that it can help
to keep everything organised and adjust the standard errors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(estimatr)}
\FunctionTok{iv\_robust}\NormalTok{(health }\SpecialCharTok{\textasciitilde{}}\NormalTok{ smoker }\SpecialCharTok{|}\NormalTok{ tax, }\AttributeTok{data =}\NormalTok{ iv\_example\_data) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
iv_robust(formula = health ~ smoker | tax, data = iv_example_data)

Standard error type:  HC2 

Coefficients:
            Estimate Std. Error t value   Pr(>|t|) CI Lower CI Upper   DF
(Intercept)   0.9163    0.04057   22.59 3.163e-110   0.8368   0.9958 9998
smoker       -0.8555    0.08047  -10.63  2.981e-26  -1.0132  -0.6977 9998

Multiple R-squared:  0.1971 ,   Adjusted R-squared:  0.197 
F-statistic:   113 on 1 and 9998 DF,  p-value: < 2.2e-16
\end{verbatim}

\hypertarget{assumptions-2}{%
\subsection{Assumptions}\label{assumptions-2}}

As discussed earlier, there are a variety of assumptions that are made
when using instrumental variables. The two most important are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Exclusion Restriction. This assumption is that the instrumental
  variable only affects the dependent variable through the independent
  variable of interest.
\item
  Relevance. There must actually be a relationship between the
  instrumental variable and the independent variable.
\end{enumerate}

There is typically a trade-off between these two. There are plenty of
variables that satisfy one, precisely because they do not satisfy the
other. Cunningham (2021, 211) describes how one test of a good
instrument is if people are initially confused before you explain it to
them, only to think it obvious in hindsight.

Relevance can be tested using regression and other tests for
correlation. The exclusion restriction cannot be tested. We need to
present evidence and convincing arguments. The difficult aspect is that
the instrument needs to seem irrelevant because that is the implication
of the exclusion restriction (Cunningham 2021, 225).

Instrumental variables is a useful approach because one can obtain
causal estimates even without explicit randomization. Finding
instrumental variables used to be a bit of a white whale, especially in
academia. But there has been increased use of IV approaches downstream
of A/B tests (Taddy 2019, 162).

\hypertarget{exercises-and-tutorial-14}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-14}}

\hypertarget{exercises-14}{%
\subsection{Exercises}\label{exercises-14}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For three months Sharla Gelfand shared two functions each day: one
  that was new to them and another that they already knew and love.
  Please go the `Two Functions Most Days' GitHub
  \href{https://github.com/sharlagelfand/twofunctionsmostdays}{repo},
  and find a package that they mention that you have never used. Find
  the relevant website for the package, and then in a paragraph or two,
  describe what the package does and a context in which it could be
  useful to you.
\item
  Please again, go to Sharla's `Two Functions Most Days' GitHub
  \href{https://github.com/sharlagelfand/twofunctionsmostdays}{repo},
  and find a function that they mention that you have never used. Please
  look at the help file for that function, and then detail the arguments
  of the function, and a context in which it could be useful to you.
\item
  What is propensity score matching? If you were matching people, then
  what are some of the features that you would like to match on? What
  sort of ethical questions does collecting and storing such information
  raise for you?
\item
  Putting the ethical issues to one side, following G. King and Nielsen
  (2019), in at least two paragraphs, please describe some of the
  statistical concerns with propensity score matching.
\item
  What is the key assumption when using difference in differences?
\item
  Please read the fascinating article in The Markup about car insurance
  algorithms:
  https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list.
  Please read the article and tell me what you think. You may wish to
  focus on ethical, legal, social, statistical, or other, aspects.
\item
  Please go to the GitHub page related to the fascinating article in The
  Markup about car insurance algorithms:
  https://github.com/the-markup/investigation-allstates-algorithm. What
  is great about their work? What could be improved?
\item
  What are the fundamental features of regression discontinuity design?
\item
  What are the conditions that are needed in order for regression
  discontinuity design to be able to be used?
\item
  Can you think of a situation in your own life where regression
  discontinuity design may be useful?
\item
  What are some threats to the validity of regression discontinuity
  design estimates?
\item
  Please read and reproduce the main findings from Eggers, Fowler,
  Hainmueller, Hall, Snyder, 2015.
\item
  What is an instrumental variable?
\item
  What are some circumstances in which instrumental variables might be
  useful?
\item
  What conditions must instrumental variables satisfy?
\item
  Who were some of the early instrumental variable authors?
\item
  Can you please think of and explain an application of instrumental
  variables in your own life?
\item
  What is the key assumption in difference in differences

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Parallel trends.
  \item
    Heteroscedasticity.
  \end{enumerate}
\item
  If you are using regression discontinuity, whare are some aspects to
  be aware of and think hard about (select all that apply)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Is the cut-off free of manipulation?
  \item
    Is the forcing function continuous?
  \item
    To what extent is the functional form driving the estimate?
  \item
    Would different fitted lines affect the results?
  \end{enumerate}
\item
  What is the main reason that Oostrom (2021) finds that the outcome of
  an RCT can depend on who is funding it (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Publication bias
  \item
    Explicit manipulation
  \item
    Specialisation
  \item
    Larger number of arms
  \end{enumerate}
\item
  What is the key coefficient of interest in Angelucci and Cagé, 2019
  (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \(\beta_0\)
  \item
    \(\beta_1\)
  \item
    \(\lambda\)
  \item
    \(\gamma\)
  \end{enumerate}
\item
  The instrumental variable is (please pick all that apply):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Correlated with the treatment variable.
  \item
    Not correlated with the outcome.
  \item
    Heteroskedastic.
  \end{enumerate}
\item
  Who are the two candidates to have invented instrumental variables?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Sewall Wright
  \item
    Philip G. Wright
  \item
    Sewall Cunningham
  \item
    Philip G. Cunningham
  \end{enumerate}
\item
  What are the two main assumptions of instrumental variables?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Exclusion Restriction.
  \item
    Relevance.
  \item
    Ignorability.
  \item
    Randomization.
  \end{enumerate}
\item
  According to Meng (2021) `Data science can persuade via\ldots{}' (pick
  all that apply):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    the careful establishment of evidence from fair-minded and
    high-quality data collection
  \item
    processing and analysis
  \item
    the honest interpretation and communication of findings
  \item
    large sample sizes
  \end{enumerate}
\item
  According to Reiderer, 2021, if I have `disjoint treated and untreated
  groups partitioned by a sharp cut-off' then which method should I use
  to measure the local treatment effect at the juncture between groups
  (pick one)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    regression discontinuity
  \item
    matching
  \item
    difference in differences
  \item
    event study methods
  \end{enumerate}
\item
  According to Reiderer, 2021, `Causal inference requires investment in'
  (pick all that apply):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    data management
  \item
    domain knowledge
  \item
    probabilistic reasoning
  \item
    data science
  \end{enumerate}
\item
  I am an Australian 30-39 year old male living in Toronto with one
  child and a PhD. Which of the following do you think I would match
  most closely with and why (please explain in a paragraph or two)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    An Australian 30-39 year old male living in Toronto with one child
    and a bachelors degree
  \item
    A Canadian 30-39 year old male living in Toronto with one child and
    a PhD
  \item
    An Australian 30-39 year old male living in Ottawa with one child
    and a PhD
  \item
    A Canadian 18-29 year old male living in Toronto with one child and
    a PhD
  \end{enumerate}
\item
  In your most disdainful tone (jokes, I love DAGs), what is a DAG (in
  your own words please)?
\item
  What is a confounder (please select one answer)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A variable, z, that causes both x and y, where x also causes y.
  \item
    A variable, z, that is caused by both x and y, where x also causes
    y.
  \item
    A variable, z, that causes y and is caused by x, where x also causes
    y.
  \end{enumerate}
\item
  What is a mediator (please select one answer)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A variable, z, that causes y and is caused by x, where x also causes
    y.
  \item
    A variable, z, that causes both x and y, where x also causes y.
  \item
    A variable, z, that is caused by both x and y, where x also causes
    y.
  \end{enumerate}
\item
  What is a collider (please select one answer)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    A variable, z, that causes both x and y, where x also causes y.
  \item
    A variable, z, that causes y and is caused by x, where x also causes
    y.
  \item
    A variable, z, that is caused by both x and y, where x also causes
    y.
  \end{enumerate}
\item
  Please talk through a brief example of when you may want to be very
  careful about checking for Simpson's paradox.
\item
  Please talk through a brief example of when you may want to be very
  careful about checking for Berkson's paradox.
\item
  In Kahneman, Sibony, and Sunstein (2021) the authors, including the
  Nobel Prize winner Daniel Kahneman, say `\ldots{} while correlation
  does not imply causation, causation does imply correlation. Where
  there is a causal link, we should find a correlation'. With reference
  to Cunningham (2021, chap. 1), are they right or wrong, and why?
\end{enumerate}

\hypertarget{tutorial-14}{%
\subsection{Tutorial}\label{tutorial-14}}

\hypertarget{sec-multilevel-regression-with-post-stratification}{%
\chapter{Multilevel regression with
post-stratification}\label{sec-multilevel-regression-with-post-stratification}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{Analyzing name changes after marriage using a
  non-representative survey}, (M. Alexander 2019c)
\item
  Read \emph{Forecasting elections with non-representative polls}, (W.
  Wang et al. 2015).
\item
  Watch \emph{Statistical Models of Election Outcomes}, (Gelman 2020).
\item
  Read \emph{Mister P helps us understand vaccine hesitancy}, (E. Green
  2020).
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{haven} (Wickham and Miller 2020)
\item
  \texttt{labelled} (Larmarange 2021)
\item
  \texttt{modelsummary} (Arel-Bundock 2021a)
\item
  \texttt{rstanarm} (Goodrich et al. 2020)
\item
  \texttt{tidybayes} (Kay 2020)
\item
  \texttt{tidyverse} (Wickham 2017)
\end{itemize}

\hypertarget{introduction-13}{%
\section{Introduction}\label{introduction-13}}

\begin{quote}
{[}The Presidential election of{]} 2016 was the largest analytics
failure in US political history.

David Shor, 13 August 2020
\end{quote}

Multilevel regression with post-stratification (MRP) is a popular way to
adjust non-representative samples to better analyse opinion and other
survey responses. It uses a regression model to relate individual-level
survey responses to various characteristics and then rebuilds the sample
to better match the population. In this way MRP can not only allow a
better understanding of responses, but also allow us to analyse data
that may otherwise be unusable. However, it can be a challenge to get
started with MRP as the terminology may be unfamiliar, and the data
requirements can be onerous.

Let us say that we have a biased survey. Maybe we conducted a survey
about computers at an academic seminar, so folks with post-graduate
degrees are likely over-represented. We are nonetheless interested in
making claims about the population. Let us say that we found 37.5 per
cent of our respondents prefer Macs. One way forward is to just ignore
the bias and say that `37.5 per cent of people prefer Macs'. Another way
is to say, well 50 per cent of our respondents with a post-graduate
degree prefer Macs, and of those without a post-graduate degree, 25 per
cent prefer Macs. If we knew what proportion of the broader population
has post-graduate degree, let us assume 10 per cent, then we could
conduct re-weighting, or post-stratification, as follows:
\(0.5 \times 0.1 + 0.25 \times 0.9 = 0.275\), and so our estimate is
that 27.5 per cent of people prefer Macs. MRP is a third approach, and
uses a model to help do that re-weighting. So we use logistic regression
to estimate the relationship between preferring Macs and highest
educational attainment in our survey. We then apply that relationship to
population dataset.

MRP is a handy approach when dealing with survey data. Hanretty (2020)
describes how we use MRP because the alternatives are either very poor
or very expensive. Essentially, it trains a model based on the survey,
and then applies that trained model to another dataset. There are two
main, related, advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  It can allow us to `re-weight' in a way that includes uncertainty
  front-of-mind and isn't as hamstrung by small samples. The alternative
  way to deal with having a small sample is to either go and gather more
  data or throw it away.
\item
  It can allow us to use broad surveys to speak to subsets. As Hanretty
  (2020) says `A poor alternative {[}to using MRP{]} is simply splitting
  a large sample into (much) smaller geographic subsamples. This is a
  poor alternative because there is no guarantee that a sample which is
  representative at the national level will be representative when it is
  broken down into smaller groups.'.
\end{enumerate}

From a practical perspective, it tends to be less expensive to collect
non-probability samples and so there are benefits of being able to use
these types of data. That said, it is not a magic-bullet and the laws of
statistics still apply. We will have larger uncertainty around our
estimates and they will still be subject to all the usual biases. Dr
Lauren Kennedy, Lecturer, Monash University, describes how we have used
MRP for some time with probability surveys, and shows great potential
when it comes to non-probability surveys, but the limitations of MRP are
not known at the moment. It is an exciting area of research in both
academia and industry.

The workflow that we need for MRP is straight-forward, but the details
and tiny decisions that have to be made at each step can become
overwhelming. The point to keep in mind is that we are trying to create
a relationship between two datasets using a statistical model, and so we
need to establish similarity between the two datasets in terms of their
variables and levels. The steps are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  gather and prepare the survey dataset, thinking about what is needed
  for coherence with the post-stratification dataset;
\item
  gather and prepare the post-stratification dataset thinking about what
  is needed for coherence with the survey dataset;
\item
  model the variable of interest from the survey using independent
  variables and levels that are available in both the survey and the
  post-stratification datasets;
\item
  apply the model to the post-stratification data.
\end{enumerate}

In general, MRP is a good way to accomplish specific aims, but it is not
without trade-offs. If we have a good quality survey, then it may be a
way to speak to disaggregated aspects of it. Or if we are concerned
about uncertainty then it is a good way to think about that. If we have
a biased survey, then it is a great place to start, but it is not a
panacea. There is plenty of scope for exciting work from a variety of
approaches:

\begin{itemize}
\tightlist
\item
  From a more statistical perspective, there is a lot of work to do in
  terms of thinking through how survey design and modelling approaches
  interact and the extent to which we are underestimating uncertainty.
  It is also also very interested in thinking through the implications
  of small samples and uncertainty in the post-stratification dataset.
  There is an awful lot to do in terms of thinking through what the
  appropriate model is to use, and how do we even evaluate what
  `appropriate' means here? Si (2020) would be an appropriate starting
  point.
\item
  There is a lot to be done from a sociology perspective in terms of
  survey responses and how we can better design our surveys, knowing
  they are going to be used for MRP and putting respect for our
  respondents first.
\item
  From a political science perspective, we just have very little idea of
  the conditions under which we will have the stable preferences and
  relationships that are required for MRP to be accurate, and further
  understanding how this relates to uncertainty in survey design. For
  those with political science interests, a natural next step would be
  to go through Lauderdale et al. (2020) or Ghitza and Gelman (2020).
\item
  Economists might be interested to think about how we could use MRP to
  better understand the inflation and unemployment rates at local
  levels.
\item
  From a statistical software side of things, we really need to develop
  better packages around this.
\item
  And finally, from an information perspective it is exciting to think
  about how we store and protect our datasets, yet retain the ability to
  have them correspond with each other for the purposes of MRP. How do
  we put the levels together in a way that is meaningful? To what extent
  do people appreciate uncertainty estimates and how can we better
  communicate these estimates?
\end{itemize}

More generally, we could pretty much use MRP anywhere we have samples.
Determining the conditions under which we actually should, is the work
of whole generations.

In this chapter, we begin with simulating a situation in which we
pretend that we know the features of the population. We then move to a
famous example of MRP that used survey data from the Xbox platform and
exit poll data to forecast the 2012 US election. We will then move to
examples from the Australian political situation. We will then discuss
some features to be aware of when conducting MRP.

\hypertarget{simulation---toddler-bedtimes}{%
\section{Simulation - Toddler
bedtimes}\label{simulation---toddler-bedtimes}}

\hypertarget{construct-a-population}{%
\subsection{Construct a population}\label{construct-a-population}}

To get started we will simulate some data from a population that has
various properties, take a biased sample, and then conduct MRP to
demonstrate how we can get those properties back. We are going to have
two `explanatory variables' - age-group and toilet-trained - and one
dependent variable - bedtime. Bed-time will increase as age-group
increases, and will be later for children that are toilet-trained,
compared with those that are not. To be clear, in this example we will
`know' the `true' features of the population, but this isn't something
that occurs when we use real data - it is just to help to explain what
is happening in MRP. We are going to rely heavily on \texttt{tidyverse}
(Wickham et al. 2019a).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{size\_of\_population }\OtherTok{\textless{}{-}} \DecValTok{1000000}

\NormalTok{population\_for\_mrp\_example }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{age\_group =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{),}
                            \AttributeTok{size =}\NormalTok{ size\_of\_population,}
                            \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{                            ),}
         \AttributeTok{toilet\_trained =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                                 \AttributeTok{size =}\NormalTok{ size\_of\_population,}
                                 \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{                                 ),}
         \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(size\_of\_population, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{), }
         \CommentTok{\# No special reason for this intercept to be five; it could be anything.}
         \AttributeTok{bed\_time =} \DecValTok{5} \SpecialCharTok{+} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ age\_group }\SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{*}\NormalTok{ toilet\_trained }\SpecialCharTok{+}\NormalTok{ noise, }
\NormalTok{         ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{noise) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age\_group =} \FunctionTok{as\_factor}\NormalTok{(age\_group),}
         \AttributeTok{toilet\_trained =} \FunctionTok{as\_factor}\NormalTok{(toilet\_trained)}
\NormalTok{         )}

\NormalTok{population\_for\_mrp\_example }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  age_group toilet_trained bed_time
  <fct>     <fct>             <dbl>
1 1         0                  5.74
2 2         1                  6.48
3 1         0                  6.53
4 1         1                  5.39
5 1         1                  8.40
6 3         0                  6.54
\end{verbatim}

That is, as always, when we have a dataset, we first try to graph it to
better understand what is going on. As there are a million points, we
can just grab the first 1,000 so that it plots nicely.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population\_for\_mrp\_example }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ bed\_time)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ toilet\_trained), }
              \AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{, }
              \AttributeTok{width =} \FloatTok{0.1}\NormalTok{, }
              \AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Bed time"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Toilet trained"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./16-mrp_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\end{figure}

And we can also work out what the `truth' is for the information that we
are interested in (remembering that we'd never actually know this when
we move away from simulated examples).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population\_for\_mrp\_example\_summarised }\OtherTok{\textless{}{-}} 
\NormalTok{  population\_for\_mrp\_example }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(age\_group, toilet\_trained) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{median\_bed\_time =} \FunctionTok{median}\NormalTok{(bed\_time)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`summarise()` has grouped output by 'age_group'. You can override using the
`.groups` argument.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population\_for\_mrp\_example\_summarised }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{,}
               \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Age{-}group"}\NormalTok{, }\StringTok{"Is toilet trained"}\NormalTok{, }\StringTok{"Average bed time"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llr@{}}
\toprule()
Age-group & Is toilet trained & Average bed time \\
\midrule()
\endhead
1 & 0 & 5.50 \\
1 & 1 & 6.50 \\
2 & 0 & 6.00 \\
2 & 1 & 7.00 \\
3 & 0 & 6.50 \\
3 & 1 & 7.51 \\
\bottomrule()
\end{longtable}

\hypertarget{get-a-biased-sample-from-it}{%
\subsection{Get a biased sample from
it}\label{get-a-biased-sample-from-it}}

Now we want to pretend that we have some survey that has a biased
sample. We will allow that it over-samples children that are younger and
those that are not toilet-trained. For instance, perhaps we gathered our
sample based on the records of a paediatrician, so it is more likely
that they will see this biased sample of children. We are interested in
knowing what proportion of children are toilet-trained at various
age-groups.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This code based on that of Monica Alexander}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\CommentTok{\# Add a weight for each \textquotesingle{}type\textquotesingle{} (has to sum to one)}
\NormalTok{population\_for\_mrp\_example }\OtherTok{\textless{}{-}} 
\NormalTok{  population\_for\_mrp\_example }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{weight =} 
           \FunctionTok{case\_when}\NormalTok{(toilet\_trained }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\&}\NormalTok{ age\_group }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \FloatTok{0.7}\NormalTok{,}
\NormalTok{                     toilet\_trained }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \FloatTok{0.1}\NormalTok{,}
\NormalTok{                     age\_group }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \FloatTok{0.2}
\NormalTok{                     ),}
         \AttributeTok{id =} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{n}\NormalTok{()}
\NormalTok{         )}

\NormalTok{get\_these }\OtherTok{\textless{}{-}} 
  \FunctionTok{sample}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ population\_for\_mrp\_example}\SpecialCharTok{$}\NormalTok{id,}
    \AttributeTok{size =} \DecValTok{1000}\NormalTok{,}
    \AttributeTok{prob =}\NormalTok{ population\_for\_mrp\_example}\SpecialCharTok{$}\NormalTok{weight}
\NormalTok{    )}

\NormalTok{sample\_for\_mrp\_example }\OtherTok{\textless{}{-}} 
\NormalTok{  population\_for\_mrp\_example }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(id }\SpecialCharTok{\%in\%}\NormalTok{ get\_these) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{weight, }\SpecialCharTok{{-}}\NormalTok{id)}

\CommentTok{\# Clean up}
\NormalTok{poststratification\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  population\_for\_mrp\_example }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{weight, }\SpecialCharTok{{-}}\NormalTok{id)}
\end{Highlighting}
\end{Shaded}

And we can plot those also.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_for\_mrp\_example }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{toilet\_trained =} \FunctionTok{as\_factor}\NormalTok{(toilet\_trained)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ bed\_time)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ toilet\_trained), }\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Bed time"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Toilet trained"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./16-mrp_files/figure-pdf/unnamed-chunk-10-1.pdf}

}

\end{figure}

It is pretty clear that our sample has a different bedtime than the
overall population, but let us just do the same exercise as before to
look at the median, by age and toilet-trained status.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_for\_mrp\_example\_summarized }\OtherTok{\textless{}{-}} 
\NormalTok{  sample\_for\_mrp\_example }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(age\_group, toilet\_trained) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{median\_bed\_time =} \FunctionTok{median}\NormalTok{(bed\_time))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`summarise()` has grouped output by 'age_group'. You can override using the
`.groups` argument.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_for\_mrp\_example\_summarized }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{,}
               \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Age{-}group"}\NormalTok{, }\StringTok{"Is toilet trained"}\NormalTok{, }\StringTok{"Average bed time"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llr@{}}
\toprule()
Age-group & Is toilet trained & Average bed time \\
\midrule()
\endhead
1 & 0 & 5.41 \\
1 & 1 & 6.35 \\
2 & 0 & 5.89 \\
2 & 1 & 6.85 \\
3 & 0 & 6.49 \\
3 & 1 & 7.62 \\
\bottomrule()
\end{longtable}

\hypertarget{model-the-sample}{%
\subsection{Model the sample}\label{model-the-sample}}

We will quickly train a model based on the (biased) survey. We will use
\texttt{modelsummary} (Arel-Bundock 2021a) to format our estimates.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(modelsummary)}

\NormalTok{mrp\_example\_model }\OtherTok{\textless{}{-}} 
  \FunctionTok{lm}\NormalTok{(bed\_time }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age\_group }\SpecialCharTok{+}\NormalTok{ toilet\_trained, }\AttributeTok{data =}\NormalTok{ sample\_for\_mrp\_example)}

\NormalTok{mrp\_example\_model }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  modelsummary}\SpecialCharTok{::}\FunctionTok{modelsummary}\NormalTok{(}\AttributeTok{fmt =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lc}
\toprule
  & Model 1\\
\midrule
(Intercept) & \num{5.47}\\
 & (\num{0.04})\\
age\_group2 & \num{0.54}\\
 & \vphantom{1} (\num{0.09})\\
age\_group3 & \num{1.15}\\
 & (\num{0.09})\\
toilet\_trained1 & \num{0.91}\\
 & (\num{0.07})\\
\midrule
Num.Obs. & \num{1000}\\
R2 & \num{0.373}\\
R2 Adj. & \num{0.371}\\
AIC & \num{2839.3}\\
BIC & \num{2863.8}\\
Log.Lik. & \num{-1414.630}\\
F & \num{197.594}\\
\bottomrule
\end{tabular}
\end{table}

This is the `multilevel regression' part of the MRP (although this isn't
really a multilevel model just to keep things simple for now).

\hypertarget{get-a-post-stratification-dataset}{%
\subsection{Get a post-stratification
dataset}\label{get-a-post-stratification-dataset}}

Now we will use a post-stratification dataset to get some estimates of
the number in each cell. We typically use a larger dataset that may more
closely reflection the population. In the US a popular choice is the
ACS, while in other countries we typically have to use the census.

In this simulation example, we will just take a 10 per cent sample from
the population and use that as our post-stratification dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{poststratification\_dataset }\OtherTok{\textless{}{-}} 
\NormalTok{  population\_for\_mrp\_example }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{100000}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{bed\_time)}

\NormalTok{poststratification\_dataset }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
  age_group toilet_trained weight    id
  <fct>     <fct>           <dbl> <int>
1 1         0                 0.7     1
2 2         1                 0.2     2
3 1         0                 0.7     3
4 1         1                 0.2     4
5 1         1                 0.2     5
6 3         0                 0.1     6
\end{verbatim}

In an ideal world we have individual-level data in our
post-stratification dataset (that is the case above). In that world we
can apply our model to each individual. The more likely situation, in
reality, is that we just have counts by groups, so we are going to try
to construct an estimate for each group.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poststratification\_dataset\_grouped }\OtherTok{\textless{}{-}} 
\NormalTok{  poststratification\_dataset }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(age\_group, toilet\_trained) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{()}

\NormalTok{poststratification\_dataset\_grouped }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
# Groups:   age_group, toilet_trained [6]
  age_group toilet_trained     n
  <fct>     <fct>          <int>
1 1         0              16766
2 1         1              16649
3 2         0              16801
4 2         1              16617
5 3         0              16625
6 3         1              16542
\end{verbatim}

\hypertarget{post-stratify-our-model-estimates}{%
\subsection{Post-stratify our model
estimates}\label{post-stratify-our-model-estimates}}

Now we create an estimate for each group, and add some confidence
intervals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poststratification\_dataset\_grouped }\OtherTok{\textless{}{-}} 
\NormalTok{  mrp\_example\_model }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{predict}\NormalTok{(}\AttributeTok{newdata =}\NormalTok{ poststratification\_dataset\_grouped, }\AttributeTok{interval =} \StringTok{"confidence"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{cbind}\NormalTok{(poststratification\_dataset\_grouped) }
\end{Highlighting}
\end{Shaded}

At this point we can have a look at our MRP estimates (circles) along
with their confidence intervals, and compare them the raw estimates from
the data (squares). In this case, because we know the truth, we can also
compare them to the known truth (triangles) (but that is not something
we can do normally).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poststratification\_dataset\_grouped }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ fit)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ population\_for\_mrp\_example\_summarised,}
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ median\_bed\_time, }\AttributeTok{color =}\NormalTok{ toilet\_trained), }
             \AttributeTok{shape =} \DecValTok{17}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ sample\_for\_mrp\_example\_summarized,}
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ median\_bed\_time, }\AttributeTok{color =}\NormalTok{ toilet\_trained), }
             \AttributeTok{shape =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_pointrange}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin=}\NormalTok{lwr, }\AttributeTok{ymax=}\NormalTok{upr, }\AttributeTok{color =}\NormalTok{ toilet\_trained)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age{-}group"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Bed time"}\NormalTok{,}
       \AttributeTok{color =} \StringTok{"Toilet trained"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./16-mrp_files/figure-pdf/unnamed-chunk-22-1.pdf}

}

\end{figure}

\hypertarget{case-study---xbox-paper}{%
\section{Case study - Xbox paper}\label{case-study---xbox-paper}}

\hypertarget{overview-3}{%
\subsection{Overview}\label{overview-3}}

One famous MRP example is W. Wang et al. (2015). They used data from the
Xbox gaming platform to forecast the 2012 US Presidential Election.

Key facts about the set-up:

\begin{itemize}
\tightlist
\item
  Data are from an opt-in poll which was available on the Xbox gaming
  platform during the 45 days leading up to the 2012 US presidential
  election (Obama and Romney).
\item
  Each day there were three to five questions, including voter
  intention: `If the election were held today, who would you vote for?'.
\item
  Respondents were allowed to answer at most once per day.
\item
  First-time respondents were asked to provide information about
  themselves, including their sex, race, age, education, state, party
  ID, political ideology, and who they voted for in the 2008
  presidential election.
\item
  In total, 750,148 interviews were conducted, with 345,858 unique
  respondents - over 30,000 of whom completed five or more polls.
\item
  Young men dominate the Xbox population: 18-to-29-year-olds comprise 65
  per cent of the Xbox dataset, compared to 19 per cent in the exit
  poll; and men make up 93 per cent of the Xbox sample but only 47 per
  cent of the electorate.
\end{itemize}

\hypertarget{model-1}{%
\subsection{Model}\label{model-1}}

Given the structure of the US electorate, they use a two-stage modelling
approach. The details don't really matter too much, but essentially they
model how likely a respondent is to vote for Obama, given various
information such as state, education, sex, etc:

\[
Pr\left(Y_i = \mbox{Obama} | Y_i\in\{\mbox{Obama, Romney}\}\right) = \mbox{logit}^{-1}(\alpha_0 + \alpha_1(\mbox{state last vote share}) + \alpha_{j[i]}^{\mbox{state}} + \alpha_{j[i]}^{\mbox{edu}} + \alpha_{j[i]}^{\mbox{sex}} + ...)
\]

They run this in R using \texttt{glmer()} from `lme4' (Bates et al.
2015).

\hypertarget{post-stratify}{%
\subsection{Post-stratify}\label{post-stratify}}

Having a trained model that considers the effect of these various
independent variables on support for the candidates, they now
post-stratify, where each of these `cell-level estimates are weighted by
the proportion of the electorate in each cell and aggregated to the
appropriate level (i.e., state or national).'

This means that they need cross-tabulated population data. In general,
the census would have worked, or one of the other large surveys
available in the US, but the difficulty is that the variables need to be
available on a cross-tab basis. As such, they use exit polls (not a
viable option for most other countries).

They make state-specific estimates by post-stratifying to the features
of each state (Figure~\ref{fig-states}).

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/states.png}

}

\caption{\label{fig-states}Post-stratified estimates for each state
based on the Xbox survey and MRP}

\end{figure}

Similarly, they can examine demographic-differences
(Figure~\ref{fig-demographics}).

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/demographics.png}

}

\caption{\label{fig-demographics}Post-stratified estimates on a
demographic basis based on the Xbox survey and MRP}

\end{figure}

Finally, they convert their estimates into electoral college estimates
(Figure~\ref{fig-electoralcollege}).

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/electoral_college.png}

}

\caption{\label{fig-electoralcollege}Post-stratified estimates of
electoral college outcomes based on the Xbox survey and MRP}

\end{figure}

\hypertarget{simulation---australian-voting}{%
\section{Simulation - Australian
voting}\label{simulation---australian-voting}}

\hypertarget{overview-4}{%
\subsection{Overview}\label{overview-4}}

As a reminder, the workflow that we use is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  read in the poll;
\item
  model the poll;
\item
  read in the post-stratification data; and
\item
  apply the model to the post-stratification data.
\end{enumerate}

In the earlier example, we did not really do too much in the modelling
step, and despite the name `multilevel modelling with
post-stratification', we did not actually use a multilevel model. There
is nothing that says you have to use a multilevel model, but a lot of
situations will have circumstances such that it is not likely to do any
worse. To be clear, this means that although we have individual-level
data, there is some grouping of the individuals that we'll take
advantage of. For instance, in the case of trying to model elections,
usually districts/divisions/electorates/ridings/etc exist within
provinces/states so it would likely make sense to, at least, include a
coefficient that adjusts the intercept for each province.

In this section we are simulate another dataset and then fit a few
different models to it. We are going to draw on the Australian elections
set-up. In Australia we have a parliamentary system, with 151 seats in
the parliament, one for each electorate. These electorates are grouped
within six states and two territories. There are two major parties - the
Australian Labor Party (ALP) and the Liberal Party (LP). Somewhat
confusingly, the Liberal party are actually the conservative, right-wing
party, while the Labor party are the progressive, left-wing, party.

\hypertarget{construct-a-survey}{%
\subsection{Construct a survey}\label{construct-a-survey}}

To move us slightly closer to reality, we are going to simulate a survey
(rather than sample from a population as we did earlier) and then
post-stratify it using real data. The dependent variable is
`supports\_ALP', which is a binary variable - either 0 or 1. We will
just start with three independent variables here:

\begin{itemize}
\tightlist
\item
  `gender', which is either `female' or `male' (as that is what is
  available from the Australian Bureau of Statistics);
\item
  `age\_group', which is one of four groups: `ages 18 to 29', `ages 30
  to 44', `ages 45 to 59', `ages 60 plus';
\item
  `state', which is one of eight integers: 1 - 8 (inclusive).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{size\_of\_sample\_for\_australian\_polling }\OtherTok{\textless{}{-}} \DecValTok{2000}

\NormalTok{sample\_for\_australian\_polling }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{age\_group =} 
           \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{3}\NormalTok{), }
                  \AttributeTok{size =}\NormalTok{ size\_of\_sample\_for\_australian\_polling,}
                  \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{                  ),}
         \AttributeTok{gender =} 
           \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{1}\NormalTok{),}
                  \AttributeTok{size =}\NormalTok{ size\_of\_sample\_for\_australian\_polling,}
                  \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{                  ),}
         \AttributeTok{state =} 
           \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{),}
                  \AttributeTok{size =}\NormalTok{ size\_of\_sample\_for\_australian\_polling,}
                  \AttributeTok{replace =} \ConstantTok{TRUE}
\NormalTok{                  ),}
         \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(size\_of\_sample\_for\_australian\_polling, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{), }
         \AttributeTok{support\_alp =} \DecValTok{1} \SpecialCharTok{+} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ age\_group }\SpecialCharTok{+} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ gender }\SpecialCharTok{+} \FloatTok{0.01} \SpecialCharTok{*}\NormalTok{ state }\SpecialCharTok{+}\NormalTok{ noise}
\NormalTok{         ) }

\CommentTok{\# Normalize the outcome variable}
\NormalTok{sample\_for\_australian\_polling }\OtherTok{\textless{}{-}} 
\NormalTok{  sample\_for\_australian\_polling }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{support\_alp =} 
           \FunctionTok{if\_else}\NormalTok{(support\_alp }\SpecialCharTok{\textgreater{}} \FunctionTok{median}\NormalTok{(support\_alp, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{), }
                   \StringTok{\textquotesingle{}Supports ALP\textquotesingle{}}\NormalTok{, }
                   \StringTok{\textquotesingle{}Does not\textquotesingle{}}\NormalTok{)}
\NormalTok{         )}

\CommentTok{\# Clean up the simulated data}
\NormalTok{sample\_for\_australian\_polling }\OtherTok{\textless{}{-}} 
\NormalTok{  sample\_for\_australian\_polling }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{age\_group =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      age\_group }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 18 to 29\textquotesingle{}}\NormalTok{,}
\NormalTok{      age\_group }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 30 to 44\textquotesingle{}}\NormalTok{,}
\NormalTok{      age\_group }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 45 to 59\textquotesingle{}}\NormalTok{,}
\NormalTok{      age\_group }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 60 plus\textquotesingle{}}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Problem\textquotesingle{}}
\NormalTok{      ),}
    \AttributeTok{gender =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      gender }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Male\textquotesingle{}}\NormalTok{,}
\NormalTok{      gender }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Female\textquotesingle{}}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Problem\textquotesingle{}}
\NormalTok{      ),}
    \AttributeTok{state =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Queensland\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}New South Wales\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Australian Capital Territory\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Victoria\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{5} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Tasmania\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{6} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Northern Territory\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{7} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}South Australia\textquotesingle{}}\NormalTok{,}
\NormalTok{      state }\SpecialCharTok{==} \DecValTok{8} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Western Australia\textquotesingle{}}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Problem\textquotesingle{}}
\NormalTok{      ),}
    
\NormalTok{    ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{noise)}

\CommentTok{\# Tidy the class}
\NormalTok{sample\_for\_australian\_polling }\OtherTok{\textless{}{-}} 
\NormalTok{  sample\_for\_australian\_polling }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(age\_group, gender, state, support\_alp), as\_factor))}

\NormalTok{sample\_for\_australian\_polling }\SpecialCharTok{|\textgreater{}}   
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
  age_group     gender state           support_alp 
  <fct>         <fct>  <fct>           <fct>       
1 Ages 18 to 29 Female South Australia Supports ALP
2 Ages 60 plus  Male   South Australia Supports ALP
3 Ages 30 to 44 Male   Victoria        Does not    
4 Ages 18 to 29 Male   Tasmania        Does not    
5 Ages 18 to 29 Female Victoria        Does not    
6 Ages 18 to 29 Male   Queensland      Supports ALP
\end{verbatim}

Finally, we want our survey to over-sample females, so we will just get
rid of 300 males.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_for\_australian\_polling }\OtherTok{\textless{}{-}} 
\NormalTok{  sample\_for\_australian\_polling }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(gender) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1700}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-the-survey}{%
\subsection{Model the survey}\label{model-the-survey}}

This polling data was generated to make both males and older people less
likely to vote for the ALP; and females and younger people more likely
to vote for the Labor Party. Females are over-sampled. As such, we
should have an ALP skew on the dataset. We are going to use the
\texttt{gtsummary} package to quickly make a summary table (Arel-Bundock
2021a).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(modelsummary)}

\NormalTok{sample\_for\_australian\_polling }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{datasummary\_skim}\NormalTok{(}\AttributeTok{type =} \StringTok{"categorical"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{llrr}
\toprule
  &    & N & \%\\
\midrule
age\_group & Ages 18 to 29 & 458 & \num{26.9}\\
 & Ages 60 plus & 421 & \num{24.8}\\
 & Ages 30 to 44 & 401 & \num{23.6}\\
 & Ages 45 to 59 & 420 & \num{24.7}\\
gender & Female & 1023 & \num{60.2}\\
 & Male & 677 & \num{39.8}\\
state & South Australia & 233 & \num{13.7}\\
 & Victoria & 189 & \num{11.1}\\
 & Tasmania & 229 & \num{13.5}\\
 & Queensland & 214 & \num{12.6}\\
 & Western Australia & 198 & \num{11.6}\\
 & New South Wales & 219 & \num{12.9}\\
 & Australian Capital Territory & 237 & \num{13.9}\\
 & Northern Territory & 181 & \num{10.6}\\
support\_alp & Supports ALP & 896 & \num{52.7}\\
 & Does not & 804 & \num{47.3}\\
\bottomrule
\end{tabular}
\end{table}

Now we'd like to see if we can get our results back (we should find
females less likely than males to vote for Australian Labor Party and
that people are less likely to vote Australian Labor Party as they get
older). Our model is:

ADD THE MODEL.

This model says that the probability that some person, \(j\), will vote
for the Australian Labor Party depends on their gender and their
age-group. Based on our simulated data, we would like older age-groups
to be less likely to vote for the Australian Labor Party and for males
to be less likely to vote for the Australian Labor Party.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alp\_support }\OtherTok{\textless{}{-}} 
  \FunctionTok{glm}\NormalTok{(support\_alp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ age\_group }\SpecialCharTok{+}\NormalTok{ state, }
      \AttributeTok{data =}\NormalTok{ sample\_for\_australian\_polling,}
      \AttributeTok{family =} \StringTok{"binomial"}
\NormalTok{      )}

\NormalTok{alp\_support }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{modelsummary}\NormalTok{(}\AttributeTok{fmt =} \DecValTok{2}\NormalTok{, }\AttributeTok{exponentiate =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lc}
\toprule
  & Model 1\\
\midrule
(Intercept) & \num{1.44}\\
 & (\num{0.18})\\
genderMale & \num{3.22}\\
 & (\num{0.12})\\
age\_groupAges 60 plus & \num{0.07}\\
 & (\num{0.17})\\
age\_groupAges 30 to 44 & \num{0.42}\\
 & \vphantom{1} (\num{0.15})\\
age\_groupAges 45 to 59 & \num{0.17}\\
 & (\num{0.15})\\
stateVictoria & \num{1.65}\\
 & \vphantom{2} (\num{0.22})\\
stateTasmania & \num{1.24}\\
 & \vphantom{2} (\num{0.21})\\
stateQueensland & \num{1.46}\\
 & \vphantom{1} (\num{0.22})\\
stateWestern Australia & \num{1.18}\\
 & (\num{0.22})\\
stateNew South Wales & \num{1.42}\\
 & \vphantom{1} (\num{0.21})\\
stateAustralian Capital Territory & \num{1.73}\\
 & (\num{0.21})\\
stateNorthern Territory & \num{1.49}\\
 & (\num{0.23})\\
\midrule
Num.Obs. & \num{1700}\\
AIC & \num{1959.7}\\
BIC & \num{2024.9}\\
Log.Lik. & \num{-967.836}\\
F & \num{28.555}\\
\bottomrule
\end{tabular}
\end{table}

Essentially we have got our inputs back. Our dependent variable is a
binary, and so we used logistic regression so the results are a little
more difficult to interpret.

\hypertarget{post-stratify-1}{%
\subsection{Post-stratify}\label{post-stratify-1}}

Now we'd like to see if we can use what we found in the poll to get an
estimate for each state based on their demographic features.

First read in some real demographic data, on a state basis, from the
ABS.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post\_strat\_census\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"outputs/data/census\_data.csv"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(post\_strat\_census\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  state gender age_group  number cell_prop_of_division_total
  <chr> <chr>  <chr>       <dbl>                       <dbl>
1 ACT   Female ages18to29  34683                       0.125
2 ACT   Female ages30to44  42980                       0.155
3 ACT   Female ages45to59  33769                       0.122
4 ACT   Female ages60plus  30322                       0.109
5 ACT   Male   ages18to29  34163                       0.123
6 ACT   Male   ages30to44  41288                       0.149
\end{verbatim}

At this point, we have got a decision to make because we need the
variables to be the same in the survey and the post-stratification
dataset, but here the state abbreviations have been used, while in the
survey, the full names were used. We will change the post-stratification
dataset because the survey data has already modelled.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post\_strat\_census\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  post\_strat\_census\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{state =} 
      \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}ACT\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Australian Capital Territory\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}NSW\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}New South Wales\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}NT\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Northern Territory\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}QLD\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Queensland\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}SA\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}South Australia\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}TAS\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Tasmania\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}VIC\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Victoria\textquotesingle{}}\NormalTok{,}
\NormalTok{        state }\SpecialCharTok{==} \StringTok{\textquotesingle{}WA\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Western Australia\textquotesingle{}}\NormalTok{,}
        \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"Problem"}
\NormalTok{      ),}
    \AttributeTok{age\_group =} 
      \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{        age\_group }\SpecialCharTok{==} \StringTok{\textquotesingle{}ages18to29\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 18 to 29\textquotesingle{}}\NormalTok{,}
\NormalTok{        age\_group }\SpecialCharTok{==} \StringTok{\textquotesingle{}ages30to44\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 30 to 44\textquotesingle{}}\NormalTok{,}
\NormalTok{        age\_group }\SpecialCharTok{==} \StringTok{\textquotesingle{}ages45to59\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 45 to 59\textquotesingle{}}\NormalTok{,}
\NormalTok{        age\_group }\SpecialCharTok{==} \StringTok{\textquotesingle{}ages60plus\textquotesingle{}} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Ages 60 plus\textquotesingle{}}\NormalTok{,}
        \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"Problem"}
\NormalTok{      )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

We are just going to do some rough forecasts. For each gender and
age-group we want the relevant coefficient in the example data and we
can construct the estimates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post\_strat\_census\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  alp\_support }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{predict}\NormalTok{(}\AttributeTok{newdata =}\NormalTok{ post\_strat\_census\_data, }\AttributeTok{type =} \StringTok{\textquotesingle{}response\textquotesingle{}}\NormalTok{, }\AttributeTok{se.fit =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{cbind}\NormalTok{(post\_strat\_census\_data)}

\NormalTok{post\_strat\_census\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{alp\_predict\_prop =}\NormalTok{ fit}\SpecialCharTok{*}\NormalTok{cell\_prop\_of\_division\_total) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(state) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{alp\_predict =} \FunctionTok{sum}\NormalTok{(alp\_predict\_prop))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 8 x 2
  state                        alp_predict
  <chr>                              <dbl>
1 Australian Capital Territory       0.551
2 New South Wales                    0.487
3 Northern Territory                 0.546
4 Queensland                         0.491
5 South Australia                    0.403
6 Tasmania                           0.429
7 Victoria                           0.521
8 Western Australia                  0.460
\end{verbatim}

We now have post-stratified estimates for each state Our model has a
fair few weaknesses. For instance, small cell counts are going to be
problematic. And our approach ignores uncertainty, but now that we have
something working we can complicate it.

\hypertarget{improving-the-model}{%
\subsection{Improving the model}\label{improving-the-model}}

We'd like to address some of the major issues with our approach,
specifically being able to deal with small cell counts, and also taking
better account of uncertainty. As we are dealing with survey data,
prediction intervals or something similar are critical, and it is not
appropriate to only report central estimates. To do this we'll use the
same broad approach as before, but just improve the model. We are going
to change to a Bayesian model and use the \texttt{rstanarm} package
(Goodrich et al. 2020).

Now, using the same basic model as before, but in a Bayesian setting.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rstanarm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: Rcpp
\end{verbatim}

\begin{verbatim}
This is rstanarm version 2.21.1
\end{verbatim}

\begin{verbatim}
- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!
\end{verbatim}

\begin{verbatim}
- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.
\end{verbatim}

\begin{verbatim}
- For execution on a local, multicore CPU with excess RAM we recommend calling
\end{verbatim}

\begin{verbatim}
  options(mc.cores = parallel::detectCores())
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{improved\_alp\_support }\OtherTok{\textless{}{-}} 
  \FunctionTok{stan\_glm}\NormalTok{(support\_alp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ age\_group }\SpecialCharTok{+}\NormalTok{ state,}
                     \AttributeTok{data =}\NormalTok{ sample\_for\_australian\_polling,}
                     \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
                     \AttributeTok{prior =} \FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                     \AttributeTok{prior\_intercept =} \FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                     \AttributeTok{cores =} \DecValTok{2}\NormalTok{, }
                     \AttributeTok{seed =} \DecValTok{12345}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As before, we'd like an estimate for each state based on their
demographic features. We are just going to do some rough forecasts. For
each gender and age-group we want the relevant coefficient in the
example data and we can construct the estimates (this code is from
\href{https://www.monicaalexander.com/posts/2019-08-07-mrp/}{Monica
Alexander}). We are going to use \texttt{tidybayes} for this (Kay 2020).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidybayes)}

\NormalTok{post\_stratified\_estimates }\OtherTok{\textless{}{-}} 
\NormalTok{  improved\_alp\_support }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{add\_fitted\_draws}\NormalTok{(}\AttributeTok{newdata =}\NormalTok{ post\_strat\_census\_data) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{alp\_predict =}\NormalTok{ .value) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{alp\_predict\_prop =}\NormalTok{ alp\_predict}\SpecialCharTok{*}\NormalTok{cell\_prop\_of\_division\_total) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(state, .draw) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{alp\_predict =} \FunctionTok{sum}\NormalTok{(alp\_predict\_prop)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(state) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(alp\_predict), }
            \AttributeTok{lower =} \FunctionTok{quantile}\NormalTok{(alp\_predict, }\FloatTok{0.025}\NormalTok{), }
            \AttributeTok{upper =} \FunctionTok{quantile}\NormalTok{(alp\_predict, }\FloatTok{0.975}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.
Use [add_]epred_draws() to get the expectation of the posterior predictive.
Use [add_]linpred_draws() to get the distribution of the linear predictor.
For example, you used [add_]fitted_draws(..., scale = "response"), which
means you most likely want [add_]epred_draws(...).
\end{verbatim}

\begin{verbatim}
`summarise()` has grouped output by 'state'. You can override using the
`.groups` argument.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post\_stratified\_estimates}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 8 x 4
  state                         mean lower upper
  <chr>                        <dbl> <dbl> <dbl>
1 Australian Capital Territory 0.550 0.494 0.604
2 New South Wales              0.486 0.429 0.544
3 Northern Territory           0.544 0.483 0.607
4 Queensland                   0.491 0.432 0.548
5 South Australia              0.412 0.361 0.464
6 Tasmania                     0.429 0.372 0.487
7 Victoria                     0.519 0.453 0.583
8 Western Australia            0.460 0.401 0.520
\end{verbatim}

We now have post-stratified estimates for each division. Our new
Bayesian approach will enable us to think more deeply about uncertainty.
We could complicate this in a variety of ways including adding more
coefficients (but remember that we'd need to get new cell counts), or
adding some layers.

One interesting aspect is that our multilevel approach will allow us to
deal with small cell counts by borrowing information from other cells.
Even if we were to remove most of the, say, 18-to-29-year-old, male
respondents from Tasmania our model would still provide estimates. It
does this by pooling, in which the effect of these young, male,
Tasmanians is partially determined by other cells that do have
respondents.

There are many interesting aspects that we may like to communicate to
others. For instance, we may like to show how the model is affecting the
results. We can make a graph that compares the raw estimate with the
model estimate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post\_stratified\_estimates }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ mean, }\AttributeTok{x =}\NormalTok{ forcats}\SpecialCharTok{::}\FunctionTok{fct\_inorder}\NormalTok{(state), }\AttributeTok{color =} \StringTok{"MRP estimate"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ lower, }\AttributeTok{ymax =}\NormalTok{ upper), }\AttributeTok{width =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Proportion ALP support"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"State"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"bottom"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.title =} \FunctionTok{element\_blank}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./16-mrp_files/figure-pdf/unnamed-chunk-41-1.pdf}

}

\end{figure}

Similarly, we may like to plot the distribution of the coefficients. We
can work out which coefficients to be pass to \texttt{gather\_draws}
with \texttt{tidybayes::get\_variables(model}; in this example we passed
`b\_.', but that will not always be the case.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# tidybayes::get\_variables(improved\_alp\_support)}
\CommentTok{\# improved\_alp\_support |\textgreater{}}
\CommentTok{\#   gather\_draws(genderMale) |\textgreater{}}
\CommentTok{\#   ungroup() |\textgreater{}}
\CommentTok{\#   \# mutate(coefficient = stringr::str\_replace\_all(.variable, c("b\_" = ""))) |\textgreater{}}
\CommentTok{\#   mutate(coefficient = forcats::fct\_recode(coefficient,}
\CommentTok{\#                                            Intercept = "Intercept",}
\CommentTok{\#                                            \textasciigrave{}Is male\textasciigrave{} = "genderMale",}
\CommentTok{\#                                            \textasciigrave{}Age 30{-}44\textasciigrave{} = "age\_groupages30to44",}
\CommentTok{\#                                            \textasciigrave{}Age 45{-}59\textasciigrave{} = "age\_groupages45to59",}
\CommentTok{\#                                            \textasciigrave{}Age 60+\textasciigrave{} = "age\_groupages60plus"}
\CommentTok{\#                                            )) |\textgreater{} }
\CommentTok{\# }
\CommentTok{\# \# both |\textgreater{} }
\CommentTok{\#   ggplot(aes(y=fct\_rev(coefficient), x = .value)) + }
\CommentTok{\#   ggridges::geom\_density\_ridges2(aes(height = ..density..),}
\CommentTok{\#                                  rel\_min\_height = 0.01, }
\CommentTok{\#                                  stat = "density",}
\CommentTok{\#                                  scale=1.5) +}
\CommentTok{\#   xlab("Distribution of estimate") +}
\CommentTok{\#   ylab("Coefficient") +}
\CommentTok{\#   scale\_fill\_brewer(name = "Dataset: ", palette = "Set1") +}
\CommentTok{\#   theme\_minimal() +}
\CommentTok{\#   theme(panel.grid.major = element\_blank(),}
\CommentTok{\#         panel.grid.minor = element\_blank()) +}
\CommentTok{\#   theme(legend.position = "bottom")}
\end{Highlighting}
\end{Shaded}

\hypertarget{forecasting-the-2020-us-election}{%
\section{Forecasting the 2020 US
election}\label{forecasting-the-2020-us-election}}

The US election has a lot of features that are unique to the US, but the
model that we are going to build here is going to be fairly generic and,
largely a generalization of the earlier model for the Australian
election. One good thing about forecasting the US election is that there
is a lot of data around. In this case we can use survey data from the
\href{https://www.voterstudygroup.org}{Democracy Fund Voter Study
Group}.\footnote{I thank \href{http://www.chriswarshaw.com}{Chris
  Warshaw} for putting me onto this dataset.} They conducted polling in
the lead-up to the US election and make this publicly available after
registration. We will use the Integrated Public Use Microdata Series
(IPUMS), to access the 2018 American Community Survey (ACS) as a
post-stratification dataset. We will use state, age-group, gender, and
education as explanatory variables.

\hypertarget{survey-data}{%
\subsection{Survey data}\label{survey-data}}

The first step is that we need to actually get the survey data. Go to
their website: https://www.voterstudygroup.org and then you are looking
for `Nationscape' and then a button along the lines of `Get the latest
Nationscape data'. To get the dataset, you need to fill out a form,
which they will process and then email you. There is a real person on
the other side of this form, and so your request could take a few days.

Once you get access you will want to download the .dta files.
Nationscape conducted many surveys and so there are many files. The
filename is the reference date, and so `ns20200625' refers to 25 June
2020, which is the one that we will use here.

We can read in `.dta' files using the \texttt{haven} package (Wickham
and Miller 2020). This code is based on that of Alen Mitrovski, Xiaoyan
Yang, Matthew Wankiewicz:
https://github.com/matthewwankiewicz/US\_election\_forecast.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{raw\_nationscape\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_dta}\NormalTok{(here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{(}\StringTok{"dont\_push/ns20200625.dta"}\NormalTok{))}

\CommentTok{\# The Stata format separates labels so reunite those}
\NormalTok{raw\_nationscape\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  labelled}\SpecialCharTok{::}\FunctionTok{to\_factor}\NormalTok{(raw\_nationscape\_data)}

\CommentTok{\# Just keep relevant variables}
\NormalTok{nationscape\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  raw\_nationscape\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(vote\_2020,}
\NormalTok{         gender,}
\NormalTok{         education,}
\NormalTok{         state,}
\NormalTok{         age)}

\CommentTok{\# For simplicity, remove anyone undecided or planning to vote for someone other than Biden/Trump and make vote a binary variable: 1 for Biden, 0 for Trump.}
\NormalTok{nationscape\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  nationscape\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(vote\_2020 }\SpecialCharTok{==} \StringTok{"Joe Biden"} \SpecialCharTok{|}\NormalTok{ vote\_2020 }\SpecialCharTok{==} \StringTok{"Donald Trump"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{vote\_biden =} \FunctionTok{if\_else}\NormalTok{(vote\_2020 }\SpecialCharTok{==} \StringTok{"Joe Biden"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{vote\_2020)}

\CommentTok{\# Create the dependent variables by grouping the existing variables}
\NormalTok{nationscape\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  nationscape\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{age\_group =} \FunctionTok{case\_when}\NormalTok{( }\CommentTok{\# case\_when works in order and exits when there\textquotesingle{}s a match}
\NormalTok{      age }\SpecialCharTok{\textless{}=} \DecValTok{29} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_18{-}29\textquotesingle{}}\NormalTok{,}
\NormalTok{      age }\SpecialCharTok{\textless{}=} \DecValTok{44} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_30{-}44\textquotesingle{}}\NormalTok{,}
\NormalTok{      age }\SpecialCharTok{\textless{}=} \DecValTok{59} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_45{-}59\textquotesingle{}}\NormalTok{,}
\NormalTok{      age }\SpecialCharTok{\textgreater{}=} \DecValTok{60} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_60\_or\_more\textquotesingle{}}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Trouble\textquotesingle{}}
\NormalTok{      ),}
    \AttributeTok{gender =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      gender }\SpecialCharTok{==} \StringTok{"Female"} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}female\textquotesingle{}}\NormalTok{,}
\NormalTok{      gender }\SpecialCharTok{==} \StringTok{"Male"} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}male\textquotesingle{}}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Trouble\textquotesingle{}}
\NormalTok{      ),}
    \AttributeTok{education\_level =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"3rd Grade or less"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Middle School {-} Grades 4 {-} 8"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Completed some high school"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"High school graduate"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Other post high school vocational training"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Some post secondary"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Completed some college, but no degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Some post secondary"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Associate Degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Post secondary or higher"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"College Degree (such as B.A., B.S.)"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Post secondary or higher"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Completed some graduate, but no degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Post secondary or higher"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Masters degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Graduate degree"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \StringTok{"Doctorate degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Graduate degree"}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Trouble\textquotesingle{}}
\NormalTok{      )}
\NormalTok{    ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{education, }\SpecialCharTok{{-}}\NormalTok{age)}

\NormalTok{tests }\OtherTok{\textless{}{-}} 
\NormalTok{  nationscape\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{test =}\NormalTok{ stringr}\SpecialCharTok{::}\FunctionTok{str\_detect}\NormalTok{(age\_group, }\StringTok{\textquotesingle{}Trouble\textquotesingle{}}\NormalTok{),}
         \AttributeTok{test =} \FunctionTok{if\_else}\NormalTok{(test }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{, }
\NormalTok{                        stringr}\SpecialCharTok{::}\FunctionTok{str\_detect}\NormalTok{(education\_level, }\StringTok{\textquotesingle{}Trouble\textquotesingle{}}\NormalTok{)),}
         \AttributeTok{test =} \FunctionTok{if\_else}\NormalTok{(test }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{, }
\NormalTok{                        stringr}\SpecialCharTok{::}\FunctionTok{str\_detect}\NormalTok{(gender, }\StringTok{\textquotesingle{}Trouble\textquotesingle{}}\NormalTok{))}
\NormalTok{         ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(test }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{)}

\ControlFlowTok{if}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(tests) }\SpecialCharTok{!=} \DecValTok{0}\NormalTok{) \{}
  \FunctionTok{print}\NormalTok{(}\StringTok{"Check nationscape\_data"}\NormalTok{)}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
    \FunctionTok{rm}\NormalTok{(tests)}
\NormalTok{    \}}

\NormalTok{nationscape\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  gender state vote_biden age_group      education_level         
  <chr>  <chr>      <dbl> <chr>          <chr>                   
1 female WI             0 age_45-59      Post secondary or higher
2 female VA             0 age_45-59      Post secondary or higher
3 female TX             0 age_60_or_more High school or less     
4 female WA             0 age_45-59      High school or less     
5 female MA             1 age_18-29      Some post secondary     
6 female TX             1 age_30-44      Some post secondary     
\end{verbatim}

As we have seen, one of the most difficult aspects with MRP is ensuring
consistency between the datasets. In this case, we need to do some work
to make the variables consistent.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This code is very directly from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz.}
\CommentTok{\# Format state names so the whole state name is written out, to match IPUMS data}
\NormalTok{states\_names\_and\_abbrevs }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{stateicp =}\NormalTok{ state.name, }\AttributeTok{state =}\NormalTok{ state.abb)}

\NormalTok{nationscape\_data }\OtherTok{\textless{}{-}}
\NormalTok{  nationscape\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{left\_join}\NormalTok{(states\_names\_and\_abbrevs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining, by = "state"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(states\_names\_and\_abbrevs)}

\CommentTok{\# Make lowercase to match IPUMS data}
\NormalTok{nationscape\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  nationscape\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{stateicp =} \FunctionTok{tolower}\NormalTok{(stateicp))}

\CommentTok{\# Replace NAs with DC}
\NormalTok{nationscape\_data}\SpecialCharTok{$}\NormalTok{stateicp }\OtherTok{\textless{}{-}} 
  \FunctionTok{replace\_na}\NormalTok{(nationscape\_data}\SpecialCharTok{$}\NormalTok{stateicp, }\StringTok{"district of columbia"}\NormalTok{)}

\CommentTok{\# Tidy the class}
\NormalTok{nationscape\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  nationscape\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(gender, stateicp, education\_level, age\_group), as\_factor))}

\CommentTok{\# Save data}
\FunctionTok{write\_csv}\NormalTok{(nationscape\_data, }\StringTok{"outputs/data/polling\_data.csv"}\NormalTok{)}

\NormalTok{nationscape\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 6
  gender state vote_biden age_group      education_level          stateicp     
  <fct>  <chr>      <dbl> <fct>          <fct>                    <fct>        
1 female WI             0 age_45-59      Post secondary or higher wisconsin    
2 female VA             0 age_45-59      Post secondary or higher virginia     
3 female TX             0 age_60_or_more High school or less      texas        
4 female WA             0 age_45-59      High school or less      washington   
5 female MA             1 age_18-29      Some post secondary      massachusetts
6 female TX             1 age_30-44      Some post secondary      texas        
\end{verbatim}

\hypertarget{post-stratification-data}{%
\subsection{Post-stratification data}\label{post-stratification-data}}

We have a lot of options for a dataset to post-stratify by and there are
various considerations. We are after a dataset that is better quality
(however that is to be defined), and likely larger. From a strictly data
perspective, the best choice would probably be something like the
Cooperative Congressional Election Study (CCES), however for whatever
reason that is only released after the election and so it is not a
reasonable choice. W. Wang et al. (2015) use exit poll data, but again
that is only available after the election.

In most countries we'd be stuck using the census, which is of course
quite large, but likely out-of-date. Luckily in the US we have the
opportunity to use the American Community Survey (ACS) which asks
analogous questions to a census, is conducted every month, and over the
course of a year, we end up with a few million responses. In this case
we are going to access the ACS through IPUMS.

To do this go to the IPUMS website - https://ipums.org - and we are
looking for something like IPUMS USA and then `get data'. Create an
account, if you need to. That'll take a while to process. But once you
have an account, go to `Select Samples' and de-select everything apart
from the 2019 ACS. Then we need to get the variables that we are
interested in. From the household we want `STATEICP', then in person we
want `SEX', `AGE', `EDUC'. Once everything is selected, `view cart', and
we want to be careful to change the `data format' to `.dta' (there's
nothing wrong with the other formats, but we have just already got code
earlier to deal with that type). Briefly just check how many rows and
columns you are requesting. It should be around a million rows, and
around ten to twenty columns. If it is much more than 300MB then maybe
just see if you have accidentally selected something that you don't
need. Submit the request and within a day, you should get an email
saying that your data can be downloaded. It should only take 30 minutes
or so, but if you don't get an email within a day then check again the
size of the dataset, and customize the sample size to reduce the size
initially.

In any case let us tidy up the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Again, closely following code from Alen Mitrovski, Xiaoyan Yang, and Matthew Wankiewicz.}
\FunctionTok{library}\NormalTok{(haven)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{raw\_poststrat\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_dta}\NormalTok{(here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{(}\StringTok{"dont\_push/usa\_00004.dta"}\NormalTok{))}

\CommentTok{\# The Stata format separates labels so reunite those}
\NormalTok{raw\_poststrat\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  labelled}\SpecialCharTok{::}\FunctionTok{to\_factor}\NormalTok{(raw\_poststrat\_data)}
\FunctionTok{head}\NormalTok{(raw\_poststrat\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 28
  year  sample serial cbserial  hhwt cluster region stateicp strata gq    pernum
  <fct> <fct>   <dbl>    <dbl> <dbl>   <dbl> <fct>  <fct>     <dbl> <fct>  <dbl>
1 2018  2018 ~      2  2.02e12 392.  2.02e12 east ~ alabama  190001 othe~      1
2 2018  2018 ~      7  2.02e12  94.1 2.02e12 east ~ alabama   40001 grou~      1
3 2018  2018 ~     13  2.02e12  83.7 2.02e12 east ~ alabama  130301 othe~      1
4 2018  2018 ~     18  2.02e12  57.5 2.02e12 east ~ alabama  100001 grou~      1
5 2018  2018 ~     23  2.02e12 157.  2.02e12 east ~ alabama  190001 grou~      1
6 2018  2018 ~     28  2.02e12 157.  2.02e12 east ~ alabama  220001 othe~      1
# ... with 17 more variables: perwt <dbl>, sex <fct>, age <fct>, marst <fct>,
#   race <fct>, raced <fct>, hispan <fct>, hispand <fct>, bpl <fct>,
#   bpld <fct>, citizen <fct>, educ <fct>, educd <fct>, empstat <fct>,
#   empstatd <fct>, labforce <fct>, inctot <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw\_poststrat\_data}\SpecialCharTok{$}\NormalTok{age }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(raw\_poststrat\_data}\SpecialCharTok{$}\NormalTok{age)}

\NormalTok{poststrat\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  raw\_poststrat\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(inctot }\SpecialCharTok{\textless{}} \DecValTok{9999999}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(age }\SpecialCharTok{\textgreater{}=} \DecValTok{18}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{gender =}\NormalTok{ sex) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{age\_group =} \FunctionTok{case\_when}\NormalTok{( }\CommentTok{\# case\_when works in order and exits when there\textquotesingle{}s a match}
\NormalTok{      age }\SpecialCharTok{\textless{}=} \DecValTok{29} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_18{-}29\textquotesingle{}}\NormalTok{,}
\NormalTok{      age }\SpecialCharTok{\textless{}=} \DecValTok{44} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_30{-}44\textquotesingle{}}\NormalTok{,}
\NormalTok{      age }\SpecialCharTok{\textless{}=} \DecValTok{59} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_45{-}59\textquotesingle{}}\NormalTok{,}
\NormalTok{      age }\SpecialCharTok{\textgreater{}=} \DecValTok{60} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}age\_60\_or\_more\textquotesingle{}}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Trouble\textquotesingle{}}
\NormalTok{      ),}
    \AttributeTok{education\_level =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"nursery school, preschool"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"kindergarten"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 1"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 2"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 3"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 4"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 5"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 6"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 7"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 8"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 9"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 10"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"grade 11"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"12th grade, no diploma"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"regular high school diploma"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"ged or alternative credential"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"some college, but less than 1 year"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Some post secondary"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"1 or more years of college credit, no degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Some post secondary"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"associate\textquotesingle{}s degree, type not specified"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Post secondary or higher"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"bachelor\textquotesingle{}s degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Post secondary or higher"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"master\textquotesingle{}s degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Graduate degree"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"professional degree beyond a bachelor\textquotesingle{}s degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Graduate degree"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"doctoral degree"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Graduate degree"}\NormalTok{,}
\NormalTok{      educd }\SpecialCharTok{==} \StringTok{"no schooling completed"} \SpecialCharTok{\textasciitilde{}} \StringTok{"High school or less"}\NormalTok{,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{\textquotesingle{}Trouble\textquotesingle{}}
\NormalTok{      )}
\NormalTok{    )}

\CommentTok{\# Just keep relevant variables}
\NormalTok{poststrat\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  poststrat\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(gender,}
\NormalTok{         age\_group,}
\NormalTok{         education\_level,}
\NormalTok{         stateicp)}

\CommentTok{\# Tidy the class}
\NormalTok{poststrat\_data }\OtherTok{\textless{}{-}} 
\NormalTok{  poststrat\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(gender, stateicp, education\_level, age\_group), as\_factor))}

\CommentTok{\# Save data}
\FunctionTok{write\_csv}\NormalTok{(poststrat\_data, }\StringTok{"outputs/data/us\_poststrat.csv"}\NormalTok{)}

\NormalTok{poststrat\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
  gender age_group      education_level     stateicp
  <fct>  <fct>          <fct>               <fct>   
1 female age_18-29      Some post secondary alabama 
2 female age_60_or_more Some post secondary alabama 
3 male   age_45-59      Some post secondary alabama 
4 male   age_30-44      High school or less alabama 
5 female age_60_or_more High school or less alabama 
6 male   age_30-44      High school or less alabama 
\end{verbatim}

This dataset is on an individual level. So we'll create counts of each
sub-cell, and then proportions by state.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poststrat\_data\_cells }\OtherTok{\textless{}{-}} 
\NormalTok{  poststrat\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(stateicp, gender, age\_group, education\_level) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now we would like to add proportions by state.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poststrat\_data\_cells }\OtherTok{\textless{}{-}} 
\NormalTok{  poststrat\_data\_cells }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(stateicp) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop =}\NormalTok{ n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{()}

\NormalTok{poststrat\_data\_cells }\SpecialCharTok{|\textgreater{}} \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 6
  stateicp    gender age_group      education_level              n    prop
  <fct>       <fct>  <fct>          <fct>                    <int>   <dbl>
1 connecticut male   age_18-29      Some post secondary        149 0.0260 
2 connecticut male   age_18-29      High school or less        232 0.0404 
3 connecticut male   age_18-29      Post secondary or higher    96 0.0167 
4 connecticut male   age_18-29      Graduate degree             25 0.00436
5 connecticut male   age_60_or_more Some post secondary        142 0.0248 
6 connecticut male   age_60_or_more High school or less        371 0.0647 
\end{verbatim}

\hypertarget{model-2}{%
\subsection{Model}\label{model-2}}

We are going to use logistic regression to estimate a model where the
binary of support for Biden is explained by gender, age-group,
education-level, and state. We are going to do this in a Bayesian
framework using \texttt{rstanarm} (Goodrich et al. 2020). There are a
variety of reasons for using \texttt{rstanarm} here, but the main one is
that Stan is pre-compiled which eases some of the computer set-up issues
that we may otherwise have. A great further resource about implementing
MRP with \texttt{rstanarm} is Kennedy and Gabry (2020).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rstanarm)}

\NormalTok{us\_election\_model }\OtherTok{\textless{}{-}} 
\NormalTok{  rstanarm}\SpecialCharTok{::}\FunctionTok{stan\_glmer}\NormalTok{(vote\_biden }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ age\_group }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ stateicp) }\SpecialCharTok{+}\NormalTok{ education\_level,}
                       \AttributeTok{data =}\NormalTok{ nationscape\_data,}
                       \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{),}
                       \AttributeTok{prior =} \FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                       \AttributeTok{prior\_intercept =} \FunctionTok{normal}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                       \AttributeTok{cores =} \DecValTok{2}\NormalTok{, }
                       \AttributeTok{seed =} \DecValTok{853}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There are a variety of options here that we have largely unthinkingly
set, and exploring the effect of these would be a good idea, but for now
we can just have a quick look at the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelsummary}\SpecialCharTok{::}\FunctionTok{get\_estimates}\NormalTok{(us\_election\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                                     term effect    estimate conf.level
1                             (Intercept)  fixed  0.27591651       0.95
2                              gendermale  fixed -0.54094841       0.95
3                 age_groupage_60_or_more  fixed  0.04886803       0.95
4                      age_groupage_18-29  fixed  0.87817445       0.95
5                      age_groupage_30-44  fixed  0.12455081       0.95
6      education_levelHigh school or less  fixed -0.35080948       0.95
7      education_levelSome post secondary  fixed -0.14970941       0.95
8          education_levelGraduate degree  fixed -0.21988079       0.95
9 Sigma[stateicp:(Intercept),(Intercept)] random  0.08002654       0.95
     conf.low    conf.high      pd rope.percentage      rhat      ess
1  0.10322566  0.447878764 0.99850       0.1281242 1.0002399 2262.704
2 -0.65227570 -0.430121742 1.00000       0.0000000 0.9996776 5264.121
3 -0.10847716  0.200831286 0.72600       0.9765851 0.9998468 3726.472
4  0.69830955  1.061899702 1.00000       0.0000000 0.9997188 3931.709
5 -0.02530517  0.284994549 0.94125       0.7729545 0.9995061 3567.152
6 -0.51262507 -0.204222132 1.00000       0.0000000 0.9997706 3700.321
7 -0.29927232  0.006135831 0.96625       0.6771902 1.0005471 4090.628
8 -0.38172115 -0.036764485 0.99100       0.3249145 0.9999286 4589.040
9  0.02912381  0.160211514 1.00000       1.0000000 1.0031942 1284.436
  prior.distribution prior.location prior.scale
1             normal              0           1
2             normal              0           1
3             normal              0           1
4             normal              0           1
5             normal              0           1
6             normal              0           1
7             normal              0           1
8             normal              0           1
9               <NA>             NA          NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The default usage of modelsummary requires statistics that we don\textquotesingle{}t have.}
\CommentTok{\# Uncomment the following line if you want to look at what is available and specify your own:}
\CommentTok{\# modelsummary::get\_estimates(us\_election\_model)}
\NormalTok{modelsummary}\SpecialCharTok{::}\FunctionTok{modelsummary}\NormalTok{(us\_election\_model,}
                            \AttributeTok{statistic =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}conf.low\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}conf.high\textquotesingle{}}\NormalTok{)}
\NormalTok{                            )}
\end{Highlighting}
\end{Shaded}

\begin{table}
\centering
\begin{tabular}[t]{lc}
\toprule
  & Model 1\\
\midrule
(Intercept) & \num{0.276}\\
 & (\num{0.103})\\
 & (\num{0.448})\\
gendermale & \num{-0.541}\\
 & (\num{-0.652})\\
 & (\num{-0.430})\\
age\_groupage\_60\_or\_more & \num{0.049}\\
 & (\num{-0.108})\\
 & (\num{0.201})\\
age\_groupage\_18-29 & \num{0.878}\\
 & (\num{0.698})\\
 & (\num{1.062})\\
age\_groupage\_30-44 & \num{0.125}\\
 & (\num{-0.025})\\
 & (\num{0.285})\\
education\_levelHigh school or less & \num{-0.351}\\
 & (\num{-0.513})\\
 & (\num{-0.204})\\
education\_levelSome post secondary & \num{-0.150}\\
 & (\num{-0.299})\\
 & (\num{0.006})\\
education\_levelGraduate degree & \num{-0.220}\\
 & (\num{-0.382})\\
 & (\num{-0.037})\\
Sigma[stateicp × (Intercept),(Intercept)] & \num{0.080}\\
 & (\num{0.029})\\
 & (\num{0.160})\\
\midrule
Num.Obs. & \num{5200}\\
R2 & \num{0.057}\\
R2 Marg. & \num{0.045}\\
ELPD & \num{-3468.0}\\
ELPD s.e. & \num{16.4}\\
LOOIC & \num{6936.1}\\
LOOIC s.e. & \num{32.8}\\
WAIC & \num{6936.0}\\
RMSE & \num{0.48}\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{post-stratify-2}{%
\subsection{Post-stratify}\label{post-stratify-2}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{biden\_support\_by\_state }\OtherTok{\textless{}{-}} 
\NormalTok{  us\_election\_model }\SpecialCharTok{|\textgreater{}}
\NormalTok{  tidybayes}\SpecialCharTok{::}\FunctionTok{add\_fitted\_draws}\NormalTok{(}\AttributeTok{newdata=}\NormalTok{poststrat\_data\_cells) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{support\_biden\_predict =}\NormalTok{ .value) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{support\_biden\_predict\_prop =}\NormalTok{ support\_biden\_predict}\SpecialCharTok{*}\NormalTok{prop) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(stateicp, .draw) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{support\_biden\_predict =} \FunctionTok{sum}\NormalTok{(support\_biden\_predict\_prop)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(stateicp) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(support\_biden\_predict), }
            \AttributeTok{lower =} \FunctionTok{quantile}\NormalTok{(support\_biden\_predict, }\FloatTok{0.025}\NormalTok{), }
            \AttributeTok{upper =} \FunctionTok{quantile}\NormalTok{(support\_biden\_predict, }\FloatTok{0.975}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.
Use [add_]epred_draws() to get the expectation of the posterior predictive.
Use [add_]linpred_draws() to get the distribution of the linear predictor.
For example, you used [add_]fitted_draws(..., scale = "response"), which
means you most likely want [add_]epred_draws(...).
\end{verbatim}

\begin{verbatim}
`summarise()` has grouped output by 'stateicp'. You can override using the
`.groups` argument.
\end{verbatim}

And we can have a look at our estimates, if we like.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{biden\_support\_by\_state }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ mean, }\AttributeTok{x =}\NormalTok{ stateicp, }\AttributeTok{color =} \StringTok{"MRP estimate"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ lower, }\AttributeTok{ymax =}\NormalTok{ upper), }\AttributeTok{width =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =} 
\NormalTok{               nationscape\_data }\SpecialCharTok{|\textgreater{}} 
               \FunctionTok{group\_by}\NormalTok{(stateicp, vote\_biden) }\SpecialCharTok{|\textgreater{}}
               \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{|\textgreater{}} 
               \FunctionTok{group\_by}\NormalTok{(stateicp) }\SpecialCharTok{|\textgreater{}} 
               \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop =}\NormalTok{ n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n)) }\SpecialCharTok{|\textgreater{}} 
               \FunctionTok{filter}\NormalTok{(vote\_biden}\SpecialCharTok{==}\DecValTok{1}\NormalTok{), }
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ prop, }\AttributeTok{x =}\NormalTok{ stateicp, }\AttributeTok{color =} \StringTok{\textquotesingle{}Nationscape raw data\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{linetype =} \StringTok{\textquotesingle{}dashed\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}State\textquotesingle{}}\NormalTok{,}
       \AttributeTok{y =} \StringTok{\textquotesingle{}Estimated proportion support for Biden\textquotesingle{}}\NormalTok{,}
       \AttributeTok{color =} \StringTok{\textquotesingle{}Source\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{\textquotesingle{}Set1\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`summarise()` has grouped output by 'stateicp'. You can override using the
`.groups` argument.
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./16-mrp_files/figure-pdf/unnamed-chunk-61-1.pdf}

}

\end{figure}

\hypertarget{exercises-and-tutorial-15}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-15}}

\hypertarget{exercises-15}{%
\subsection{Exercises}\label{exercises-15}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Please explain what MRP is to someone who has university-education,
  but who has not necessarily taken any statistics, so you will need to
  explain any technical terms that you use, and be clear about strengths
  and weaknesses (write at least three paragraphs).
\item
  With respect to W. Wang et al. (2015): Why is this paper interesting?
  What do you like about this paper? What do you wish it did better? To
  what extent can you reproduce this paper? (Write at least four
  paragraphs).
\item
  With respect to W. Wang et al. (2015), what is not a feature they
  mention election forecasts need?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Explainable.
  \item
    Accurate.
  \item
    Cost-effective.
  \item
    Relevant.
  \item
    Timely.
  \end{enumerate}
\item
  With respect to W. Wang et al. (2015), what is a weakness of MRP?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Detailed data requirement.
  \item
    Allows use of biased data.
  \item
    Expensive to conduct.
  \end{enumerate}
\item
  With respect to W. Wang et al. (2015), what is concerning about the
  Xbox sample?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Non-representative.
  \item
    Small sample size.
  \item
    Multiple responses from the same respondent.
  \end{enumerate}
\item
  We are interested in studying how voting intentions in the 2020 US
  presidential election vary by an individual's income. We set up a
  logistic regression model to study this relationship. In this study,
  what are some possible independent variables (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Whether the respondent is registered to vote (yes/no).
  \item
    Whether the respondent is going to vote for Biden (yes/no).
  \item
    The race of the respondent (white/not white).
  \item
    The respondent's marital status (married/not).
  \end{enumerate}
\item
  Please think about Cohn (2016) Why is this type of exercise not
  carried out more? Why do you think that different groups, even with
  the same background and level of quantitative sophistication, could
  have such different estimates even when they use the same data? (Write
  at least four paragraphs).
\item
  When we think about multilevel regression with post-stratification,
  what are the key assumptions that we are making? (Write at least four
  paragraphs).
\item
  I train a model based on a survey, and then post-stratify it using the
  2020 ACS dataset. What are some of the practical considerations that I
  may have to contend when I am doing this? (Write at least four
  paragraphs).
\item
  If I have a model output from \texttt{lm()} called `my\_model\_output'
  how can I use \texttt{modelsummary} to display the output (assume the
  package has been loaded) (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{modelsummary::modelsummary(my\_model\_output)}
  \item
    \texttt{modelsummary(my\_model\_output)}
  \item
    \texttt{my\_model\_output\ \textbar{}\textgreater{}\ modelsummary()}
  \item
    \texttt{my\_model\_output\ \textbar{}\textgreater{}\ modelsummary(statistic\ =\ NULL)}
  \end{enumerate}
\item
  Which of the following are examples of linear models (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \texttt{lm(y\ \textasciitilde{}\ x\_1\ +\ x\_2\ +\ x\_3,\ data\ =\ my\_data)}
  \item
    \texttt{lm(y\ \textasciitilde{}\ x\_1\ +\ x\_2\^{}2\ +\ x\_3,\ data\ =\ my\_data)}
  \item
    \texttt{lm(y\ \textasciitilde{}\ x\_1\ *\ x\_2\ +\ x\_3,\ data\ =\ my\_data)}
  \item
    \texttt{lm(y\ \textasciitilde{}\ x\_1\ +\ x\_1\^{}2\ +\ x\_2\ +\ x\_3,\ data\ =\ my\_data)}
  \end{enumerate}
\item
  Consider a situation in which you have a survey dataset with these
  age-groups: 18-29; 30-44; 45- 60; and 60+. And a post-stratification
  dataset with these age-groups: 18-24; 25-29; 30-34; 35-39; 40-44;
  45-49; 50-54; 55-59; and 60+. What approach would you take to bringing
  these together? {[}Please write a paragraph.{]}
\item
  Consider a situation in which you again have a survey dataset with
  these age-groups: 18-29; 30-44; 45- 60; and 60+. But this time the
  post-stratification dataset has these age-groups: 18-34; 35-49; 50-64;
  and 65+. What approach would you take to bringing these together?
  (Write at least one paragraph).
\item
  Please consider Kennedy et al. (2020). What are some statistical
  facets when considering a survey focused on gender, with a
  post-stratification survey that is not (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Impute all non-male as female
  \item
    Estimate gender using auxiliary information
  \item
    Impute population
  \item
    Impute sample values
  \item
    Model population distribution using auxiliary data
  \item
    Remove all non-binary respondents
  \item
    Remove respondents
  \item
    Assume population distribution
  \end{enumerate}
\item
  Please consider Kennedy et al. (2020). What are some ethical facets
  when considering a survey focused on gender, with a
  post-stratification survey that is not (select all)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Impute all non-male as female
  \item
    Estimate gender using auxiliary information
  \item
    Impute population
  \item
    Impute sample values
  \item
    Model population distribution using auxiliary data
  \item
    Remove all non-binary respondents
  \item
    Remove respondents
  \item
    Assume population distribution
  \end{enumerate}
\item
  Please consider Kennedy et al. (2020). How do they define ethics?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Respecting the perspectives and dignity of individual survey
    respondents.
  \item
    Generating estimates of the general population and for
    subpopulations of interest.
  \item
    Using more complicated procedures only when they serve some useful
    function.
  \end{enumerate}
\end{enumerate}

\hypertarget{tutorial-15}{%
\subsection{Tutorial}\label{tutorial-15}}

In a similar manner to Ghitza and Gelman (2020) pretend you have got
access to a US voter file record from a private company. You train a
model on the 2020 US CCES, and post-stratify it, on an individual-basis,
based on that voter file. a. Could you please put-together a datasheet
for the voter file dataset following Gebru et al. (2021)? As a reminder,
datasheets accompany datasets and document `motivation, composition,
collection process, recommended uses,' among other aspects. b. Could you
also please put together a model card for your model, following Mitchell
et al. (2019)? As a reminder, model cards are deliberately
straight-forward one- or two-page documents that report aspects such as:
model details; intended use; metrics; training data; ethical
considerations; as well as caveats and recommendations (Mitchell et al.
2019). c.~Could you please discuss three ethical aspects around the
features that you are using in your model? {[}Please write a paragraph
or two for each point.{]} d.~Could you please detail the protections
that you would put in place in terms of the dataset, the model, and the
predictions?

\hypertarget{paper-4}{%
\subsection{Paper}\label{paper-4}}

At about this point, the Paper Five (Appendix @ref(paper-five)) would be
appropriate.

\hypertarget{sec-text-as-data}{%
\chapter{Text as data}\label{sec-text-as-data}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Read \emph{The Naked Truth: How the names of 6,816 complexion products
  can reveal bias in beauty}, (Amaka and Thomas 2021).
\item
  Read \emph{Supervised Machine Learning for Text Analysis in R},
  Chapters 2 `Tokenization', 3 `Stop words', 6 `Regression', and 7
  `Classification', (Hvitfeldt and Silge 2021).
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Understanding text as a dataset that we can use.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{gutenbergr} (D. Robinson 2021)
\item
  \texttt{janitor} (Firke 2020).
\item
  \texttt{tidytext} (Silge and Robinson 2016).
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{glmnet::glmnet()}
\item
  \texttt{gutenbergr::gutenberg\_download()}
\item
  \texttt{tidytext::bind\_tf\_idf()}
\item
  \texttt{tidytext::cast\_dtm()}
\end{itemize}

\hypertarget{introduction-14}{%
\section{Introduction}\label{introduction-14}}

Text can be considered an unwieldy, but in general similar, version of
the datasets that we have used throughout this book. The main difference
is that we will typically begin with very wide data, insofar as often
each column is a word, or token more generally. Each entry is then often
a count. We would then typically transform this into rather long data,
with a column of the word and another column of the count.

The larger size of text datasets means that it is especially important
to simulate, and start small, when it comes to their analysis. Using
text as data is exciting because of the quantity and variety of text
that is available to us. In general, dealing with text datasets is
messy. There is a lot of cleaning and preparation that is typically
required. Often text datasets are large. As such, having a workflow in
place, in which you work in a reproducible way, simulating data first,
and then clearly communicating your findings becomes critical, if only
to keep everything organized in your own mind. Nonetheless, it is an
exciting area.

In terms of next steps there are two, related, concerns: data and
analysis.

In terms of data there are many places to get large amounts of text data
relatively easily, from sources that we have already used, including:

\begin{itemize}
\tightlist
\item
  Accessing the Twitter API using \texttt{rtweet} (Kearney 2019).
\item
  Using the Inside Airbnb, which provides text from reviews.
\item
  Getting the text of out-of-copyright books using \texttt{gutenbergr}
  (D. Robinson 2021).
\item
  And finally, scraping Wikipedia.
\end{itemize}

In this chapter we first consider preparing text datasets. We then
consider logistic and lasso regression. We finally consider topic
models.

\hypertarget{tf-idf}{%
\section{TF-IDF}\label{tf-idf}}

Inspired by Gelfand (2019) and following Amaka and Thomas (2021), we
will draw on the dataset they put together of makeup names and
descriptions from Sephora and Ulta. We are interested in the counts of
each word. We can read in the data using \texttt{read\_csv()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{makeup }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} 
             \StringTok{"https://raw.githubusercontent.com/the{-}pudding/data/master/foundation{-}names/allNumbers.csv"}\NormalTok{)}

\NormalTok{makeup}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3,117 x 9
   brand        product name  specific lightness hex   lightToDark numbers    id
   <chr>        <chr>   <chr> <chr>        <dbl> <chr> <lgl>         <dbl> <dbl>
 1 Makeup Revo~ Concea~ <NA>  F0           0.949 #F2F~ TRUE            0       1
 2 HOURGLASS    Veil F~ Porc~ No. 0        0.818 #F6D~ TRUE            0       2
 3 TOM FORD     Tracel~ Pearl 0.0          0.851 #F0D~ TRUE            0       3
 4 Armani Beau~ Neo Nu~ <NA>  0            0.912 #F0E~ TRUE            0       4
 5 TOM FORD     Tracel~ Pearl 0.0          0.912 #FDE~ TRUE            0       5
 6 Charlotte T~ Magic ~ <NA>  0            0.731 #D9A~ TRUE            0       6
 7 Bobbi Brown  Skin W~ Porc~ 0            0.822 #F3C~ TRUE            0       7
 8 Givenchy     Matiss~ <NA>  N00          0.831 #F5D~ TRUE            0       8
 9 Smashbox     Studio~ <NA>  0.1          0.814 #F8C~ TRUE            0.1     9
10 Smashbox     Studio~ <NA>  0.1          0.910 #F9E~ TRUE            0.1    10
# ... with 3,107 more rows
\end{verbatim}

We will focus on `product', which provides the name of the item, and
`lightness' which is a value between 0 and 1. We are interested in
whether products with lightness values that are less than 0.5, typically
use different words to those with lightness values that are at least
0.5.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{makeup }\OtherTok{\textless{}{-}}
\NormalTok{  makeup }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(product, lightness) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lightness\_above\_half =} \FunctionTok{if\_else}\NormalTok{(lightness }\SpecialCharTok{\textgreater{}=} \FloatTok{0.5}\NormalTok{, }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)}
\NormalTok{         )}

\FunctionTok{table}\NormalTok{(makeup}\SpecialCharTok{$}\NormalTok{lightness\_above\_half)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

  No  Yes 
 702 2415 
\end{verbatim}

In this example we are going to split everything into separate words.
When we do this, it is just searching for a space. And so, it will be
the case that more than just words will be considered `words', for
instance, numbers. We use \texttt{unnest\_tokens()} from
\texttt{tidytext} (Silge and Robinson 2016) to do this.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidytext)}

\NormalTok{makeup\_by\_words }\OtherTok{\textless{}{-}}
\NormalTok{  makeup }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =}\NormalTok{ word, }
                \AttributeTok{input =}\NormalTok{ product, }
                \AttributeTok{token =} \StringTok{"words"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(makeup\_by\_words)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  lightness lightness_above_half word      
      <dbl> <chr>                <chr>     
1     0.949 Yes                  conceal   
2     0.949 Yes                  define    
3     0.949 Yes                  full      
4     0.949 Yes                  coverage  
5     0.949 Yes                  foundation
6     0.818 Yes                  veil      
\end{verbatim}

We now want to count the number of times each word is used by each of
the lightness classifications.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{makeup\_by\_words }\OtherTok{\textless{}{-}}
\NormalTok{  makeup\_by\_words }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(lightness\_above\_half, word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{makeup\_by\_words }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(lightness\_above\_half }\SpecialCharTok{==} \StringTok{"Yes"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 3
  lightness_above_half word           n
  <chr>                <chr>      <int>
1 Yes                  foundation  2214
2 Yes                  skin         452
3 Yes                  spf          443
4 Yes                  matte        422
5 Yes                  powder       327
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{makeup\_by\_words }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(lightness\_above\_half }\SpecialCharTok{==} \StringTok{"No"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 3
  lightness_above_half word           n
  <chr>                <chr>      <int>
1 No                   foundation   674
2 No                   matte        106
3 No                   spf          103
4 No                   skin          98
5 No                   liquid        89
\end{verbatim}

We can see that the most popular words appear to be similar between the
two categories. At this point, we could use the data in a variety of
ways. We might be interested to know which words characterize each
group---that is to say, which words are commonly used only in each
group. We can do that by first looking at a word's term frequency (tf),
which is how many times a word is used in the product name. The issue is
that there are a lot of words that are commonly used regardless of
context. As such, we may also like to look at the inverse document
frequency (idf) in which we `penalize' words that occur in both groups.
For instance, we have seen that `foundation' occurs in both products
with high and low lightness values. And so, its idf would be lower than
another word which only occurred in products that did not have a
lightness value above half. The term frequency--inverse document
frequency (tf-idf) is then the product of these.

We can create this value using \texttt{bind\_tf\_idf()} from
\texttt{tidytext}. It will create a bunch of new columns, one for each
word and star combination.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{makeup\_by\_words\_tf\_idf }\OtherTok{\textless{}{-}}
\NormalTok{  makeup\_by\_words }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{bind\_tf\_idf}\NormalTok{(}\AttributeTok{term =}\NormalTok{ word, }
              \AttributeTok{document =}\NormalTok{ lightness\_above\_half, }
              \AttributeTok{n =}\NormalTok{ n) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{tf\_idf)}

\NormalTok{makeup\_by\_words\_tf\_idf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 505 x 6
   lightness_above_half word        n       tf   idf   tf_idf
   <chr>                <chr>   <int>    <dbl> <dbl>    <dbl>
 1 Yes                  cushion    25 0.00187  0.693 0.00130 
 2 Yes                  combo      16 0.00120  0.693 0.000831
 3 Yes                  custom     16 0.00120  0.693 0.000831
 4 Yes                  oily       16 0.00120  0.693 0.000831
 5 Yes                  perfect    16 0.00120  0.693 0.000831
 6 Yes                  refill     16 0.00120  0.693 0.000831
 7 Yes                  50         14 0.00105  0.693 0.000727
 8 Yes                  compact    13 0.000974 0.693 0.000675
 9 Yes                  lifting    12 0.000899 0.693 0.000623
10 Yes                  satte      12 0.000899 0.693 0.000623
# ... with 495 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{makeup\_by\_words\_tf\_idf }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(lightness\_above\_half) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 6
# Groups:   lightness_above_half [2]
   lightness_above_half word            n       tf   idf   tf_idf
   <chr>                <chr>       <int>    <dbl> <dbl>    <dbl>
 1 No                   able            2 0.000525 0.693 0.000364
 2 No                   concentrate     2 0.000525 0.693 0.000364
 3 No                   marc            2 0.000525 0.693 0.000364
 4 No                   re              2 0.000525 0.693 0.000364
 5 No                   look            1 0.000263 0.693 0.000182
 6 Yes                  cushion        25 0.00187  0.693 0.00130 
 7 Yes                  combo          16 0.00120  0.693 0.000831
 8 Yes                  custom         16 0.00120  0.693 0.000831
 9 Yes                  oily           16 0.00120  0.693 0.000831
10 Yes                  perfect        16 0.00120  0.693 0.000831
\end{verbatim}

\hypertarget{lasso-regression}{%
\section{Lasso regression}\label{lasso-regression}}

One of the nice aspects of text is that we can adapt our existing
methods to use it as an input. Here we are going to use logistic
regression, along with text inputs, to forecast. Inspired by Silge
(2018) we are going to have two different text inputs, train a model on
a sample of text from each of them, and then try to use that model to
forecast the text in a training set. Although this is a arbitrary
example, we could imagine many real-world applications. For instance, we
may be interested in whether some text was likely written by a bot or a
human

First we need to get some data. We use books from Project Gutenberg
using \texttt{gutenberg\_download()} from \texttt{gutenbergr} (D.
Robinson 2021). We will consider \emph{Jane Eyre} (Bronte 1847) and
\emph{Alice's Adventures in Wonderland} (Carroll 1865).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gutenbergr)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# The books that we are interested in have the keys of 1260 and 11, respectively.}
\NormalTok{alice\_and\_jane }\OtherTok{\textless{}{-}} 
  \FunctionTok{gutenberg\_download}\NormalTok{(}
    \AttributeTok{gutenberg\_id =} \FunctionTok{c}\NormalTok{(}\DecValTok{1260}\NormalTok{, }\DecValTok{11}\NormalTok{), }
    \AttributeTok{meta\_fields =} \StringTok{"title"}\NormalTok{)}

\FunctionTok{write\_csv}\NormalTok{(alice\_and\_jane, }\StringTok{"alice\_and\_jane.csv"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(alice\_and\_jane)}
\end{Highlighting}
\end{Shaded}

One of the great things about this is that the dataset is a tibble. So
we can work with all our familiar skills. Each line of the book is read
in as a different row in the dataset. Notice that we have downloaded two
books here at once, and so we added the title. The two books are one
after each other.

By looking at the number of lines in each, it looks like \emph{Jane
Eyre} is much longer than \emph{Alice in Wonderland}. We will start by
getting rid of blank lines using \texttt{remove\_empty()} from
\texttt{janitor} (Firke 2020).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(janitor)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Attaching package: 'janitor'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:stats':

    chisq.test, fisher.test
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alice\_and\_jane }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{blank\_line =} \FunctionTok{if\_else}\NormalTok{(text }\SpecialCharTok{==} \StringTok{""}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(blank\_line }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{blank\_line)}

\FunctionTok{table}\NormalTok{(alice\_and\_jane}\SpecialCharTok{$}\NormalTok{title)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Alice's Adventures in Wonderland      Jane Eyre: An Autobiography 
                            2481                            16395 
\end{verbatim}

There is still an overwhelming amount of \emph{Jane Eyre}, compared with
\emph{Alice in Wonderland}, so we will sample from \emph{Jane Eyre} to
make it more equal.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{alice\_and\_jane}\SpecialCharTok{$}\NormalTok{rows }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(alice\_and\_jane))}
\NormalTok{sample\_from\_me }\OtherTok{\textless{}{-}}\NormalTok{ alice\_and\_jane }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(title }\SpecialCharTok{==} \StringTok{"Jane Eyre: An Autobiography"}\NormalTok{)}
\NormalTok{keep\_me }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sample\_from\_me}\SpecialCharTok{$}\NormalTok{rows, }\AttributeTok{size =} \DecValTok{2481}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}

\NormalTok{alice\_and\_jane }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(title }\SpecialCharTok{==} \StringTok{"Alice\textquotesingle{}s Adventures in Wonderland"} \SpecialCharTok{|}\NormalTok{ rows }\SpecialCharTok{\%in\%}\NormalTok{ keep\_me) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{rows)}

\FunctionTok{table}\NormalTok{(alice\_and\_jane}\SpecialCharTok{$}\NormalTok{title)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Alice's Adventures in Wonderland      Jane Eyre: An Autobiography 
                            2481                             2481 
\end{verbatim}

There are a variety of issues here, for instance, we have the whole of
Alice, but we only have random bits of Jane, but nonetheless we will
continue and add a counter with the line number for each book.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alice\_and\_jane }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(title) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{line\_number =} \FunctionTok{paste}\NormalTok{(gutenberg\_id, }\FunctionTok{row\_number}\NormalTok{(), }\AttributeTok{sep =} \StringTok{"\_"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We now want to unnest the tokes. We will use \texttt{unnest\_tokens()}
from \texttt{tidytext} (Silge and Robinson 2016).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidytext)}

\NormalTok{alice\_and\_jane\_by\_word }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(word) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{n}\NormalTok{() }\SpecialCharTok{\textgreater{}} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We remove any word that was not used more than 10 times. Nonetheless we
still have more than 500 unique words. (If we did not require that the
word be used by the author at least 10 times then we end up with more
than 6,000 words.)

The reason this is relevant is because these are our independent
variables. So where we may be used to having something less than 10
explanatory variables, in this case we are going to have 585 As such, we
need a model that can handle this.

However, as mentioned before, we are going to have some rows that
essentially just had one word. And so we filter for that also, which
ensures that the model will have at least some words to work with.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alice\_and\_jane\_by\_word }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane\_by\_word }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(title, line\_number) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{number\_of\_words\_in\_line =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(number\_of\_words\_in\_line }\SpecialCharTok{\textgreater{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{number\_of\_words\_in\_line)}
\end{Highlighting}
\end{Shaded}

We'll create a test/training split, and load in \texttt{tidymodels}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidymodels)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{alice\_and\_jane\_by\_word\_split }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane\_by\_word }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(title, line\_number) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{distinct}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{initial\_split}\NormalTok{(}\AttributeTok{prop =} \DecValTok{3}\SpecialCharTok{/}\DecValTok{4}\NormalTok{, }\AttributeTok{strata =}\NormalTok{ title)}
\end{Highlighting}
\end{Shaded}

Then we can use \texttt{cast\_dtm()} to create a document-term matrix.
This provides a count of how many times each word is used.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alice\_and\_jane\_dtm\_training }\OtherTok{\textless{}{-}} 
\NormalTok{  alice\_and\_jane\_by\_word }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(line\_number, word) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{inner\_join}\NormalTok{(}\FunctionTok{training}\NormalTok{(alice\_and\_jane\_by\_word\_split) }\SpecialCharTok{|\textgreater{}} \FunctionTok{select}\NormalTok{(line\_number)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{cast\_dtm}\NormalTok{(}\AttributeTok{term =}\NormalTok{ word, }\AttributeTok{document =}\NormalTok{ line\_number, }\AttributeTok{value =}\NormalTok{ n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining, by = "line_number"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(alice\_and\_jane\_dtm\_training)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3413  585
\end{verbatim}

So we have our independent variables sorted, now we need our binary
dependent variable, which is whether the book is Alice in Wonderland or
Jane Eyre.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{response }\OtherTok{\textless{}{-}} 
  \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{id =} \FunctionTok{dimnames}\NormalTok{(alice\_and\_jane\_dtm\_training)[[}\DecValTok{1}\NormalTok{]]) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{separate}\NormalTok{(id, }\AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"book"}\NormalTok{, }\StringTok{"line"}\NormalTok{, }\AttributeTok{sep =} \StringTok{"\_"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{is\_alice =} \FunctionTok{if\_else}\NormalTok{(book }\SpecialCharTok{==} \DecValTok{11}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: Expected 3 pieces. Missing pieces filled with `NA` in 3413 rows [1, 2,
3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predictor }\OtherTok{\textless{}{-}}\NormalTok{ alice\_and\_jane\_dtm\_training[] }\SpecialCharTok{|\textgreater{}} \FunctionTok{as.matrix}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now we can run our model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(glmnet)}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(}\AttributeTok{x =}\NormalTok{ predictor,}
                   \AttributeTok{y =}\NormalTok{ response}\SpecialCharTok{$}\NormalTok{is\_alice,}
                   \AttributeTok{family =} \StringTok{"binomial"}\NormalTok{,}
                   \AttributeTok{keep =} \ConstantTok{TRUE}
\NormalTok{                   )}

\FunctionTok{save}\NormalTok{(model, }\AttributeTok{file =} \StringTok{"alice\_vs\_jane.rda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(glmnet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: Matrix
\end{verbatim}

\begin{verbatim}

Attaching package: 'Matrix'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:tidyr':

    expand, pack, unpack
\end{verbatim}

\begin{verbatim}
Loaded glmnet 4.1-3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}

\NormalTok{coefs }\OtherTok{\textless{}{-}}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{glmnet.fit }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tidy}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(lambda }\SpecialCharTok{==}\NormalTok{ model}\SpecialCharTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se)}

\NormalTok{coefs }\SpecialCharTok{|\textgreater{}} \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  term         step estimate  lambda dev.ratio
  <chr>       <dbl>    <dbl>   <dbl>     <dbl>
1 (Intercept)    36 -0.335   0.00597     0.562
2 in             36 -0.144   0.00597     0.562
3 she            36  0.390   0.00597     0.562
4 so             36  0.00249 0.00597     0.562
5 a              36 -0.117   0.00597     0.562
6 about          36  0.279   0.00597     0.562
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coefs }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(estimate }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{10}\NormalTok{, }\FunctionTok{abs}\NormalTok{(estimate)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\FunctionTok{fct\_reorder}\NormalTok{(term, estimate), estimate, }\AttributeTok{fill =}\NormalTok{ estimate }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Coefficient"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Word"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./17-text_files/figure-pdf/unnamed-chunk-40-1.pdf}

}

\end{figure}

Perhaps unsurprisingly, if a line mentions Alice then it is likely to be
a Alice in Wonderland and if it mention Jane then it is likely to be
Jane Eyre.

\hypertarget{topic-models}{%
\section{Topic models}\label{topic-models}}

Sometimes we have a statement and we want to know what it is about.
Sometimes this will be easy, but we do not always have titles for
statements, and even when we do, sometimes we do not have titles that
define topics in a well-defined and consistent way. One way to get
consistent estimates of the topics of each statement is to use topic
models. While there are many variants, one way is to use the latent
Dirichlet allocation (LDA) method of Blei, Ng, and Jordan (2003), as
implemented by the R package `topicmodels' by Grün and Hornik (2011).

The key assumption behind the LDA method is that each statement, `a
document', is made by a person who decides the topics they would like to
talk about in that document, and then chooses words, `terms', that are
appropriate to those topics. A topic could be thought of as a collection
of terms, and a document as a collection of topics. The topics are not
specified \emph{ex ante}; they are an outcome of the method. Terms are
not necessarily unique to a particular topic, and a document could be
about more than one topic. This provides more flexibility than other
approaches such as a strict word count method. The goal is to have the
words found in documents group themselves to define topics.

LDA considers each statement to be a result of a process where a person
first chooses the topics they want to speak about. After choosing the
topics, the person then chooses appropriate words to use for each of
those topics. More generally, the LDA topic model works by considering
each document as having been generated by some probability distribution
over topics. For instance, if there were five topics and two documents,
then the first document may be comprised mostly of the first few topics;
the other document may be mostly about the final few topics (Figure
\textbf{?@fig-topicsoverdocuments}).

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{./17-text_files/figure-pdf/fig-topicsoverdocuments-1.pdf}

}

\caption{\label{fig-topicsoverdocuments-1}Probability distributions over
topics}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{./17-text_files/figure-pdf/fig-topicsoverdocuments-2.pdf}

}

\caption{\label{fig-topicsoverdocuments-2}Probability distributions over
topics}

\end{figure}

Similarly, each topic could be considered a probability distribution
over terms. To choose the terms used in each document the speaker picks
terms from each topic in the appropriate proportion. For instance, if
there were ten terms, then one topic could be defined by giving more
weight to terms related to immigration; and some other topic may give
more weight to terms related to the economy (Figure
\textbf{?@fig-topicsoverterms}).

\begin{figure}

{\centering \includegraphics{./17-text_files/figure-pdf/fig-topicsoverterms-1.pdf}

}

\caption{\label{fig-topicsoverterms-1}Probability distributions over
terms}

\end{figure}

\begin{figure}

{\centering \includegraphics{./17-text_files/figure-pdf/fig-topicsoverterms-2.pdf}

}

\caption{\label{fig-topicsoverterms-2}Probability distributions over
terms}

\end{figure}

Following Blei and Lafferty (2009), Blei (2012) and Griffiths and
Steyvers (2004), the process by which a document is generated is more
formally considered to be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  There are \(1, 2, \dots, k, \dots, K\) topics and the vocabulary
  consists of \(1, 2, \dots, V\) terms. For each topic, decide the terms
  that the topic uses by randomly drawing distributions over the terms.
  The distribution over the terms for the \(k\)th topic is \(\beta_k\).
  Typically a topic would be a small number of terms and so the
  Dirichlet distribution with hyperparameter \(0<\eta<1\) is used:
  \(\beta_k \sim \mbox{Dirichlet}(\eta)\).{[}\^{}Dirichletfootnote{]}
  Strictly, \(\eta\) is actually a vector of hyperparameters, one for
  each \(K\), but in practice they all tend to be the same value.
\item
  Decide the topics that each document will cover by randomly drawing
  distributions over the \(K\) topics for each of the
  \(1, 2, \dots, d, \dots, D\) documents. The topic distributions for
  the \(d\)th document are \(\theta_d\), and \(\theta_{d,k}\) is the
  topic distribution for topic \(k\) in document \(d\). Again, the
  Dirichlet distribution with the hyperparameter \(0<\alpha<1\) is used
  here because usually a document would only cover a handful of topics:
  \(\theta_d \sim \mbox{Dirichlet}(\alpha)\). Again, strictly \(\alpha\)
  is vector of length \(K\) of hyperparameters, but in practice each is
  usually the same value.
\item
  If there are \(1, 2, \dots, n, \dots, N\) terms in the \(d\)th
  document, then to choose the \(n\)th term, \(w_{d, n}\):

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Randomly choose a topic for that term \(n\), in that document \(d\),
    \(z_{d,n}\), from the multinomial distribution over topics in that
    document, \(z_{d,n} \sim \mbox{Multinomial}(\theta_d)\).
  \item
    Randomly choose a term from the relevant multinomial distribution
    over the terms for that topic,
    \(w_{d,n} \sim \mbox{Multinomial}(\beta_{z_{d,n}})\).
  \end{enumerate}
\end{enumerate}

The Dirichlet distribution is a variation of the beta distribution that
is commonly used as a prior for categorical and multinomial variables.
If there are just two categories, then the Dirichlet and the beta
distributions are the same. In the special case of a symmetric Dirichlet
distribution, \(\eta=1\), it is equivalent to a uniform distribution. If
\(\eta<1\), then the distribution is sparse and concentrated on a
smaller number of the values, and this number decreases as \(\eta\)
decreases. A hyperparameter is a parameter of a prior distribution.

Given this set-up, the joint distribution for the variables is (Blei
(2012), p.6):
\[p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}) = \prod^{K}_{i=1}p(\beta_i) \prod^{D}_{d=1}p(\theta_d) \left(\prod^N_{n=1}p(z_{d,n}|\theta_d)p\left(w_{d,n}|\beta_{1:K},z_{d,n}\right) \right).\]

Based on this document generation process the analysis problem,
discussed in the next section, is to compute a posterior over
\(\beta_{1:K}\) and \(\theta_{1:D}\), given \(w_{1:D, 1:N}\). This is
intractable directly, but can be approximated (Griffiths and Steyvers
(2004) and Blei (2012)).

After the documents are created, they are all that we have to analyze.
The term usage in each document, \(w_{1:D, 1:N}\), is observed, but the
topics are hidden, or `latent'. We do not know the topics of each
document, nor how terms defined the topics. That is, we do not know the
probability distributions of Figures \textbf{?@fig-topicsoverdocuments}
or \textbf{?@fig-topicsoverterms}. In a sense we are trying to reverse
the document generation process -- we have the terms and we would like
to discover the topics.

If the earlier process around how the documents were generated is
assumed and we observe the terms in each document, then we can obtain
estimates of the topics (Steyvers and Griffiths (2006)). The outcomes of
the LDA process are probability distributions and these define the
topics. Each term will be given a probability of being a member of a
particular topic, and each document will be given a probability of being
about a particular topic. That is, we are trying to calculate the
posterior distribution of the topics given the terms observed in each
document (Blei (2012), p.7):
\[p(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N} | w_{1:D, 1:N}) = \frac{p\left(\beta_{1:K}, \theta_{1:D}, z_{1:D, 1:N}, w_{1:D, 1:N}\right)}{p(w_{1:D, 1:N})}.\]

The initial practical step when implementing LDA given a corpus of
documents is to remove `stop words'. These are words that are common,
but that do not typically help to define topics. There is a general list
of stop words such as: ``a''; ``a's''; ``able''; ``about'';
``above''\ldots{} We also remove punctuation and capitalization. The
documents need to then be transformed into a document-term-matrix. This
is essentially a table with a column of the number of times each term
appears in each document.

After the dataset is ready, the R package `topicmodels' by Grün and
Hornik (2011) can be used to implement LDA and approximate the
posterior. It does this using Gibbs sampling or the variational
expectation-maximization algorithm. Following Steyvers and Griffiths
(2006) and Darling (2011), the Gibbs sampling process attempts to find a
topic for a particular term in a particular document, given the topics
of all other terms for all other documents. Broadly, it does this by
first assigning every term in every document to a random topic,
specified by Dirichlet priors with \(\alpha = \frac{50}{K}\) and
\(\eta = 0.1\) (Steyvers and Griffiths (2006) recommends
\(\eta = 0.01\)), where \(\alpha\) refers to the distribution over
topics and \(\eta\) refers to the distribution over terms (Grün and
Hornik (2011), p.7). It then selects a particular term in a particular
document and assigns it to a new topic based on the conditional
distribution where the topics for all other terms in all documents are
taken as given (Grün and Hornik (2011), p.6):
\[p(z_{d, n}=k | w_{1:D, 1:N}, z'_{d, n}) \propto \frac{\lambda'_{n\rightarrow k}+\eta}{\lambda'_{.\rightarrow k}+V\eta} \frac{\lambda'^{(d)}_{n\rightarrow k}+\alpha}{\lambda'^{(d)}_{-i}+K\alpha} \]
where \(z'_{d, n}\) refers to all other topic assignments;
\(\lambda'_{n\rightarrow k}\) is a count of how many other times that
term has been assigned to topic \(k\); \(\lambda'_{.\rightarrow k}\) is
a count of how many other times that any term has been assigned to topic
\(k\); \(\lambda'^{(d)}_{n\rightarrow k}\) is a count of how many other
times that term has been assigned to topic \(k\) in that particular
document; and \(\lambda'^{(d)}_{-i}\) is a count of how many other times
that term has been assigned in that document. Once \(z_{d,n}\) has been
estimated, then estimates for the distribution of words into topics and
topics into documents can be backed out.

This conditional distribution assigns topics depending on how often a
term has been assigned to that topic previously, and how common the
topic is in that document (Steyvers and Griffiths (2006)). The initial
random allocation of topics means that the results of early passes
through the corpus of document are poor, but given enough time the
algorithm converges to an appropriate estimate.

The choice of the number of topics, \emph{k}, affects the results, and
must be specified \emph{a priori}. If there is a strong reason for a
particular number, then this can be used. Otherwise, one way to choose
an appropriate number is to use a test and training set process.
Essentially, this means running the process on a variety of possible
values for \emph{k} and then picking an appropriate value that performs
well.

One weakness of the LDA method is that it considers a `bag of words'
where the order of those words does not matter (Blei (2012)). It is
possible to extend the model to reduce the impact of the bag-of-words
assumption and add conditionality to word order. Additionally,
alternatives to the Dirichlet distribution can be used to extend the
model to allow for correlation. For instance, in Hansard topics related
the army may be expected to be more commonly found with topics related
to the navy, but less commonly with topics related to banking.

\hypertarget{exercises-and-tutorial-16}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-16}}

\hypertarget{exercises-16}{%
\subsection{Exercises}\label{exercises-16}}

\hypertarget{tutorial-16}{%
\subsection{Tutorial}\label{tutorial-16}}

Consider a situation where we run a news website and are trying to
understand whether to allow anonymous comments. We decide to do an A/B
test, where we will keep everything the same, but only allow anonymous
comments on one version of the site. Please simulate some text data that
we may obtain from our test. And then build a model that could test
whether there is a difference between the situations.

\part{Enrichment}

\hypertarget{sec-deploying-models}{%
\chapter{Deploying models}\label{sec-deploying-models}}

\textbf{Required reading}

\begin{itemize}
\tightlist
\item
  Read \emph{Machine learning is going real-time}, (Huyen 2020)
\item
  Read \emph{Real-time machine learning: challenges and solutions},
  (Huyen 2022)
\item
  Watch \emph{Democratizing R with Plumber APIs}, (J. Blair 2019)
\item
  Read \emph{Science Storms the Cloud}, (Gentemann et al. 2021)
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Benefits/costs of cloud.
\item
  Getting started in the cloud.
\item
  Starting virtual machines with R Studio.
\item
  Stopping virtual machines.
\item
  Putting models into production requires a different set of skills to
  building a model. We need a familiarity with some cloud provider,
  APIs, and of course modelling. But the biggest difficulty, for me, is
  getting things set-up.
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{plumber} (Schloerke and Allen 2021)
\item
  \texttt{shiny} (Chang et al. 2021)
\end{itemize}

\hypertarget{introduction-15}{%
\section{Introduction}\label{introduction-15}}

Having done the work to develop a dataset and explore it with a model
that we are confident can be used, we may wish to enable this to be used
more widely thank just our own computer. There are a variety of ways of
doing this, including:

\begin{itemize}
\tightlist
\item
  using the cloud,
\item
  creating R packages,
\item
  making \texttt{shiny} applications, and
\item
  using \texttt{plumber} to create an API.
\end{itemize}

The general idea here is that we need to know, and allow others to come
to trust, the whole workflow. That is what our approach to this point
brings. After this, then we may like to use our model more broadly. Say
we been able to scrape some data from a website, bring some order to
that chaos, make some charts, appropriately model it, and write this all
up. In most academic settings that is more than enough. But in many
industry settings we would like to use the model to do something. For
instance, setting up a website that allows a model to be used to
generate an insurance quote given several inputs.

In this chapter, we begin by moving our compute from our local computer
to the cloud. We then describe the use of R packages and Shiny for
sharing models. That works well, but in some settings other users may
like to interact with our model in ways that we are not focused on. One
way to allow this is to make our results available to other computers,
and for that we will want to make an APIs. Hence, we introduce
\texttt{plumber} (Schloerke and Allen 2021), which is a way of creating
APIs.

\hypertarget{amazon-web-services}{%
\section{Amazon Web Services}\label{amazon-web-services}}

The apocryphal quote is that the cloud is another name for someone
else's computer. And while that is true to various degrees, for our
purposes that is enough. Learning to use someone else's computer can be
great for a number of reasons including:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Scalability: It can be quite expensive to buy a new computer,
  especially if we only need it to run something every now and then, but
  by using someone else's computer, we can just rent for a few hours or
  days. This allows use to amortize this cost and work out what we
  actually need before committing to a purchase. It also allows us to
  easily increase or decrease the compute scale if we suddenly have a
  substantial increase in demand.
\item
  Portability: If we can shift our analysis workflow from a local
  machine to the cloud, then that suggests that there is we are likely
  doing good things in terms of reproducibility and portability. At the
  very least, code can run both locally and on the cloud, which is a big
  step in terms of reproducibility.
\item
  Set-and-forget: If we are doing something that will take a while, then
  it can be great to not have to worry about our own computer needing to
  run overnight, or, say, not being able to watch Netflix on that same
  computer. Additionally, on many cloud options, open-source statistical
  software, such as R and Python, is either already available, or
  relatively easy to set-up.
\end{enumerate}

That said, there are downsides, including:

\begin{itemize}
\tightlist
\item
  Cost: While most cloud options are cheap, they are rarely free. To
  provide an idea of cost, using a well-featured AWS instance for a few
  days, may end up being a few dollars. It is also easy to accidentally
  forget about something, and generate unexpectedly large bills,
  especially initially.
\item
  Public: It can be easy to make mistakes and accidentally make
  everything public.
\item
  Time: It takes time to get set-up and comfortable on the cloud.
\end{itemize}

When we use the cloud, we are typically running code on a `virtual
machine' (VM). This is an allocation that is part of a larger collection
of computers that has been designed to act like a computer with specific
features. For instance, we may specify that our virtual machine has,
say, 8 GB RAM, 128 storage, and 4 CPUs. The VM would then act like a
computer with those specifications. The cost to use cloud options
increases based on the specifications of the virtual machine.

In a sense, we started with a cloud option, through our initial
recommendation, in Chapter @ref(drinking-from-a-fire-hose) of using R
Studio Cloud, before we moved to our local machine in Chapter
@ref(r-essentials). That cloud option was specifically designed for
beginners. We will now introduce a more general cloud option: Amazon Web
Services (AWS). Often a particular business will use a particular cloud
option, such as Google, AWS, or Azure, but developing familiarity with
one will make the use of the others easier.

Amazon Web Services is a cloud service from Amazon. To get started we
need to create an AWS Developer account
\href{https://aws.amazon.com/developer/}{here} (Figure
Figure~\ref{fig-awsone}).

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/aws_one.png}

}

\caption{\label{fig-awsone}AWS Developer website}

\end{figure}

After we have created an account, we need to select a region where the
computer that we will access is located. After this, we want to ``Launch
a virtual machine'' with EC2 (Figure Figure~\ref{fig-awstwo}).

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/aws_two.png}

}

\caption{\label{fig-awstwo}AWS Developer console}

\end{figure}

The first step is to choose an Amazon Machine Image (AMI). This provides
the details of the computer that you will be using. For instance, a
local computer may be a MacBook running Monterey. Louis Aslett provides
AMIs that are already set-up with R Studio and much else
\href{http://www.louisaslett.com/RStudio_AMI/}{here}. We can either
search for the AMI of the region that we registered for, or click on the
relevant link on Aslett's website. For instance, to use the AMI set-up
for the Canadian central region we search for `ami-0bdd24fd36f07b638'.
The benefit of using these AMIs is that they are set-up specifically for
R Studio, but the trade-off is that they are a little outdated, as they
were compiled in August 2020.

In the next step we can choose how powerful the computer will be. The
free tier is basic computer, but we can choose better ones when we need
them. At this point we can pretty much just launch the instance (Figure
Figure~\ref{fig-awsthree}). If we start using AWS more seriously we
could go back and select different options, especially around the
security of the account. AWS relies on key pairs. And so we will need to
create a PEM and save it locally (Figure Figure~\ref{fig-awsfive}). We
can then launch the instance.

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/aws_three.png}

}

\caption{\label{fig-awsthree}AWS Developer launch instance}

\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.95\textwidth,height=\textheight]{./figures/aws_five.png}

}

\caption{\label{fig-awsfive}AWS Developer establishing a key-pair}

\end{figure}

After a few minutes, the instance will be running. We can use it by
pasting the `public DNS' into a browser. The username is `rstudio' and
the password is the instance ID.

We should have R Studio running, which is exciting. The first thing to
do is probably to change the default password using the instructions in
the instance.

We do not need to install, say, \texttt{tidyverse}, instead we can just
call the library and keep going. This is because this AMI comes with
many packages already installed. We can see the list of packages that
are installed with \texttt{installed.packages()}. For instance,
\texttt{rstan} is already installed, and we could set-up an instance
with GPUs if we needed.

Perhaps as important as being able to start an AWS instance is being
able to stop it (so that we do not get billed). The free tier is pretty
great, but we do need to turn it off. To stop an instance, in the AWS
instances page, select it, then `Actions -\textgreater{} Instance State
-\textgreater{} Terminate'.

\hypertarget{plumber-and-model-apis}{%
\section{Plumber and model APIs}\label{plumber-and-model-apis}}

The general idea behind the \texttt{plumber} package (Schloerke and
Allen 2021) is that we can train a model and make it available via an
API that we can call when we want a forecast. It is pretty great.

Just to get something working, let us make a function that returns
`Hello Toronto' regardless of the output. Open a new R file, add the
following, and then save it as `plumber.R' (you may need to install the
\texttt{plumber} package if you've not done that yet).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plumber)}

\CommentTok{\#* @get /print\_toronto}
\NormalTok{print\_toronto }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() \{}
\NormalTok{  result }\OtherTok{\textless{}{-}} \StringTok{"Hello Toronto"}
  \FunctionTok{return}\NormalTok{(result)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

After that is saved, in the top right of the editor you should get a
button to `Run API'. Click that, and your API should load. It will be a
`Swagger' application, which provides a GUI around our API. Expand the
GET method, and then click `Try it out' and `Execute'. In the response
body, you should get `Toronto'.

To more closely reflect the fact that this is an API designed for
computers, you can copy/paste the `request HTML' into a browser and it
should return `Hello Toronto'.

\hypertarget{local-model}{%
\subsection{Local model}\label{local-model}}

Now, we are going to update the API so that it serves a model output,
given some input. We are going to follow Buhr (2017) fairly closely.

At this point, we should start a new R Project. To get started, let us
simulate some data and then train a model on it. In this case we are
interested in forecasting how long a baby may sleep overnight, given we
know how long they slept during their afternoon nap.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}

\NormalTok{number\_of\_observations }\OtherTok{\textless{}{-}} \DecValTok{1000}

\NormalTok{baby\_sleep }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{afternoon\_nap\_length =} \FunctionTok{rnorm}\NormalTok{(number\_of\_observations, }\DecValTok{120}\NormalTok{, }\DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{abs}\NormalTok{(),}
         \AttributeTok{noise =} \FunctionTok{rnorm}\NormalTok{(number\_of\_observations, }\DecValTok{0}\NormalTok{, }\DecValTok{120}\NormalTok{),}
         \AttributeTok{night\_sleep\_length =}\NormalTok{ afternoon\_nap\_length }\SpecialCharTok{*} \DecValTok{4} \SpecialCharTok{+}\NormalTok{ noise,}
\NormalTok{         )}

\NormalTok{baby\_sleep }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ afternoon\_nap\_length, }\AttributeTok{y =}\NormalTok{ night\_sleep\_length)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Baby\textquotesingle{}s afternoon nap length (minutes)"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Baby\textquotesingle{}s overnight sleep length (minutes)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Let us now use \texttt{tidymodels} to quickly make a dodgy model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{853}\NormalTok{)}
\FunctionTok{library}\NormalTok{(tidymodels)}

\NormalTok{baby\_sleep\_split }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{initial\_split}\NormalTok{(baby\_sleep, }\AttributeTok{prop =} \FloatTok{0.80}\NormalTok{)}
\NormalTok{baby\_sleep\_train }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{training}\NormalTok{(baby\_sleep\_split)}
\NormalTok{baby\_sleep\_test }\OtherTok{\textless{}{-}}\NormalTok{ rsample}\SpecialCharTok{::}\FunctionTok{testing}\NormalTok{(baby\_sleep\_split)}

\NormalTok{model }\OtherTok{\textless{}{-}} 
\NormalTok{  parsnip}\SpecialCharTok{::}\FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
\NormalTok{  parsnip}\SpecialCharTok{::}\FunctionTok{set\_engine}\NormalTok{(}\AttributeTok{engine =} \StringTok{"lm"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  parsnip}\SpecialCharTok{::}\FunctionTok{fit}\NormalTok{(night\_sleep\_length }\SpecialCharTok{\textasciitilde{}}\NormalTok{ afternoon\_nap\_length, }
               \AttributeTok{data =}\NormalTok{ baby\_sleep\_train}
\NormalTok{               )}

\FunctionTok{write\_rds}\NormalTok{(}\AttributeTok{x =}\NormalTok{ model, }\AttributeTok{file =} \StringTok{"baby\_sleep.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

At this point, we have a model. One difference from what you might be
used to is that we have saved the model as an `.rds' file. We are going
to read that in.

Now that we have our model we want to put that into a file that we will
use the API to access, again called `plumber.R'. And we also want a file
that sets up the API, called `server.R'. So make an R script called
`server.R' and add the following content:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plumber)}

\NormalTok{serve\_model }\OtherTok{\textless{}{-}} \FunctionTok{plumb}\NormalTok{(}\StringTok{"plumber.R"}\NormalTok{)}
\NormalTok{serve\_model}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(}\AttributeTok{port =} \DecValTok{8000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then in `plumber.R' add the following content:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plumber)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"baby\_sleep.rds"}\NormalTok{)}

\NormalTok{version\_number }\OtherTok{\textless{}{-}} \StringTok{"0.0.1"}

\NormalTok{variables }\OtherTok{\textless{}{-}} 
  \FunctionTok{list}\NormalTok{(}
    \AttributeTok{afternoon\_nap\_length =} \StringTok{"A value in minutes, likely between 0 and 240."}\NormalTok{,}
    \AttributeTok{night\_sleep\_length =} \StringTok{"A forecast, in minutes, likely between 0 and 1000."}
\NormalTok{  )}

\CommentTok{\#* @param afternoon\_nap\_length}
\CommentTok{\#* @get /survival}
\NormalTok{predict\_sleep }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{afternoon\_nap\_length=}\DecValTok{0}\NormalTok{) \{}
\NormalTok{  afternoon\_nap\_length }\OtherTok{=} \FunctionTok{as.integer}\NormalTok{(afternoon\_nap\_length)}
  
\NormalTok{  payload }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{afternoon\_nap\_length=}\NormalTok{afternoon\_nap\_length)}
  
\NormalTok{  prediction }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model, payload)}

\NormalTok{  result }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{input =} \FunctionTok{list}\NormalTok{(payload),}
    \AttributeTok{response =} \FunctionTok{list}\NormalTok{(}\StringTok{"estimated\_night\_sleep"} \OtherTok{=}\NormalTok{ prediction),}
    \AttributeTok{status =} \DecValTok{200}\NormalTok{,}
    \AttributeTok{model\_version =}\NormalTok{ version\_number)}

  \FunctionTok{return}\NormalTok{(result)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Again, after we save the `plumber.R' file we should have an option to
`Run API'. Click that and you can try out the API locally in the same
way as before.

\hypertarget{cloud-model}{%
\subsection{Cloud model}\label{cloud-model}}

To this point, we have got an API working on our own machine, but what
we really want to do is to get it working on a computer such that the
API can be accessed by anyone. To do this we are going to use
\href{https://www.digitalocean.com}{DigitalOcean}. It is a charged
service, but when you create an account, it will come with \$100 in
credit, which will be enough to get started.

This set-up process will take some time, but we only need to do it once.
Install two additional packages that will assist here are
\texttt{plumberDeploy} (Allen 2021) and \texttt{analogsea} (Chamberlain,
Wickham, and Chang 2021).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"plumberDeploy"}\NormalTok{)}
\NormalTok{remotes}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"sckott/analogsea"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we need to connect the local computer with the DigitalOcean account.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{account}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now we need to authenticate the connection, and this is done using a SSH
public key.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{key\_create}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

What you want is to have a `.pub' file on our computer. Then copy the
public key aspect in that file, and add it to the SSH keys section in
the account security settings. When we have the key on our local
computer, then we can check this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssh}\SpecialCharTok{::}\FunctionTok{ssh\_key\_info}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Again, this will all take a while to validate. DigitalOcean calls every
computer that we start a `droplet'. So if we start three computers, then
we will have started three droplets. We can check the droplets that are
running.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{droplets}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

If everything is set-up properly, then this will print the information
about all droplets that you have associated with the account (which at
this point, is probably none). We first create a droplet.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{id }\OtherTok{\textless{}{-}}\NormalTok{ plumberDeploy}\SpecialCharTok{::}\FunctionTok{do\_provision}\NormalTok{(}\AttributeTok{example =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then we get asked for the SSH passphrase and then it will just set-up a
bunch of things. After this we are going to need to install a whole
bunch of things onto our droplet.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{install\_r\_package}\NormalTok{(}\AttributeTok{droplet =}\NormalTok{ id, }\FunctionTok{c}\NormalTok{(}\StringTok{"plumber"}\NormalTok{, }
                                             \StringTok{"remotes"}\NormalTok{, }
                                             \StringTok{"here"}\NormalTok{))}
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{debian\_apt\_get\_install}\NormalTok{(id, }\StringTok{"libssl{-}dev"}\NormalTok{, }
                                  \StringTok{"libsodium{-}dev"}\NormalTok{, }
                                  \StringTok{"libcurl4{-}openssl{-}dev"}\NormalTok{)}
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{debian\_apt\_get\_install}\NormalTok{(id, }
                                  \StringTok{"libxml2{-}dev"}\NormalTok{)}

\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{install\_r\_package}\NormalTok{(id, }\FunctionTok{c}\NormalTok{(}\StringTok{"config"}\NormalTok{,}
                                   \StringTok{"httr"}\NormalTok{,}
                                   \StringTok{"urltools"}\NormalTok{,}
                                   \StringTok{"plumber"}\NormalTok{))}

\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{install\_r\_package}\NormalTok{(id, }\FunctionTok{c}\NormalTok{(}\StringTok{"xml2"}\NormalTok{))}
\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{install\_r\_package}\NormalTok{(id, }\FunctionTok{c}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{))}

\NormalTok{analogsea}\SpecialCharTok{::}\FunctionTok{install\_r\_package}\NormalTok{(id, }\FunctionTok{c}\NormalTok{(}\StringTok{"tidymodels"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

And then when that is finally set-up (it will take 30 minutes or so) we
can deploy our API.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plumberDeploy}\SpecialCharTok{::}\FunctionTok{do\_deploy\_api}\NormalTok{(}\AttributeTok{droplet =}\NormalTok{ id, }
                             \AttributeTok{path =} \StringTok{"example"}\NormalTok{, }
                             \AttributeTok{localPath =} \FunctionTok{getwd}\NormalTok{(), }
                             \AttributeTok{port =} \DecValTok{8000}\NormalTok{, }
                             \AttributeTok{docs =} \ConstantTok{TRUE}\NormalTok{, }
                             \AttributeTok{overwrite=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{exercises-and-tutorial-17}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-17}}

\hypertarget{exercises-17}{%
\subsection{Exercises}\label{exercises-17}}

\hypertarget{tutorial-17}{%
\subsection{Tutorial}\label{tutorial-17}}

\hypertarget{sec-efficiency}{%
\chapter{Efficiency}\label{sec-efficiency}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Watch \emph{Code smells and feels}, (Jenny Bryan 2018).
\item
  Read \emph{Notes from a data witch: Getting started with Apache
  Arrow}, (Navarro 2021).
\end{itemize}

\textbf{Key concepts and skills}

\begin{itemize}
\tightlist
\item
  Refactoring code
\item
  Developing an appreciation for SQL
\end{itemize}

\textbf{Key libraries}

\begin{itemize}
\tightlist
\item
  \texttt{arrow} (Richardson et al. 2022)
\item
  \texttt{tictoc} (Izrailev 2014)
\end{itemize}

\textbf{Key functions}

\begin{itemize}
\tightlist
\item
  \texttt{arrow::read\_parquet()}
\item
  \texttt{arrow::write\_parquet()}
\item
  \texttt{tictoc::tic()}
\item
  \texttt{tictoc::toc()}
\end{itemize}

\hypertarget{introduction-16}{%
\section{Introduction}\label{introduction-16}}

For much of this book we have been largely concerned with just getting
something done. Not necessarily getting it done in the best or most
efficient way. To a large extent, being worried about getting something
done in the best or most efficient way is almost always a waste of time.
Until it is not. Eventually inefficient ways of storing data, ugly or
slow code, and an insistence on using R do have an effect. And it is at
that point that we need to be open to new approaches to ensure
efficiency.

In this chapter we briefly cover ways to be more efficient with data, by
using SQL, feather and parquet. We then discuss code efficiency,
particularly, the need to measure and refactor code. Then we discuss
experimental efficiency, in particular, the multi-armed bandit which
enables us to more quickly test different effects. Finally, we briefly
introduce other languages, such as Python and Julia, that have an
important role to play in data science.

\hypertarget{code-efficiency}{%
\section{Code efficiency}\label{code-efficiency}}

By and large, worrying about performance is a waste of time. For the
most part we are better off just pushing things into the cloud, letting
them run for a reasonable time, and using that time to worry about other
aspects of the pipeline. But, eventually this becomes unfeasible. For
instance, if something takes more than a day, then it becomes a pain
because of the need to completely switch tasks and then return to it.
There is rarely a most common area for obvious performance gains.
Instead it is important to develop the ability to measure and then
refactor code.

Being fast is valuable but it is mostly about being able to iterate fast
not necessarily that the code runs fast. The first thing we should do if
we find that the speed at which code is running is becoming a bottle
neck is to shard. Then we should throw more machines at it. But
eventually we should go back and refactor the code.

To refactor code means to re-write it so that the new code achieves the
same outcome as the old code, it is just that the new code does it
better. We can use \texttt{tic()} and \texttt{toc()} from
\texttt{tictoc} (Izrailev 2014) to time various aspects of our code and
find where the largest delays are.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tictoc)}
\FunctionTok{tic}\NormalTok{(}\StringTok{"First bit of code"}\NormalTok{)}
\FunctionTok{print}\NormalTok{(}\StringTok{"Fast code"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Fast code"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{toc}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
First bit of code: 0.003 sec elapsed
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tic}\NormalTok{(}\StringTok{"Second bit of code"}\NormalTok{)}
\FunctionTok{Sys.sleep}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\FunctionTok{print}\NormalTok{(}\StringTok{"Slow code"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Slow code"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{toc}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Second bit of code: 3.009 sec elapsed
\end{verbatim}

And so we know that there is something slowing down the code; which in
this artificial case is \texttt{Sys.sleep()} causing a delay of 3
seconds.

When we start to refactor our code, we want to make sure that the
re-written code achieves the same outcomes as the original code. This
means that it is important to have tests written. We generally want to
reduce the size of functions, by breaking them into smaller ones.

\hypertarget{data-efficiency}{%
\section{Data efficiency}\label{data-efficiency}}

\hypertarget{sql}{%
\subsection{SQL}\label{sql}}

While it may be true that the SQL is never as good as the original, SQL
is a popular way of working with data. Advanced users probably do a lot
with it alone, but even just having a working knowledge of SQL increases
the number of datasets that we can access. We can use SQL within RStudio

SQL is a straightforward variant of the \texttt{dplyr} verbs that we
have used throughout this book. Having used \texttt{mutate()},
\texttt{filter()} and \texttt{left\_join()} in the \texttt{tidyverse}
means that much of the core commands will be familiar. That means that
the main difficulty will be getting on top of the order of operations
because SQL can be pedantic.

SQL (``see-quell'' or ``S.Q.L.'') is used with relational databases. A
relational database is just a collection of at least one table, and a
table is just some data organized into rows and columns. If there is
more than one table in the database, then there should be some column
that links them. Using it feels a bit like HTML/CSS in terms of being
halfway between markup and programming. One fun aspect is that line
spaces mean nothing: include them or do not, but always end a SQL
command in a semicolon;

We can create an empty table of three columns of type: int, text, int:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{CREATE} \KeywordTok{TABLE}\NormalTok{ table\_name (}
\NormalTok{  column1 }\DataTypeTok{INTEGER}\NormalTok{,}
\NormalTok{  column2 TEXT,}
\NormalTok{  column3 }\DataTypeTok{INTEGER}
\NormalTok{);}
\end{Highlighting}
\end{Shaded}

Add a row of data:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{INSERT} \KeywordTok{INTO}\NormalTok{ table\_name (column1, column2, column3)}
  \KeywordTok{VALUES}\NormalTok{ (}\DecValTok{1234}\NormalTok{, }\StringTok{\textquotesingle{}Gough Menzies\textquotesingle{}}\NormalTok{, }\DecValTok{32}\NormalTok{);}
\end{Highlighting}
\end{Shaded}

Add a column:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ALTER} \KeywordTok{TABLE}\NormalTok{ table\_name}
  \KeywordTok{ADD} \KeywordTok{COLUMN}\NormalTok{ column4 TEXT;}
\end{Highlighting}
\end{Shaded}

We can view particular aspects of the data, using SELECT in a similar
way to \texttt{select()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ column2}
  \KeywordTok{FROM}\NormalTok{ table\_name;}
\end{Highlighting}
\end{Shaded}

See two columns:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ column1, column2}
  \KeywordTok{FROM}\NormalTok{ table\_name;}
\end{Highlighting}
\end{Shaded}

See all columns:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
  \KeywordTok{FROM}\NormalTok{ table\_name;}
\end{Highlighting}
\end{Shaded}

See unique rows in a column (similar to R's distinct):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \KeywordTok{DISTINCT}\NormalTok{ column2}
  \KeywordTok{FROM}\NormalTok{ table\_name;}
\end{Highlighting}
\end{Shaded}

See the rows that match a criteria (similar idea to R's which or
filter):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
  \KeywordTok{FROM}\NormalTok{ table\_name}
    \KeywordTok{WHERE}\NormalTok{ column3 }\OperatorTok{\textgreater{}} \DecValTok{30}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

All the usual operators are fine with WHERE: =, !=, \textgreater,
\textless, \textgreater=, \textless=. Just make sure the condition
evaluates to true/false.

See the rows that are pretty close to a criteria:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
  \KeywordTok{FROM}\NormalTok{ table\_name}
    \KeywordTok{WHERE}\NormalTok{ column2 }\KeywordTok{LIKE}  \StringTok{\textquotesingle{}\_ough Menzies\textquotesingle{}}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

The \_ above is a wildcard that matches to any character e.g.~`Cough
Menzies' would be matched here, as would `Gough Menzies'. LIKE is not
case-sensitive: `Gough Menzies' and `gough menzies' would both match
here.

Use \% as an anchor to matches pieces:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
  \KeywordTok{FROM}\NormalTok{ table\_name}
    \KeywordTok{WHERE}\NormalTok{ column2 }\KeywordTok{LIKE}  \StringTok{\textquotesingle{}\%Menzies\textquotesingle{}}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

That matches anything ending with `Menzies', so `Cough Menzies', `Gough
Menzies', `Sir Menzies' etc, would all be matched here. Use surrounding
percentages to match within, e.g.~\%Menzies\% would also match `Sir
Menzies Jr' whereas \%Menzies would not.

This is wild: NULL values (!) (True/False/NULL) are possible, not just
True/False, but they need to be explicitly matched for:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
  \KeywordTok{FROM}\NormalTok{ table\_name}
    \KeywordTok{WHERE}\NormalTok{ column2 }\KeywordTok{IS} \KeywordTok{NOT} \KeywordTok{NULL}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

This too is wild: There's an underlying ordering build into number, date
and text fields that allows you to use BETWEEN on all those, not just
numeric! The following looks for text that starts with a letter between
A and M (not including M) so would match `Gough Menzies', but not `Sir
Gough Menzies'!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
  \KeywordTok{FROM}\NormalTok{ table\_name}
    \KeywordTok{WHERE}\NormalTok{ column2 }\KeywordTok{BETWEEN} \StringTok{\textquotesingle{}A\textquotesingle{}} \KeywordTok{AND} \StringTok{\textquotesingle{}M\textquotesingle{}}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

If you look for a numeric (as opposed to text) then BETWEEN is
inclusive.

Combine conditions with AND (both must be true to be returned) or OR (at
least one must be true):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
  \KeywordTok{FROM}\NormalTok{ table\_name}
    \KeywordTok{WHERE}\NormalTok{ column2 }\KeywordTok{BETWEEN} \StringTok{\textquotesingle{}A\textquotesingle{}} \KeywordTok{AND} \StringTok{\textquotesingle{}M\textquotesingle{}}
    \KeywordTok{AND}\NormalTok{ column3 }\OperatorTok{=} \DecValTok{32}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

You can order the result:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
  \KeywordTok{FROM}\NormalTok{ table\_name}
    \KeywordTok{ORDER} \KeywordTok{BY}\NormalTok{ column3;}
\end{Highlighting}
\end{Shaded}

Ascending is the default, add DESC for alternative:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
  \KeywordTok{FROM}\NormalTok{ table\_name}
    \KeywordTok{ORDER} \KeywordTok{BY}\NormalTok{ column3 }\KeywordTok{DESC}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

Restrict the return to a certain number of values by adding LIMIT at the
end:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
  \KeywordTok{FROM}\NormalTok{ table\_name}
    \KeywordTok{ORDER} \KeywordTok{BY}\NormalTok{ column3 }\KeywordTok{DESC}
    \KeywordTok{LIMIT} \DecValTok{1}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

(This doesn't work all the time - only certain SQL databases.)

We can modify data and use logic. For instance we can edit a value.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{UPDATE}\NormalTok{ table\_name}
  \KeywordTok{SET}\NormalTok{ column3 }\OperatorTok{=} \DecValTok{33}
    \KeywordTok{WHERE}\NormalTok{ column1 }\OperatorTok{=} \DecValTok{1234}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

Implement if/else logic:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}\NormalTok{,}
  \ControlFlowTok{CASE}
    \ControlFlowTok{WHEN}\NormalTok{ column2 }\OperatorTok{=} \StringTok{\textquotesingle{}Gough Whitlam\textquotesingle{}} \ControlFlowTok{THEN} \StringTok{\textquotesingle{}Labor\textquotesingle{}}
    \ControlFlowTok{WHEN}\NormalTok{ column2 }\OperatorTok{=} \StringTok{\textquotesingle{}Robert Menzies\textquotesingle{}} \ControlFlowTok{THEN} \StringTok{\textquotesingle{}Liberal\textquotesingle{}}
    \ControlFlowTok{ELSE} \StringTok{\textquotesingle{}Who knows\textquotesingle{}}
  \ControlFlowTok{END} \KeywordTok{AS} \StringTok{\textquotesingle{}Party\textquotesingle{}}
  \KeywordTok{FROM}\NormalTok{ table\_name;}
\end{Highlighting}
\end{Shaded}

This returns a column called `Party' that looks at the name of the
person to return a party.

Delete some rows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{DELETE} \KeywordTok{FROM}\NormalTok{ table\_name}
  \KeywordTok{WHERE}\NormalTok{ column3 }\KeywordTok{IS} \KeywordTok{NULL}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

Add an alias to a column name (this just shows in the output):

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ column2 }\KeywordTok{AS} \StringTok{\textquotesingle{}Names\textquotesingle{}}
  \KeywordTok{FROM}\NormalTok{ table\_name;}
\end{Highlighting}
\end{Shaded}

We can use COUNT, SUM, MAX, MIN, AVG and ROUND in the place of
\texttt{summarize()}. COUNT counts the number of rows that are not empty
for some column by passing the column name, or for all using *.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \FunctionTok{COUNT}\NormalTok{(}\OperatorTok{*}\NormalTok{)}
  \KeywordTok{FROM}\NormalTok{ table\_name;}
\end{Highlighting}
\end{Shaded}

Similarly, we can pass a column to SUM, MAX, MIN, and AVG.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \FunctionTok{SUM}\NormalTok{(column1)}
  \KeywordTok{FROM}\NormalTok{ table\_name;}
\end{Highlighting}
\end{Shaded}

ROUND takes a column and an integer to specify how many decimal places.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \FunctionTok{ROUND}\NormalTok{(column1, }\DecValTok{0}\NormalTok{)}
  \KeywordTok{FROM}\NormalTok{ table\_name;}
\end{Highlighting}
\end{Shaded}

SELECT and GROUP BY is similar to group\_by in R.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ column3, }\FunctionTok{COUNT}\NormalTok{(}\OperatorTok{*}\NormalTok{)}
  \KeywordTok{FROM}\NormalTok{ table\_name}
    \KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ column3;}
\end{Highlighting}
\end{Shaded}

We can GROUP BY column number instead of name e.g.~1 instead of column3
in the GROUP BY line or 2 instead of COUNT(*) if that was of interest.

HAVING for aggregates, is similar to filter in R or the WHERE for rows
from earlier. Use it after GROUP BY and before ORDER BY and LIMIT.

We can combine two tables using JOIN or LEFT JOIN.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \OperatorTok{*}
  \KeywordTok{FROM}\NormalTok{ table1\_name}
  \KeywordTok{JOIN}\NormalTok{ table2\_name}
    \KeywordTok{ON}\NormalTok{ table1\_name.colum1 }\OperatorTok{=}\NormalTok{ table2\_name.column1;}
\end{Highlighting}
\end{Shaded}

Be careful to specify the matching columns using dot notation. Primary
key columns uniquely identify rows and are: 1) never NULL; 2) unique; 3)
only one column per table. A primary key can be primary in one table and
foreign in another. Unique columns have a different value for every row
and there can be many in one table.

UNION is the equivalent of cbind if the tables are already fairly
similar.

\hypertarget{parquet}{%
\subsection{Parquet}\label{parquet}}

While the use of CSVs is great because they are so widely used and have
very little overhead, they are also very minimal. This can lead to
issues, especially in terms of class. There are various modern
alternatives, including \texttt{arrow} (Richardson et al. 2022). Where
we use \texttt{write\_csv()} and \texttt{read\_csv()} we can use
\texttt{write\_parquet()} and \texttt{read\_parquet()}. One advantage is
that it should retain the class between R and Python. It should also be
faster than CSV.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(arrow)}
\FunctionTok{library}\NormalTok{(tictoc)}
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{number\_of\_draws }\OtherTok{\textless{}{-}} \DecValTok{1000000}

\NormalTok{some\_data }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{first =} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ number\_of\_draws),}
    \AttributeTok{second =} \FunctionTok{sample}\NormalTok{(}\AttributeTok{x =}\NormalTok{ LETTERS, }\AttributeTok{size =}\NormalTok{ number\_of\_draws, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}

\FunctionTok{tic}\NormalTok{(}\StringTok{"CSV"}\NormalTok{)}
\FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{x =}\NormalTok{ some\_data,}
          \AttributeTok{file =} \StringTok{"some\_data.csv"}\NormalTok{)}
\FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"some\_data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000,000 x 2
    first second
    <dbl> <chr> 
 1 0.220  Q     
 2 0.768  N     
 3 0.902  Y     
 4 0.564  V     
 5 0.388  H     
 6 0.895  Z     
 7 0.341  Y     
 8 0.145  X     
 9 0.462  C     
10 0.0403 H     
# ... with 999,990 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{toc}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
CSV: 0.351 sec elapsed
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tic}\NormalTok{(}\StringTok{"parquet"}\NormalTok{)}
\FunctionTok{write\_parquet}\NormalTok{(}\AttributeTok{x =}\NormalTok{ some\_data,}
              \AttributeTok{sink =} \StringTok{"some\_data.parquet"}\NormalTok{)}
\FunctionTok{read\_parquet}\NormalTok{(}\AttributeTok{file =} \StringTok{"some\_data.parquet"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000,000 x 2
    first second
    <dbl> <chr> 
 1 0.220  Q     
 2 0.768  N     
 3 0.902  Y     
 4 0.564  V     
 5 0.388  H     
 6 0.895  Z     
 7 0.341  Y     
 8 0.145  X     
 9 0.462  C     
10 0.0403 H     
# ... with 999,990 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{toc}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
parquet: 0.329 sec elapsed
\end{verbatim}

\hypertarget{exercises-and-tutorial-18}{%
\section{Exercises and tutorial}\label{exercises-and-tutorial-18}}

\hypertarget{exercises-18}{%
\subsection{Exercises}\label{exercises-18}}

\hypertarget{tutorial-18}{%
\subsection{Tutorial}\label{tutorial-18}}

\hypertarget{paper-5}{%
\subsection{Paper}\label{paper-5}}

At about this point, the Final Paper Appendix~\ref{sec-final-paper}
would be appropriate.

\hypertarget{sec-concluding-remarks}{%
\chapter{Concluding remarks}\label{sec-concluding-remarks}}

\textbf{Required material}

\begin{itemize}
\tightlist
\item
  Watch \emph{Wrong Again! 30+ Years of Statistical Mistakes}, (Gelman
  2021)
\item
  Read \emph{Five ways to fix statistics}, (Leek et al. 2017)
\end{itemize}

\hypertarget{concluding-remarks}{%
\section{Concluding remarks}\label{concluding-remarks}}

There is an old saying, something along the lines of `may you live in
interesting times'. Possibly every generation feels this way, but we
sure live in interesting times. In this book, we have covered a broad
range of essential skills that would allow you to tell stories with
data. But we are just getting started.

In a little over a decade data science has gone from something that
barely existed, to a defining part of academia and industry. The extent
and pace of this change has many implications for those learning data
science. For instance, it may imply that one should not just make
decisions that optimize for what data science looks like right now, but
also what could happen. While that is a little difficult, that is also
one of the things that makes data science so exciting. That might mean
choices like:

\begin{itemize}
\tightlist
\item
  taking courses on fundamentals, not just fashionable applications;
\item
  reading books, not just whatever is trending; and
\item
  trying to be at the intersection of at least a few different areas,
  rather than hyper-specialized.
\end{itemize}

One of the most exciting times when you learn data science is realizing
that you just love playing with data. A decade ago, this did not fit
into any particular department, these days it fits into almost any of
them. Data science needs diversity, both in terms of approaches and
applications. It is increasingly the most important work in the world
and hegemonic approaches have no place. It is just such an exciting time
to be enthusiastic about data and able to build.

\hypertarget{some-issues}{%
\section{Some issues}\label{some-issues}}

Data science draws from a variety of disciplines. But there are a
variety of concerns that are common across them. Here we detail a few.

\textbf{1. How do we write unit tests for data science?}

One thing that computer scientists know is the importance of unit tests.
Basically this just means writing down the small checks that we do in
our heads all the time. Like if we have a column that purports to the
year, then it's unlikely that it's a character, and it's unlikely that
it's an integer larger than 2500, and it's unlikely that it's a negative
integer. We know all this, but writing unit tests has us write this all
down.

In this case it's obvious what the unit test looks like. But more
generally, we often have little idea what our results should look like
if they're running well. The approach that I have taken is to add
simulation---so we simulate reasonable results, write unit tests based
on that, and then bring the real data to bear and adjust as necessary.
But I really think that we need extensive work in this area because the
current state-of-the-art is lacking.

\textbf{2. What happened to the machine learning revolution?}

I don't understand what happened to the promised machine learning
revolution in social sciences. Specifically, I am yet to see any
convincing application of machine learning methods that are designed for
prediction to a social sciences problem where what we care about is
understanding. I would like to either see evidence of them or a
definitive thesis about why this can't happen. The current situation is
untenable where folks, especially those in fields that have been
historically female, are made to feel inferior even though their results
are no worse.

\textbf{3. What do we do about p-values and power?}

As someone who learnt statistics from economists, but now is partly in a
statistics department, I do think that everyone should learn statistics
from statisticians. This isn't anything against economists, but the
conversations that I have in the statistics department about what
statistical methods are and how they should be used are very different
to those that I have had in other departments.

I think the problem is that people outside statistics, treat statistics
as a recipe in which they follow various steps and then out comes a
cake. With regard to `power'---it turns out that there were a bunch of
instructions that no one bothered to check---they turned the oven on to
some temperature without checking that it was 180C, and that's fine
because whatever mess came out was accepted because the people
evaluating the cake didn't know that they needed to check the
temperature had been appropriately set. (I am ditching this analogy
right now).

As you know, the issue with power is related to the broader discussion
about p-values, which basically no one is taught properly, because it
would require changing an awful lot about how we teach statistics
i.e.~moving away from the recipe approach.

And so, my specific issue is that people think that statistics is a
recipe to be followed. They think that because that's how they are
trained especially in social sciences like political science and
economics, and that's what is rewarded. But that's not what these
methods are. Instead, statistics is a collection of different
instruments that let us look at our data in a certain way. I think that
we need a revolution here, not a metaphorical tucking in of one's shirt.

\textbf{4. How do we teach data science?}

We are beginning to start to have agreement on what the foundations of
data science are. It involves computational thinking, concern for
sampling, statistics, graphics, Git/GitHub, SQL, command line, comfort
with messy data, comfort across a few languages including R and Python.
But we have very little agreement on how best to teach it. Partly this
is because data science instructors often come different fields, but
also it is also partly a difference in resources and priorities.

\textbf{5. What is happening at the data cleaning and preparation
stage?}

We basically do not have a good understanding how much any of this
matters. Huntington-Klein et al. (2021) showed that hidden research
decisions have a big effect on subsequent estimates, sometimes greater
than the standard errors. Such findings invalidate claims. We need much
more investigation of how these early stages of the data science
workflow affect the conclusions.

\hypertarget{next-steps}{%
\section{Next steps}\label{next-steps}}

This book has covered much ground, and while we are toward the end of
it, as the butler Stevens is told in the novel \emph{The Remains of the
Day} (Ishiguro 1989):

\begin{quote}
The evening's the best part of the day. You've done your day's work. Now
you can put your feet up and enjoy it.
\end{quote}

Chances are there are aspects that you want to explore further, building
on the foundation that you have established. If so, then I have
accomplished what I set out to do.

If you were new to data science at the start of this book, then the next
step would be to backfill that which I skipped over, and I would
recommend T.-A. Timbers, Campbell, and Lee (2022). After that, you
should learn more about R in terms of data science by going through
Wickham and Grolemund (2017). To deepen your understanding of R itself,
go next to Wickham (2019a).

If you are interested in learning more about causality then start with
Cunningham (2021) and Huntington-Klein (2021).

If you are interested to learn more about statistics then begin with
McElreath (2020), and then backfill with A. A. Johnson, Ott, and Dogucu
(2022) and solidify the foundation with Gelman et al. (2014). You should
probably also backfill some of the fundamentals around probability,
starting with Wasserman (2005).

There is only one next natural step if you are interested in learning
more about statistical (what has come to be called machine) learning and
that's James et al. (2017) followed by Friedman, Tibshirani, and Hastie
(2009).

If you are interested in sampling then the next book to turn to is Lohr
(2019). To deepen your understanding of surveys and experiments, go next
to Gerber and Green (2012) in combination with Kohavi, Tang, and Xu
(2020).

For developing better data visualization skills, begin by turning to
Healy (2018), but then after that, develop strong foundations, such as
L. Wilkinson (2005). For writing, it would be best to turn inward. Force
yourself to write and publish everyday for a month. Then do it again and
again. You will get better. That said, there are some useful books,
including Caro (2019) and S. King (2000).

We often hear the phrase let the data speak. Hopefully by this point you
understand that never happens. All that we can do is to acknowledge that
we are the ones using data to tell stories, and strive and seek to make
them worthy.

\begin{quote}
It was her voice that made\\
The sky acutest at its vanishing.\\
She measured to the hour its solitude.\\
She was the single artificer of the world\\
In which she sang. And when she sang, the sea,\\
Whatever self it had, became the self\\
That was her song, for she was the maker.

`The Idea of Order at Key West', (Stevens 1934)
\end{quote}

\appendix
\addcontentsline{toc}{part}{Appendices}

\hypertarget{sec-papers}{%
\chapter{Papers}\label{sec-papers}}

\hypertarget{sec-paper-one}{%
\section{Paper One}\label{sec-paper-one}}

\hypertarget{task}{%
\subsection{Task}\label{task}}

\begin{itemize}
\tightlist
\item
  Working individually and in an entirely reproducible way, please find
  a dataset of interest on \href{https://open.toronto.ca}{Open Data
  Toronto} and write a short paper telling a story about the data.

  \begin{itemize}
  \tightlist
  \item
    Create a well-organized folder with appropriate sub-folders, and add
    it to GitHub. You are welcome to use this
    \href{https://github.com/RohanAlexander/starter_folder}{starter
    folder}.
  \item
    Find a dataset of interest on \href{https://open.toronto.ca}{Open
    Data Toronto}.

    \begin{itemize}
    \tightlist
    \item
      Put together an R script, `scripts/00-simulation.R', that
      simulates the dataset of interest. Push to GitHub and include an
      informative commit message
    \item
      Write an R script, `scripts/00-download\_data.R' to download the
      actual data in a reproducible way using \texttt{opendatatoronto}
      (Gelfand 2020). Save the data: `inputs/data/raw\_data.csv'. Push
      to GitHub and include an informative commit message
    \end{itemize}
  \item
    Prepare a PDF using Quarto `outputs/paper/paper.qmd' with these
    sections: title, author, date, abstract, introduction, data, and
    references.

    \begin{itemize}
    \tightlist
    \item
      The title should be descriptive, informative, and specific.
    \item
      The date should be in an unambiguous format. Add a link to the
      GitHub repo in the acknowledgements.
    \item
      The abstract should be three or four sentences. The abstract must
      tell the reader the top-level finding. What is the one thing that
      we learn about the world because of this paper?
    \item
      The introduction should be two or three paragraphs of content. And
      there should be an additional final paragraph that sets out the
      remainder of the paper.
    \item
      The data section should thoroughly and precisely discuss the
      source of the data and the bias this brings (ethical, statistical,
      and otherwise). Comprehensively describe and summarize the data
      using text, graphs, and tables. Graphs must be made with
      \texttt{ggplot2} (Wickham 2016) and tables must be made with
      \texttt{knitr} (Xie 2021) or \texttt{gt} (Iannone, Cheng, and
      Schloerke 2020). Graphs must show the actual data, or as close to
      it as possible, not summary statistics. Graphs and tables should
      be cross-referenced in the text e.g.~`Table 1 shows\ldots{}').
    \item
      References should be added using BibTeX. Be sure to reference R
      and any R packages you use, as well as the dataset. Strong
      submissions will draw on related literature and reference those.
    \item
      The paper should be well-written, draw on relevant literature, and
      explain all technical concepts. Pitch it at an educated, but
      non-specialist, audience.
    \item
      Use appendices for supporting, but not critical, material.
    \item
      Push to GitHub and include an informative commit message
    \end{itemize}
  \end{itemize}
\item
  Submit a PDF of your paper.
\item
  There should be no evidence that this is a class assignment.
\end{itemize}

\hypertarget{checks}{%
\subsection{Checks}\label{checks}}

\begin{itemize}
\tightlist
\item
  There should be no R code or raw R output in the final PDF.
\item
  Code should be entirely reproducible, well-documented, commented, and
  readable.
\item
  The paper should knit directly to PDF i.e.~use `Knit to PDF'.

  \begin{itemize}
  \tightlist
  \item
    Do not use `Knit to html' and then save as a PDF.
  \item
    Do not use `Knit to Word' and then save as a PDF
  \end{itemize}
\item
  Graphs, tables, and text should be clear, and of comparable quality to
  those of FiveThirtyEight.
\item
  The date should be up-to-date and unambiguous (e.g.~2/3/2022 is
  ambiguous, 2 March 2022 is not).
\item
  The entire workflow should be entirely reproducible.
\item
  There should not be any typos.
\item
  There should be no sign this is a school paper.
\item
  There must be a link to the paper's GitHub repo using a footnote.
\item
  The GitHub repo should be well-organized, and contain an informative
  README.
\item
  The paper should be well-written and able to be understood by the
  average reader of, say, FiveThirtyEight. This means that you are
  allowed to use mathematical notation, but you must explain all of it
  in plain language. All statistical concepts and terminology must be
  explained. Your reader is someone with a university education, but not
  necessarily someone who understands what a p-value is.
\end{itemize}

\hypertarget{faq}{%
\subsection{FAQ}\label{faq}}

\begin{itemize}
\tightlist
\item
  Can I use a dataset from Kaggle instead? No, because they have done
  the hard work for you.
\item
  I cannot use code to download my dataset, can I just manually download
  it? No, because your entire workflow needs to be reproducible. Please
  fix the download problem or pick a different dataset.
\item
  How much should I write? Most students submit something in the
  two-to-six-page range, but it is up to you. Be precise and thorough.
\item
  My data is about apartment blocks/NBA/League of Legends so there's no
  ethical or bias aspect, what do I do? Please re-read the readings to
  better understand bias and ethics. If you really cannot think of
  something, then it might be worth picking a different dataset.
\item
  Can I use Python? No.~If you already know Python then it doesn't hurt
  to learn another language.
\item
  Why do I need to cite R, when I don't need to cite Word? R is a free
  statistical programming language with academic origins, so it is
  appropriate to acknowledge the work of others. it is also important
  for reproducibility.
\item
  What reference style should I use? Any major reference style is fine
  (APA, Harvard, Chicago, etc); just pick one that you are used to.
\item
  The paper in the starter folder has a model section, so do I need to
  put together a model? No.~The starter folder is designed to be
  applicable to all of the papers; just delete the aspects that you do
  not need.
\item
  What does `graph the actual data' mean? If you have, say 5,000
  observations in the dataset and three variables, then for every
  variable there should be a graph that has 5,000 points in the case of
  dots, or adds up to 5,000 in the case of bar charts and histograms.
\end{itemize}

\hypertarget{rubric}{%
\subsection{Rubric}\label{rubric}}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{lll}
\toprule
Component & Range & Requirement \\ 
\midrule
R is appropriately cited & 0 - 'No'; 1 - 'Yes' & Must be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall. \\ 
Title & 0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional' & An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper. \\ 
Author, date, and repo & 0 - 'Poor or not done'; 2 - 'Yes' & The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK'). \\ 
Abstract & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper. \\ 
Introduction & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total. \\ 
Data & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & A sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used. \\ 
Cross-references & 0 - 'Poor or not done'; 2 - 'Yes' & All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. \\ 
Prose & 0 - 'Poor or not done'; 2 - 'Yes' & All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear. \\ 
Graphs/tables/etc & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables. \\ 
Reference list & 0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect' & All data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. \\ 
Commits & 0 - 'Poor or not done'; 2 - 'Excellent' & There are at least two different commits, and they have meaningful commit messages. \\ 
Simulation & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The script is clearly commented and structured. All variables are appropriately simulated. \\ 
Reproducibility & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used. \\ 
General excellence & 0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional' & There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that. \\ 
 \bottomrule
\end{longtable}

\hypertarget{previous-examples}{%
\subsection{Previous examples}\label{previous-examples}}

\href{inputs/pdfs/paper_one-2022-adam_labas.pdf}{Adam Labas},
\href{inputs/pdfs/paper_one-2022-alicia_yang.pdf}{Alicia Yang},
\href{inputs/pdfs/paper_one-2022-alyssa_schleifer.pdf}{Alyssa
Schleifer}, \href{inputs/pdfs/paper_one-2021-Amy_Farrow.pdf}{Amy
Farrow}, \href{inputs/pdfs/paper_one-2022-ethan_sansom.pdf}{Ethan
Sansom}, \href{inputs/pdfs/paper_one-2022-hudson_yuen.pdf}{Hudson Yuen},
\href{inputs/pdfs/paper_one-2022-jack_mckay.pdf}{Jack McKay},
\href{inputs/pdfs/paper_one-2021-Morgaine_Westin.pdf}{Morgaine Westin},
\href{inputs/pdfs/paper_one-2021-Rachael_Lam.pdf}{Rachael Lam},
\href{inputs/pdfs/paper_one-2022-roy_chan.pdf}{Roy Chan},
\href{inputs/pdfs/paper_one-2022-thomas_donofrio.pdf}{Thomas D'Onofrio},
and \href{inputs/pdfs/paper_one-2022-william_gerecke.pdf}{William
Gerecke}.

\newpage

\hypertarget{sec-paper-two}{%
\section{Paper Two}\label{sec-paper-two}}

\hypertarget{task-1}{%
\subsection{Task}\label{task-1}}

\begin{itemize}
\tightlist
\item
  Working as part of a team of one to three people, please pick a paper
  of interest to you, with code and data that are available, published
  anytime since 2019, in an American Economic Association
  \href{https://www.aeaweb.org/journals}{journal}. These journals are:
  `American Economic Review', `AER: Insights', `AEJ: Applied Economics',
  `AEJ: Economic Policy', `AEJ: Macroeconomics', `AEJ: Microeconomics',
  `Journal of Economic Literature', `Journal of Economic Perspectives',
  `AEA Papers \& Proceedings'.
\item
  Following the \href{https://bitss.github.io/ACRE/}{\emph{Guide for
  Accelerating Computational Reproducibility in the Social Sciences}},
  please complete a \textbf{replication}\footnote{This terminology is
    used following Barba (2018), but it is the opposite of that used by
    BITSS.} of at least three graphs, tables, or a combination, from
  that paper, using the
  \href{https://www.socialsciencereproduction.org}{Social Science
  Reproduction Platform}. Note the DOI of your replication.
\item
  Working in an entirely reproducible way then conduct a
  \textbf{reproduction} based on two or three aspects of the paper, and
  write a short paper about that.

  \begin{itemize}
  \tightlist
  \item
    Create a well-organized folder with appropriate sub-folders, add it
    to GitHub, and then prepare a PDF using Quarto with these sections
    (you are welcome to use this
    \href{https://github.com/RohanAlexander/starter_folder}{starter
    folder}): title, author, date, abstract, introduction, data,
    results, discussion, and references.
  \item
    The aspects that you focus on in your paper could be the same
    aspects that you replicated, but they do not need to be. Follow the
    direction of the paper, but make it your own. That means you should
    ask a slightly different question, or answer the same question in a
    slightly different way, but still use the same dataset.
  \item
    Include the DOI of your replication in your paper and a link to the
    GitHub repo that underpins your paper.
  \item
    The results section should convey findings.
  \item
    The discussion should include three or four sub-sections that each
    focus on an interesting point, and there should be another
    sub-section on the weaknesses of your paper, and another on
    potential next steps for it.
  \item
    In the discussion section, and any other relevant section, please be
    sure to discuss ethics and bias, with reference to relevant
    literature.
  \item
    The paper should be well-written, draw on relevant literature, and
    explain all technical concepts. Pitch it at an educated, but
    non-specialist, audience.
  \item
    Use appendices for supporting, but not critical, material.
  \item
    Code should be entirely reproducible, well-documented, and readable.
  \end{itemize}
\item
  Submit a PDF of your paper.
\item
  There should be no evidence that this is a class assignment.
\end{itemize}

\hypertarget{checks-1}{%
\subsection{Checks}\label{checks-1}}

\begin{itemize}
\tightlist
\item
  The paper should not just copy/paste the code from the original paper,
  but have instead used that as a foundation to work from.
\item
  Your paper should have a link to the associated GitHub repository and
  the DOI of the Social Science Reproduction Platform replication that
  you conducted.
\item
  Make sure you have referenced everything, including R. Strong
  submissions will draw on related literature in the discussion (and
  other sections) and would be sure to also reference those. The style
  of references does not matter, provided it is consistent.
\end{itemize}

\hypertarget{faq-1}{%
\subsection{FAQ}\label{faq-1}}

\begin{itemize}
\tightlist
\item
  How much should I write? Most students submit something in the
  10-to-15-page range, but it is up to you. Be precise and thorough.
\item
  Do I have to focus on a model result? No, it is likely best to stay
  away from that at this point, and instead focus on tables or graphs of
  summary or explanatory statistics.
\item
  What if the paper I choose is in a language other than R? Both your
  replication and reproduction code should be in R. So you will need to
  translate the code into R for the replication. And the reproduction
  should be your own work, so that also should be in R. One common
  language is Stata, and Huntington-Klein (2022) might be useful as a
  `Rosetta Stone' of sorts, for R, Python, and Stata.
\item
  Can I work by myself? Yes.
\item
  Do the graphs/tables have to look identical to the original? No, you
  are welcome to, and should, make them look better as part of the
  reproduction. And even as part of the replication, they do not have to
  be identical, just similar enough.
\end{itemize}

\hypertarget{rubric-1}{%
\subsection{Rubric}\label{rubric-1}}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{lll}
\toprule
Component & Range & Requirement \\ 
\midrule
R is appropriately cited & 0 - 'No'; 1 - 'Yes' & Must be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall. \\ 
Replication & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & SSRP submission needs to be filled out completely for three elements. \\ 
Title & 0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional' & An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper. \\ 
Author, date, and repo & 0 - 'Poor or not done'; 2 - 'Yes' & The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK'). \\ 
Abstract & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper. \\ 
Introduction & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total. \\ 
Data & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & A sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used. \\ 
Results & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars. \\ 
Discussion & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future? \\ 
Cross-references & 0 - 'Poor or not done'; 2 - 'Yes' & All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. \\ 
Prose & 0 - 'Poor or not done'; 2 - 'Yes' & All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear. \\ 
Graphs/tables/etc & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables. \\ 
Reference list & 0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect' & All data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. \\ 
Commits & 0 - 'Poor or not done'; 2 - 'Excellent' & There are at least two different commits, and they have meaningful commit messages. \\ 
Simulation & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The script is clearly commented and structured. All variables are appropriately simulated. \\ 
Reproducibility & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used. \\ 
General excellence & 0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional' & There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that. \\ 
 \bottomrule
\end{longtable}

\hypertarget{previous-examples-1}{%
\subsection{Previous examples}\label{previous-examples-1}}

\href{inputs/pdfs/paper_two-2022-Alyssa_Schleifer_Hudson_Yuen_Tamsen_Yau.pdf}{Alyssa
Schleifer, Hudson Yuen, Tamsen Yau};
\href{inputs/pdfs/paper_two-2022-Olaedo_Okpareke_Arsh_Lakhanpal_Swarnadeep_Chattopadhyay.pdf}{Olaedo
Okpareke, Arsh Lakhanpal, Swarnadeep Chattopadhyay}; and
\href{inputs/pdfs/paper_two-2022-Kimlin_Chin.pdf}{Kimlin Chin}.

\newpage

\hypertarget{sec-paper-three}{%
\section{Paper Three}\label{sec-paper-three}}

\hypertarget{task-2}{%
\subsection{Task}\label{task-2}}

\begin{itemize}
\tightlist
\item
  Working as part of a team of one to three people, and in an entirely
  reproducible way, please obtain data from the
  \href{https://gss.norc.org/Get-The-Data}{US General Social
  Survey}\footnote{The US GSS is recommended here because
    individual-level data are publicly available, and the dataset is
    well-documented. But, often university students in particular
    countries have access to individual level data that are not
    available to the public, and if this is the case then you are
    welcome to use that instead. Students at Australian universities
    will likely have access to individual-level data from the Australian
    General Social Survey, and could use that. Students at Canadian
    universities will likely have access to individual-level data from
    the Canadian General Social and may like to use that.}. (You are
  welcome to use a different government-run survey, but please obtain
  permission before starting.)
\item
  Obtain the data, focus on one aspect of the survey, and then use it to
  tell a story.

  \begin{itemize}
  \tightlist
  \item
    Create a well-organized folder with appropriate sub-folders, add it
    to GitHub, and then use Quarto to prepare a PDF with these sections
    (you are welcome to use this
    \href{https://github.com/RohanAlexander/starter_folder}{starter
    folder}): title, author, date, abstract, introduction, data,
    results, discussion, an appendix that will, at least, contain a
    survey, and references.
  \item
    In addition to conveying a sense of the dataset of interest, the
    data section should include, but not be limited to:

    \begin{itemize}
    \tightlist
    \item
      A discussion of the survey's methodology, and its key features,
      strengths, and weaknesses. For instance: what is the population,
      frame, and sample; how is the sample recruited; what sampling
      approach is taken, and what are some of the trade-offs of this;
      how is non-response handled.
    \item
      A discussion of the questionnaire: what is good and bad about it?
    \item
      If this becomes too detailed, then use appendices for supporting
      but not essential aspects.
    \end{itemize}
  \item
    In an appendix, please put together a supplementary survey that
    could be used to augment the general social survey the paper focuses
    on. The purpose of the supplementary survey is to gain additional
    information on the topic that is the focus of the paper, beyond that
    gathered by the general social survey. The survey would be
    distributed in the same manner as the general social survey but
    needs to stand independently. The supplementary survey should be put
    together using a survey platform. A link to this should be included
    in the appendix. Additionally, a copy of the survey should be
    included in the appendix.
  \item
    Please be sure to discuss ethics and bias, with reference to
    relevant literature.
  \item
    Code should be entirely reproducible, well-documented, and readable.
  \end{itemize}
\item
  Submit a PDF of the paper.
\item
  The paper should be well-written, draw on relevant literature, and
  explain all technical concepts. Pitch it at a university-educated, but
  non-specialist, audience. Use survey, sampling, and statistical
  terminology, but be sure to explain it. The paper should flow, and be
  easy to follow and understand.
\item
  There should be no evidence that this is a class paper.
\end{itemize}

\hypertarget{checks-2}{%
\subsection{Checks}\label{checks-2}}

\begin{itemize}
\tightlist
\item
  An appendix should contain both a link to the supplementary survey and
  the details of it, including questions (in case the link fails, and to
  make the paper self-contained).
\end{itemize}

\hypertarget{faq-2}{%
\subsection{FAQ}\label{faq-2}}

\begin{itemize}
\tightlist
\item
  What should I focus on? You may focus on any year, aspect, or
  geography that is reasonable given the focus and constraints of the
  general social survey that you are interested in. Please consider the
  year and topics that you are interested in together, as some surveys
  focus on particular topics in some years.
\item
  Do I need to include the raw GSS data in the repo? For most of the
  general social surveys you will not have permission to share the GSS
  data. If that is the case, then you should add clear details in the
  README explaining how the data could be obtained.
\item
  How many graphs do I need? In general, you need at least as many
  graphs as you have variables, because you need to show all the
  observations for all variables. But you may be able to combine a few;
  or, vice versa, you may be interested in looking at different aspects
  or relationships.
\end{itemize}

\hypertarget{rubric-2}{%
\subsection{Rubric}\label{rubric-2}}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{lll}
\toprule
Component & Range & Requirement \\ 
\midrule
R is appropriately cited & 0 - 'No'; 1 - 'Yes' & Must be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall. \\ 
Title & 0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional' & An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper. \\ 
Author, date, and repo & 0 - 'Poor or not done'; 2 - 'Yes' & The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK'). \\ 
Abstract & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper. \\ 
Introduction & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total. \\ 
Data & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & A sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used. \\ 
Results & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars. \\ 
Discussion & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future? \\ 
Cross-references & 0 - 'Poor or not done'; 2 - 'Yes' & All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. \\ 
Prose & 0 - 'Poor or not done'; 2 - 'Yes' & All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear. \\ 
Graphs/tables/etc & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables. \\ 
Survey & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & The survey should have an introductory section and include the details of a contact person. The survey questions should be well constructed and appropriate to the task. The questions should have an appropriate ordering. A final section should thank the respondent. \\ 
Reference list & 0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect' & All data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. \\ 
Commits & 0 - 'Poor or not done'; 2 - 'Excellent' & There are at least two different commits, and they have meaningful commit messages. \\ 
Simulation & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The script is clearly commented and structured. All variables are appropriately simulated. \\ 
Reproducibility & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used. \\ 
General excellence & 0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional' & There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that. \\ 
 \bottomrule
\end{longtable}

\hypertarget{previous-examples-2}{%
\subsection{Previous examples}\label{previous-examples-2}}

\href{inputs/pdfs/paper3-2022-Li_Sheikh.pdf}{Anna Li and Mohammad Sardar
Sheikh}; \href{inputs/pdfs/paper3-2022-hui_chau.pdf}{Chyna Hui and Marco
Chau}; \href{inputs/pdfs/paper3-2022-Ethan_Sansom.pdf}{Ethan Sansom};
\href{inputs/pdfs/paper3-2022-LuckynaLaurent_SamitaPrabhasavat_ZoieSo.pdf}{Luckyna
Laurent, Samita Prabhasavat, and Zoie So};
\href{inputs/pdfs/paper3-2022-Pascal_Lee_Slew_Yunkyung_Park.pdf}{Pascal
Lee Slew, and Yunkyung\_Park}; and
\href{inputs/pdfs/paper3-2022-Ray_Wen_Isfandyar_Virani_Rayhan_Walia.pdf}{Ray
Wen, Isfandyar Virani, and Rayhan Walia}.

\newpage

\hypertarget{sec-paper-four}{%
\section{Paper Four}\label{sec-paper-four}}

\hypertarget{task-3}{%
\subsection{Task}\label{task-3}}

\begin{itemize}
\tightlist
\item
  Working as part of a team of one to three people, and in an entirely
  reproducible way, please convert one full-page table from one DHS
  Program `Final Report', from the 1980s or 1990s, as available
  \href{https://dhsprogram.com/search/index.cfm?_srchd=1\&bydoctype=publication\&bypubtype=26\%2C5\%2C39\%2C30\%2C21\%2C100\&byyear=1999\&byyear=1998\&byyear=1997\&byyear=1996\&byyear=1995\&byyear=1994\&byyear=1993\&byyear=1992\&byyear=1991\&byyear=1990\&byyear=1989\&byyear=1988\&byyear=1987\&bylanguage=2}{here},
  into a usable dataset, then write a short paper telling a story with
  the data.
\item
  Create a well-organized folder with appropriate sub-folders, and add
  it to GitHub. You are welcome to use this
  \href{https://github.com/RohanAlexander/starter_folder}{starter
  folder}.
\item
  Create and document a dataset:

  \begin{itemize}
  \tightlist
  \item
    Save the PDF to `inputs'.
  \item
    Put together a simulation of your plan for the usable dataset and
    save the script to `scripts/00-simulation.R'.
  \item
    Write R code, saved as `scripts/01-gather\_data.R', to either OCR or
    parse the PDF, as appropriate, and save the output to
    `outputs/data/raw\_data.csv'.
  \item
    Write R code, saved as `scripts/02-clean\_and\_prepare\_data.R',
    that draws on `raw\_data.csv' to clean and prepare the dataset. Use
    \texttt{pointblank} to put together tests that the dataset passes
    (at a minimum, every variable should have a test for class and
    another for content). Save the dataset to
    `outputs/data/cleaned\_data.csv'.
  \item
    Following Gebru et al. (2021), put together a data sheet for the
    dataset you put together (put this in the appendix of your paper).
    You are welcome to start from the template
    `inputs/data/datasheet\_template.qmd' in the starter folder,
    although, again, you should add it to the appendix of your paper,
    rather than a stand-alone document.
  \end{itemize}
\item
  Use the dataset to tell a story by using Quarto to prepare a PDF with
  these sections: title, author, date, abstract, introduction, data,
  results, discussion, an appendix that will, at least, contain a
  datasheet for the dataset, and references.

  \begin{itemize}
  \tightlist
  \item
    In addition to conveying a sense of the dataset of interest, the
    data section should include details of the methodology used by the
    DHS you used, and its key features, strengths, and weaknesses.
  \end{itemize}
\item
  Submit a PDF of the paper.
\item
  There should be no evidence that this is a class paper.
\end{itemize}

\hypertarget{checks-3}{%
\subsection{Checks}\label{checks-3}}

\begin{itemize}
\tightlist
\item
  Use GitHub in a well-developed way by making at least a few commits
  and using descriptive commit messages.
\end{itemize}

\hypertarget{faq-3}{%
\subsection{FAQ}\label{faq-3}}

\hypertarget{rubric-3}{%
\subsection{Rubric}\label{rubric-3}}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{lll}
\toprule
Component & Range & Requirement \\ 
\midrule
R is appropriately cited & 0 - 'No'; 1 - 'Yes' & Must be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall. \\ 
Title & 0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional' & An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper. \\ 
Author, date, and repo & 0 - 'Poor or not done'; 2 - 'Yes' & The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK'). \\ 
Abstract & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper. \\ 
Introduction & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total. \\ 
Data & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & A sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used. \\ 
Results & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars. \\ 
Discussion & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future? \\ 
Cross-references & 0 - 'Poor or not done'; 2 - 'Yes' & All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. \\ 
Prose & 0 - 'Poor or not done'; 2 - 'Yes' & All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear. \\ 
Graphs/tables/etc & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables. \\ 
Reference list & 0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect' & All data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. \\ 
Commits & 0 - 'Poor or not done'; 2 - 'Excellent' & There are at least two different commits, and they have meaningful commit messages. \\ 
Simulation & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The script is clearly commented and structured. All variables are appropriately simulated. \\ 
Reproducibility & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used. \\ 
Datasheet & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & A thorough datasheet for the dataset that was constructed is included. \\ 
General excellence & 0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional' & There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that. \\ 
 \bottomrule
\end{longtable}

\hypertarget{previous-examples-3}{%
\subsection{Previous examples}\label{previous-examples-3}}

\href{inputs/pdfs/paper-4-2022-BilalHaq_RitvikPuri.pdf}{Bilal Haq and
Ritvik Puri};
\href{paper-4-2022-CharlesLu_MahakJain_YujunJiao.pdf}{Charles Lu, Mahak
Jain, and Yujun Jiao}; and
\href{inputs/pdfs/paper-4-2022-PascalLeeSlew_YunkyungPark.pdf}{Pascal
Lee Slew and Yunkyung Park}.

\newpage

\hypertarget{sec-paper-five}{%
\section{Paper Five}\label{sec-paper-five}}

\hypertarget{task-4}{%
\subsection{Task}\label{task-4}}

\begin{itemize}
\tightlist
\item
  Working as part of a team of one to three people, please forecast the
  popular vote of the 2020 US election using multilevel regression with
  post-stratification and then write a short paper telling a story. This
  requires individual-level survey data, post-stratification data, and a
  model that brings them together. Given the expense of collecting these
  data, and the privilege of having access to them, please be sure to
  properly cite all datasets that you use.
\item
  Individual-level survey data:

  \begin{itemize}
  \tightlist
  \item
    Request access to the Democracy Fund + UCLA Nationscape
    \href{https://www.voterstudygroup.org/data/nationscape}{`Full Data
    Set'}. This could take a day or two. Please start early.
  \item
    Simulate the survey dataset that you will use, and save the script
    to `scripts/00-simulation-survey.R'.
  \item
    Once you have access then pick one survey of interest (they were
    conducted at different times).
  \item
    This will be a large file and is not yours to share. Do not push it
    to GitHub. Use a .gitignore file to accomplish this. Instead
    document how to get the raw data in the README.
  \item
    Clean and prepare the dataset based on what you need.
  \end{itemize}
\item
  Post-stratification data:

  \begin{itemize}
  \tightlist
  \item
    Create an account with
    \href{https://usa.ipums.org/usa/index.shtml}{IPUMS} and then use
    this to access the American Community Surveys (ACS).
  \item
    Simulate the post-stratification dataset that you will use, and save
    the script to `scripts/00-simulation-poststratification.R'.
  \item
    Pick an appropriate 1-year ACS (there is one every year). Then
    select some variables. This will depend on what you want to model
    and the survey data, but some options include: REGION, STATEICP,
    AGE, SEX, MARST, RACE, HISPAN, BPL, CITIZEN, EDUC, LABFORCE, or
    INCTOT. Have a look around and see what you are interested in,
    remembering that you will need to establish a correspondence to the
    survey.
  \item
    Download the relevant post-stratification data (it is probably
    easiest to change the data format to .dta).
  \item
    Again, this will be a large file and is not yours to share. Do not
    push it to GitHub. Use a .gitignore file to accomplish this. Instead
    document how to get the raw data in the README.
  \item
    Clean and prepare the post-stratification dataset. Remember that you
    need cell counts for the sub-populations in the model.
  \end{itemize}
\item
  Modelling:

  \begin{itemize}
  \tightlist
  \item
    You will want to explain vote intention based on a variety of
    explanatory variables. The decision is yours, but you should
    probably use logistic regression. In that case, construct the vote
    intention variable so that it is binary (either `supports Trump' or
    `supports Biden'). In increasing level of complexity, you would then
    build a model using: \texttt{glm()}, \texttt{lme4::glmer()},
    \texttt{rstanarm::\ stan\_glm}, or \texttt{brms::brm()}.
  \item
    Think about model fit, diagnostics, and other similar aspects that
    you need to convince someone that the model is appropriate.
  \item
    You have flexibility of the model that you use, (and hence the cells
    that you will need to create). In general, the more cells the
    better, but you may want fewer cells for simplicity in the writing
    process and to ensure a decent sample in each cell. It would be best
    to start with a simple model and then complicate it, rather than
    vice versa.
  \item
    Apply the trained model to the post-stratification dataset to
    forecast the election result. The specifics will depend on your
    modelling approach but will likely involve \texttt{predict()},
    \texttt{add\_predicted\_draws()}, or similar. The primary aspect of
    interest is the forecast distribution of the popular vote, and how
    the explanatory variables affect this. Strong submissions would go
    beyond that.
  \end{itemize}
\item
  Write-up:

  \begin{itemize}
  \tightlist
  \item
    Create a well-organized folder with appropriate sub-folders, add it
    to GitHub, and then prepare a PDF using Quarto with these sections
    (you are welcome to use this
    \href{https://github.com/RohanAlexander/starter_folder}{starter
    folder}): title, author, date, abstract, introduction, data, model,
    results, discussion, and references. Use appendices for supporting,
    but not critical, material.

    \begin{itemize}
    \tightlist
    \item
      In the model section, you should carefully spell out the
      statistical model that you are using, being sure to define and
      explain each aspect and why it is important. The model should be
      appropriately complex; that is, not inappropriately simple, but
      not unnecessarily complicated. The model should have well-defined
      variables and these should correspond to what is discussed in the
      data section. You should explain how the aspects discussed in the
      data section assert themselves in the modelling decisions that you
      made. The model should be written out in appropriate mathematical
      notation but also in plain English. Every aspect of that notation
      should be defined. The model should make sense based on the
      substantive area, and the form of the model. If the model is
      Bayesian, then priors should be defined and sensible. There should
      be explanation of how features enter the model and why. For
      instance, why use age rather than age-groups, why does province
      have a levels effect, why is gender categorical, etc? In general,
      there should be a clear justification that this is the model for
      the situation. The assumptions underpinning the model should be
      clearly discussed. Alternative models, or variants, should be
      discussed, and strengths and weaknesses made clear. Why was this
      model chosen? You should mention the software that you used to run
      the model. There should be evidence of thought about the
      circumstances in which the model may not be appropriate. There
      should be evidence of model validation and checking, whether that
      is out-of-sample, RMSE, a test/training split, or appropriate
      sensitivity checks. You should be clear about model convergence,
      model checks, and diagnostic issues.
    \end{itemize}
  \end{itemize}
\item
  Submit a PDF of your paper.
\item
  There should be no evidence that this is a class assignment.
\end{itemize}

\hypertarget{checks-4}{%
\subsection{Checks}\label{checks-4}}

\begin{itemize}
\tightlist
\item
  Use GitHub in a well-developed way by making at least a few commits
  and using descriptive commit messages.
\item
  Do not include p-values, stars, or similar, in tables. If you invoke
  statistical significance, then you should draw on and integrate R. A.
  Fisher (1926) and others.
\end{itemize}

\hypertarget{faq-4}{%
\subsection{FAQ}\label{faq-4}}

\begin{itemize}
\tightlist
\item
  How much should I write? Most students submit something in the
  10-to-15-page range, but it is up to you. Be precise and thorough.
\end{itemize}

\hypertarget{rubric-4}{%
\subsection{Rubric}\label{rubric-4}}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{lll}
\toprule
Component & Range & Requirement \\ 
\midrule
R is appropriately cited & 0 - 'No'; 1 - 'Yes' & Must be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall. \\ 
Title & 0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional' & An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper. \\ 
Author, date, and repo & 0 - 'Poor or not done'; 2 - 'Yes' & The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK'). \\ 
Abstract & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper. \\ 
Introduction & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total. \\ 
Data & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & A sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used. \\ 
Model & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & The model should be nicely written out, well-explained, justified, and appropriate. \\ 
Results & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars. \\ 
Discussion & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future? \\ 
Cross-references & 0 - 'Poor or not done'; 2 - 'Yes' & All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. \\ 
Prose & 0 - 'Poor or not done'; 2 - 'Yes' & All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear. \\ 
Graphs/tables/etc & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables. \\ 
Reference list & 0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect' & All data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. \\ 
Commits & 0 - 'Poor or not done'; 2 - 'Excellent' & There are at least two different commits, and they have meaningful commit messages. \\ 
Simulation & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The script is clearly commented and structured. All variables are appropriately simulated. \\ 
Reproducibility & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used. \\ 
General excellence & 0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional' & There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that. \\ 
 \bottomrule
\end{longtable}

\hypertarget{previous-examples-4}{%
\subsection{Previous examples}\label{previous-examples-4}}

\href{inputs/pdfs/paper_five_2020-Mitrovski_Yang_Wankiewicz.pdf}{Alen
Mitrovski, Xiaoyan Yang, Matthew Wankiewicz} (this paper received an
`Honorable Mention' in the ASA December 2020 Undergraduate Statistics
Research Project competition.)

\newpage

\hypertarget{sec-final-paper}{%
\section{Final paper}\label{sec-final-paper}}

\hypertarget{task-5}{%
\subsection{Task}\label{task-5}}

\begin{itemize}
\tightlist
\item
  Working \textbf{individually} and in an entirely reproducible way
  please write a paper that involves original work to tell a story with
  data.
\item
  Options include (pick one):

  \begin{itemize}
  \tightlist
  \item
    Develop a research question that is of interest to you based on your
    own interests, background, and expertise, then obtain or create a
    relevant dataset.
  \item
    A reproduction, being sure to use the paper as a foundation rather
    than as an end-in-itself.
  \end{itemize}
\item
  Create a well-organized folder with appropriate sub-folders, add it to
  GitHub, and then prepare a PDF using Quarto with these sections (you
  are welcome to use this
  \href{https://github.com/RohanAlexander/starter_folder}{starter
  folder}):

  \begin{itemize}
  \tightlist
  \item
    Title, date, author, abstract, introduction, data, model, results,
    discussion, appendix (optional, for supporting, but not critical,
    material), and a reference list.
  \item
    It must also include an enhancement, and this would either be
    contained, or linked to, in the appendix.
  \end{itemize}
\end{itemize}

\hypertarget{peer-review-submission}{%
\subsection{Peer review submission}\label{peer-review-submission}}

\begin{itemize}
\tightlist
\item
  This is an initial `submission' where you get comments and feedback on
  a draft.
\item
  Submit a PDF of your draft.
\item
  The paper does not have to be finished at this point, but the
  following sections must be filled out: title, author, date, abstract,
  and introduction.
\item
  All other sections must be present in the paper, but do not have to be
  filled out (e.g.~you must have a `Data' heading, but you do not need
  to have content in that section).
\item
  To be clear, it is fine to later change any aspect of what you submit
  at this checkpoint.
\item
  You will be awarded one percentage point just for submitting a draft
  that meets this minimum.
\item
  There are no extensions possible for this submission because the
  following submission is dependent on this date.
\end{itemize}

\hypertarget{conduct-peer-review}{%
\subsection{Conduct peer-review}\label{conduct-peer-review}}

\begin{itemize}
\tightlist
\item
  As an individual, you will randomly be assigned a handful of rough
  drafts to provide feedback. You have three days to provide feedback to
  your peers.
\item
  If you provide feedback to one peer you will receive one percentage
  point, if you provide feedback to two peers you will receive two
  percentage points, etc.
\item
  Your feedback must include at least five comments (meaningful and
  useful bullet points). These must be well-written and thoughtful.
\item
  There are no extensions granted for this submission since the
  following submission is dependent on this date.
\item
  Please remember that you are providing feedback here to help your
  colleagues. All comments should be professional and kind. It is
  challenging to receive criticism. Please remember that your goal here
  is to help your peers advance their writing/analysis.
\end{itemize}

\hypertarget{faq-5}{%
\subsection{FAQ}\label{faq-5}}

\begin{itemize}
\tightlist
\item
  Can I work as part of a team? No.~it is important that you have some
  work that is entirely your own. You really need your own work to show
  off for job applications etc.
\item
  How much should I write? Most students submit something that has
  10-to-16-pages of main content, with additional pages devoted to
  appendices, but it is up to you. Be precise and thorough.
\item
  Do I have to submit an initial paper in order to do the peer-review?
  Yes.
\item
  Can I use the same paper for the reproduction as in Paper 3? No.
\item
  Can I use any model? You are welcome to use any model, but you need to
  thoroughly explain it and this can be difficult for more complicated
  models.
\end{itemize}

\hypertarget{rubric-5}{%
\subsection{Rubric}\label{rubric-5}}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{lll}
\toprule
Component & Range & Requirement \\ 
\midrule
R is appropriately cited & 0 - 'No'; 1 - 'Yes' & Must be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall. \\ 
Title & 0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional' & An informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper. \\ 
Author, date, and repo & 0 - 'Poor or not done'; 2 - 'Yes' & The author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK'). \\ 
Abstract & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & An abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper. \\ 
Introduction & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total. \\ 
Data & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & A sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used. \\ 
Model & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & The model should be nicely written out, well-explained, justified, and appropriate. \\ 
Results & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & Results will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars. \\ 
Discussion & 0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional' & Some questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future? \\ 
Cross-references & 0 - 'Poor or not done'; 2 - 'Yes' & All figures, tables, and equations, should be numbered, and referred to in the text using cross-references. \\ 
Prose & 0 - 'Poor or not done'; 2 - 'Yes' & All aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear. \\ 
Graphs/tables/etc & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & Graphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables. \\ 
Reference list & 0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect' & All data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list. \\ 
Commits & 0 - 'Poor or not done'; 2 - 'Excellent' & There are at least two different commits, and they have meaningful commit messages. \\ 
Simulation & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The script is clearly commented and structured. All variables are appropriately simulated. \\ 
Reproducibility & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & The paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used. \\ 
Enhancements & 0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional' & You should pick at least one of the following and include it to enhance your submission: 1) A datasheet for the dataset; 2) A model card for the model; 3) A Shiny application; 4) An R package; or 5) API for the model. \\ 
General excellence & 0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional' & There are always students that excel in a way that is not anticipated in the rubric. This item accounts for that. \\ 
 \bottomrule
\end{longtable}

\hypertarget{previous-examples-5}{%
\subsection{Previous examples}\label{previous-examples-5}}

\href{inputs/pdfs/final_paper-2021-amy_farrow.pdf}{Amy Farrow},
\href{inputs/pdfs/final_paper-2020-annie_collins.pdf}{Annie Collins},
\href{inputs/pdfs/final_paper-2021-hong_shi.pdf}{Hong Shi},
\href{inputs/pdfs/final_paper-2021-jia_jia_ji.pdf}{Jia Jia Ji},
\href{inputs/pdfs/final_paper-2021-laura_cline.pdf}{Laura Cline},
\href{inputs/pdfs/final_paper-2021-lorena_almaraz_de_la_garza.pdf}{Lorena
Almaraz De La Garza}, and
\href{inputs/pdfs/final_paper-2021-rachael_lam.pdf}{Rachael Lam}.

\hypertarget{sec-datasets}{%
\chapter{Datasets}\label{sec-datasets}}

\emph{This is just a holding place for this content while it is being
developed.}

\hypertarget{oh-you-think-we-have-good-data-on-that-1}{%
\section{Oh, you think we have good data on
that!}\label{oh-you-think-we-have-good-data-on-that-1}}

Chapter 8:

\begin{quote}
\textbf{Oh, you think we have good data on that!} City boundaries. What
constitues `Atlanta'? Different definitions - metro, X, Y. (also an
issue in countries with boundaries changing over time)
\end{quote}

Chapter 10:

\begin{quote}
\textbf{Oh, you think we have good data on that!} One representation of
reality that is commonplace, is in chess. A chess board (see Figure X -
add photo of a chess board) is a 8 x 8 board of alternating black and
white squares. The squares are denonated by a unique combination of a
letter (A-G) and a number (1-8). And each piece has a unique
abbreviation, for instance pawns are X, and knights are Y. A game is
recorded by each player noting the move. In this way the entire game can
be recreated. The 2021 World Championship was contested by Magnus
Carlsen and Ian Nepomniachtchi. Figure X shows a score sheet from Game
6. There were a variety of reasons this game was particularly
noteworthy, but one the uncharactertic mistakes that both Carlsen and
Nepomniachtchi made. For instance, at Move 32 Carlsen did not exploit an
opportunity; and Move 36 a different move would have provided
Nepomniachtchi with a promising endgame (CITATION). One reason for this
may have been that both players at that point in the game had very
little time remaining---they had to decide on their moves very quickly.
But there is no sense of that in the representation provided by the game
sheet. It is a `correct' representation of what happened in the game,
but not necessarily why it happened.
\end{quote}

\begin{quote}
\textbf{Oh, you think we have good data on that!} Migration.
\end{quote}

\begin{quote}
\textbf{Oh, you think we have good data on that!} Weather stations
\end{quote}

\begin{quote}
\textbf{Oh, you think we have good data on that!} Olympics events. Who
decides on the scoring?. Who does the timing?
\end{quote}

\begin{quote}
\textbf{Oh, you think we have good data on that!} Personality scores.
Myers Briggs and Big 5 more generally.
\end{quote}

\begin{quote}
\textbf{Oh, you think we have good data on that!} Cause of death
\end{quote}

\begin{quote}
\textbf{Oh, you think we have good data on that!} Timing
\end{quote}

\hypertarget{shoulders-of-giants-6}{%
\section{Shoulders of giants}\label{shoulders-of-giants-6}}

Chapter 8:

\begin{quote}
\textbf{Shoulders of giants}
\end{quote}

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Barbara Bailar
\end{tcolorbox}

Chapter 8:

\begin{quote}
\textbf{Shoulders of giants}
\end{quote}

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Leo Goodman
\end{tcolorbox}

Chapter 9:

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Donald B. Rubin
\end{tcolorbox}

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Marcella Alsan
\end{tcolorbox}

Chapter 10:

\begin{quote}
\textbf{Shoulders of giants}
\end{quote}

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Susan Athey
\end{tcolorbox}

Chapter 12:

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Timnit Gebru
\end{tcolorbox}

Chapter 12:

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Katherine Wallman
\end{tcolorbox}

Chapter 13

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
John Tukey
\end{tcolorbox}

Chapter 14:

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Dr Daniela Witten
\end{tcolorbox}

Chapter 15:

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Evelyn Kitagawa
\end{tcolorbox}

Chapter 16:

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Andrew Gelman
\end{tcolorbox}

Chapter 17:

(Link to Lasso)

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
Rob Tibshirani
\end{tcolorbox}

Chapter 18:

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
???
\end{tcolorbox}

Chapter 19:

\begin{tcolorbox}[standard jigsaw,bottomtitle=1mm, toptitle=1mm, titlerule=0mm, arc=.35mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, toprule=.15mm, bottomrule=.15mm, left=2mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Shoulders of giants}, colbacktitle=quarto-callout-note-color!10!white, colback=white, coltitle=black]
???
\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  Elizabeth Scott
\item
  Gertrude Mary Cox
\item
  Simon Kuznets
\item
  Stella Cunliffe
\end{itemize}

\hypertarget{possible-datasets}{%
\section{Possible datasets}\label{possible-datasets}}

\begin{itemize}
\tightlist
\item
  https://som.yale.edu/faculty-research/our-centers/international-center-finance/data
\item
  Alex cookson
\item
  David Andrew's book
\item
  Tidycensus
\item
  https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.html
\item
  https://www.historicalstatistics.org/
\item
  https://data.cityofberkeley.info/browse?limitTo=datasets\&utf8
\item
  https://data.gov.hk/en-datasets/category/education
\item
  https://data.rijksmuseum.nl/object-metadata/download/
\item
  World bank https://data.worldbank.org/ eg development indicators
\item
  South sea bubble
\item
  OECD
\item
  Aer R package and that paper?
\item
  CESr
\item
  Paspaley
\item
  Canlang
\item
  Fred - does that have an api?
\item
  538
\item
  The Economist
\item
  https://pds.nasa.gov/datasearch/subscription-service/SS-Release.shtml
\item
  https://github.com/BuzzFeedNews/nics-firearm-background-checks
\item
  The markup
\item
  Tom Cardoso
\item
  List of APIs: https://bookdown.org/paul/apis\_for\_social\_scientists/
\end{itemize}

\hypertarget{sec-cocktails}{%
\chapter{Cocktails}\label{sec-cocktails}}

\hypertarget{cocktails}{%
\chapter{Cocktails}\label{cocktails}}

Each of the chapters inspired a cocktail.

\textbf{Chapter 1}

\textbf{Chapter 2}

\(2\) oz Hennessy\\
\(1\) oz lemon juice\\
\(1\) oz strawberry syrup\\
Top with lemonade

\textbf{Chapter 3}

\(1\frac{1}{2}\) oz bourbon\\
\(\frac{1}{2}\) oz Benedictine\\
\(1\) oz cherry syrup\\
\(\frac{1}{2}\) oz raspberry syrup\\
\(1\) oz lemon juice\\
Cherry garnish

\textbf{Chapter 4}

\(1\) oz cognac\\
\(\frac{1}{2}\) oz Grand Marnier\\
\(\frac{1}{2}\) oz Amaro Nonino\\
\(\frac{1}{4}\) oz lemon juice\\
\(\frac{1}{4}\) oz honey syrup\\
Cherry garnish

\textbf{Chapter 5}

\(1\frac{1}{2}\) oz vodka\\
\(\frac{3}{4}\) oz lime juice\\
\(\frac{1}{2}\) oz ginger syrup\\
One dash Angostura bitters\\
Top with lemonade

\textbf{Chapter 6}

\(1\frac{1}{2}\) oz cherry-infused vodka\\
\(1\) oz sweet vermouth\\
\(1\) oz raspberry syrup\\
\(1\) oz lime juice\\
One dash Angostura bitters

\textbf{Chapter 7}

\(1\) oz cherry-infused vodka\\
\(1\) oz lemon juice\\
\(\frac{1}{2}\) oz St Germain\\
\(\frac{1}{2}\) oz limoncello\\
\(\frac{1}{2}\) oz strawberry syrup\\
One dash elderflower bitters

\textbf{Chapter 8}

\(1\frac{1}{4}\) oz rye\\
\(\frac{1}{2}\) oz Grand Marnier\\
\(\frac{3}{4}\) oz lemon juice\\
\(\frac{3}{4}\) oz orange juice\\
\(\frac{1}{2}\) oz grenadine\\
\(\frac{1}{4}\) oz raspberry syrup

\textbf{Chapter 9}

\(1\frac{1}{4}\) oz Four Roses bourbon\\
\(1\) oz apple cider\\
\(\frac{1}{2}\) oz Grand Marnier\\
\(\frac{1}{4}\) oz ameretto\\
\(\frac{1}{2}\) oz lemon juice\\
\(\frac{1}{2}\) oz ginger syrup\\
One dash Angostura bitters

\textbf{Chapter 10}

\emph{January has April showers and 2+2 always makes up 5 (measurement
error)}

\(2\) oz bourbon\\
\(1\) oz sweet vermouth\\
\(1\frac{1}{2}\) oz cherry syrup\\
\(1\frac{1}{2}\) oz lemon juice\\
One dash Old Fashioned bitters

\textbf{Chapter 11}

\emph{You can try the best you can, the best you can is good enough
(some models are useful)}

\(2\) oz bourbon\\
\(1\) oz Galliano Vanilla\\
\(1\) oz raspberry syrup\\
\(1\) oz lemon juice\\
One dash Angostura bitters

\textbf{Chapter 12}

\emph{Just cause you feel it doesn't mean it's there (p hacking)}

\(1\frac{1}{2}\) oz dark rum\\
\(1\) oz honey syrup\\
\(1\) oz lemon juice\\
\(\frac{1}{2}\) oz ginger syrup\\
One dash Old Fashioned bitters\\
Cherry garnish

\textbf{Chapter 13}

\emph{We are accidents waiting to happen (model overfit)}

\(1\frac{1}{2}\) oz bourbon\\
\(\frac{1}{2}\) oz Amaro Averna\\
\(\frac{1}{2}\) oz lemon juice\\
\(\frac{1}{2}\) oz ginger syrup

\textbf{Chapter 14}

\(1\frac{1}{2}\) oz Empress gin\\
\(\frac{1}{2}\) oz St Germain\\
\(\frac{1}{2}\) oz lime juice\\
\(\frac{1}{4}\) oz raspberry syrup\\
One dash Angostura bitters

\textbf{Chapter 15}

\(1\frac{1}{2}\) oz Amaro Averna\\
\(\frac{1}{2}\) oz bourbon\\
\(\frac{3}{4}\) oz lemon juice\\
Cherry garnish

\textbf{Chapter 16}

\(1\frac{1}{2}\) oz Canadian whiskey\\
\(\frac{3}{4}\) oz Sweet vermouth\\
\(\frac{3}{4}\) oz Benedictine\\
One dash absinthe\\
Two dashes Peychaud's bitters\\
Two dashes Angostura bitters

\textbf{Chapter 17}

\(1\frac{1}{2}\) oz dark rum\\
\(\frac{3}{4}\) oz cane sugar syrup\\
\(\frac{1}{2}\) oz lemon juice\\
\(\frac{1}{2}\) oz orange juice\\
\(\frac{1}{2}\) oz ginger syrup\\
One dash Old Fashioned bitters

\textbf{Chapter 18}

\(1\frac{1}{2}\) oz Empress gin\\
\(1\) oz raspberry syrup\\
\(\frac{3}{4}\) oz lemon juice\\
One dash Angostura bitters

\textbf{Chapter 19}

\(2\) oz Reid's gin\\
\(\frac{3}{4}\) oz lemon juice\\
\(\frac{3}{4}\) oz ginger syrup\\
One dash Angostura bitters

\textbf{Chapter 20}

\(2\) oz rye whiskey\\
\(1\) oz sweet vermouth\\
\(\frac{1}{2}\) oz blackberry syrup\\
One dash Angostura bitters

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-omggenes}{}}%
Abeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu, and Mark Ziemann.
2021. {``Gene Name Errors: Lessons Not Learned.''} \emph{PLOS
Computational Biology} 17 (7): 1--13.
\url{https://doi.org/10.1371/journal.pcbi.1008984}.

\leavevmode\vadjust pre{\hypertarget{ref-Alexander2019}{}}%
Alexander, Monica. 2019a. {``Reproducibility in Demographic Research.''}
\url{https://www.monicaalexander.com/posts/2019-10-20-reproducibility/}.

\leavevmode\vadjust pre{\hypertarget{ref-monicababynames}{}}%
---------. 2019b. {``The Concentration and Uniqueness of Baby Names in
Australia and the US,''} January.
\url{https://www.monicaalexander.com/posts/2019-20-01-babynames/}.

\leavevmode\vadjust pre{\hypertarget{ref-monicanamechanges}{}}%
---------. 2019c. {``Analyzing Name Changes After Marriage Using a
Non-Representative Survey,''} August.
\url{https://www.monicaalexander.com/posts/2019-08-07-mrp/}.

\leavevmode\vadjust pre{\hypertarget{ref-monicatalks}{}}%
---------. 2021. {``Overcoming Barriers to Sharing Code.''}
\emph{YouTube}, February. \url{https://youtu.be/yvM2C6aZ94k}.

\leavevmode\vadjust pre{\hypertarget{ref-alexander2018trends}{}}%
Alexander, Monica J, Mathew V Kiang, and Magali Barbieri. 2018.
{``Trends in Black and White Opioid Mortality in the United States,
1979--2015.''} \emph{Epidemiology (Cambridge, Mass.)} 29 (5): 707.

\leavevmode\vadjust pre{\hypertarget{ref-alexander2018global}{}}%
Alexander, Monica, and Leontine Alkema. 2018. {``Global Estimation of
Neonatal Mortality Using a Bayesian Hierarchical Splines Regression
Model.''} \emph{Demographic Research} 38: 335--72.

\leavevmode\vadjust pre{\hypertarget{ref-alexander2021bayesian}{}}%
---------. 2021. {``A Bayesian Cohort Component Projection Model to
Estimate Adult Populations at the Subnational Level in Data-Sparse
Settings.''} \url{https://arxiv.org/abs/2102.06121}.

\leavevmode\vadjust pre{\hypertarget{ref-Alexander2020}{}}%
Alexander, Rohan, and Monica Alexander. 2021. {``The Increased Effect of
Elections and Changing Prime Ministers on Topics Discussed in the
Australian Federal Parliament Between 1901 and 2018.''}
\url{https://arxiv.org/abs/2111.09299}.

\leavevmode\vadjust pre{\hypertarget{ref-citeaustralianpoliticians}{}}%
Alexander, Rohan, and Paul A. Hodgetts. 2021.
\emph{AustralianPoliticians: Provides Datasets about Australian
Politicians}.
\url{https://CRAN.R-project.org/package=AustralianPoliticians}.

\leavevmode\vadjust pre{\hypertarget{ref-alexander2018age}{}}%
Alexander, Rohan, and Zachary Ward. 2018. {``Age at Arrival and
Assimilation During the Age of Mass Migration.''} \emph{The Journal of
Economic History} 78 (3): 904--37.

\leavevmode\vadjust pre{\hypertarget{ref-citedistill}{}}%
Allaire, JJ, Rich Iannone, Alison Presmanes Hill, and Yihui Xie. 2021.
\emph{Distill: 'R Markdown' Format for Scientific and Technical
Writing}.

\leavevmode\vadjust pre{\hypertarget{ref-plumberdeploy}{}}%
Allen, Jeff. 2021. \emph{plumberDeploy: Plumber Deployment}.
\url{https://CRAN.R-project.org/package=plumberDeploy}.

\leavevmode\vadjust pre{\hypertarget{ref-tuskegeeandthehealthofblackmen}{}}%
Alsan, Marcella, and Marianne Wanamaker. 2018. {``Tuskegee and the
Health of Black Men.''} \emph{The Quarterly Journal of Economics} 133
(1): 407--55.

\leavevmode\vadjust pre{\hypertarget{ref-thenakedtruth}{}}%
Amaka, Ofunne, and Amber Thomas. 2021. \emph{The Naked Truth: How the
Names of 6,816 Complexion Products Can Reveal Bias in Beauty}.
\url{https://pudding.cool/2021/03/foundation-names/}.

\leavevmode\vadjust pre{\hypertarget{ref-angelucci2019newspapers}{}}%
Angelucci, Charles, and Julia Cagé. 2019. {``Newspapers in Times of Low
Advertising Revenues.''} \emph{American Economic Journal:
Microeconomics} 11 (3): 319--64.

\leavevmode\vadjust pre{\hypertarget{ref-angrist2010credibility}{}}%
Angrist, Joshua D, and Jörn-Steffen Pischke. 2010. {``The Credibility
Revolution in Empirical Economics: How Better Research Design Is Taking
the Con Out of Econometrics.''} \emph{Journal of Economic Perspectives}
24 (2): 3--30.

\leavevmode\vadjust pre{\hypertarget{ref-annas2003hipaa}{}}%
Annas, George J. 2003. {``HIPAA Regulations: A New Era of Medical-Record
Privacy?''} \emph{New England Journal of Medicine} 348: 1486.

\leavevmode\vadjust pre{\hypertarget{ref-citemodelsummary}{}}%
Arel-Bundock, Vincent. 2021a. \emph{Modelsummary: Summary Tables and
Plots for Statistical Models and Data: Beautiful, Customizable, and
Publication-Ready}.
\url{https://CRAN.R-project.org/package=modelsummary}.

\leavevmode\vadjust pre{\hypertarget{ref-WDI}{}}%
---------. 2021b. \emph{WDI: World Development Indicators and Other
World Bank Data}. \url{https://CRAN.R-project.org/package=WDI}.

\leavevmode\vadjust pre{\hypertarget{ref-ggthemes}{}}%
Arnold, Jeffrey B. 2021. \emph{Ggthemes: Extra Themes, Scales and Geoms
for 'Ggplot2'}. \url{https://CRAN.R-project.org/package=ggthemes}.

\leavevmode\vadjust pre{\hypertarget{ref-american1848code}{}}%
Association, American Medical, and New York Academy of Medicine. 1848.
\emph{Code of Medical Ethics}. Academy of Medicine.

\leavevmode\vadjust pre{\hypertarget{ref-athey2017state}{}}%
Athey, Susan, and Guido W Imbens. 2017. {``The State of Applied
Econometrics: Causality and Policy Evaluation.''} \emph{Journal of
Economic Perspectives} 31 (2): 3--32.

\leavevmode\vadjust pre{\hypertarget{ref-athey2021using}{}}%
Athey, Susan, Guido W Imbens, Jonas Metzger, and Evan Munro. 2021.
{``Using Wasserstein Generative Adversarial Networks for the Design of
Monte Carlo Simulations.''} \emph{Journal of Econometrics}.

\leavevmode\vadjust pre{\hypertarget{ref-thatrandyauperson}{}}%
Au, Randy. 2020. {``Data Cleaning IS Analysis, Not Grunt Work.''}
Counting Stuff.
\url{https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt}.

\leavevmode\vadjust pre{\hypertarget{ref-nonprobabilitysmaples}{}}%
Baker, Reg, J. Michael Brick, Nancy A. Bates, Mike Battaglia, Mick P.
Couper, Jill A. Dever, Krista J. Gile, and Roger Tourangeau. 2013.
{``{Summary Report of the AAPOR Task Force on Non-probability
Sampling}.''} \emph{Journal of Survey Statistics and Methodology} 1 (2):
90--143. \url{https://doi.org/10.1093/jssam/smt008}.

\leavevmode\vadjust pre{\hypertarget{ref-bandy2021addressing}{}}%
Bandy, Jack, and Nicholas Vincent. 2021. {``Addressing "Documentation
Debt" in Machine Learning Research: A Retrospective Datasheet for
BookCorpus.''} \url{https://arxiv.org/abs/2105.05241}.

\leavevmode\vadjust pre{\hypertarget{ref-barba2018terminologies}{}}%
Barba, Lorena A. 2018. {``Terminologies for Reproducible Research.''}
\url{https://arxiv.org/abs/1802.03311}.

\leavevmode\vadjust pre{\hypertarget{ref-citeBarrett}{}}%
Barrett, Malcolm. 2021a. \emph{Data Science as an Atomic Habit}.
\url{https://malco.io/2021/01/04/data-science-as-an-atomic-habit/}.

\leavevmode\vadjust pre{\hypertarget{ref-citeggdag}{}}%
---------. 2021b. \emph{Ggdag: Analyze and Create Elegant Directed
Acyclic Graphs}. \url{https://CRAN.R-project.org/package=ggdag}.

\leavevmode\vadjust pre{\hypertarget{ref-barron2018individuals}{}}%
Barron, Alexander TJ, Jenny Huang, Rebecca L Spang, and Simon DeDeo.
2018. {``Individuals, Institutions, and Innovation in the Debates of the
French Revolution.''} \emph{Proceedings of the National Academy of
Sciences} 115 (18): 4607--12.

\leavevmode\vadjust pre{\hypertarget{ref-citelme}{}}%
Bates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015.
{``Fitting Linear Mixed-Effects Models Using {lme4}.''} \emph{Journal of
Statistical Software} 67 (1): 1--48.
\url{https://doi.org/10.18637/jss.v067.i01}.

\leavevmode\vadjust pre{\hypertarget{ref-moderndatascience}{}}%
Baumer, Benjamin, Daniel Kaplan, and Nicholas Horton. 2021. \emph{Modern
Data Science with r}. 2nd ed. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-beauregard2021antiwomen}{}}%
Beauregard, Katrine, and Jill Sheppard. 2021. {``Antiwomen but Proquota:
Disaggregating Sexism and Support for Gender Quota Policies.''}
\emph{Political Psychology} 42 (2): 219--37.

\leavevmode\vadjust pre{\hypertarget{ref-washingtonpostmaps}{}}%
Bensinger, Greg. 2020. \emph{Google Redraws the Borders on Maps
Depending on Who's Looking}. \emph{Washington Post}.

\leavevmode\vadjust pre{\hypertarget{ref-citeberkson}{}}%
Berkson, Joseph. 1946. {``Limitations of the Application of Fourfold
Table Analysis to Hospital Data.''} \emph{Biometrics Bulletin} 2 (3):
47--53. \url{http://www.jstor.org/stable/3002000}.

\leavevmode\vadjust pre{\hypertarget{ref-berners1989information}{}}%
Berners-Lee, Timothy J. 1989. {``Information Management: A Proposal.''}

\leavevmode\vadjust pre{\hypertarget{ref-berry1989investigating}{}}%
Berry, Donald A. 1989. {``{[}Investigating Therapies of Potentially
Great Benefit: ECMO{]}: Comment: Ethics and ECMO.''} \emph{Statistical
Science} 4 (4): 306--10.

\leavevmode\vadjust pre{\hypertarget{ref-bertrand2004emily}{}}%
Bertrand, Marianne, and Sendhil Mullainathan. 2004. {``Are Emily and
Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor
Market Discrimination.''} \emph{American Economic Review} 94 (4):
991--1013.

\leavevmode\vadjust pre{\hypertarget{ref-bickel1975sex}{}}%
Bickel, Peter J, Eugene A Hammel, and J William O'Connell. 1975. {``Sex
Bias in Graduate Admissions: Data from Berkeley: Measuring Bias Is
Harder Than Is Usually Assumed, and the Evidence Is Sometimes Contrary
to Expectation.''} \emph{Science} 187 (4175): 398--404.

\leavevmode\vadjust pre{\hypertarget{ref-biderman2022datasheet}{}}%
Biderman, Stella, Kieran Bicheno, and Leo Gao. 2022. {``Datasheet for
the Pile.''} \url{https://arxiv.org/abs/2201.07311}.

\leavevmode\vadjust pre{\hypertarget{ref-estimatr}{}}%
Blair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and
Luke Sonnet. 2021. \emph{Estimatr: Fast Estimators for Design-Based
Inference}. \url{https://CRAN.R-project.org/package=estimatr}.

\leavevmode\vadjust pre{\hypertarget{ref-democratizingr}{}}%
Blair, James. 2019. \emph{Democratizing r with Plumber APIs}.
\url{https://www.rstudio.com/resources/rstudioconf-2019/democratizing-r-with-plumber-apis/}.

\leavevmode\vadjust pre{\hypertarget{ref-bland1986statistical}{}}%
Bland, J Martin, and DouglasG Altman. 1986. {``Statistical Methods for
Assessing Agreement Between Two Methods of Clinical Measurement.''}
\emph{The Lancet} 327 (8476): 307--10.

\leavevmode\vadjust pre{\hypertarget{ref-blei2012}{}}%
Blei, David M. 2012. {``Probabilistic Topic Models.''}
\emph{Communications of the ACM} 55 (4): 77--84.

\leavevmode\vadjust pre{\hypertarget{ref-BleiLafferty2009}{}}%
Blei, David M, and John D Lafferty. 2009. {``Topic Models.''} In
\emph{Text Mining}, 101--24. Chapman; Hall/CRC.

\leavevmode\vadjust pre{\hypertarget{ref-Blei2003latent}{}}%
Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. {``Latent
Dirichlet Allocation.''} \emph{Journal of Machine Learning Research} 3
(Jan): 993--1022.

\leavevmode\vadjust pre{\hypertarget{ref-bloombellreiman2020}{}}%
Bloom, Howard, Andrew Bell, and Kayla Reiman. 2020. {``Using Data from
Randomized Trials to Assess the Likely Generalizability of Educational
Treatment-Effect Estimates from Regression Discontinuity Designs.''}
\emph{Journal of Research on Educational Effectiveness}, 1--30.
\url{https://doi.org/10.1080/19345747.2019.1634169}.

\leavevmode\vadjust pre{\hypertarget{ref-boland1984biographical}{}}%
Boland, Philip J. 1984. {``A Biographical Glimpse of William Sealy
Gosset.''} \emph{The American Statistician} 38 (3): 179--83.

\leavevmode\vadjust pre{\hypertarget{ref-boltonruth}{}}%
Bolton, Ruth, and Randall Chapman. 1986. {``Searching for Positive
Returns at the Track.''} \emph{Management Science} 32 (August):
1040--60. \url{https://doi.org/10.1287/mnsc.32.8.1040}.

\leavevmode\vadjust pre{\hypertarget{ref-americaslavery}{}}%
Bouie, Jamelle. 2022. \emph{We Still Can't See American Slavery for What
It Was}.

\leavevmode\vadjust pre{\hypertarget{ref-bowers2011six}{}}%
Bowers, Jake. 2011. {``Six Steps to a Better Relationship with Your
Future Self.''} \emph{The Political Methodologist} 18 (2): 2--8.

\leavevmode\vadjust pre{\hypertarget{ref-bowley}{}}%
Bowley, Arthur Lyon. 1901. \emph{Elements of Statistics}. P. S. King.

\leavevmode\vadjust pre{\hypertarget{ref-bowley1913working}{}}%
---------. 1913. {``Working-Class Households in Reading.''}
\emph{Journal of the Royal Statistical Society} 76 (7): 672--701.

\leavevmode\vadjust pre{\hypertarget{ref-brandt1978racism}{}}%
Brandt, Allan M. 1978. {``Racism and Research: The Case of the Tuskegee
Syphilis Study.''} \emph{Hastings Center Report}, 21--29.

\leavevmode\vadjust pre{\hypertarget{ref-briggs2021does}{}}%
Briggs, Ryan C. 2021. {``Why Does Aid Not Target the Poorest?''}
\emph{International Studies Quarterly} 65 (3): 739--52.

\leavevmode\vadjust pre{\hypertarget{ref-brokowski2019crispr}{}}%
Brokowski, Carolyn, and Mazhar Adli. 2019. {``CRISPR Ethics: Moral
Considerations for Applications of a Powerful Tool.''} \emph{Journal of
Molecular Biology} 431 (1): 88--101.

\leavevmode\vadjust pre{\hypertarget{ref-janeeyre}{}}%
Bronte, Charlotte. 1847. \emph{Jane Eyre}.
\url{https://www.gutenberg.org/files/1260/1260-h/1260-h.htm}.

\leavevmode\vadjust pre{\hypertarget{ref-theprofessor}{}}%
Brontë, Charlotte. 1857. \emph{The Professor}.

\leavevmode\vadjust pre{\hypertarget{ref-randhealth}{}}%
Brook, Robert H, John E Ware, William H Rogers, Emmett B Keeler, Allyson
Ross Davies, Cathy D Sherbourne, George A Goldberg, Kathleen N Lohr,
Patricia Camp, and Joseph P Newhouse. 1984. {``The Effect of Coinsurance
on the Health of Adults: Results from the RAND Health Insurance
Experiment.''}

\leavevmode\vadjust pre{\hypertarget{ref-Bryan2018}{}}%
Bryan, Jennifer. 2018. {``Excuse Me, Do You Have a Moment to Talk about
Version Control?''} \emph{The American Statistician} 72 (1): 20--27.
\url{https://doi.org/10.1080/00031305.2017.1399928}.

\leavevmode\vadjust pre{\hypertarget{ref-whattheyforgot}{}}%
Bryan, Jennifer, and Jim Hester. 2020. \emph{What They Forgot to Teach
You about r}. \url{https://rstats.wtf/index.html}.

\leavevmode\vadjust pre{\hypertarget{ref-citereprex}{}}%
Bryan, Jennifer, Jim Hester, David Robinson, and Hadley Wickham. 2019.
\emph{Reprex: Prepare Reproducible Example Code via the Clipboard}.
\url{https://CRAN.R-project.org/package=reprex}.

\leavevmode\vadjust pre{\hypertarget{ref-codesmells}{}}%
Bryan, Jenny. 2018. {``Code Smells and Feels.''} \emph{YouTube}, July.
\url{https://youtu.be/7oyiPBjLAWY}.

\leavevmode\vadjust pre{\hypertarget{ref-happygit}{}}%
---------. 2020. \emph{{Happy Git and GitHub for the useR}}.
\url{https://happygitwithr.com}.

\leavevmode\vadjust pre{\hypertarget{ref-de2021thinking}{}}%
Bueno de Mesquita, Ethan, and Anthony Fowler. 2021. \emph{Thinking
Clearly with Data: A Guide to Quantitative Reasoning and Analysis}.
Princeton University Press.

\leavevmode\vadjust pre{\hypertarget{ref-buhrplumber}{}}%
Buhr, Ray. 2017. \emph{Using r as a Production Machine Learning Language
(Part i)}.
\url{https://raybuhr.github.io/blog/posts/making-predictions-over-http/}.

\leavevmode\vadjust pre{\hypertarget{ref-buja1996interactive}{}}%
Buja, Andreas, Dianne Cook, and Deborah F Swayne. 1996. {``Interactive
High-Dimensional Data Visualization.''} \emph{Journal of Computational
and Graphical Statistics} 5 (1): 78--99.

\leavevmode\vadjust pre{\hypertarget{ref-vannevarbush}{}}%
Bush, Vannevar. 1945. {``As We May Think.''} \emph{The Atlantic Monthly}
176 (1): 101--8.

\leavevmode\vadjust pre{\hypertarget{ref-cahill2020increase}{}}%
Cahill, Niamh, Michelle Weinberger, and Leontine Alkema. 2020. {``What
Increase in Modern Contraceptive Use Is Needed in Fp2020 Countries to
Reach 75\% Demand Satisfied by 2030? An Assessment Using the Accelerated
Transition Method and Family Planning Estimation Model.''} \emph{Gates
Open Research} 4.

\leavevmode\vadjust pre{\hypertarget{ref-rdrobust}{}}%
Calonico, Sebastian, Matias D. Cattaneo, Max H. Farrell, and Rocio
Titiunik. 2021. \emph{Rdrobust: Robust Data-Driven Statistical Inference
in Regression-Discontinuity Designs}.
\url{https://CRAN.R-project.org/package=rdrobust}.

\leavevmode\vadjust pre{\hypertarget{ref-citetidygeocoder}{}}%
Cambon, Jesse, and Christopher Belanger. 2021. {``Tidygeocoder:
Geocoding Made Easy.''} Zenodo.
\url{https://doi.org/10.5281/zenodo.3981510}.

\leavevmode\vadjust pre{\hypertarget{ref-veryhungry}{}}%
Carle, Eric. 1969. \emph{The Very Hungry Caterpillar}. World Publishing
Company.

\leavevmode\vadjust pre{\hypertarget{ref-chris_carleton_2021_4550688}{}}%
Carleton, Chris. 2021. \emph{Wccarleton/Conflict-Europe: Acce} (version
v1.0.0). Zenodo. \url{https://doi.org/10.5281/zenodo.4550688}.

\leavevmode\vadjust pre{\hypertarget{ref-carleton2021reassessment}{}}%
Carleton, W Christopher, Dave Campbell, and Mark Collard. 2021. {``A
Reassessment of the Impact of Temperature Change on European Conflict
During the Second Millennium CE Using a Bespoke Bayesian Time-Series
Model.''} \emph{Climatic Change} 165 (1): 1--16.

\leavevmode\vadjust pre{\hypertarget{ref-caroonworking}{}}%
Caro, Robert. 2019. \emph{Working}. 1st ed. Knopf.

\leavevmode\vadjust pre{\hypertarget{ref-citealice}{}}%
Carroll, Lewis. 1865. \emph{Alice's Adventures in Wonderland}.
Macmillan.

\leavevmode\vadjust pre{\hypertarget{ref-throughthelookingglass}{}}%
---------. 1871. \emph{Through the Looking-Glass}. Macmillan.

\leavevmode\vadjust pre{\hypertarget{ref-citeanalogsea}{}}%
Chamberlain, Scott, Hadley Wickham, and Winston Chang. 2021.
\emph{Analogsea: Interface to 'Digital Ocean'}.

\leavevmode\vadjust pre{\hypertarget{ref-chambliss1989mundanity}{}}%
Chambliss, Daniel F. 1989. {``The Mundanity of Excellence: An
Ethnographic Report on Stratification and Olympic Swimmers.''}
\emph{Sociological Theory} 7 (1): 70--86.

\leavevmode\vadjust pre{\hypertarget{ref-citeshiny}{}}%
Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke,
Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara
Borges. 2021. \emph{Shiny: Web Application Framework for r}.
\url{https://CRAN.R-project.org/package=shiny}.

\leavevmode\vadjust pre{\hypertarget{ref-chellel2018gambler}{}}%
Chellel, Kit. 2018. {``The Gambler Who Cracked the Horse-Racing Code.''}
\emph{Bloomberg Businessweek (May 2018). Featured in Bloomberg
Businessweek, May} 14.

\leavevmode\vadjust pre{\hypertarget{ref-chen2019forensic}{}}%
Chen, Wei, Xilu Chen, Chang-Tai Hsieh, and Zheng Song. 2019. {``A
Forensic Examination of China's National Accounts.''} National Bureau of
Economic Research.

\leavevmode\vadjust pre{\hypertarget{ref-ChengKarambelkarXie2017}{}}%
Cheng, Joe, Bhaskar Karambelkar, and Yihui Xie. 2021. \emph{Leaflet:
Create Interactive Web Maps with the JavaScript 'Leaflet' Library}.
\url{https://CRAN.R-project.org/package=leaflet}.

\leavevmode\vadjust pre{\hypertarget{ref-chouldechova18a}{}}%
Chouldechova, Alexandra, Diana Benavides-Prado, Oleksandr Fialko, and
Rhema Vaithianathan. 2018. {``A Case Study of Algorithm-Assisted
Decision Making in Child Maltreatment Hotline Screening Decisions.''} In
\emph{Proceedings of the 1st Conference on Fairness, Accountability and
Transparency}, edited by Sorelle A. Friedler and Christo Wilson,
81:134--48. Proceedings of Machine Learning Research. PMLR.
\url{https://proceedings.mlr.press/v81/chouldechova18a.html}.

\leavevmode\vadjust pre{\hypertarget{ref-jeanchretien}{}}%
Chrétien, Jean. 2007. \emph{My Years as Prime Minister}. Knopf Canada.

\leavevmode\vadjust pre{\hypertarget{ref-christensen2019study}{}}%
Christensen, Garret, Allan Dafoe, Edward Miguel, Don A Moore, and Andrew
K Rose. 2019. {``A Study of the Impact of Data Sharing on Article
Citations Using Journal Policies as a Natural Experiment.''} \emph{PLoS
One} 14 (12): e0225883.

\leavevmode\vadjust pre{\hypertarget{ref-christensen2019transparent}{}}%
Christensen, Garret, Jeremy Freese, and Edward Miguel. 2019.
\emph{Transparent and Reproducible Social Science Research}. University
of California Press.

\leavevmode\vadjust pre{\hypertarget{ref-churchill}{}}%
Churchill, Winston. 1956. \emph{A History of the English-Speaking
Peoples}.

\leavevmode\vadjust pre{\hypertarget{ref-cirone}{}}%
Cirone, Alexandra, and Arthur Spirling. 2021. {``Turning History into
Data: Data Collection, Measurement, and Inference in HPE.''}
\emph{Journal of Historical Political Economy} 1 (1): 127--54.
\url{https://doi.org/10.1561/115.00000005}.

\leavevmode\vadjust pre{\hypertarget{ref-torontohomeless}{}}%
City of Toronto. 2021. \emph{2021 Street Needs Assessment}.
\url{https://www.toronto.ca/city-government/data-research-maps/research-reports/housing-and-homelessness-research-and-reports/}.

\leavevmode\vadjust pre{\hypertarget{ref-elementsofgraphingdata}{}}%
Cleveland, William. 1994. \emph{The Elements of Graphing Data}. 2nd ed.
Hobart Press.

\leavevmode\vadjust pre{\hypertarget{ref-Cohen2018}{}}%
Cohen, I. Glenn, and Michelle M. Mello. 2018. {``{HIPAA} and Protecting
Health Information in the 21st Century.''} \emph{{JAMA}} 320 (3): 231.
\url{https://doi.org/10.1001/jama.2018.5630}.

\leavevmode\vadjust pre{\hypertarget{ref-cohn2016}{}}%
Cohn, Nate. 2016. \emph{We Gave Four Good Pollsters the Same Raw Data.
They Had Four Different Results}.

\leavevmode\vadjust pre{\hypertarget{ref-Cook2021Foundation}{}}%
Cook, Dianne, Nancy Reid, and Emi Tanaka. 2021. {``The Foundation Is
Available for Thinking about Data Visualization Inferentially.''}
\emph{Harvard Data Science Review}, July.
\url{https://doi.org/10.1162/99608f92.8453435d}.

\leavevmode\vadjust pre{\hypertarget{ref-citemapdeck}{}}%
Cooley, David. 2020. \emph{Mapdeck: Interactive Maps Using 'Mapbox GL
JS' and 'Deck.gl'}. \url{https://CRAN.R-project.org/package=mapdeck}.

\leavevmode\vadjust pre{\hypertarget{ref-gdpr}{}}%
Council of European Union. 2016. {``General Data Protection Regulation
2016/679.''}

\leavevmode\vadjust pre{\hypertarget{ref-coxtalks}{}}%
Cox, David. 2018. {``In Gentle Praise of Significance Tests.''}
\emph{YouTube}, October. \url{https://youtu.be/txLj_P9UlCQ}.

\leavevmode\vadjust pre{\hypertarget{ref-cox1987parameter}{}}%
Cox, David Roxbee, and Nancy Reid. 1987. {``Parameter Orthogonality and
Approximate Conditional Inference.''} \emph{Journal of the Royal
Statistical Society: Series B (Methodological)} 49 (1): 1--18.

\leavevmode\vadjust pre{\hypertarget{ref-airbnbdata}{}}%
Cox, Murray. 2021. {``{Inside Airbnb - Toronto Data}.''}
\url{http://insideairbnb.com/get-the-data.html}.

\leavevmode\vadjust pre{\hypertarget{ref-craiu2019hiring}{}}%
Craiu, Radu V. 2019. {``The Hiring Gambit: In Search of the Twofer Data
Scientist.''} \emph{Harvard Data Science Review} 1 (1).

\leavevmode\vadjust pre{\hypertarget{ref-cramer2002origins}{}}%
Cramer, Jan Salomon. 2002. {``The Origins of Logistic Regression.''}

\leavevmode\vadjust pre{\hypertarget{ref-crawford}{}}%
Crawford, Kate. 2021. \emph{{Atlas of AI}}. Yale University Press.

\leavevmode\vadjust pre{\hypertarget{ref-Cunningham2021}{}}%
Cunningham, Scott. 2021. \emph{Causal Inference: The Mixtape}. Yale
Press.

\leavevmode\vadjust pre{\hypertarget{ref-datafeminism2020}{}}%
D'Ignazio, Catherine, and Lauren F Klein. 2020. \emph{Data Feminism}.
Mit Press.

\leavevmode\vadjust pre{\hypertarget{ref-dagan2021bnt162b2}{}}%
Dagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark A
Katz, Miguel A Hernán, Marc Lipsitch, Ben Reis, and Ran D Balicer. 2021.
{``BNT162b2 mRNA Covid-19 Vaccine in a Nationwide Mass Vaccination
Setting.''} \emph{New England Journal of Medicine}.

\leavevmode\vadjust pre{\hypertarget{ref-Darling2011}{}}%
Darling, William M. 2011. {``A Theoretical and Practical Implementation
Tutorial on Topic Modeling and Gibbs Sampling.''} In \emph{Proceedings
of the 49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies}, 642--47.

\leavevmode\vadjust pre{\hypertarget{ref-helendewitt}{}}%
DeWitt, Helen. 2000. \emph{The Last Samurai}. Talk Mirimax Books.

\leavevmode\vadjust pre{\hypertarget{ref-doll1950smoking}{}}%
Doll, Richard, and A Bradford Hill. 1950. {``Smoking and Carcinoma of
the Lung.''} \emph{British Medical Journal} 2 (4682): 739.

\leavevmode\vadjust pre{\hypertarget{ref-edgeworth1885methods}{}}%
Edgeworth, Francis Ysidro. 1885. {``Methods of Statistics.''}
\emph{Journal of the Statistical Society of London}, 181--217.

\leavevmode\vadjust pre{\hypertarget{ref-eghbal2020working}{}}%
Eghbal, Nadia. 2020. \emph{Working in Public: The Making and Maintenance
of Open Source Software}. Stripe Press.

\leavevmode\vadjust pre{\hypertarget{ref-farrugia2010research}{}}%
Farrugia, Patricia, Bradley A Petrisor, Forough Farrokhyar, and Mohit
Bhandari. 2010. {``Research Questions, Hypotheses and Objectives.''}
\emph{Canadian Journal of Surgery} 53 (4): 278.

\leavevmode\vadjust pre{\hypertarget{ref-finkelstein2012oregon}{}}%
Finkelstein, Amy, Sarah Taubman, Bill Wright, Mira Bernstein, Jonathan
Gruber, Joseph P Newhouse, Heidi Allen, Katherine Baicker, and Oregon
Health Study Group. 2012. {``The Oregon Health Insurance Experiment:
Evidence from the First Year.''} \emph{The Quarterly Journal of
Economics} 127 (3): 1057--1106.

\leavevmode\vadjust pre{\hypertarget{ref-janitor}{}}%
Firke, Sam. 2020. \emph{Janitor: Simple Tools for Examining and Cleaning
Dirty Data}. \url{https://CRAN.R-project.org/package=janitor}.

\leavevmode\vadjust pre{\hypertarget{ref-fisherdesignofexperiments}{}}%
Fisher, Ronald. 1935. \emph{The Design of Experiments}. Oliver; Boyd.

\leavevmode\vadjust pre{\hypertarget{ref-fisherarrangement}{}}%
Fisher, Ronald Aylmer. 1926. {``{The Arrangement of Field
Experiments},''} 503--15.
\url{https://doi.org/10.23637/ROTHAMSTED.8V61Q}.

\leavevmode\vadjust pre{\hypertarget{ref-fiske2021words}{}}%
Fiske, Susan T, and Shiro Kuriwaki. 2021. {``Words to the Wise on
Writing Scientific Papers.''}

\leavevmode\vadjust pre{\hypertarget{ref-aboutupworthy}{}}%
Fitts, Alexis Sobel. 2014. {``The King of Content: How Upworthy Aims to
Alter the Web, and Could End up Altering the World.''} \emph{Columbia
Journalism Review}.
\url{https://archives.cjr.org/feature/the_king_of_content.php}.

\leavevmode\vadjust pre{\hypertarget{ref-troopdata}{}}%
Flynn, Michael. 2021. \emph{Troopdata: Tools for Analyzing
Cross-National Military Deployment and Basing Data}.
\url{https://CRAN.R-project.org/package=troopdata}.

\leavevmode\vadjust pre{\hypertarget{ref-forster}{}}%
Forster, E M. 1927. \emph{Aspects of the Novel}. Edward Arnold.

\leavevmode\vadjust pre{\hypertarget{ref-foster}{}}%
Foster, Gordon. 1968. {``Computers, Statistics and Planning: Systems or
Chaos?''} \emph{Geary Lecture}.
\url{https://www.esri.ie/system/files/media/file-uploads/2016-03/GLS2.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-fourcade2017seeing}{}}%
Fourcade, Marion, and Kieran Healy. 2017. {``Seeing Like a Market.''}
\emph{Socio-Economic Review} 15 (1): 9--29.

\leavevmode\vadjust pre{\hypertarget{ref-fox2006effect}{}}%
Fox, John, and Robert Andersen. 2006. {``Effect Displays for Multinomial
and Proportional-Odds Logit Models.''} \emph{Sociological Methodology}
36 (1): 225--55.

\leavevmode\vadjust pre{\hypertarget{ref-franconeri2021science}{}}%
Franconeri, Steven L, Lace M Padilla, Priti Shah, Jeffrey M Zacks, and
Jessica Hullman. 2021. {``The Science of Visual Data Communication: What
Works.''} \emph{Psychological Science in the Public Interest} 22 (3):
110--61.

\leavevmode\vadjust pre{\hypertarget{ref-franklin2005exploratory}{}}%
Franklin, Laura R. 2005. {``Exploratory Experiments.''} \emph{Philosophy
of Science} 72 (5): 888--99.

\leavevmode\vadjust pre{\hypertarget{ref-esl}{}}%
Friedman, Jerome H., Robert Tibshirani, and Trevor Hastie. 2009.
\emph{The Elements of Statistical Learning}. Springer.

\leavevmode\vadjust pre{\hypertarget{ref-historyofdataviz}{}}%
Friendly, Michael, and Howard Wainer. 2021. \emph{A History of Data
Visaulization and Graphic Communication}. 1st ed. Harvard University
Press.

\leavevmode\vadjust pre{\hypertarget{ref-fry2020big}{}}%
Fry, Hannah. 2020. {``Big Tech Is Testing You.''} \emph{The New Yorker},
61--65.

\leavevmode\vadjust pre{\hypertarget{ref-stringi}{}}%
Gagolewski, Marek. 2020. \emph{R Package Stringi: Character String
Processing Facilities}.
\url{http://www.gagolewski.com/software/stringi/}.

\leavevmode\vadjust pre{\hypertarget{ref-viridis}{}}%
Garnier, Simon, Ross, Noam, Rudis, Robert, Camargo, et al. 2021.
\emph{{viridis} - Colorblind-Friendly Color Maps for r}.
\url{https://doi.org/10.5281/zenodo.4679424}.

\leavevmode\vadjust pre{\hypertarget{ref-gebru2021datasheets}{}}%
Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman
Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021.
{``Datasheets for Datasets.''} \emph{Communications of the ACM} 64 (12):
86--92.

\leavevmode\vadjust pre{\hypertarget{ref-sharlasephora}{}}%
Gelfand, Sharla. 2019. \emph{Crying @ Sephora}.
\url{https://sharla.party/post/crying-sephora/}.

\leavevmode\vadjust pre{\hypertarget{ref-citeSharla}{}}%
---------. 2020. \emph{Opendatatoronto: Access the City of Toronto Open
Data Portal}. \url{https://CRAN.R-project.org/package=opendatatoronto}.

\leavevmode\vadjust pre{\hypertarget{ref-sharlatalks}{}}%
---------. 2021. {``Make a ReprEx... Please.''} \emph{YouTube},
February. \url{https://youtu.be/G5Nm-GpmrLw}.

\leavevmode\vadjust pre{\hypertarget{ref-Gelman2016}{}}%
Gelman, Andrew. 2016. {``What Has Happened down Here Is the Winds Have
Changed.''}
\url{https://statmodeling.stat.columbia.edu/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/}.

\leavevmode\vadjust pre{\hypertarget{ref-gelmanonrdd}{}}%
---------. 2019. \emph{Another Regression Discontinuity Disaster and
What Can We Learn from It}.
\url{https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/}.

\leavevmode\vadjust pre{\hypertarget{ref-gelmantalks}{}}%
---------. 2020. {``Statistical Models of Election Outcomes.''}
\emph{YouTube}, August. \url{https://youtu.be/7gjDnrbLQ4k}.

\leavevmode\vadjust pre{\hypertarget{ref-gelmansmistakes}{}}%
---------. 2021. {``Wrong Again! 30+ Years of Statistical Mistakes.''}
\emph{YouTube}, October. \url{https://youtu.be/mB9Q26uptao}.

\leavevmode\vadjust pre{\hypertarget{ref-bda}{}}%
Gelman, Andrew, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and
Donald Rubin. 2014. \emph{Bayesian Data Analysis}. 3rd ed. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-gelmanandhill}{}}%
Gelman, Andrew, and Jennifer Hill. 2007. \emph{Data Analysis Using
Regression and Multilevel/Hierarchical Models}.

\leavevmode\vadjust pre{\hypertarget{ref-gelman2013garden}{}}%
Gelman, Andrew, and Eric Loken. 2013. {``The Garden of Forking Paths:
Why Multiple Comparisons Can Be a Problem, Even When There Is No
{`Fishing Expedition'} or {`p-Hacking'} and the Research Hypothesis Was
Posited Ahead of Time.''} \emph{Department of Statistics, Columbia
University} 348.

\leavevmode\vadjust pre{\hypertarget{ref-Gelman_2018}{}}%
Gelman, Andrew, Greggor Mattson, and Daniel Simpson. 2018. {``Gaydar and
the Fallacy of Decontextualized Measurement.''} \emph{Sociological
Science} 5 (12): 270--80. \url{https://doi.org/10.15195/v5.a12}.

\leavevmode\vadjust pre{\hypertarget{ref-gelman2002let}{}}%
Gelman, Andrew, Cristian Pasarica, and Rahul Dodhia. 2002. {``Let's
Practice What We Preach: Turning Tables into Graphs.''} \emph{The
American Statistician} 56 (2): 121--30.

\leavevmode\vadjust pre{\hypertarget{ref-gelman2020most}{}}%
Gelman, Andrew, and Aki Vehtari. 2020. {``What Are the Most Important
Statistical Ideas of the Past 50 Years?''} \emph{arXiv Preprint
arXiv:2012.00174}.

\leavevmode\vadjust pre{\hypertarget{ref-Gentemann2021}{}}%
Gentemann, C. L., C. Holdgraf, R. Abernathey, D. Crichton, J.
Colliander, E. J. Kearns, Y. Panda, and R. P. Signell. 2021. {``Science
Storms the Cloud.''} \emph{{AGU} Advances} 2 (2).
\url{https://doi.org/10.1029/2020av000354}.

\leavevmode\vadjust pre{\hypertarget{ref-fieldexperiments}{}}%
Gerber, Alan, and Donald Green. 2012. \emph{Field Experiments: Design,
Analysis, and Interpretation}. W W Norton.

\leavevmode\vadjust pre{\hypertarget{ref-gertler2016impact}{}}%
Gertler, Paul J, Sebastian Martinez, Patrick Premand, Laura B Rawlings,
and Christel MJ Vermeersch. 2016. \emph{Impact Evaluation in Practice}.
The World Bank.

\leavevmode\vadjust pre{\hypertarget{ref-geuenich2021automated}{}}%
Geuenich, Michael J, Jinyu Hou, Sunyun Lee, Shanza Ayub, Hartland W
Jackson, and Kieran R Campbell. 2021. {``Automated Assignment of Cell
Identity from Single-Cell Multiplexed Imaging and Proteomic Data.''}
\emph{Cell Systems} 12 (12): 1173--86.

\leavevmode\vadjust pre{\hypertarget{ref-ghitza2020voter}{}}%
Ghitza, Yair, and Andrew Gelman. 2020. {``Voter Registration Databases
and MRP: Toward the Use of Large-Scale Databases in Public Opinion
Research.''} \emph{Political Analysis} 28 (4): 507--31.

\leavevmode\vadjust pre{\hypertarget{ref-goodman1961snowball}{}}%
Goodman, Leo A. 1961. {``Snowball Sampling.''} \emph{The Annals of
Mathematical Statistics}, 148--70.

\leavevmode\vadjust pre{\hypertarget{ref-citerstanarm}{}}%
Goodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2020.
{``Rstanarm: {Bayesian} Applied Regression Modeling via {Stan}.''}
\url{https://mc-stan.org/rstanarm}.

\leavevmode\vadjust pre{\hypertarget{ref-gould2013median}{}}%
Gould, Stephen Jay. 2013. {``The Median Isn't the Message.''} \emph{AMA
Journal of Ethics} 15 (1): 77--81.

\leavevmode\vadjust pre{\hypertarget{ref-grahamhowtowriteusefully}{}}%
Graham, Paul. 2020. \emph{How to Write Usefully}.
\url{http://paulgraham.com/useful.html}.

\leavevmode\vadjust pre{\hypertarget{ref-green2009testing}{}}%
Green, Donald P, Terence Y Leong, Holger L Kern, Alan S Gerber, and
Christopher W Larimer. 2009. {``Testing the Accuracy of Regression
Discontinuity Analysis Using Experimental Benchmarks.''} \emph{Political
Analysis} 17 (4): 400--417.

\leavevmode\vadjust pre{\hypertarget{ref-green2020mister}{}}%
Green, Eric. 2020. {``Nivi Research: Mister p Helps Us Understand
Vaccine Hesitancy.''}
\url{https://research.nivi.io/posts/2020-12-08-mister-p-helps-us-understand-vaccine-hesitancy/}.

\leavevmode\vadjust pre{\hypertarget{ref-greenland2016statistical}{}}%
Greenland, Sander, Stephen J Senn, Kenneth J Rothman, John B Carlin,
Charles Poole, Steven N Goodman, and Douglas G Altman. 2016.
{``Statistical Tests, p Values, Confidence Intervals, and Power: A Guide
to Misinterpretations.''} \emph{European Journal of Epidemiology} 31
(4): 337--50.

\leavevmode\vadjust pre{\hypertarget{ref-GriffithsSteyvers2004}{}}%
Griffiths, Thomas, and Mark Steyvers. 2004. {``Finding Scientific
Topics.''} \emph{PNAS} 101: 5228--35.

\leavevmode\vadjust pre{\hypertarget{ref-GrolemundWickham2011}{}}%
Grolemund, Garrett, and Hadley Wickham. 2011. {``Dates and Times Made
Easy with {lubridate}.''} \emph{Journal of Statistical Software} 40 (3):
1--25. \url{http://www.jstatsoft.org/v40/i03/}.

\leavevmode\vadjust pre{\hypertarget{ref-Grun2011}{}}%
Grün, Bettina, and Kurt Hornik. 2011. {``{topicmodels}: An {R} Package
for Fitting Topic Models.''} \emph{Journal of Statistical Software} 40
(13): 1--30. \url{https://doi.org/10.18637/jss.v040.i13}.

\leavevmode\vadjust pre{\hypertarget{ref-halberstam}{}}%
Halberstam, David. 1972. \emph{{The Best and the Brightest}}. Random
House.

\leavevmode\vadjust pre{\hypertarget{ref-hamming1996}{}}%
Hamming, Richard W. 1996. \emph{{The Art of Doing Science and
Engineering}}. Stripe Press.

\leavevmode\vadjust pre{\hypertarget{ref-handcock2011comment}{}}%
Handcock, Mark S, and Krista J Gile. 2011. {``Comment: On the Concept of
Snowball Sampling.''} \emph{Sociological Methodology} 41 (1): 367--71.

\leavevmode\vadjust pre{\hypertarget{ref-hangartner2021monitoring}{}}%
Hangartner, Dominik, Daniel Kopp, and Michael Siegenthaler. 2021.
{``Monitoring Hiring Discrimination Through Online Recruitment
Platforms.''} \emph{Nature} 589 (7843): 572--76.

\leavevmode\vadjust pre{\hypertarget{ref-hanretty2020}{}}%
Hanretty, Chris. 2020. {``An Introduction to Multilevel Regression and
Post-Stratification for Estimating Constituency Opinion.''}
\emph{Political Studies Review} 18 (4): 630--45.

\leavevmode\vadjust pre{\hypertarget{ref-hao2019}{}}%
Hao, Karen. 2019. {``{This is how AI bias really happens---and why it's
so hard to fix}.''} \emph{MIT Technology Review}.

\leavevmode\vadjust pre{\hypertarget{ref-hart2016ten}{}}%
Hart, Edmund M, Pauline Barmby, David LeBauer, François Michonneau,
Sarah Mount, Patrick Mulrooney, Timothée Poisot, Kara H Woo, Naupaka B
Zimmerman, and Jeffrey W Hollister. 2016. {``Ten Simple Rules for
Digital Data Storage.''} Public Library of Science San Francisco, CA
USA.

\leavevmode\vadjust pre{\hypertarget{ref-hastie1990generalized}{}}%
Hastie, Trevor J, and Robert J Tibshirani. 1990. \emph{Generalized
Additive Models}. Vol. 43. CRC press.

\leavevmode\vadjust pre{\hypertarget{ref-hayotacademicstyle}{}}%
Hayot, Eric. 2014. \emph{The Elements of Academic Style}. Columbia
University Press.

\leavevmode\vadjust pre{\hypertarget{ref-healyviz}{}}%
Healy, Kieran. 2018. \emph{Data Visualization}. Princeton University
Press.

\leavevmode\vadjust pre{\hypertarget{ref-kieranskitchen}{}}%
---------. 2020. \emph{The Kitchen Counter Observatory}.
\url{https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/}.

\leavevmode\vadjust pre{\hypertarget{ref-heckathorn1997respondent}{}}%
Heckathorn, Douglas D. 1997. {``Respondent-Driven Sampling: A New
Approach to the Study of Hidden Populations.''} \emph{Social Problems}
44 (2): 174--99.

\leavevmode\vadjust pre{\hypertarget{ref-heil2021reproducibility}{}}%
Heil, Benjamin J, Michael M Hoffman, Florian Markowetz, Su-In Lee, Casey
S Greene, and Stephanie C Hicks. 2021. {``Reproducibility Standards for
Machine Learning in the Life Sciences.''} \emph{Nature Methods} 18 (10):
1132--35.

\leavevmode\vadjust pre{\hypertarget{ref-citepurrr}{}}%
Henry, Lionel, and Hadley Wickham. 2020. \emph{Purrr: Functional
Programming Tools}. \url{https://CRAN.R-project.org/package=purrr}.

\leavevmode\vadjust pre{\hypertarget{ref-hernanrobins2020}{}}%
Hernan, Miguel A, and James M Robins. 2020. \emph{What If}. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-herndon2014does}{}}%
Herndon, Thomas, Michael Ash, and Robert Pollin. 2014. {``Does High
Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart
and Rogoff.''} \emph{Cambridge Journal of Economics} 38 (2): 257--79.

\leavevmode\vadjust pre{\hypertarget{ref-hillelwayne}{}}%
Hillel, Wayne. 2017. \emph{How Do We Trust Our Science Code?}
\url{https://www.hillelwayne.com/how-do-we-trust-science-code/}.

\leavevmode\vadjust pre{\hypertarget{ref-MatchIt}{}}%
Ho, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2011.
{``{MatchIt}: Nonparametric Preprocessing for Parametric Causal
Inference.''} \emph{Journal of Statistical Software} 42 (8): 1--28.
\url{https://doi.org/10.18637/jss.v042.i08}.

\leavevmode\vadjust pre{\hypertarget{ref-shorternamestakelonger}{}}%
Hofmeister, Johannes, Janet Siegmund, and Daniel V. Holt. 2017.
{``Shorter Identifier Names Take Longer to Comprehend.''} In \emph{2017
IEEE 24th International Conference on Software Analysis, Evolution and
Reengineering (SANER)}, 217--27.
\url{https://doi.org/10.1109/SANER.2017.7884623}.

\leavevmode\vadjust pre{\hypertarget{ref-holland1986statistics}{}}%
Holland, Paul W. 1986. {``Statistics and Causal Inference.''}
\emph{Journal of the American Statistical Association} 81 (396):
945--60.

\leavevmode\vadjust pre{\hypertarget{ref-palmerpenguins}{}}%
Horst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020.
\emph{Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data}.
\url{https://allisonhorst.github.io/palmerpenguins/}.

\leavevmode\vadjust pre{\hypertarget{ref-hug2019national}{}}%
Hug, Lucia, Monica Alexander, Danzhen You, Leontine Alkema, and UN
Inter-agency Group for Child. 2019. {``National, Regional, and Global
Levels and Trends in Neonatal Mortality Between 1990 and 2017, with
Scenario-Based Projections to 2030: A Systematic Analysis.''} \emph{The
Lancet Global Health} 7 (6): e710--20.

\leavevmode\vadjust pre{\hypertarget{ref-institueforgovernment}{}}%
Hughes, Nicola, and Jill Rutter. 2016. \emph{Oliver Letwin}.
\url{https://www.instituteforgovernment.org.uk/ministers-reflect/person/oliver-letwin/}.

\leavevmode\vadjust pre{\hypertarget{ref-hulley2007designing}{}}%
Hulley, Stephen B. 2007. \emph{Designing Clinical Research}. Lippincott
Williams \& Wilkins.

\leavevmode\vadjust pre{\hypertarget{ref-Hullman2021Designing}{}}%
Hullman, Jessica, and Andrew Gelman. 2021. {``Designing for Interactive
Exploratory Data Analysis Requires Theories of Graphical Inference.''}
\emph{Harvard Data Science Review}, July.
\url{https://doi.org/10.1162/99608f92.3ab8a587}.

\leavevmode\vadjust pre{\hypertarget{ref-theeffect}{}}%
Huntington-Klein, Nick. 2021. \emph{The Effect: An Introduction to
Research Design and Causality}. Chapman \& Hall.

\leavevmode\vadjust pre{\hypertarget{ref-lost2022}{}}%
---------. 2022. {``Library of Statistical Techniques.''}
\url{https://lost-stats.github.io}.

\leavevmode\vadjust pre{\hypertarget{ref-huntington2021influence}{}}%
Huntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni,
Jeffrey R Bloem, Pralhad Burli, Naibin Chen, et al. 2021. {``The
Influence of Hidden Researcher Decisions in Applied Microeconomics.''}
\emph{Economic Inquiry}.

\leavevmode\vadjust pre{\hypertarget{ref-huntington2020influence}{}}%
Huntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni,
Jeffrey Bloem, Pralhad H Burli, Naibin Chen, et al. 2020. {``The
Influence of Hidden Researcher Decisions in Applied Microeconomics.''}

\leavevmode\vadjust pre{\hypertarget{ref-chiphuyenone}{}}%
Huyen, Chip. 2020. \emph{Machine Learning Is Going Real-Time}.
\url{https://huyenchip.com/2020/12/27/real-time-machine-learning.html}.

\leavevmode\vadjust pre{\hypertarget{ref-chiphuyentwo}{}}%
---------. 2022. \emph{Real-Time Machine Learning: Challenges and
Solutions}.
\url{https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html}.

\leavevmode\vadjust pre{\hypertarget{ref-hvitfeldt2021supervised}{}}%
Hvitfeldt, Emil, and Julia Silge. 2021. \emph{Supervised Machine
Learning for Text Analysis in r}. Chapman; Hall/CRC.

\leavevmode\vadjust pre{\hypertarget{ref-citeDiagrammeR}{}}%
Iannone, Richard. 2020. \emph{{DiagrammeR: Graph/Network
Visualization}}. \url{https://CRAN.R-project.org/package=DiagrammeR}.

\leavevmode\vadjust pre{\hypertarget{ref-citegt}{}}%
Iannone, Richard, Joe Cheng, and Barret Schloerke. 2020. \emph{Gt:
Easily Create Presentation-Ready Display Tables}.
\url{https://CRAN.R-project.org/package=gt}.

\leavevmode\vadjust pre{\hypertarget{ref-pointblank}{}}%
Iannone, Richard, and Mauricio Vargas. 2022. \emph{Pointblank: Data
Validation and Organization of Metadata for Local and Remote Tables}.
\url{https://CRAN.R-project.org/package=pointblank}.

\leavevmode\vadjust pre{\hypertarget{ref-citeerik}{}}%
Igelström, Erik. 2020. {``Causal Graphs in r with DiagrammeR.''}
\url{https://www.erikigelstrom.com/articles/causal-graphs-in-r-with-diagrammer/}.

\leavevmode\vadjust pre{\hypertarget{ref-ioannidis2005most}{}}%
Ioannidis, John PA. 2005. {``Why Most Published Research Findings Are
False.''} \emph{PLoS Medicine} 2 (8): e124.

\leavevmode\vadjust pre{\hypertarget{ref-stevejobs}{}}%
Isaacson, Walter. 2011. \emph{Steve Jobs}. Simon \& Schuster.

\leavevmode\vadjust pre{\hypertarget{ref-ishiguro}{}}%
Ishiguro, Kazuo. 1989. \emph{The Remains of the Day}. Faber; Faber.

\leavevmode\vadjust pre{\hypertarget{ref-Izrailev2014}{}}%
Izrailev, Sergei. 2014. \emph{Tictoc: Functions for Timing r Scripts}.
\url{https://CRAN.R-project.org/package=tictoc}.

\leavevmode\vadjust pre{\hypertarget{ref-islr}{}}%
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.
2017. \emph{An Introduction to Statistical Learning with Applications in
r}.

\leavevmode\vadjust pre{\hypertarget{ref-bayesrules}{}}%
Johnson, Alicia A., Miles Ott, and Mine Dogucu. 2022. \emph{Bayes Rules!
An Introduction to Bayesian Modeling with r}. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-Johnson2021Two}{}}%
Johnson, Kaneesha R. 2021. {``Two Regimes of Prison Data Collection.''}
\emph{Harvard Data Science Review}, July.
\url{https://doi.org/10.1162/99608f92.72825001}.

\leavevmode\vadjust pre{\hypertarget{ref-jones1953census}{}}%
Jones, Arnold HM. 1953. {``Census Records of the Later Roman Empire.''}
\emph{The Journal of Roman Studies} 43 (1-2): 49--64.

\leavevmode\vadjust pre{\hypertarget{ref-Jordan2019Artificial}{}}%
Jordan, Michael I. 2019. {``Artificial Intelligence---the Revolution
Hasn't Happened Yet.''} \emph{Harvard Data Science Review} 1 (1).
\url{https://doi.org/10.1162/99608f92.f06c6e61}.

\leavevmode\vadjust pre{\hypertarget{ref-joyner1991modeling}{}}%
Joyner, MICHAEL J. 1991. {``Modeling: Optimal Marathon Performance on
the Basis of Physiological Factors.''} \emph{Journal of Applied
Physiology} 70 (2): 683--87.

\leavevmode\vadjust pre{\hypertarget{ref-KahleWickham2013}{}}%
Kahle, David, and Hadley Wickham. 2013. {``Ggmap: Spatial Visualization
with Ggplot2.''} \emph{The R Journal} 5 (1): 144--61.
\url{http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-NoiseKahneman}{}}%
Kahneman, Daniel, Olivier Sibony, and Cass Sunstein. 2021. \emph{Noise:
A Flaw in Human Judgment}. William Collins.

\leavevmode\vadjust pre{\hypertarget{ref-citetidybayes}{}}%
Kay, Matthew. 2020. \emph{{tidybayes}: Tidy Data and Geoms for
{Bayesian} Models}. \url{https://doi.org/10.5281/zenodo.1308151}.

\leavevmode\vadjust pre{\hypertarget{ref-rtweet}{}}%
Kearney, Michael W. 2019. {``Rtweet: Collecting and Analyzing Twitter
Data.''} \emph{Journal of Open Source Software} 4 (42): 1829.
\url{https://doi.org/10.21105/joss.01829}.

\leavevmode\vadjust pre{\hypertarget{ref-kennedygabry2020}{}}%
Kennedy, Lauren, and Jonah Gabry. 2020. {``MRP with Rstanarm.''}
\url{https://mc-stan.org/rstanarm/articles/mrp.html}.

\leavevmode\vadjust pre{\hypertarget{ref-kennedy2020know}{}}%
Kennedy, Lauren, and Andrew Gelman. 2020. {``Know Your Population and
Know Your Model: Using Model-Based Regression and Poststratification to
Generalize Findings Beyond the Observed Sample.''}
\url{https://arxiv.org/abs/1906.11323}.

\leavevmode\vadjust pre{\hypertarget{ref-kennedy2020using}{}}%
Kennedy, Lauren, Katharine Khanna, Daniel Simpson, and Andrew Gelman.
2020. {``Using Sex and Gender in Survey Adjustment.''}
\url{https://arxiv.org/abs/2009.14401}.

\leavevmode\vadjust pre{\hypertarget{ref-kenny2021impact}{}}%
Kenny, Christopher T., Shiro Kuriwaki, Cory McCartan, Evan Rosenman,
Tyler Simko, and Kosuke Imai. 2021. {``The Impact of the u.s. Census
Disclosure Avoidance System on Redistricting and Voting Rights
Analysis.''} \url{https://arxiv.org/abs/2105.14197}.

\leavevmode\vadjust pre{\hypertarget{ref-keyes2019}{}}%
Keyes, Os. 2019. {``Counting the Countless.''} \emph{Real Life}.
\url{https://reallifemag.com/counting-the-countless/}.

\leavevmode\vadjust pre{\hypertarget{ref-kharecha2013prevented}{}}%
Kharecha, Pushker A, and James E Hansen. 2013. {``Prevented Mortality
and Greenhouse Gas Emissions from Historical and Projected Nuclear
Power.''} \emph{Environmental Science \& Technology} 47 (9): 4889--95.

\leavevmode\vadjust pre{\hypertarget{ref-kiang2021racial}{}}%
Kiang, Mathew V, Alexander C Tsai, Monica J Alexander, David H Rehkopf,
and Sanjay Basu. 2021. {``Racial/Ethnic Disparities in Opioid-Related
Mortality in the USA, 1999--2019: The Extreme Case of Washington DC.''}
\emph{Journal of Urban Health} 98 (5): 589--95.

\leavevmode\vadjust pre{\hypertarget{ref-kimmerer2013}{}}%
Kimmerer, Robin Wall. 2012. \emph{Braiding Sweetgrass}. Milkweed
Editions.

\leavevmode\vadjust pre{\hypertarget{ref-king2006publication}{}}%
King, Gary. 2006. {``Publication, Publication.''} \emph{PS: Political
Science \& Politics} 39 (1): 119--25.

\leavevmode\vadjust pre{\hypertarget{ref-king2019propensity}{}}%
King, Gary, and Richard Nielsen. 2019. {``Why Propensity Scores Should
Not Be Used for Matching.''} \emph{Political Analysis} 27 (4): 435--54.

\leavevmode\vadjust pre{\hypertarget{ref-stephenking}{}}%
King, Stephen. 2000. \emph{On Writing: A Memoir of the Craft}. Scribner.

\leavevmode\vadjust pre{\hypertarget{ref-citeaer}{}}%
Kleiber, Christian, and Achim Zeileis. 2008. \emph{Applied Econometrics
with {R}}. New York: Springer-Verlag.
\url{https://CRAN.R-project.org/package=AER}.

\leavevmode\vadjust pre{\hypertarget{ref-Knuth1984}{}}%
Knuth, D. E. 1984. {``Literate Programming.''} \emph{The Computer
Journal} 27 (2): 97--111. \url{https://doi.org/10.1093/comjnl/27.2.97}.

\leavevmode\vadjust pre{\hypertarget{ref-knuth}{}}%
Knuth, Donald E. 1998. \emph{Art of Computer Programming, Volume 2:
Seminumerical Algorithms}. 2nd ed.

\leavevmode\vadjust pre{\hypertarget{ref-koenecke2020synthetic}{}}%
Koenecke, Allison, and Hal Varian. 2020. {``Synthetic Data Generation
for Economists.''} \url{https://arxiv.org/abs/2011.01374}.

\leavevmode\vadjust pre{\hypertarget{ref-koenker2009reproducible}{}}%
Koenker, Roger, and Achim Zeileis. 2009. {``On Reproducible Econometric
Research.''} \emph{Journal of Applied Econometrics} 24 (5): 833--47.

\leavevmode\vadjust pre{\hypertarget{ref-kohavi}{}}%
Kohavi, Ron, Diane Tang, and Ya Xu. 2020. \emph{Trustworthy Online
Controlled Experiments: A Practical Guide to a/b Testing}. Cambridge
University Press.

\leavevmode\vadjust pre{\hypertarget{ref-kroger2021data}{}}%
Kröger, Jacob Leon, Milagros Miceli, and Florian Müller. 2021. {``How
Data Can Be Used Against People: A Classification of Personal Data
Misuses.''} \emph{Available at SSRN 3887097}.

\leavevmode\vadjust pre{\hypertarget{ref-citepostcards}{}}%
Kross, Sean. 2021. \emph{Postcards: Create Beautiful, Simple Personal
Websites}. \url{https://CRAN.R-project.org/package=postcards}.

\leavevmode\vadjust pre{\hypertarget{ref-poissonreg}{}}%
Kuhn, Max. 2021. \emph{Poissonreg: Model Wrappers for Poisson
Regression}. \url{https://CRAN.R-project.org/package=poissonreg}.

\leavevmode\vadjust pre{\hypertarget{ref-citeTidymodels}{}}%
Kuhn, Max, and Hadley Wickham. 2020. \emph{Tidymodels: A Collection of
Packages for Modeling and Machine Learning Using Tidyverse Principles.}
\url{https://www.tidymodels.org}.

\leavevmode\vadjust pre{\hypertarget{ref-dataverse}{}}%
Kuriwaki, Shiro, Will Beasley, and Thomas J. Leeper. 2022.
\emph{Dataverse: R Client for Dataverse 4+ Repositories}.

\leavevmode\vadjust pre{\hypertarget{ref-NationalIncomeAndItsComposition}{}}%
Kuznets, Simon. 1941. \emph{{National Income and Its Composition,
1919-1938}}. National Bureau of Economic Research.

\leavevmode\vadjust pre{\hypertarget{ref-birdbybird}{}}%
Lamott, Anne. 1994. \emph{Bird by Bird: Some Instructions on Writing and
Life}. Anchor Books.

\leavevmode\vadjust pre{\hypertarget{ref-citelabelled}{}}%
Larmarange, Joseph. 2021. \emph{Labelled: Manipulating Labelled Data}.
\url{https://CRAN.R-project.org/package=labelled}.

\leavevmode\vadjust pre{\hypertarget{ref-latour}{}}%
Latour, Bruno. 1996. {``On Actor-Network Theory: A Few
Clarifications.''} \emph{Soziale Welt} 47 (4): 369--81.
\url{http://www.jstor.org/stable/40878163}.

\leavevmode\vadjust pre{\hypertarget{ref-lauderdale2020model}{}}%
Lauderdale, Benjamin E, Delia Bailey, Jack Blumenau, and Douglas Rivers.
2020. {``Model-Based Pre-Election Polling for National and Sub-National
Outcomes in the US and UK.''} \emph{International Journal of
Forecasting} 36 (2): 399--413.

\leavevmode\vadjust pre{\hypertarget{ref-lazear2000economic}{}}%
Lazear, Edward P. 2000. {``Economic Imperialism.''} \emph{The Quarterly
Journal of Economics} 115 (1): 99--146.

\leavevmode\vadjust pre{\hypertarget{ref-lee2018ten}{}}%
Lee, Benjamin D. 2018. {``Ten Simple Rules for Documenting Scientific
Software.''} Public Library of Science San Francisco, CA USA.

\leavevmode\vadjust pre{\hypertarget{ref-Leek2017}{}}%
Leek, Jeff, Blakeley B. McShane, Andrew Gelman, David Colquhoun, Michèle
B. Nuijten, and Steven N. Goodman. 2017. {``Five Ways to Fix
Statistics.''} \emph{Nature} 551 (7682): 557--59.
\url{https://doi.org/10.1038/d41586-017-07522-z}.

\leavevmode\vadjust pre{\hypertarget{ref-leekandpeng}{}}%
Leek, Jeff, and Roger D. Peng. 2020. {``{Advanced Data Science 2020}.''}
\url{http://jtleek.com/ads2020/index.html}.

\leavevmode\vadjust pre{\hypertarget{ref-lin2020ten}{}}%
Lin, Sarah, Ibraheem Ali, and Greg Wilson. 2020. {``Ten Quick Tips for
Making Things Findable.''} \emph{PLoS Computational Biology} 16 (12):
e1008469.

\leavevmode\vadjust pre{\hypertarget{ref-Little2021}{}}%
Little, Roderick J., and Roger J. Lewis. 2021. {``Estimands, Estimators,
and Estimates.''} \emph{{JAMA}} 326 (10): 967.
\url{https://doi.org/10.1001/jama.2021.2886}.

\leavevmode\vadjust pre{\hypertarget{ref-citedatasauRus}{}}%
Locke, Steph, and Lucy D'Agostino McGowan. 2018. \emph{datasauRus:
Datasets from the Datasaurus Dozen}.
\url{https://CRAN.R-project.org/package=datasauRus}.

\leavevmode\vadjust pre{\hypertarget{ref-lohr}{}}%
Lohr, Sharon L. 2019. \emph{Sampling: Design and Analysis}. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-lovelace2019geocomputation}{}}%
Lovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019.
\emph{Geocomputation with r}. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-lucas1978asset}{}}%
Lucas Jr, Robert E. 1978. {``Asset Prices in an Exchange Economy.''}
\emph{Econometrica: Journal of the Econometric Society}, 1429--45.

\leavevmode\vadjust pre{\hypertarget{ref-luebke1994locating}{}}%
Luebke, David Martin, and Sybil Milton. 1994. {``Locating the Victim: An
Overview of Census-Taking, Tabulation Technology, and Persecution in
Nazi Germany.''} \emph{IEEE Annals of the History of Computing} 16 (3):
25.

\leavevmode\vadjust pre{\hypertarget{ref-Lundberg2021}{}}%
Lundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021. {``What Is
Your Estimand? Defining the Target Quantity Connects Statistical
Evidence to Theory.''} \emph{American Sociological Review} 86 (3):
532--65. \url{https://doi.org/10.1177/00031224211004187}.

\leavevmode\vadjust pre{\hypertarget{ref-luscombe2021algorithmic}{}}%
Luscombe, Alex, Kevin Dick, and Kevin Walby. 2021. {``Algorithmic
Thinking in the Public Interest: Navigating Technical, Legal, and
Ethical Hurdles to Web Scraping in the Social Sciences.''} \emph{Quality
\& Quantity}, 1--22.

\leavevmode\vadjust pre{\hypertarget{ref-luscombe2020policing}{}}%
Luscombe, Alex, and Alexander McClelland. 2020. {``Policing the
Pandemic: Tracking the Policing of COVID-19 Across Canada.''}

\leavevmode\vadjust pre{\hypertarget{ref-macaulay}{}}%
Macaulay, Thomas Babington. 1848. \emph{The History of England from the
Accession of James the Second}.

\leavevmode\vadjust pre{\hypertarget{ref-macdorman2018failure}{}}%
MacDorman, Marian F, and Eugene Declercq. 2018. {``The Failure of United
States Maternal Mortality Reporting and Its Impact on Women's Lives.''}
\emph{Birth (Berkeley, Calif.)} 45 (2): 105.

\leavevmode\vadjust pre{\hypertarget{ref-martinez2019much}{}}%
Martinez, Luis R. 2019. {``How Much Should We Trust the Dictator's GDP
Growth Estimates?''} \emph{Available at SSRN 3093296}.

\leavevmode\vadjust pre{\hypertarget{ref-upworthy}{}}%
Matias, J. Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles
Ebersole. 2019. {``The Upworthy Research Archive.''}
\url{https://upworthy.natematias.com}.

\leavevmode\vadjust pre{\hypertarget{ref-Mattson2017}{}}%
Mattson, Greggor. 2017. {``Artificial Intelligence Discovers Gayface.
Sigh.''}
\url{https://greggormattson.com/2017/09/09/artificial-intelligence-discovers-gayface/amp/}.

\leavevmode\vadjust pre{\hypertarget{ref-mcclelland2019lock}{}}%
McClelland, Alexander. 2019. {``"Lock This Whore up": Legal Violence and
Flows of Information Precipitating Personal Violence Against People
Criminalised for HIV-Related Crimes in Canada.''} \emph{European Journal
of Risk Regulation} 10 (1): 132--47.

\leavevmode\vadjust pre{\hypertarget{ref-citemcelreath}{}}%
McElreath, Richard. 2020. \emph{{Statistical Rethinking: A Bayesian
Course with Examples in R and Stan}}. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-draftnumberfour}{}}%
McPhee, John. 2017. \emph{Draft No. 4}. Farrar, Straus; Giroux.

\leavevmode\vadjust pre{\hypertarget{ref-mcquire2019one}{}}%
McQuire, Scott. 2019. {``One Map to Rule Them All? Google Maps as
Digital Technical Object.''} \emph{Communication and the Public} 4 (2):
150--65.

\leavevmode\vadjust pre{\hypertarget{ref-meng2018statistical}{}}%
Meng, Xiao-Li. 2018. {``Statistical Paradises and Paradoxes in Big Data
(i): Law of Large Populations, Big Data Paradox, and the 2016 US
Presidential Election.''} \emph{The Annals of Applied Statistics} 12
(2): 685--726.

\leavevmode\vadjust pre{\hypertarget{ref-Meng2021What}{}}%
---------. 2021. {``What Are the Values of Data, Data Science, or Data
Scientists?''} \emph{Harvard Data Science Review}, January.
\url{https://doi.org/10.1162/99608f92.ee717cf7}.

\leavevmode\vadjust pre{\hypertarget{ref-merali2010computational}{}}%
Merali, Zeeya. 2010. {``Computational Science:... Error.''}
\emph{Nature} 467 (7317): 775--77.

\leavevmode\vadjust pre{\hypertarget{ref-geuenich_michael_2021_5156049}{}}%
Michael, Geuenich, Hou Jinyu, Lee Sunyun, Ayub Shanza, Jackson Hartland,
and Campbell Kieran. 2021. {``{Automated assignment of cell identity
from single- cell multiplexed imaging and proteomic data}.''} Zenodo.
\url{https://doi.org/10.5281/zenodo.5156049}.

\leavevmode\vadjust pre{\hypertarget{ref-michener2015ten}{}}%
Michener, William K. 2015. {``Ten Simple Rules for Creating a Good Data
Management Plan.''} \emph{PLoS Computational Biology} 11 (10): e1004525.
https://doi.org/\url{https://doi.org/10.1371/journal.pcbi.1004525}.

\leavevmode\vadjust pre{\hypertarget{ref-GoodResearchCode}{}}%
Mineault, Patrick, and The Good Research Code Handbook Community. 2021.
{``The Good Research Code Handbook.''}
\url{https://doi.org/10.5281/ZENODO.5796873}.

\leavevmode\vadjust pre{\hypertarget{ref-Mitchell_2019}{}}%
Mitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy
Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and
Timnit Gebru. 2019. {``Model Cards for Model Reporting.''}
\emph{Proceedings of the Conference on Fairness, Accountability, and
Transparency}, January. \url{https://doi.org/10.1145/3287560.3287596}.

\leavevmode\vadjust pre{\hypertarget{ref-miyakawa2020no}{}}%
Miyakawa, Tsuyoshi. 2020. {``No Raw Data, No Science: Another Possible
Source of the Reproducibility Crisis.''} \emph{Molecular Brain}.
Springer.

\leavevmode\vadjust pre{\hypertarget{ref-murphy2017}{}}%
Murphy, Heather. 2017. \emph{Why Stanford Researchers Tried to Create a
'Gaydar' Machine}.

\leavevmode\vadjust pre{\hypertarget{ref-navarro2021getting}{}}%
Navarro, Danielle. 2021. {``Notes from a Data Witch: Getting Started
with Apache Arrow.''}
\url{https://blog.djnavarro.net/starting-apache-arrow-in-r}.

\leavevmode\vadjust pre{\hypertarget{ref-nelder1972generalized}{}}%
Nelder, John Ashworth, and Robert WM Wedderburn. 1972. {``Generalized
Linear Models.''} \emph{Journal of the Royal Statistical Society: Series
A (General)} 135 (3): 370--84.

\leavevmode\vadjust pre{\hypertarget{ref-neufeld2002wernher}{}}%
Neufeld, Michael J. 2002. {``Wernher von Braun, the SS, and
Concentration Camp Labor: Questions of Moral, Political, and Criminal
Responsibility.''} \emph{German Studies Review} 25 (1): 57--78.

\leavevmode\vadjust pre{\hypertarget{ref-RColorBrewer}{}}%
Neuwirth, Erich. 2014. \emph{RColorBrewer: ColorBrewer Palettes}.
\url{https://CRAN.R-project.org/package=RColorBrewer}.

\leavevmode\vadjust pre{\hypertarget{ref-neyman1934two}{}}%
Neyman, Jerzy. 1934. {``On the Two Different Aspects of the
Representative Method: The Method of Stratified Sampling and the Method
of Purposive Selection.''} \emph{Journal of the Royal Statistical
Society} 97 (4): 558--625.

\leavevmode\vadjust pre{\hypertarget{ref-obermeyer2019dissecting}{}}%
Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil
Mullainathan. 2019. {``Dissecting Racial Bias in an Algorithm Used to
Manage the Health of Populations.''} \emph{Science} 366 (6464): 447--53.

\leavevmode\vadjust pre{\hypertarget{ref-EssentialMacroAggregates}{}}%
OECD. 2014. {``The Essential Macroeconomic Aggregates.''} In
\emph{Understanding National Accounts}, 13--46. {OECD}.
\url{https://doi.org/10.1787/9789264214637-2-en}.

\leavevmode\vadjust pre{\hypertarget{ref-citeoecdgdp}{}}%
---------. 2022. \emph{Quarterly GDP}.
\url{https://data.oecd.org/gdp/quarterly-gdp.htm}.

\leavevmode\vadjust pre{\hypertarget{ref-jsonlite}{}}%
Ooms, Jeroen. 2014. {``The Jsonlite Package: A Practical and Consistent
Mapping Between JSON Data and r Objects.''} \emph{arXiv:1403.2805
{[}Stat.CO{]}}. \url{https://arxiv.org/abs/1403.2805}.

\leavevmode\vadjust pre{\hypertarget{ref-Ooms2018pdftools}{}}%
---------. 2018a. \emph{Pdftools: Text Extraction, Rendering and
Converting of PDF Documents}.
\url{https://CRAN.R-project.org/package=pdftools}.

\leavevmode\vadjust pre{\hypertarget{ref-Ooms2018tesseract}{}}%
---------. 2018b. \emph{Tesseract: Open Source OCR Engine}.
\url{https://CRAN.R-project.org/package=tesseract}.

\leavevmode\vadjust pre{\hypertarget{ref-citepdftools}{}}%
---------. 2019a. \emph{Pdftools: Text Extraction, Rendering and
Converting of PDF Documents}.
\url{https://CRAN.R-project.org/package=pdftools}.

\leavevmode\vadjust pre{\hypertarget{ref-pdftools}{}}%
---------. 2019b. \emph{Pdftools: Text Extraction, Rendering and
Converting of PDF Documents}.
\url{https://CRAN.R-project.org/package=pdftools}.

\leavevmode\vadjust pre{\hypertarget{ref-citetesseract}{}}%
---------. 2019c. \emph{Tesseract: Open Source OCR Engine}.
\url{https://CRAN.R-project.org/package=tesseract}.

\leavevmode\vadjust pre{\hypertarget{ref-openssl}{}}%
---------. 2021. \emph{Openssl: Toolkit for Encryption, Signatures and
Certificates Based on OpenSSL}.
\url{https://CRAN.R-project.org/package=openssl}.

\leavevmode\vadjust pre{\hypertarget{ref-oostrom2021}{}}%
Oostrom, Tamar. 2021. {``Funding of Clinical Trials and Reported Drug
Efficacy.''}
\url{https://drive.google.com/file/d/1EQLCH0ns99IxYBkxPNbagcZtGgE9a8MQ/view}.

\leavevmode\vadjust pre{\hypertarget{ref-politicsandtheenglishlanguage}{}}%
Orwell, George. 1946. \emph{Politics and the English Language}.
\url{https://www.orwellfoundation.com/the-orwell-foundation/orwell/essays-and-other-works/politics-and-the-english-language/}.

\leavevmode\vadjust pre{\hypertarget{ref-patki2016synthetic}{}}%
Patki, Neha, Roy Wedge, and Kalyan Veeramachaneni. 2016. {``The
Synthetic Data Vault.''} In \emph{2016 IEEE International Conference on
Data Science and Advanced Analytics (DSAA)}, 399--410. IEEE.

\leavevmode\vadjust pre{\hypertarget{ref-Paullada2021}{}}%
Paullada, Amandalynne, Inioluwa Deborah Raji, Emily M. Bender, Emily
Denton, and Alex Hanna. 2021. {``Data and Its (Dis)contents: A Survey of
Dataset Development and Use in Machine Learning Research.''}
\emph{Patterns} 2 (11): 100336.
\url{https://doi.org/10.1016/j.patter.2021.100336}.

\leavevmode\vadjust pre{\hypertarget{ref-kaylinpavlik}{}}%
Pavlik, Kaylin. 2019. {``Understanding + Classifying Genres Using
Spotify Audio Features.''}
\url{https://www.kaylinpavlik.com/classifying-songs-genres/}.

\leavevmode\vadjust pre{\hypertarget{ref-citepatchwork}{}}%
Pedersen, Thomas Lin. 2020. \emph{Patchwork: The Composer of Plots}.
\url{https://CRAN.R-project.org/package=patchwork}.

\leavevmode\vadjust pre{\hypertarget{ref-phillips1958relation}{}}%
Phillips, Alban W. 1958. {``The Relation Between Unemployment and the
Rate of Change of Money Wage Rates in the United Kingdom, 1861-1957.''}
\emph{Economica} 25 (100): 283--99.

\leavevmode\vadjust pre{\hypertarget{ref-pineau2021improving}{}}%
Pineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent
Larivière, Alina Beygelzimer, Florence d'Alché-Buc, Emily Fox, and Hugo
Larochelle. 2021. {``Improving Reproducibility in Machine Learning
Research: A Report from the NeurIPS 2019 Reproducibility Program.''}
\emph{Journal of Machine Learning Research} 22.

\leavevmode\vadjust pre{\hypertarget{ref-pitman}{}}%
Pitman, Jim. 1993. \emph{Probability}.

\leavevmode\vadjust pre{\hypertarget{ref-hillpostcards}{}}%
Presmanes Hill, Alison. 2021a. \emph{{M-F-E-O: postcards + distill}}.
\url{https://alison.rbind.io/post/2020-12-22-postcards-distill/}.

\leavevmode\vadjust pre{\hypertarget{ref-citealisonhillblogdown}{}}%
---------. 2021b. \emph{Up \& Running with Blogdown in 2021}.

\leavevmode\vadjust pre{\hypertarget{ref-citeR}{}}%
R Core Team. 2021. \emph{R: A Language and Environment for Statistical
Computing}. Vienna, Austria: R Foundation for Statistical Computing.
\url{https://www.R-project.org/}.

\leavevmode\vadjust pre{\hypertarget{ref-register2020}{}}%
Register, Yim. 2020. {``Data Science Ethics in 6 Minutes.''}
\url{https://youtu.be/mA4gypAiRYU}.

\leavevmode\vadjust pre{\hypertarget{ref-reid2003asymptotics}{}}%
Reid, Nancy. 2003. {``Asymptotics and the Theory of Inference.''}
\emph{The Annals of Statistics} 31 (6): 1695--2095.

\leavevmode\vadjust pre{\hypertarget{ref-arrow}{}}%
Richardson, Neal, Ian Cook, Nic Crane, Jonathan Keane, Romain François,
Jeroen Ooms, and Apache Arrow. 2022. \emph{Arrow: Integration to
'Apache' 'Arrow'}. \url{https://CRAN.R-project.org/package=arrow}.

\leavevmode\vadjust pre{\hypertarget{ref-columnnamesascontracts}{}}%
Riederer, Emily. 2020. {``Column Names as Contracts.''}
\url{https://emilyriederer.netlify.app/post/column-name-contracts/}.

\leavevmode\vadjust pre{\hypertarget{ref-convo}{}}%
---------. 2022. \emph{Convo: Enables Conversations and Contracts
Through Controlled Vocabulary Naming Conventions}.
\url{https://github.com/emilyriederer/convo}.

\leavevmode\vadjust pre{\hypertarget{ref-rilke}{}}%
Rilke, Rainer Maria. 1929. \emph{Letters to a Young Poet}.

\leavevmode\vadjust pre{\hypertarget{ref-gutenbergr}{}}%
Robinson, David. 2021. \emph{Gutenbergr: Download and Process Public
Domain Works from Project Gutenberg}.
\url{https://CRAN.R-project.org/package=gutenbergr}.

\leavevmode\vadjust pre{\hypertarget{ref-broom}{}}%
Robinson, David, Alex Hayes, and Simon Couch. 2021. \emph{Broom: Convert
Statistical Objects into Tidy Tibbles}.
\url{https://CRAN.R-project.org/package=broom}.

\leavevmode\vadjust pre{\hypertarget{ref-robinsonnolis2020}{}}%
Robinson, Emily, and Jacqueline Nolis. 2020. \emph{Build a Career in
Data Science}.
\url{https://livebook.manning.com/book/build-a-career-in-data-science?origin=product-look-inside}.

\leavevmode\vadjust pre{\hypertarget{ref-rockoff2019controversies}{}}%
Rockoff, Hugh. 2019. {``On the Controversies Behind the Origins of the
Federal Economic Statistics.''} \emph{Journal of Economic Perspectives}
33 (1): 147--64.

\leavevmode\vadjust pre{\hypertarget{ref-ibmdataset}{}}%
Ross, Casey. 2022. {``How a Decades-Old Database Became a Hugely
Profitable Dossier on the Health of 270 Million Americans.''}
\emph{Stat}, February.
\url{https://www.statnews.com/2022/02/01/ibm-watson-health-marketscan-data/}.

\leavevmode\vadjust pre{\hypertarget{ref-hrbrthemes}{}}%
Rudis, Bob. 2020. \emph{Hrbrthemes: Additional Themes, Theme Components
and Utilities for 'Ggplot2'}.
\url{https://CRAN.R-project.org/package=hrbrthemes}.

\leavevmode\vadjust pre{\hypertarget{ref-ruggles2019differential}{}}%
Ruggles, Steven, Catherine Fitch, Diana Magnuson, and Jonathan
Schroeder. 2019. {``Differential Privacy and Census Data: Implications
for Social and Economic Research.''} In \emph{AEA Papers and
Proceedings}, 109:403--8.

\leavevmode\vadjust pre{\hypertarget{ref-ipumsusa}{}}%
Ruggles, Steven, Sarah Flood, Sophia Foster, Ronald Goeken, Jose Pacas,
Megan Schouweiler, and Matthew Sobek. 2021. {``IPUMS USA: Version
11.0.''} Minneapolis, MN: IPUMS.
\url{https://doi.org/10.18128/D010.V11.0}.

\leavevmode\vadjust pre{\hypertarget{ref-Salganik2018}{}}%
Salganik, Matthew. 2018. \emph{Bit by Bit: Social Research in the
Digital Age}. Princeton University Press.

\leavevmode\vadjust pre{\hypertarget{ref-salganik2006experimental}{}}%
Salganik, Matthew J, Peter Sheridan Dodds, and Duncan J Watts. 2006.
{``Experimental Study of Inequality and Unpredictability in an
Artificial Cultural Market.''} \emph{Science} 311 (5762): 854--56.

\leavevmode\vadjust pre{\hypertarget{ref-salganik2004sampling}{}}%
Salganik, Matthew J, and Douglas D Heckathorn. 2004. {``Sampling and
Estimation in Hidden Populations Using Respondent-Driven Sampling.''}
\emph{Sociological Methodology} 34 (1): 193--240.

\leavevmode\vadjust pre{\hypertarget{ref-samuel1959some}{}}%
Samuel, Arthur L. 1959. {``Some Studies in Machine Learning Using the
Game of Checkers.''} \emph{IBM Journal of Research and Development} 3
(3): 210--29.

\leavevmode\vadjust pre{\hypertarget{ref-plumber}{}}%
Schloerke, Barret, and Jeff Allen. 2021. \emph{Plumber: An API Generator
for r}. \url{https://CRAN.R-project.org/package=plumber}.

\leavevmode\vadjust pre{\hypertarget{ref-sekhon2017understanding}{}}%
Sekhon, Jasjeet S, and Rocio Titiunik. 2017. {``Understanding Regression
Discontinuity Designs as Observational Studies.''} \emph{Observational
Studies} 3 (2): 174--82.

\leavevmode\vadjust pre{\hypertarget{ref-si2020use}{}}%
Si, Yajuan. 2020. {``On the Use of Auxiliary Variables in Multilevel
Regression and Poststratification.''}
\url{https://arxiv.org/abs/2011.00360}.

\leavevmode\vadjust pre{\hypertarget{ref-sidesvavreckwarshaw}{}}%
Sides, John, Lynn Vavreck, and Christopher Warshaw. 2021. {``The Effect
of Television Advertising in United States Elections.''} \emph{American
Political Science Review}, 1--17.
\url{https://doi.org/10.1017/S000305542100112X}.

\leavevmode\vadjust pre{\hypertarget{ref-silberzahn2018many}{}}%
Silberzahn, Raphael, Eric L Uhlmann, Daniel P Martin, Pasquale Anselmi,
Frederik Aust, Eli Awtrey, Štěpán Bahnı́k, et al. 2018. {``Many Analysts,
One Data Set: Making Transparent How Variations in Analytic Choices
Affect Results.''} \emph{Advances in Methods and Practices in
Psychological Science} 1 (3): 337--56.

\leavevmode\vadjust pre{\hypertarget{ref-silge2018}{}}%
Silge, Julia. 2018. \emph{Text Classification with Tidy Data
Principles}.
\url{https://juliasilge.com/blog/tidy-text-classification/}.

\leavevmode\vadjust pre{\hypertarget{ref-SilgeRobinson2016}{}}%
Silge, Julia, and David Robinson. 2016. {``Tidytext: Text Mining and
Analysis Using Tidy Data Principles in r.''} \emph{JOSS} 1 (3).
\url{https://doi.org/10.21105/joss.00037}.

\leavevmode\vadjust pre{\hypertarget{ref-natefixesmistake}{}}%
Silver, Nate. 2020. \emph{We Fixed an Issue with How Our Primary
Forecast Was Calculating Candidates' Demographic Strengths}.
\url{https://fivethirtyeight.com/features/we-fixed-a-mistake-in-how-our-primary-forecast-was-calculating-candidates-demographic-strengths/}.

\leavevmode\vadjust pre{\hypertarget{ref-simonsohn2013just}{}}%
Simonsohn, Uri. 2013. {``Just Post It: The Lesson from Two Cases of
Fabricated Data Detected by Statistics Alone.''} \emph{Psychological
Science} 24 (10): 1875--88.

\leavevmode\vadjust pre{\hypertarget{ref-simpson1951interpretation}{}}%
Simpson, Edward H. 1951. {``The Interpretation of Interaction in
Contingency Tables.''} \emph{Journal of the Royal Statistical Society:
Series B (Methodological)} 13 (2): 238--41.

\leavevmode\vadjust pre{\hypertarget{ref-somers2017torching}{}}%
Somers, James. 2017. {``Torching the Modern-Day Library of
Alexandria.''} \emph{The Atlantic} 20.

\leavevmode\vadjust pre{\hypertarget{ref-sprint2019mining}{}}%
Sprint, Gina, and Jason Conci. 2019. {``Mining Github Classroom Commit
Behavior in Elective and Introductory Computer Science Courses.''}
\emph{The Journal of Computing Sciences in Colleges} 35 (1).

\leavevmode\vadjust pre{\hypertarget{ref-staniak2019landscape}{}}%
Staniak, Mateusz, and Przemyslaw Biecek. 2019. {``The Landscape of r
Packages for Automated Exploratory Data Analysis.''} \emph{arXiv
Preprint arXiv:1904.02101}.

\leavevmode\vadjust pre{\hypertarget{ref-statcanhistory}{}}%
Statistics Canada. 2017. {``Guide to the Census of Population, 2016.''}
Statistics Canada.
\url{https://www12.statcan.gc.ca/census-recensement/2016/ref/98-304/98-304-x2016001-eng.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-statcan2020}{}}%
---------. 2020. {``Sex at Birth and Gender: Technical Report on Changes
for the 2021 Census.''} Statistics Canada.
\url{https://www12.statcan.gc.ca/census-recensement/2021/ref/98-20-0002/982000022020002-eng.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-wallacestevens}{}}%
Stevens, Wallace. 1934. \emph{The Idea of Order at Key West}.
\url{https://www.poetryfoundation.org/poems/43431/the-idea-of-order-at-key-west}.

\leavevmode\vadjust pre{\hypertarget{ref-SteyversGriffiths2006}{}}%
Steyvers, Mark, and Tom Griffiths. 2006. {``Probabilistic Topic
Models.''} In \emph{Latent Semantic Analysis: A Road to Meaning}, edited
by T. Landauer, D McNamara, S. Dennis, and W. Kintsch.

\leavevmode\vadjust pre{\hypertarget{ref-stigler}{}}%
Stigler, Stephen. 1986. \emph{The History of Statistics}. Harvard
University Press.

\leavevmode\vadjust pre{\hypertarget{ref-stock2003retrospectives}{}}%
Stock, James H, and Francesco Trebbi. 2003. {``Retrospectives: Who
Invented Instrumental Variable Regression?''} \emph{Journal of Economic
Perspectives} 17 (3): 177--94.

\leavevmode\vadjust pre{\hypertarget{ref-stolberg2006inventing}{}}%
Stolberg, Michael. 2006. {``Inventing the Randomized Double-Blind Trial:
The Nuremberg Salt Test of 1835.''} \emph{Journal of the Royal Society
of Medicine} 99 (12): 642--43.

\leavevmode\vadjust pre{\hypertarget{ref-student1908probable}{}}%
Student. 1908. {``The Probable Error of a Mean.''} \emph{Biometrika},
1--25.

\leavevmode\vadjust pre{\hypertarget{ref-sunstein2017economics}{}}%
Sunstein, Cass R, and Lucia A Reisch. 2017. \emph{The Economics of
Nudge}. Routledge.

\leavevmode\vadjust pre{\hypertarget{ref-Suriyakumar2021}{}}%
Suriyakumar, Vinith M., Nicolas Papernot, Anna Goldenberg, and Marzyeh
Ghassemi. 2021. {``Chasing Your Long Tails.''} In \emph{Proceedings of
the 2021 {ACM} Conference on Fairness, Accountability, and
Transparency}. {ACM}. \url{https://doi.org/10.1145/3442188.3445934}.

\leavevmode\vadjust pre{\hypertarget{ref-taddy2019}{}}%
Taddy, Matt. 2019. \emph{Business Data Science}. McGraw Hill.

\leavevmode\vadjust pre{\hypertarget{ref-johnsontheeconomist}{}}%
The Economist. 2013. \emph{Johnson: Those Six Little Rules: George
Orwell on Writing}.
\url{https://www.economist.com/prospero/2013/07/29/johnson-those-six-little-rules}.

\leavevmode\vadjust pre{\hypertarget{ref-theeconomistonspotify}{}}%
---------. 2022. \emph{What Spotify Data Show about the Decline of
English}.
\url{https://www.economist.com/interactives/graphic-detail/2022/01/29/what-spotify-data-show-about-the-decline-of-english}.

\leavevmode\vadjust pre{\hypertarget{ref-Thieme2018}{}}%
Thieme, Nick. 2018. {``R Generation.''} \emph{Significance} 15 (4):
14--19. \url{https://doi.org/10.1111/j.1740-9713.2018.01169.x}.

\leavevmode\vadjust pre{\hypertarget{ref-thistlethwaite1960regression}{}}%
Thistlethwaite, Donald L, and Donald T Campbell. 1960.
{``Regression-Discontinuity Analysis: An Alternative to the Ex Post
Facto Experiment.''} \emph{Journal of Educational Psychology} 51 (6):
309.

\leavevmode\vadjust pre{\hypertarget{ref-spotifyr}{}}%
Thompson, Charlie, Josiah Parry, Donal Phipps, and Tom Wolff. 2020.
\emph{Spotifyr: R Wrapper for the 'Spotify' Web API}.
\url{http://github.com/charlie86/spotifyr}.

\leavevmode\vadjust pre{\hypertarget{ref-hannahfryft}{}}%
Thornhill, John. 2021. {``Lunch with the FT: Mathematician Hannah
Fry.''} \emph{Financial Times}.

\leavevmode\vadjust pre{\hypertarget{ref-citevisdat}{}}%
Tierney, Nicholas. 2017. {``Visdat: Visualising Whole Data Frames.''}
\emph{JOSS} 2 (16): 355. \url{https://doi.org/10.21105/joss.00355}.

\leavevmode\vadjust pre{\hypertarget{ref-quartoforscientists}{}}%
---------. 2022. \emph{Quarto for Scientists}.
\url{https://qmd4sci.njtierney.com}.

\leavevmode\vadjust pre{\hypertarget{ref-tierney2020realistic}{}}%
Tierney, Nicholas J, and Karthik Ram. 2020. {``A Realistic Guide to
Making Data Available Alongside Code to Improve Reproducibility.''}
\url{https://arxiv.org/abs/2002.11626}.

\leavevmode\vadjust pre{\hypertarget{ref-canlang}{}}%
Timbers, Tiffany. 2020. \emph{Canlang: Canadian Census Language Data}.
\url{https://ttimbers.github.io/canlang/}.

\leavevmode\vadjust pre{\hypertarget{ref-timbersandfriends}{}}%
Timbers, Tiffany-Anne, Trevor Campbell, and Melissa Lee. 2022.
\emph{Data Science: A First Introduction}. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-tolley2021gender}{}}%
Tolley, Erin, and Mireille Paquet. 2021. {``Gender, Municipal Party
Politics, and Montreal's First Woman Mayor.''} \emph{Canadian Journal of
Urban Research} 30 (1): 40--52.

\leavevmode\vadjust pre{\hypertarget{ref-Trisovic2022}{}}%
Trisovic, Ana, Matthew K. Lau, Thomas Pasquier, and Mercè Crosas. 2022.
{``A Large-Scale Study on Research Code Quality and Execution.''}
\emph{Scientific Data} 9 (1).
\url{https://doi.org/10.1038/s41597-022-01143-6}.

\leavevmode\vadjust pre{\hypertarget{ref-tukey1962future}{}}%
Tukey, John W. 1962. {``The Future of Data Analysis.''} \emph{The Annals
of Mathematical Statistics} 33 (1): 1--67.

\leavevmode\vadjust pre{\hypertarget{ref-unigme}{}}%
UN IGME. 2021. {``Levels and Trends in Child Mortality, 2021.''}
\url{https://childmortality.org/wp-content/uploads/2021/12/UNICEF-2021-Child-Mortality-Report.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-van2005data}{}}%
Van den Broeck, Jan, Solveig Argeseanu Cunningham, Roger Eeckels, and
Kobus Herbst. 2005. {``Data Cleaning: Detecting, Diagnosing, and Editing
Data Abnormalities.''} \emph{PLoS Medicine} 2 (10): e267.

\leavevmode\vadjust pre{\hypertarget{ref-vanderplas2020testing}{}}%
Vanderplas, Susan, Dianne Cook, and Heike Hofmann. 2020. {``Testing
Statistical Charts: What Makes a Good Graph?''} \emph{Annual Review of
Statistics and Its Application} 7: 61--88.

\leavevmode\vadjust pre{\hypertarget{ref-varin2011overview}{}}%
Varin, Cristiano, Nancy Reid, and David Firth. 2011. {``An Overview of
Composite Likelihood Methods.''} \emph{Statistica Sinica}, 5--42.

\leavevmode\vadjust pre{\hypertarget{ref-citecancensus}{}}%
von Bergmann, Jens, Dmitry Shkolnik, and Aaron Jacobs. 2021.
\emph{Cancensus: R Package to Access, Retrieve, and Work with Canadian
Census Data and Geography}.
\url{https://mountainmath.github.io/cancensus/}.

\leavevmode\vadjust pre{\hypertarget{ref-wang2015forecasting}{}}%
Wang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman. 2015.
{``Forecasting Elections with Non-Representative Polls.''}
\emph{International Journal of Forecasting} 31 (3): 980--91.

\leavevmode\vadjust pre{\hypertarget{ref-wang2018deep}{}}%
Wang, Yilun, and Michal Kosinski. 2018. {``Deep Neural Networks Are More
Accurate Than Humans at Detecting Sexual Orientation from Facial
Images.''} \emph{Journal of Personality and Social Psychology} 114 (2):
246.

\leavevmode\vadjust pre{\hypertarget{ref-wardrop1995simpson}{}}%
Wardrop, Robert L. 1995. {``Simpson's Paradox and the Hot Hand in
Basketball.''} \emph{The American Statistician} 49 (1): 24--28.

\leavevmode\vadjust pre{\hypertarget{ref-ware1989}{}}%
Ware, James. 1989. {``Investigating Therapies of Potentially Great
Benefit: ECMO.''} \emph{Statistical Science}, no. 4: 298--306.

\leavevmode\vadjust pre{\hypertarget{ref-ware1989investigating}{}}%
Ware, James H. 1989. {``Investigating Therapies of Potentially Great
Benefit: ECMO.''} \emph{Statistical Science} 4 (4): 298--306.

\leavevmode\vadjust pre{\hypertarget{ref-wasserman}{}}%
Wasserman, Larry. 2005. \emph{All of Statistics}. Springer.

\leavevmode\vadjust pre{\hypertarget{ref-wei1978randomized}{}}%
Wei, LJ, and S Durham. 1978. {``The Randomized Play-the-Winner Rule in
Medical Trials.''} \emph{Journal of the American Statistical
Association} 73 (364): 840--43.

\leavevmode\vadjust pre{\hypertarget{ref-weissgerber2015beyond}{}}%
Weissgerber, Tracey L, Natasa M Milic, Stacey J Winham, and Vesna D
Garovic. 2015. {``Beyond Bar and Line Graphs: Time for a New Data
Presentation Paradigm.''} \emph{PLoS Biology} 13 (4): e1002128.

\leavevmode\vadjust pre{\hypertarget{ref-whitby}{}}%
Whitby, Andrew. 2020. \emph{{The Sum of the People}}. Basic Books.

\leavevmode\vadjust pre{\hypertarget{ref-whatasurvey}{}}%
Whitelaw, James. 1905. \emph{An Essay on the Population of Dublin. Being
the Result of an Actual Survey Taken in 1798, with Great Care and
Precision, and Arranged in a Manner Entirely New}. Graisberry; Campbell.

\leavevmode\vadjust pre{\hypertarget{ref-matmortality}{}}%
WHO. 2019. {``Trends in Maternal Mortality 2000 to 2017: Estimates by
WHO, UNICEF, UNFPA, World Bank Group and the United Nations Population
Division.''}
\url{https://www.who.int/reproductivehealth/publications/maternal-mortality-2000-2017/en/}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham2010layered}{}}%
Wickham, Hadley. 2010. {``A Layered Grammar of Graphics.''}
\emph{Journal of Computational and Graphical Statistics} 19 (1): 3--28.

\leavevmode\vadjust pre{\hypertarget{ref-testthat}{}}%
---------. 2011. {``Testthat: Get Started with Testing.''} \emph{The R
Journal} 3: 5--10.
\url{https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham2014tidy}{}}%
---------. 2014. {``Tidy Data.''} \emph{Journal of Statistical Software}
59 (1): 1--23.

\leavevmode\vadjust pre{\hypertarget{ref-citeggplot}{}}%
---------. 2016. \emph{Ggplot2: Elegant Graphics for Data Analysis}.
Springer-Verlag New York. \url{https://ggplot2.tidyverse.org}.

\leavevmode\vadjust pre{\hypertarget{ref-Wickham2017}{}}%
---------. 2017. \emph{Tidyverse: Easily Install and Load the
'Tidyverse'}. \url{https://CRAN.R-project.org/package=tidyverse}.

\leavevmode\vadjust pre{\hypertarget{ref-advancedr}{}}%
---------. 2019a. \emph{Advanced r}. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-citebabynames}{}}%
---------. 2019b. \emph{Babynames: US Baby Names 1880-2017}.
\url{https://CRAN.R-project.org/package=babynames}.

\leavevmode\vadjust pre{\hypertarget{ref-citehttr}{}}%
---------. 2019c. \emph{Httr: Tools for Working with URLs and HTTP}.
\url{https://CRAN.R-project.org/package=httr}.

\leavevmode\vadjust pre{\hypertarget{ref-citervest}{}}%
---------. 2019d. \emph{Rvest: Easily Harvest (Scrape) Web Pages}.
\url{https://CRAN.R-project.org/package=rvest}.

\leavevmode\vadjust pre{\hypertarget{ref-citestringr}{}}%
---------. 2019e. \emph{Stringr: Simple, Consistent Wrappers for Common
String Operations}. \url{https://CRAN.R-project.org/package=stringr}.

\leavevmode\vadjust pre{\hypertarget{ref-citeforcats}{}}%
---------. 2020a. \emph{Forcats: Tools for Working with Categorical
Variables (Factors)}. \url{https://CRAN.R-project.org/package=forcats}.

\leavevmode\vadjust pre{\hypertarget{ref-tidyversewebsite}{}}%
---------. 2020b. \emph{Tidyverse}. \url{https://www.tidyverse.org/}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham2021mastering}{}}%
---------. 2021a. \emph{Mastering Shiny}.

\leavevmode\vadjust pre{\hypertarget{ref-tidyversestyleguide}{}}%
---------. 2021b. \emph{The Tidyverse Style Guide}.
\url{https://style.tidyverse.org/index.html}.

\leavevmode\vadjust pre{\hypertarget{ref-citetidyr}{}}%
---------. 2021c. \emph{Tidyr: Tidy Messy Data}.
\url{https://CRAN.R-project.org/package=tidyr}.

\leavevmode\vadjust pre{\hypertarget{ref-citetidyverse}{}}%
Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy
D'Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019a.
{``Welcome to the {tidyverse}.''} \emph{Journal of Open Source Software}
4 (43): 1686. \url{https://doi.org/10.21105/joss.01686}.

\leavevmode\vadjust pre{\hypertarget{ref-tidyverse}{}}%
---------, et al. 2019b. {``Welcome to the {tidyverse}.''} \emph{Journal
of Open Source Software} 4 (43): 1686.
\url{https://doi.org/10.21105/joss.01686}.

\leavevmode\vadjust pre{\hypertarget{ref-citeusethis}{}}%
Wickham, Hadley, and Jennifer Bryan. 2020. \emph{Usethis: Automate
Package and Project Setup}.
\url{https://CRAN.R-project.org/package=usethis}.

\leavevmode\vadjust pre{\hypertarget{ref-citedplyr}{}}%
Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2020.
\emph{Dplyr: A Grammar of Data Manipulation}.
\url{https://CRAN.R-project.org/package=dplyr}.

\leavevmode\vadjust pre{\hypertarget{ref-r4ds}{}}%
Wickham, Hadley, and Garrett Grolemund. 2017. \emph{R for Data Science}.
\url{https://r4ds.had.co.nz/}.

\leavevmode\vadjust pre{\hypertarget{ref-citereadr}{}}%
Wickham, Hadley, Jim Hester, and Jennifer Bryan. 2021. \emph{Readr: Read
Rectangular Text Data}. \url{https://CRAN.R-project.org/package=readr}.

\leavevmode\vadjust pre{\hypertarget{ref-citeDevtools}{}}%
Wickham, Hadley, Jim Hester, and Winston Chang. 2020. \emph{Devtools:
Tools to Make Developing r Packages Easier}.
\url{https://CRAN.R-project.org/package=devtools}.

\leavevmode\vadjust pre{\hypertarget{ref-xml2}{}}%
Wickham, Hadley, Jim Hester, and Jeroen Ooms. 2021. \emph{Xml2: Parse
XML}. \url{https://CRAN.R-project.org/package=xml2}.

\leavevmode\vadjust pre{\hypertarget{ref-citehaven}{}}%
Wickham, Hadley, and Evan Miller. 2020. \emph{Haven: Import and Export
'SPSS', 'Stata' and 'SAS' Files}.
\url{https://CRAN.R-project.org/package=haven}.

\leavevmode\vadjust pre{\hypertarget{ref-scales}{}}%
Wickham, Hadley, and Dana Seidel. 2020. \emph{Scales: Scale Functions
for Visualization}. \url{https://CRAN.R-project.org/package=scales}.

\leavevmode\vadjust pre{\hypertarget{ref-wiessner2014embers}{}}%
Wiessner, Polly W. 2014. {``Embers of Society: Firelight Talk Among the
Ju/'Hoansi Bushmen.''} \emph{Proceedings of the National Academy of
Sciences} 111 (39): 14027--35.

\leavevmode\vadjust pre{\hypertarget{ref-wilde}{}}%
Wilde, Oscar. 1891. \emph{The Picture of Dorian Gray}.

\leavevmode\vadjust pre{\hypertarget{ref-grammarofgraphics}{}}%
Wilkinson, Leland. 2005. \emph{The Grammar of Graphics}. 2nd ed.
Springer.

\leavevmode\vadjust pre{\hypertarget{ref-wilkinson2016fair}{}}%
Wilkinson, Mark D, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle
Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. {``The
FAIR Guiding Principles for Scientific Data Management and
Stewardship.''} \emph{Scientific Data} 3 (1): 1--9.

\leavevmode\vadjust pre{\hypertarget{ref-buildingsoftwaretogether}{}}%
Wilson, Greg. 2021. \emph{Building Software Together}. CRC Books.

\leavevmode\vadjust pre{\hypertarget{ref-wilsongoodenough}{}}%
Wilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex
Nederbragt, and Tracy K. Teal. 2017. {``Good Enough Practices in
Scientific Computing.''} \emph{PLOS Computational Biology} 13 (6):
1--20. \url{https://doi.org/10.1371/journal.pcbi.1005510}.

\leavevmode\vadjust pre{\hypertarget{ref-facebookapitrump}{}}%
Wong, Julia Carrie. 2020. \emph{One Year Inside Trump's Monumental
Facebook Campaign}.

\leavevmode\vadjust pre{\hypertarget{ref-wright1928tariff}{}}%
Wright, Philip G. 1928. \emph{The Tariff on Animal and Vegetable Oils}.
Macmillan Company.

\leavevmode\vadjust pre{\hypertarget{ref-wuandthompson}{}}%
Wu, Changbao, and Mary E Thompson. 2020. \emph{Sampling Theory and
Practice}. Springer.

\leavevmode\vadjust pre{\hypertarget{ref-tinytex}{}}%
Xie, Yihui. 2019. {``TinyTeX: A Lightweight, Cross-Platform, and
Easy-to-Maintain LaTeX Distribution Based on TeX Live.''}
\emph{TUGboat}, no. 1: 30--32.
\url{https://tug.org/TUGboat/Contents/contents40-1.html}.

\leavevmode\vadjust pre{\hypertarget{ref-citeknitr}{}}%
---------. 2021. \emph{Knitr: A General-Purpose Package for Dynamic
Report Generation in r}. \url{https://yihui.org/knitr/}.

\leavevmode\vadjust pre{\hypertarget{ref-citeblogdown}{}}%
Xie, Yihui, Christophe Dervieux, and Alison Presmanes Hill. 2021.
\emph{Blogdown: Create Blogs and Websites with r Markdown}.
\url{https://github.com/rstudio/blogdown}.

\leavevmode\vadjust pre{\hypertarget{ref-blogdownbook}{}}%
Xie, Yihui, Amber Thomas, and Alison Presmanes Hill. 2021.
\emph{Blogdown: Creating Websites with r Markdown}.

\leavevmode\vadjust pre{\hypertarget{ref-citekableextra}{}}%
Zhu, Hao. 2020. \emph{kableExtra: Construct Complex Table with 'Kable'
and Pipe Syntax}. \url{https://CRAN.R-project.org/package=kableExtra}.

\leavevmode\vadjust pre{\hypertarget{ref-zinsser}{}}%
Zinsser, William. 1976. \emph{On Writing Well}.

\leavevmode\vadjust pre{\hypertarget{ref-zook2017ten}{}}%
Zook, Matthew, Solon Barocas, Danah Boyd, Kate Crawford, Emily Keller,
Seeta Peña Gangadharan, Alyssa Goodman, et al. 2017. {``Ten Simple Rules
for Responsible Big Data Research.''} \emph{PLoS Computational Biology}.
Public Library of Science San Francisco, CA USA.

\end{CSLReferences}

\end{document}
