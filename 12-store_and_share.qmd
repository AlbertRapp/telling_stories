---
engine: knitr
---

# Store and share {#sec-store-and-share}

**Required material**

- Read *Transparent and reproducible social science research*, Chapter 10 'Data Sharing', [@christensen2019transparent]
- Read *Datasheets for datasets*, [@gebru2021datasheets]
- Read *Data and its (dis)contents: A survey of dataset development and use in machine learning research*, [@Paullada2021]
- Watch *Code smells and feels*, [@codesmells]

**Key concepts and skills**

- The FAIR principles provide the foundation from which we consider data sharing and storage. The most important stage is the first one, and that is to get the data off our local computer, and to then make it accessible by others. After that, we build documentation, and datasheets, to make it easier for others to understand and use it. Finally, we ideally enable access without our involvement.
- At the same time as wanting to share our datasets are widely as possible, we must respect those whose data are contained in them. For instance, protect, to a reasonable extent, personally identifying information through selective disclosure, hashing, data simulation, and differential privacy.
- Finally, as our data get larger, approaches that were viable when they were small start to break down. We need to consider efficiency with regard to both our code and our data.

**Key packages and functions**

- `arrow` [@arrow]
  - `read_parquet()`
  - `write_parquet()`
- `openssl` [@openssl]
  - `md5()`
  - `sha512()`
- `tictoc` [@Izrailev2014]
  - `tic()`
  - `toc()`

## Introduction

After we have put together a dataset it is important to store it appropriately and enable easy retrieval both for ourselves and others. @Wicherts2011 found that a reluctance to share data was associated with weaker evidence and more potential errors in research papers. While it is certainly possible to be especially concerned about this, and entire careers and disciplines are based on the storage and retrieval of data, to a certain extent, the baseline is not onerous. If we can get our dataset off our own computer, then we are much of the way there. Further confirming that someone else can retrieve it and use it, ideally without our involvement, puts us much further than most. Just achieving that for our data, models, and code meets the 'bronze' standard of @heil2021reproducibility.

The FAIR principles are useful when we come to think more formally about data sharing and management. This requires that datasets are [@wilkinson2016fair]:

1. Findable. There is one, unchanging, identifier for the dataset and the dataset has high-quality descriptions and explanations.
2. Accessible. Standardized approaches can be used to retrieve the data, and these are open and free, possibly with authentication, and that the metadata persists even if the dataset is removed.
3. Interoperable. The dataset and its metadata use a broadly applicable language, and vocabulary.
4. Reusable. There are extensive descriptions of the dataset and the usage conditions are made clear along with provenance.

Just because a dataset is FAIR, it is not necessarily an unbiased representation of the world. Further, it is not necessarily fair in the everyday way that word is used i.e. impartial and honest [@deLima2022]. FAIR reflects whether a dataset is appropriately available, not whether it is appropriate.

One reason for the rise of data science is that humans are at the heart of it. And often the data that we are interested in directly concern humans. This means there is a tension between sharing a dataset to facilitate reproducibility and maintaining privacy. Medicine developed approaches to this over a long time. And out of that we have seen Health Insurance Portability and Accountability Act (HIPAA) in the US, and the related, and more recent, General Data Protection Regulation (GDPR) in Europe. Our concerns in data science tend to be about personally identifying information (PII). We have a variety of ways to protect especially private information, such as emails, including hashing. And sometimes we simulate data and distribute that instead of sharing the actual dataset. More recently, differential privacy approaches are being implemented. The fundamental problem of data privacy is that increased privacy reduces the usefulness of a dataset. The appropriate choice is nuanced and depends on costs and benefits, and we should be especially concerned about differentiated effects on population minorities.

As datasets and code bases get larger it becomes more difficult to deal with them, especially if we want them to be shared. We come to concerns around efficiency, not for its own sake, but to enable us to tell stories that could not otherwise be told. 

In particular, we are concerned with more efficient code. This means making it easier for others to read, and contribute to, it by linting and reviewing. It also means making it faster to run by actively timing how long aspects take to run, evaluating aspects in parallel, and refactoring. We are also concerned with more efficient data. This might mean moving beyond CSV files to formats with other properties, or even using databases such as SQL.

<!-- In this chapter we will consider how we plan and organize our datasets to meet essential requirements. To a large extent we put these in place to make our own life easier when we come back to use our dataset later. We then go through putting our dataset on GitHub, building R packages for data, and finally depositing it in various archives. Then we consider documentation, and in particular focus on datasheets. -->

## Plan

The storage and retrieval of information is especially connected with libraries. These have existed since antiquity and have well-established protocols for deciding what information to store and what to discard, as well as information retrieval. One of the defining aspects of libraries is deliberate curation and organization. The use of a cataloging system ensures that books on similar topics are located close to each other, and there are typically also deliberate plans for ensuring the collection is up-to-date. This ensures that information storage and retrieval is appropriate and efficient.

Vannevar Bush, the twentieth-century engineer, defined a 'memex' in 1945 as a device used to store books, records, and communications, in a way that supplements memory [@vannevarbush]. And the key is the indexing, or linking together of items. We can see this concept echoed in the proposal by Tim Berners-Lee for hypertext [@berners1989information], which led to the World Wide Web. This is the way that resources are identified. They are then transported over the internet, using Hypertext Transfer Protocol (HTTP).

At its most fundamental, the internet is about storing and retrieving data. It is based on making various files on our computer available to others. The internet is famously brittle, but this is due to economies of scale which encourage monopolies, not the fundamental protocols, which are robust. 

When we consider the storage and retrieval of our datasets we want to especially contemplate, for how long it is important that they are stored and for whom [@michener2015ten]. For instance, if we want some dataset to be available for a decade and widely available then it becomes important to store it in open and persistent formats, such as CSVs [@hart2016ten]. But if we are just using a dataset as part of an intermediate step, and we have the raw data and the scripts to create it, then it might be fine to not worry too much about such considerations.

Storing raw data is important and there are many cases where raw data have revealed or hinted at fraud [@simonsohn2013just]. Shared data also enhances the credibility of our work, by enabling others to verify it, and can lead to the generation of new knowledge as others use it to answer different questions [@christensen2019transparent]. @christensen2019study suggests that research that shares its data may be more highly cited.

## Share data

### GitHub

The easiest place to get started with storing a dataset is GitHub because that is already built into our workflow. For instance, if we push to a public repository, then our dataset becomes available. One benefit of this is that if we have set-up our workspace appropriately, then we likely store our raw data, and the tidy data, as well as the scripts that are needed to transform one to the other. We are most of the way to the 'bronze' standard of @heil2021reproducibility without changing anything.

As an example of how we have stored some data, we can access 'raw_data.csv' from the 'starter_folder', (which we recommend using for the papers in Appendix @sec-papers). To get the file that we pass to `read_csv()` we navigate to the file in GitHub, and then click 'Raw' (@fig-githubraw).

![Getting the necessary link to be able to read a CSV from a GitHub repository](figures/github_raw_data.png){#fig-githubraw width=70% fig-align="center"}

We can then add that URL as an argument to `read_csv()`.

```{r}
#| message: false
#| warning: false

library(tidyverse)

starter_data <- 
  read_csv(
    "https://raw.githubusercontent.com/RohanAlexander/starter_folder/main/inputs/data/raw_data.csv"
    )

starter_data
```

While we can store and retrieve the dataset easily in this way, it lacks much explanation, a formal dictionary, and aspects such as a license that would bring our dataset closer to aligning with the FAIR principles. Another is that the maximum file size on GitHub is 100MB.

### R Packages for data

To this point we have largely used R packages for their code, although we have seen a few that were focused on sharing data, for instance, `troopdata` [@troopdata] and `babynames` [@citebabynames] in @sec-interactive-communication. We can build an R package for our dataset and then add it to GitHub. This will make it easy to store and retrieve because we can obtain the dataset by loading the package. In contrast to the CSV and GitHub approach, it also brings documentation along with it. This will be the first R package that we build. In @sec-its-just-a-linear-model, we return to R packages and use them to deploy models.

To get started, create a new package ('File' -> 'New project' -> 'New Directory' -> 'R Package'). Give the package a name, such as 'favcolordata' and select 'Open in new session'. Create a new folder called 'data'. We will simulate a dataset that we will include.

```{r}
#| include: true
#| message: false
#| warning: false
#| eval: false

library(tidyverse)

set.seed(853)

color_data <-
    tibble(
        name =
            c("Edward", "Helen", "Hugo", "Ian", "Monica",
              "Myles", "Patricia", "Roger", "Rohan", "Ruth"
            ),
        fav_color =
            sample(
                x = c("Black", "White", "Rainbow"),
                size = 10,
                replace = TRUE
            )
    )
```

To this point we have largely been trying to use CSV files for our datasets. To include our data in this R package, we will save our dataset in a different format, '.rda', using `save()`.

```{r}
#| eval: false
#| include: true

save(color_data, file = "data/color_data.rda")
```

Then create an R file 'data.R' in the 'R' folder. This R file will only contain documentation using `roxygen2` comments, which start with `#'`, and we follow the documentation for `troopdata` closely.

```{r}
#| eval: false
#| include: true

#' Favorite color of various people data
#'
#' @description \code{favcolordata} returns a dataframe 
#' of the favorite color of various people.
#' 
#' @return Returns a dataframe of the favorite color 
#' of various people.
#' 
#' @docType data
#'
#' @usage data(color_data)
#'
#' @format An dataframe of individual-level observations 
#' with the following variables: 
#'
#' \describe{
#' \item{\code{name}}{A character vector of individual names.}
#' \item{\code{fav_color}}{A character vector of one 
#' of: black, white, rainbow.}
#' }
#'
#' @keywords datasets
#'
#' @source \url{https://tellingstorieswithdata.com/12-store_and_share.html}
#'
"color_data"

```

Finally, we add a README which provides a summary of all of this for someone coming to the project for the first time.

We can now go to the 'Build' tab and then 'Install and Restart'. After this happens, the package 'favcolordata', is loaded and the data can be accessed locally using 'color_data'. If we were to push this to GitHub, then anyone would be able to install the package using `devtools` [@citeDevtools] and then use our dataset.

```{r}
#| eval: false
#| include: true

devtools::install_github("RohanAlexander/favcolordata")

library(favcolordata)

color_data
```

This has addressed many of the issues that we faced earlier. For instance, we have included a README and a data dictionary of sorts in terms of the descriptions that we added. But if we were to try to put this package onto CRAN, then we might face some issues. For instance, the maximum size of a package is 5MB and we would quickly come up against that. We have also largely forced users to use R, and while there are considerable benefits of that, we may like to be more language agnostic [@tierney2020realistic].

The definitive guide to including data in R packages is @rpackages [Chapter 8].

### Depositing data

While it is possible that a dataset will be cited if it is available through GitHub or an R package, this becomes more likely if the dataset is deposited somewhere. There are several reasons for this, but one is that it seems a bit more formal, and another is that it is associated with a DOI. [Zenodo](https://zenodo.org) and [Open Science Framework](https://osf.io) (OSF) are two that are commonly used. For instance, @chris_carleton_2021_4550688 use Zenodo to share the dataset and analysis supporting @carleton2021reassessment and @geuenich_michael_2021_5156049 use Zenodo to share the dataset that underpins @geuenich2021automated. Similarly, @ryansnewpaper use OSF to share code and data.

Another option is to use a dataverse, such as the [Harvard Dataverse](https://dataverse.harvard.edu). This is a common requirement for journal publications. One nice aspect of this is that we can use `dataverse` [@dataverse] to retrieve the dataset as part of a reproducible workflow.

In general, these options are free and provide a DOI that can be useful for citation purposes. The use of data deposits such as these is a critical way to offload responsibility, and prevent the dataset from being lost. It also establishes a single point of truth, which should act to reduce errors [@byrd2020responsible]. Finally, it makes access to the dataset independent of the original researchers, and results in persistent metadata.

## Documentation

Datasheets [@gebru2021datasheets] are an increasingly critical aspect of data science. Datasheets are basically nutrition labels for datasets. The process of creating them enables us to think more carefully about what we will feed our model. More importantly, they enable others to better understand what we fed our model. One important task is going back and putting together datasheets for datasets that are widely used. For instance, researchers went back and wrote a datasheet for one of the most popular datasets in computer science, and they found that around 30 per cent of the data were duplicated [@bandy2021addressing].


:::{.callout-note}
## Shoulders of giants

Timnit Gebru is the founder of the Distributed Artificial Intelligence Research Institute (DAIR). After taking a PhD in Computer Science from Stanford University, she joined Microsoft and then Google. She was fired from Google in 2020 for writing @Bender2021, which discussed the dangers of language models being too large. She has made many other substantial contributions to fairness and accountability, especially @buolamwini2018gender, which demonstrated racial bias in facial analysis algorithms.
:::


Instead of telling us how unhealthy various foods are, a datasheet tells us things like:

- Who put the dataset together?
- Who paid for the dataset to be created?
- How complete is the dataset?
- Which variables are present, and, equally, not present, for particular observations?

Sometimes we have done a lot of work to create a dataset. In that case, we may like to publish and share it on its own, for instance, @biderman2022datasheet and @bandy2021addressing. But typically a datasheet might live in an appendix to the paper, for instance @zhang2022opt, or be included in a file adjacent to the dataset.

We will put together a datasheet for the dataset that underpins @citeaustralianpoliticians and include it in Appendix @sec-datasheet. The text of the questions directly comes from @gebru2021datasheets. When we create datasheets for a dataset, especially a dataset that we did not put together ourselves, it is possible that the answer to some questions will simply be "Unknown", but we should do what we can to minimize that. Finally, the datasheet template created by @gebru2021datasheets is not the final word. For instance, it is possible to improve on them, and add additional detail sometimes. For instance, @Miceli2022 argue for the addition of questions to do with power relations.

## Personally identifying information

Personally identifying information (PII) is that which enables us to link an observation in our dataset with an actual person. For instance, email addresses are often PII, as are names and addresses. While some variable may not be PII for almost everyone in the dataset, it could be PII for some. For instance, if we have a survey that is representative of the age distribution of the population, then there is not likely to be many respondents aged over 100, and so the variable age may then become PII. The same scenario happens with income, wealth, and many other variables. One response to this is for data to be censored, which was discussed in @sec-farm-data. For instance, we may record age between 0 and 90, and then group everyone over that into '91+'. Another is to construct age-groups. Notice that with both these solutions we have had to trade-off privacy for usefulness. A variable may be PII, not by itself, but when combined with another variable.

Our primary concern should be with ensuring that the privacy of our dataset is appropriate, given the expectations of the reasonable person. This requires weighing costs and benefits. In national security settings, there has been considerable concern about the over-classification of documents [@overclassification]. This may result in unrealized benefits. The test of the need to protect a dataset needs to be made by the reasonable person weighing up costs and benefits. It is easy, but wrong, to say that no data should be released unless it is perfectly anonymized, because the fundamental problem of data privacy would mean such data would have little utility. That approach, possibly motivated by the precautionary principle, would be too conservative and cause considerable potential loss in terms of unrealized benefits.

From @christensen2019transparent [p. 180] a dataset is confidential if the researchers know who is associated with each observation, but the public version of the dataset removes this association. A dataset is anonymous if even the researchers do not know. 

Randomized response [@randomizedresponse] is a clever way that anonymity can be ensured without much overhead. Each respondent, metaphorically, flips a coin before they answer a question and does not show the researcher the outcome of the coin flip. The respondent is instructed to respond truthfully to the question if the coin lands on heads, but to always give some particular (still plausible) response if tails. The results of the other options can then be re-weighted to provide an estimate, without a researcher ever knowing the truth about any particular respondent. This is especially used in association with snowball sampling, discussed in @sec-farm-data. 

One issue with randomized response is that the resulting dataset can be only used to answer specific questions. This requires careful planning and the dataset will be of less general value. Expectations of what should be public, confidential, or anonymous, differ by context. For instance, in Ontario, Canada, public sector employees who earn more than \$100,000 annually have their salary published on a publicly available website. As many universities in Ontario are public, this includes professors, and we could see that, say, in 2021, the author of this book was paid \$131,186.08. Outside this context, we would normally expect such information would be confidential.

@zook2017ten recommend considering whether data even need to be gathered in the first place. For instance, if a phone number is not absolutely required then it might be better to not ask for it, rather than need to worry about protecting it before data dissemination. 

GDPR and HIPAA are two legal structures that govern data in Europe and the US, respectively. Due to the influence of these regions, they have a significant effect outside those regions also. GDPR concerns data generally, while HIPAA is focused on healthcare. GDPR applies to all personal data, which is defined as:

> ...any information relating to an identified or identifiable natural person ('data subject'); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person;
>
> @gdpr, Article 4, 'Definitions'

HIPAA refers to the privacy of medical records in the US and codifies the idea that the patient should have access to their medical records, and that only the patient should be able to authorize access to their medical records [@annas2003hipaa]. HIPAA only applies to certain entities so it sets a standard, but coverage is inconsistent. For instance, a person's social media posts about their health would generally not subject to it, nor would knowledge of a person's location and how active they are even though based on that information we may be able to get some idea of their health [@Cohen2018]. Such data are hugely valuable [@ibmdataset].

There are a variety of ways of protecting PII, while still sharing some data, that we will now go through. We focus here initially on what we can do when the dataset is considered by itself, which is the main concern. Interestingly, sometimes the combination of several variables, none of which are PII in and of themselves, can be PII. For instance, age is unlikely PII by itself, but age combined with city, education, and a few other variables could be. One concern is that this re-identification could occur by combining datasets and this is a potential role for differential privacy.

### Hashing and salting

A hash is a one-way transformation of data, such that the same input always provides the same output, but given the output, it is not reasonably possible to obtain the input. For instance, a function that doubled its input always gives the same output, for the same input, but is also easy to reverse. 

@knuth [p. 514] relates an interesting etymology for 'hash' by first defining 'to hash' as relating to chop up or make a mess, and then explaining that hashing relates to scrambling the input and using this partial information to define the output. A collision is when different inputs map to the same output, and one feature of a good hashing algorithm is that collisions are reduced. One simple approach is to rely on the modulo operator. For instance, if we were interested in 10 different groupings for the integers 1 through to 10. A better approach would be for the number of groupings to be a prime number, such as 11 or 853.

```{r}
#| message: false
#| warning: false

library(tidyverse)

hashing <- 
  tibble(ppi_data = c(1:10),
         modulo_ten = ppi_data %% 10,
         modulo_eleven = ppi_data %% 11,
         modulo_eightfivethree = ppi_data %% 853)

hashing
```

Rather than worry about things ourselves, we can use various hash functions from `openssl` [@openssl] including `sha512()` and `md5()`.

```{r}
#| message: false
#| warning: false

library(openssl)

openssl_hashing <- 
  tibble(names =
            c("Edward", "Helen", "Hugo", "Ian", "Monica",
              "Myles", "Patricia", "Roger", "Rohan", "Ruth"
            )) |> 
  mutate(md5 = md5(names),
         sha512 = sha512(names)
  )

openssl_hashing
```

We could share either of these and be comfortable that, in general, it would be difficult for someone to use only that information to recover the names of our respondents. That is not to say that it is impossible. If we made a mistake, such as accidentally committing the original dataset to GitHub then they could be recovered. And it is likely that various governments have the ability to reverse the cryptographic hashes used here.

One issue that remains is that anyone can take advantage of the key feature of hashing being that the same input always gets the same output, to test various options for inputs. For instance, they could, themselves try to hash 'Rohan', and then noticing that the hash is the same as the one that we published in our dataset, know that data relates to that particular individual. We could try to keep our hashing approach secret, but that is difficult as there are only a few that are widely used. One approach is to add a salt that we keep secret. This slightly changes the input. For instance, we could add the salt `_is_a_person' to all our names and then hash that, although a large random number might be a better option. Provided the salt is not shared, then it would be difficult for most folks to reverse our approach in that way.

```{r}
#| message: false
#| warning: false

openssl_hashing_with_salt <- 
  tibble(names =
            c("Edward", "Helen", "Hugo", "Ian", "Monica",
              "Myles", "Patricia", "Roger", "Rohan", "Ruth"
            )
         ) |> 
  mutate(names = paste0(names, "_is_a_person")) |> 
  mutate(md5 = md5(names),
         sha512 = sha512(names)
         )

openssl_hashing_with_salt
```


### Data simulation

One common approach to deal with the issue of being unable to share the actual data that underpins an analysis, is to use data simulation. We have used data simulation throughout this book toward the start of the workflow to help us to think more deeply about our dataset before we turn to it. We can use data simulation again at the end, to ensure that others cannot think about our actual dataset. The workflow advocated in this book makes this relatively straight-forward.

The approach is to understand the critical features of the dataset and the appropriate distribution. For instance, if our data were the ages of some population, then we may want to use the Poisson distribution and experiment with different parameters for lambda. Having simulated a dataset, we conduct our analysis using this simulated dataset and ensure that the results are broadly similar to when we use the real data. We can then release the simulated dataset along with our code.

For more nuanced situations, @koenecke2020synthetic recommend using the synthetic data vault [@patki2016synthetic] and then the use of Generative Adversarial Networks, such as implemented by @athey2021using.

### Differential privacy

Differential privacy is a mathematical definition of privacy [@Dwork2013, p. 6]. It is important to be clear that it is not an algorithm. The main issue it solves is that there are a lot of datasets available, and there is always the possibility that some combination of them could be combined to identify respondents even if PII were removed from each of these individual datasets. Rather than needing to anticipate how various datasets could be combined to re-identify individuals and adjust variables to remove this possibility, a dataset that is created using a differentially private approach provides assurances that privacy will be maintained. 

:::{.callout-note}
## Shoulders of giants

Cynthia Dwork is Gordon McKay Professor of Computer Science, Harvard University. After taking a PhD in Computer Science from Cornell University, she was a Post-Doctoral Research Fellow at MIT and then worked at IBM, Compaq, and Microsoft Research where she is a Distinguished Scientist. She joined Harvard in 2017. One of her major contributions is differential privacy [@dwork2006calibrating], which has been widely adapted.
:::

To motivate the definition, consider a dataset of responses and PII that only has one person in it. The release of that dataset, as is, would perfectly identify them. At the other end of the scale, consider a dataset that does not contain a particular person. The release of that dataset could never be linked to them because they are not in it. Differential privacy, then, is about the inclusion or exclusion of particular individuals in a dataset. An algorithm is differentially private if the inclusion or exclusion of any particular person in a dataset has at most some given factor of an effect on the probability of some output [@Oberski2020Differential].

More specifically, from @Asquith2022Assessing, consider @eq-macroidentity:

$$
\frac{\Pr [M(d)\in S]}{\Pr [M(d')\in S]}\leq e^{\epsilon} 
$$ {#eq-macroidentity}

In @eq-macroidentity, from @Asquith2022Assessing, '$M$ is a differentially private algorithm, $d$ and $d'$ are datasets that differ only in terms of one row, $S$ is a set of output from the algorithm' and $\epsilon$ controls the amount of privacy that is provided to respondents. The fundamental problem of data privacy is that we cannot have completely anonymized data that remains useful [@Dwork2013, p. 6]. Instead, we must trade-off utility and privacy.

A dataset is differentially private to different levels of privacy, based on how much it changes when one person's results are included or excluded. This is the key parameter, because at the same time as deciding how much of an individual's information we are prepared to give up, or 'leak', we are deciding how much random noise to add. The choice of this level is a nuanced one and should involve extensive consideration of the costs of undesired disclosures, compared with the benefits of additional research. For public data that will be released under differential privacy, this *ratio decidendi* must be public because of the costs that are being imposed. Indeed, @differentialprivacyatapple argue that even in the case of private companies, such as Apple, users should have a choice about the level of privacy loss. 
A variant of differential privacy has recently been implemented by the US census. This has been shown to not universally protect respondent privacy, and yet it is expected to have a significant effect on redistricting [@kenny2021impact] even though redistricting is considered a high priority use case [@Hawes2020Implementing]. @Suriyakumar2021 found that such model will be disproportionately affected by large demographic groups. The implementation of differential privacy is expected to result in some publicly available data that are unusable in the social sciences [@ruggles2019differential]. And even the ACS, introduced in @sec-farm-data may be replaced with synthetic content. The issue with this approach is that it is fine for what is of interest and known now but cannot account for the unknowable answers to questions that we do not even think to ask. 

The implementation of differential privacy is a costs and benefits issue. Stronger privacy protection fundamentally must mean less information [@clairemckaybowen, p. 39]. The costs of this have been convincingly shown, while the benefits have not. As always in data science, it is important to consider who this benefits. For instance, one can imagine that in the 1970s it would have been rare to evaluate whether the proportion of gay people was accurate. And given recent policy changes one can imagine that soon abortion will be similarly overlooked in much of the US. At issue, with all privacy enhancing approaches, but especially differential privacy given how much it changes the data, is concerns around our ability to answer questions, that we cannot even think to ask today.


<!-- :::{.callout-note} -->
<!-- ## Shoulders of giants -->

<!-- Katherine Wallman was Chief Statistician of the United States from 1992 to 2017. After majoring in sociology as an undergraduate at Wellesley, she began at New England Bell telephone company, and then began working for the US government [@Citro2015]. She served as President of the American Statistical Association in 1992.  -->
<!-- ::: -->



## Efficiency

In general we are, and will continue to be, largely concerned with just getting something done. Not necessarily getting it done in the best or most efficient way. And to a large extent, being worried about getting something done in the best or most efficient way is almost always a waste of time. Until it is not. Eventually inefficient ways of storing data, ugly or slow code, and dogmatic insistence on a particular way of doing things, have an effect. And it is at that point that we need to be open to new approaches to ensure efficiency.

We firstly discuss code efficiency. We start with code linting, which was mentioned in dispatches in  @sec-reproducible-workflows, and enables code review. This does not speed up our code, per se, but we will instead use linters to make out code easier to read, which makes it more efficient when another person comes to it, or we revisit it. We then turn to measurement of run time, and introduce parallel processing, where we allow our computer to run code multiple processes at the same time. Finally we consider code refactoring, which is where we re-write code to make it better, while not changing what it does. After this we turn to ways to be more efficient with data, by using SQL and parquet.

### Code efficiency

By and large, worrying about performance is a waste of time. For the most part we are better off just pushing things into the cloud, letting them run for a reasonable time, and using that time to worry about other aspects of the pipeline. But, eventually this becomes unfeasible. For instance, if something takes more than a day, then it becomes a pain because of the need to completely switch tasks and then return to it. There is rarely a most common area for obvious performance gains. Instead it is important to develop the ability to measure, evaluate, and think.

The first thing we should do is clean up our code. If we then find that the speed at which code is running is becoming a bottle neck then we should shard. Then we should throw more machines at it. But eventually we should go back and refactor the code.

#### Code linting and review

Being fast is valuable but it is mostly about being able to iterate fast, not necessarily having code that runs fast. For almost all applications we will work in teams. @oldlanguages [p. 26] describes how even in 1954 a programmer cost at least as much as a computer, and these days compute is usually much cheaper than a programmer. Hence, it is important to use other people's time efficiently. 

Linting is the process of checking code, mostly for stylistic issues, and re-arranging code to make it easier to read. (There is another aspect of linting, which is programming errors, but here we focus on stylistic issues.) In general, the best efficiency gain that we can make is to make it easier for others to read our code, even if this is just ourselves returning to the code after a break.

We use `lint()` from `lintr` [@lintr] to lint our code. For instance, consider the following R code (saved as linting_example.R).

```{r}
#| include: true
#| message: false
#| warning: false
#| eval: false

SIMULATED_DATA <-
  tibble(
        division = c(1:150,151),
         party = sample(
           x = c('Liberal'),
           size = 151,
           replace = T
         ))
```

```{r}
library(lintr)
lint(filename = "inputs/linting_example.R")
```

The result is that the file 'inputs/linting_example.R' is opened and the issues that `lint()` found are printed in 'Markers' (@fig-linter).

![Linting results](figures/linter.png){#fig-linter width=70% fig-align="center"}

Making the recommended changes would result in code that is more readable.

```{r}
#| include: true
#| message: false
#| warning: false
#| eval: false

simulated_data <-
  tibble(
        division = c(1:150, 151),
         party = sample(
           x = c("Liberal"),
           size = 151,
           replace = TRUE
         ))

```

Code review


#### Parallel processing

Parallel processing enables us to 

#### Code refactoring

To refactor code means to re-write it so that the new code achieves the same outcome as the old code, it is just that the new code does it better. We can use `tic()` and `toc()` from `tictoc` [@Izrailev2014] to time various aspects of our code and find where the largest delays are.

```{r}
#| message: false
#| warning: false

library(tictoc)
tic("First bit of code")
print("Fast code")
toc()

tic("Second bit of code")
Sys.sleep(3)
print("Slow code")
toc()
```

And so we know that there is something slowing down the code; which in this artificial case is `Sys.sleep()` causing a delay of 3 seconds.

When we start to refactor our code, we want to make sure that the re-written code achieves the same outcomes as the original code. This means that it is important to have tests written. We generally want to reduce the size of functions, by breaking them into smaller ones. 

We also want to make sure that our code is satisfying best practice. @Trisovic2022 details some core recommendations based on examining 9,000 R scripts, and a refactor is a great opportunity to ensure that these are met. 

1. Using `renv` [@renv] to document the version of packages that was used.
2. Removing any absolute paths, and ensuring that only relative paths (in relation to the .rproj file) are used.
3. Ensuring there is a clear order of execution. We have recommended using numbers in filenames to achieve this, but more sophisticated approaches, such as `targets` [@targets], could be used instead.
4. Having someone else run everything on their computer.


<!-- Start with an example of bad code, and then how it gets fixed. -->





### Data efficiency

#### SQL

Structured Query Language (SQL) ("see-quell" or "S.Q.L.") is used with relational databases. A relational database is a collection of at least one table, and a table is just some data organized into rows and columns. If there is more than one table in the database, then there should be some column that links them. Using it feels a bit like HTML/CSS in terms of being halfway between markup and programming. One fun aspect is that line spaces mean nothing: include them or do not, but always end a SQL command in a semicolon;

SQL was developed in the 1970s at IBM. While it may be true that the SQL is never as good as the original, SQL is a popular way of working with data. There are many 'flavors' of SQL, including both closed and open options. Here we introduce SQLite, which is open source, and pre-installed on Macs. Windows users can install it from [here](https://www.sqlite.org/download.html).

Advanced SQL users do a lot with it alone, but even just having a working knowledge of SQL increases the number of datasets that we can access. A working knowledge of SQL is especially useful for our efficiency because a large number of datasets are stored on SQL servers, and being able to get data from them ourselves is handy. 

We can use SQL within RStudio, especially drawing on `DBI` [@dbi]. although given the demand for SQL skills, independent of demand for R skills, it may be a better idea, from a career perspective to have a working knowledge of it that is independent of R. We can consider many SQL functions as straightforward variants of the `dplyr` verbs that we have used throughout this book. Indeed `dbplyr` [@dbplyr] would explicitly allow us to use `dplyr` functions and would then automatically translate them into SQL. Having used `mutate()`, `filter()` and `left_join()` in the `tidyverse` means that many of the core SQL commands will be familiar. That means that the main difficulty will be getting on top of the order of operations because SQL can be pedantic.

To get started, open 'Terminal' and then type 'sqlite3' and then press return. This will invoke SQL (@fig-sql). And we can then enter the following commands in the terminal.

![Getting started with SQL in Mac Terminal](figures/sql.png){#fig-sql width=70% fig-align="center"}

We can create an empty table of three columns of type: int, text, int.

```{sql}
#| include: true
#| eval: false

CREATE TABLE table_name (
  column1 INTEGER,
  column2 TEXT,
  column3 INTEGER
);
```

Add a row of data:

```{sql}
#| include: true
#| eval: false

INSERT INTO table_name (column1, column2, column3)
  VALUES (1234, 'Gough Menzies', 32);
```

Add a column:

```{sql}
#| include: true
#| eval: false

ALTER TABLE table_name
  ADD COLUMN column4 TEXT;
```

We can view particular aspects of the data, using SELECT in a similar way to `select()`.

```{sql}
#| include: true
#| eval: false

SELECT column2
  FROM table_name;
```

See two columns:

```{sql}
#| include: true
#| eval: false

SELECT column1, column2
  FROM table_name;
```

See all columns:

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name;
```

See unique rows in a column (similar to `distinct()`):

```{sql}
#| include: true
#| eval: false

SELECT DISTINCT column2
  FROM table_name;
```

See the rows that match a criteria (similar to `which()` or `filter()`):

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    WHERE column3 > 30;
```

All the usual operators are fine with WHERE: =, !=, >, <, >=, <=. Just make sure the condition evaluates to true/false.

See the rows that are pretty close to a criteria:

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    WHERE column2 LIKE  '_ough Menzies';
```

The '_' above is a wildcard that matches to any character e.g. 'Cough Menzies' would be matched here, as would 'Gough Menzies'. LIKE is not case-sensitive: 'Gough Menzies' and 'gough menzies' would both match here.

We can use '%' as an anchor to matches pieces:

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    WHERE column2 LIKE  '%Menzies';
```

That matches anything ending with 'Menzies', so 'Cough Menzies', 'Gough Menzies', 'Sir Menzies' etc, would all be matched here. Use surrounding percentages to match within, e.g. %Menzies% would also match 'Sir Menzies Jr' whereas %Menzies would not.

NULL values (True/False/NULL) are possible, not just True/False, but they need to be explicitly matched for.

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    WHERE column2 IS NOT NULL;
```

There is an underlying ordering build into number, date and text fields that allows you to use BETWEEN on all those, not just numeric. The following looks for text that starts with a letter between A and M (not including M) so would match 'Gough Menzies', but not 'Sir Gough Menzies'.

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    WHERE column2 BETWEEN 'A' AND 'M';
```

If you look for a numeric (as opposed to text) then BETWEEN is inclusive.

We can combine conditions with AND (both must be true to be returned) or OR (at least one must be true).

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    WHERE column2 BETWEEN 'A' AND 'M'
    AND column3 = 32;
```

And we can order the result with ORDER.

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    ORDER BY column3;
```

Ascending is the default, add DESC for alternative:

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    ORDER BY column3 DESC;
```

Restrict the return to a certain number of values by adding LIMIT at the end:

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    ORDER BY column3 DESC
    LIMIT 1;
```

(This doesn't work all the time - only certain SQL databases.)

We can modify data and use logic. For instance we can edit a value.

```{sql}
#| include: true
#| eval: false

UPDATE table_name
  SET column3 = 33
    WHERE column1 = 1234;
```

Implement if/else logic:

```{sql}
#| include: true
#| eval: false

SELECT *,
  CASE
    WHEN column2 = 'Gough Whitlam' THEN 'Labor'
    WHEN column2 = 'Robert Menzies' THEN 'Liberal'
    ELSE 'Who knows'
  END AS 'Party'
  FROM table_name;
```

This returns a column called 'Party' that looks at the name of the person to return a party.

Delete some rows:

```{sql}
#| include: true
#| eval: false

DELETE FROM table_name
  WHERE column3 IS NULL;
```

Add an alias to a column name (this just shows in the output):

```{sql}
#| include: true
#| eval: false

SELECT column2 AS 'Names'
  FROM table_name;
```

We can use COUNT, SUM, MAX, MIN, AVG and ROUND in the place of `summarize()`. COUNT counts the number of rows that are not empty for some column by passing the column name, or for all using *.

```{sql}
#| include: true
#| eval: false

SELECT COUNT(*)
  FROM table_name;
```

Similarly, we can pass a column to SUM, MAX, MIN, and AVG.

```{sql}
#| include: true
#| eval: false

SELECT SUM(column1)
  FROM table_name;
```

ROUND takes a column and an integer to specify how many decimal places.

```{sql}
#| include: true
#| eval: false

SELECT ROUND(column1, 0)
  FROM table_name;
```

SELECT and GROUP BY is similar to group_by in R.

```{sql}
#| include: true
#| eval: false

SELECT column3, COUNT(*)
  FROM table_name
    GROUP BY column3;
```

We can GROUP BY column number instead of name e.g. 1 instead of column3 in the GROUP BY line or 2 instead of COUNT(*) if that was of interest.

HAVING for aggregates, is similar to filter in R or the WHERE for rows from earlier. Use it after GROUP BY and before ORDER BY and LIMIT.

We can combine two tables using JOIN or LEFT JOIN.

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table1_name
  JOIN table2_name
    ON table1_name.colum1 = table2_name.column1;
```

Be careful to specify the matching columns using dot notation. Primary key columns uniquely identify rows and are: 1) never NULL; 2) unique; 3) only one column per table. A primary key can be primary in one table and foreign in another. Unique columns have a different value for every row and there can be many in one table.

UNION is the equivalent of `cbind()` if the tables are already fairly similar.


#### Parquet

While the use of CSVs is great because they are so widely used and have very little overhead, they are also very minimal. This can lead to issues, especially in terms of class. There are various alternatives, including `arrow` [@arrow] which has the advantage of requiring very little change from us. Where we use `write_csv()` and `read_csv()` we can use `write_parquet()` and `read_parquet()`. One advantage is that it should retain the class between R and Python. It should also be faster than CSVs.

```{r}
#| message: false
#| warning: false

library(arrow)
library(tictoc)
library(tidyverse)

number_of_draws <- 10000000

some_data <- 
  tibble(
    first = runif(n = number_of_draws),
    second = sample(x = LETTERS, size = number_of_draws, replace = TRUE)
  )

tic("CSV")
write_csv(x = some_data,
          file = "some_data.csv")
read_csv(file = "some_data.csv")
toc()

tic("parquet")
write_parquet(x = some_data,
              sink = "some_data.parquet")
read_parquet(file = "some_data.parquet")
toc()
```

The size of a Parquet file will also be smaller than a CSV.

```{r}
#| eval: true

file.size("some_data.parquet")
file.size("some_data.csv")
```



```{r}
#| include: false
#| eval: true

file.remove("some_data.parquet")
file.remove("some_data.csv")
```


Reading in only certain columns `col_select = `

Separate into different datasets.

Then reading them all in. (purrr map)


<!-- Reproducibilty -->
<!-- renv -->





## Exercises and tutorial


### Exercises {.unnumbered}

1. *(Plan)* Consider the following scenario: **TBD**. Please sketch out what that dataset could look like and then sketch a graph that you could build to show all observations.
2. *(Simulate)* Please further consider the scenario described and simulate the situation with every variable independent of each other.
3. *(Acquire)* Please describe three possible sources of such a dataset.
4. *(Explore)* Please use `ggplot2` to build the graph that you sketched.
5. *(Communicate)* Please write two paragraphs about what you did.
6. Following @wilkinson2016fair, which of the following are FAIR principles (please select all that apply)?
    a. Findable.
    b. Approachable.
    c. Interoperable.
    d. Reusable.
    e. Integrated.
    f. Fungible.
    g. Reduced.
    h. Accessible.
7. Please create an R package for a simulated dataset, push it to GitHub, and submit the link.
8. Please simulate some data, add it to a GitHub repository and then submit the link.
9. According to @gebru2021datasheets, a datasheet should document a dataset's (please select all that apply):
    a. composition.
    b. recommended uses.
    c. motivation.
    d. collection process.
10. Do you think that a person's name is PII? 
    a.  Yes.
    b. No.
11. Under what circumstances do you think income is PII (please write a paragraph or two)?
12. Using `openssl::md5()` what is the hash of "Rohan" (pick one)?
    a. 243f63354f4c1cc25d50f6269b844369
    b. 09084cc0cda34fd80bfa3cc0ae8fe3dc
    c.  02df8936eee3d4d2568857ed530671b2
    d. 1b3840b0b70d91c17e70014c8537dbba


### Tutorial {.unnumbered}

Please identify a dataset you consider interesting and important, that does not have a datasheet [@gebru2021datasheets]. As a reminder, datasheets accompany datasets and document 'motivation, composition, collection process, recommended uses,' among other aspects. Please put together a datasheet for this dataset. You are welcome to use the template [here](https://github.com/RohanAlexander/starter_folder/blob/main/inputs/data/datasheet_template.Rmd) as a starting point. The datasheet should be completely contained in its own GitHub repository. Please submit a PDF.


### Paper {.unnumbered}

At about this point, Paper Four in Appendix @sec-papers would be appropriate.




<!-- Look into how IQ tests are conducted and what goes into them. To what extent do you think they measure intelligence? Some aspects that you may like to think about in answering that question include: Who decides what is intelligence? How is this updated? What is missing from that definition? To what extent is this generalisable? You should write a page or two. -->








<!-- The purpose of this tutorial is to ensure that it is clear in your mind how thoroughly you should know your dataset. It builds on the 'memory palace' technique used by professional memorisers, as described by @moonwalkingwitheinstein. -->

<!-- Please think about your childhood home, or another building that you know intimately. Imagine yourself standing at the front of it. Describe what it looks like. Then 'walk' into the front and throughout the house, again describing each aspect in as much detail as you can imagine. What are each of the rooms used for and what are their distinguishing features? How does it smell? What does this all evoke in you? Please write a page or two.  -->

<!-- Now think about a dataset that you are interested in. Please do this same exercise, but for the dataset. -->


