---
engine: knitr
---

# Store and share {#sec-store-and-share}

**Required material**

- Read *Transparent and reproducible social science research*, Chapter 10 "Data Sharing", [@christensen2019transparent]
- Read *Datasheets for datasets*, [@gebru2021datasheets]
- Read *Data and its (dis)contents: A survey of dataset development and use in machine learning research*, [@Paullada2021]
- Watch *Code smells and feels*, [@codesmells]

**Key concepts and skills**

- The FAIR principles provide the foundation from which we consider data sharing and storage. These specify that data should be findable, accessible, interoperable, and reusable. 
- The most important step is the first one, and that is to get the data off our local computer, and to then make it accessible by others. After that, we build documentation, and datasheets, to make it easier for others to understand and use it. Finally, we ideally enable access without our involvement.
- At the same time as wanting to share our datasets are widely as possible, we must respect those whose data are contained in them. For instance, protect, to a reasonable extent, personally identifying information through selective disclosure, hashing, data simulation, and differential privacy.
- Finally, as our data get larger, approaches that were viable when they were smaller start to break down. We need to consider efficiency with regard to both our code and our data.

**Key packages and functions**

- Base R
  - `%%` "modulo"
- `arrow` [@arrow]
  - `read_parquet()`
  - `write_parquet()`
- `openssl` [@openssl]
  - `md5()`
  - `sha512()`
- `tictoc` [@Izrailev2014]
  - `tic()`
  - `toc()`

## Introduction

After we have put together a dataset it is important to store it appropriately and enable easy retrieval both for ourselves and others. @Wicherts2011 found that a reluctance to share data was associated with research papers that had weaker evidence and more potential errors. While it is certainly possible to be especially concerned about this, and entire careers and disciplines are based on the storage and retrieval of data, to a certain extent, the baseline is not onerous. If we can get our dataset off our own computer, then we are much of the way there. Further confirming that someone else can retrieve it and use it, ideally without our involvement, puts us much further than most. Just achieving that for our data, models, and code, meets the "bronze" standard of @heil2021reproducibility.

The FAIR principles are useful when we come to think more formally about data sharing and management. This requires that datasets are [@wilkinson2016fair]:

1. Findable. There is one, unchanging, identifier for the dataset and the dataset has high-quality descriptions and explanations.
2. Accessible. Standardized approaches can be used to retrieve the data, and these are open and free, possibly with authentication, and their metadata persist even if the dataset is removed.
3. Interoperable. The dataset and its metadata use a broadly applicable language and vocabulary.
4. Reusable. There are extensive descriptions of the dataset and the usage conditions are made clear along with provenance.

Just because a dataset is FAIR, it is not necessarily an unbiased representation of the world. Further, it is not necessarily fair in the everyday way that word is used i.e. impartial and honest [@deLima2022]. FAIR reflects whether a dataset is appropriately available, not whether it is appropriate.

One reason for the rise of data science is that humans are at the heart of it. And often the data that we are interested in directly concern humans. This means that there can be tension between sharing a dataset to facilitate reproducibility and maintaining privacy. Medicine developed approaches to this over a long time. And out of that we have seen Health Insurance Portability and Accountability Act (HIPAA) in the US, and then the more general General Data Protection Regulation (GDPR) in Europe. Our concerns in data science tend to be about personally identifying information (PII). We have a variety of ways to protect especially private information, such as emails and home addresses. For instance, we can hash those variables. Sometimes we may simulate data and distribute that instead of sharing the actual dataset. More recently, approaches based on differential privacy are being implemented. The fundamental problem of data privacy is that increased privacy reduces the usefulness of a dataset. The trade-off means the appropriate decision is nuanced and depends on costs and benefits, and we should be especially concerned about differentiated effects on population minorities.

Finally, in this chapter we consider efficiency. As datasets and code bases get larger it becomes more difficult to deal with them, especially if we want them to be shared. We come to concerns around efficiency, not for its own sake, but to enable us to tell stories that could not otherwise be told. 

In particular, we are concerned with more efficient code. This means making it easier for others to both read and contribute to it, by linting and adopting code review. It also means making code faster to run by actively timing how long aspects take, evaluating aspects in parallel, and refactoring. We are also concerned with more efficient data. This might mean moving beyond CSV files to formats with other properties, or even using databases such as SQL.


## Plan

The storage and retrieval of information is especially connected with libraries. These have existed since antiquity and have well-established protocols for deciding what information to store and what to discard, as well as information retrieval. One of the defining aspects of libraries is deliberate curation and organization. The use of a cataloging system ensures that books on similar topics are located close to each other, and there are typically also deliberate plans for ensuring the collection is up-to-date. This ensures that information storage and retrieval is appropriate and efficient.

Data science relies heavily on the internet when it comes to storage and retrieval. Vannevar Bush, the twentieth-century engineer, defined a "memex" in 1945 as a device to store books, records, and communications, in a way that supplements memory [@vannevarbush]. The key to it was the indexing, or linking together, of items. We see this concept echoed just four decades later in the proposal by Tim Berners-Lee for hypertext [@berners1989information]. This led to the World Wide Web and defines the way that resources are identified. They are then transported over the internet, using Hypertext Transfer Protocol (HTTP).

At its most fundamental, the internet is about storing and retrieving data. It is based on making various files on a computer available to others. The internet is famously brittle, but this is due to economies of scale which encourage monopolies, not the fundamental protocols, which are robust. 

When we consider the storage and retrieval of our datasets we want to especially contemplate for how long it is important that they are stored and for whom [@michener2015ten]. For instance, if we want some dataset to be available for a decade, and widely available, then it becomes important to store it in open and persistent formats, such as CSV [@hart2016ten]. But if we are just using a dataset as part of an intermediate step, and we have the raw data and the scripts to create it, then it might be fine to not worry too much about such considerations.

Storing raw data is important and there are many cases where raw data have revealed or hinted at fraud [@simonsohn2013just]. Shared data also enhances the credibility of our work, by enabling others to verify it, and can lead to the generation of new knowledge as others use it to answer different questions [@christensen2019transparent]. @christensen2019study suggests that research that shares its data may be more highly cited.

## Share data

### GitHub

The easiest place to get started with storing a dataset is GitHub because that is already built into our workflow. For instance, if we push a dataset to a public repository, then our dataset becomes available. One benefit of this is that if we have set-up our workspace appropriately, then we likely store our raw data, and the tidy data, as well as the scripts that are needed to transform one to the other. We are most of the way to the "bronze" standard of @heil2021reproducibility without changing anything.

As an example of how we have stored some data, we can access "raw_data.csv" from the ["starter_folder"](https://github.com/RohanAlexander/starter_folder), (which we recommend using for the papers in Appendix [-@sec-papers]). We navigate to the file in GitHub ("inputs" -> "data" -> "raw_data.csv"), and then click "Raw" (@fig-githubraw).

![Getting the necessary link to be able to read a CSV from a GitHub repository](figures/github_raw_data.png){#fig-githubraw width=95% fig-align="center"}

We can then add that URL as an argument to `read_csv()`.

```{r}
#| message: false
#| warning: false

library(tidyverse)

data_location <-
  paste0("https://raw.githubusercontent.com/RohanAlexander/",
         "starter_folder/main/inputs/data/raw_data.csv")

starter_data <-
  read_csv(file = data_location)

starter_data
```

While we can store and retrieve a dataset easily in this way, it lacks explanation, a formal dictionary, and aspects such as a license that would bring it closer to aligning with the FAIR principles. Another practical concern is that the maximum file size on GitHub is 100MB. And a final concern, for some, is that GitHub is owned by Microsoft, a for-profit US technology firm.

### R packages for data

To this point we have largely used R packages for their code, although we have seen a few that were focused on sharing data, for instance, `troopdata` [@troopdata] and `babynames` [@citebabynames] in @sec-interactive-communication. We can build an R package for our dataset and then add it to GitHub and potentially eventually CRAN. This will make it easy to store and retrieve because we can obtain the dataset by loading the package. In contrast to the CSV-based approach, it also means a dataset brings its documentation along with it. This will be the first R package that we build. In @sec-its-just-a-linear-model, we return to R packages and use them to deploy models.

To get started, create a new package ("File" -> "New project" -> "New Directory" -> "R Package"). Give the package a name, such as "favcolordata" and select "Open in new session". Create a new folder called "data". We simulate a dataset of people and their favourite colors to include in our R package.

```{r}
#| include: true
#| message: false
#| warning: false
#| eval: false

library(tidyverse)

set.seed(853)

color_data <-
    tibble(
        name =
            c("Edward", "Helen", "Hugo", "Ian", "Monica",
              "Myles", "Patricia", "Roger", "Rohan", "Ruth"
            ),
        fav_color =
            sample(
                x = c("Black", "White", "Rainbow"),
                size = 10,
                replace = TRUE
            )
    )
```

To this point we have largely been using CSV files for our datasets. To include our data in this R package, we save our dataset in a different format, ".rda", using `save()`.

```{r}
#| eval: false
#| include: true

save(color_data, file = "data/color_data.rda")
```

Then we create an R file "data.R" in the "R" folder. This file will only contain documentation using `roxygen2` comments. These start with `#'`, and we follow the documentation for `troopdata` closely.

```{r}
#| eval: false
#| include: true

#' Favorite color of various people data
#'
#' @description \code{favcolordata} returns a dataframe
#' of the favorite color of various people.
#'
#' @return Returns a dataframe of the favorite color
#' of various people.
#'
#' @docType data
#'
#' @usage data(color_data)
#'
#' @format An dataframe of individual-level observations
#' with the following variables:
#'
#' \describe{
#' \item{\code{name}}{A character vector of individual names.}
#' \item{\code{fav_color}}{A character vector of one
#' of: black, white, rainbow.}
#' }
#'
#' @keywords datasets
#'
#' @source \url{tellingstorieswithdata.com/12-store_and_share.html}
#'
"color_data"

```

Finally, add a README that provides a summary of all of this for someone coming to the project for the first time.

HERE ADD README CONTENT

We can now go to the "Build" tab and click "Install and Restart". After this, the package "favcolordata", will be loaded and the data can be accessed locally using "color_data". If we were to push this package to GitHub, then anyone would be able to install the package using `devtools` [@citeDevtools] and use our dataset. Indeed, the following will work.

```{r}
#| eval: false
#| include: true

library(devtools)

install_github("RohanAlexander/favcolordata")

library(favcolordata)

color_data
```

This has addressed many of the issues that we faced earlier. For instance, we have included a README and a data dictionary, of sorts, in terms of the descriptions that we added. But if we were to try to put this package onto CRAN, then we might face some issues. For instance, the maximum size of a package is 5MB and we would quickly come up against that. We have also largely forced users to use R. While there are benefits of that, we may like to be more language agnostic [@tierney2020realistic], especially if we are concerned about the FAIR principles.

The definitive guide to including data in R packages is @rpackages [Chapter 8].

### Depositing data

While it is possible that a dataset will be cited if it is available through GitHub or an R package, this becomes more likely if the dataset is deposited somewhere. There are several reasons for this, but one is that it seems a bit more formal. Another is that it is associated with a DOI. [Zenodo](https://zenodo.org) and [Open Science Framework](https://osf.io) (OSF) are two that are commonly used. For instance, @chris_carleton_2021_4550688 use Zenodo to share the dataset and analysis supporting @carleton2021reassessment and @geuenich_michael_2021_5156049 use Zenodo to share the dataset that underpins @geuenich2021automated. Similarly, @ryansnewpaper use OSF to share code and data.

Another option is to use a dataverse, such as the [Harvard Dataverse](https://dataverse.harvard.edu) or the [Australian Data Archive](https://ada.edu.au). This is a common requirement for journal publications. One nice aspect of this is that we can use `dataverse` [@dataverse] to retrieve the dataset as part of a reproducible workflow.

In general, these options are free and provide a DOI that can be useful for citation purposes. The use of data deposits such as these is a critical way to offload responsibility for the continued hosting of the dataset (which in this case is a good thing) and prevent the dataset from being lost. It also establishes a single point of truth, which should act to reduce errors [@byrd2020responsible]. Finally, it makes access to the dataset independent of the original researchers, and results in persistent metadata.


## Data documentation

Dataset documentation has long consisted of simply a data dictionary. This may be just a list of the variables, a few sentences of description, and ideally a source. For instance, [the data dictionary of the ACS](https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2016-2020.pdf), which was introduced in @sec-farm-data, is particularly comprehensive. And OSF provides [instructions](https://help.osf.io/article/217-how-to-make-a-data-dictionary) for how to make a data dictionary.

Datasheets [@gebru2021datasheets] are an increasingly critical aspect of data science because of the contribution they make to documentation. Datasheets are basically nutrition labels for datasets. The process of creating them enables us to think more carefully about what we will feed our model. More importantly, they enable others to better understand what we fed our model. One important task is going back and putting together datasheets for datasets that are widely used. For instance, researchers went back and wrote a datasheet for one of the most popular datasets in computer science, and they found that around 30 per cent of the data were duplicated [@bandy2021addressing].


:::{.callout-note}
## Shoulders of giants

Timnit Gebru is the founder of the Distributed Artificial Intelligence Research Institute (DAIR). After taking a PhD in Computer Science from Stanford University, she joined Microsoft and then Google. She was fired from Google in 2020 for writing @Bender2021, which discussed the dangers of language models being too large. She has made many other substantial contributions to fairness and accountability, especially @buolamwini2018gender, which demonstrated racial bias in facial analysis algorithms.
:::


Instead of telling us how unhealthy various foods are, a datasheet tells us things like:

- Who put the dataset together?
- Who paid for the dataset to be created?
- How complete is the dataset?
- Which variables are present, and, equally, not present, for particular observations?

Sometimes we have done a lot of work to create a datasheet. In that case, we may like to publish and share it on its own, for instance, @biderman2022datasheet and @bandy2021addressing. But typically a datasheet might live in an appendix to the paper, for instance @zhang2022opt, or be included in a file adjacent to the dataset.

As an example, a datasheet for the dataset that underpins @citeaustralianpoliticians is included in Appendix [-@sec-datasheet]. The text of the questions directly comes from @gebru2021datasheets. When we create datasheets for a dataset, especially a dataset that we did not put together ourselves, it is possible that the answer to some questions will simply be "Unknown", but we should do what we can to minimize that. 

The datasheet template created by @gebru2021datasheets is not the final word. It is possible to improve on it, and add additional detail sometimes. For instance, @Miceli2022 argue for the addition of questions to do with power relations.

## Personally identifying information

By way of background, @christensen2019transparent [p. 180] define a variable as confidential if the researchers know who is associated with each observation, but the public version of the dataset removes this association. A variable is anonymous if even the researchers do not know. 

Personally identifying information (PII) is that which enables us to link an observation in our dataset with an actual person. For instance, email addresses are often PII, as are names and addresses. While some variable may not be PII for many respondents, it could be PII for some. For instance, consider a survey that is representative of the population age distribution. There is not likely to be many respondents aged over 100, and so the variable age may then become PII. The same scenario happens with income, wealth, and many other variables. One response to this is for data to be censored, which was discussed in @sec-farm-data. For instance, we may record age between 0 and 90, and then group everyone over that into "91+". Another is to construct age-groups: "18-29", "30-44", .... Notice that with both these solutions we have had to trade-off privacy for usefulness. More concerningly, a variable may be PII, not by itself, but when combined with another variable.

Our primary concern should be with ensuring that the privacy of our dataset is appropriate, given the expectations of the reasonable person. This requires weighing costs and benefits. In national security settings there has been considerable concern about the over-classification of documents [@overclassification]. The reduced circulation of information because of this may result in unrealized benefits. To avoid this in data science, the test of the need to protect a dataset needs to be made by the reasonable person weighing up costs and benefits. It is easy, but wrong, to say that no data should be released unless it is perfectly anonymized, because the fundamental problem of data privacy would mean such data would have little utility. That approach, possibly motivated by the precautionary principle, would be too conservative and could cause considerable loss in terms of unrealized benefits.

Randomized response [@randomizedresponse] is a clever way that anonymity can be ensured without much overhead. Each respondent, flips a coin before they answer a question and does not show the researcher the outcome of the coin flip. The respondent is instructed to respond truthfully to the question if the coin lands on heads, but to always give some particular (but still plausible) response if tails. The results of the other options can then be re-weighted to provide an estimate, without a researcher ever knowing the truth about any particular respondent. This is especially used in association with snowball sampling, discussed in @sec-farm-data. 

One issue with randomized response is that the resulting dataset can be only used to answer specific questions. This requires careful planning and the dataset will be of less general value. Expectations of what should be public, confidential, or anonymous, differ by context. For instance, in Ontario, Canada, public sector employees who earn more than \$100,000 annually have their salary published on a publicly available website. As many universities in Ontario are public, this includes professors, and we could see that, say, in 2021, the author of this book was paid \$131,186.08 by the University of Toronto. Outside the context of Ontario, Canada, we would normally expect such information to be confidential.

@zook2017ten recommend considering whether data even need to be gathered in the first place. For instance, if a phone number is not absolutely required then it might be better to not ask for it, rather than need to worry about protecting it before data dissemination. 

GDPR and HIPAA are two legal structures that govern data in Europe and the US, respectively. Due to the influence of these regions, they have a significant effect outside those regions also. GDPR concerns data generally, while HIPAA is focused on healthcare. GDPR applies to all personal data, which is defined as:

> ...any information relating to an identified or identifiable natural person ("data subject"); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person;
>
> @gdpr, Article 4, "Definitions"

HIPAA refers to the privacy of medical records in the US and codifies the idea that the patient should have access to their medical records, and that only the patient should be able to authorize access to their medical records [@annas2003hipaa]. HIPAA only applies to certain entities. This means it sets a standard, but coverage is inconsistent. For instance, a person's social media posts about their health would generally not be subject to it, nor would knowledge of a person's location and how active they are, even though based on that information we may be able to get some idea of their health [@Cohen2018]. Such data are hugely valuable [@ibmdataset].

There are a variety of ways of protecting PII, while still sharing some data, that we will now go through. We focus here initially on what we can do when the dataset is considered by itself, which is the main concern. But sometimes the combination of several variables, none of which are PII in and of themselves, can be PII. For instance, age is unlikely PII by itself, but age combined with city, education, and a few other variables could be. One concern is that re-identification could occur by combining datasets and this is a potential role for differential privacy.

### Hashing and salting

A hash is a one-way transformation, such that the same input always provides the same output, but given the output, it is not reasonably possible to obtain the input. For instance, a function that doubled its input always gives the same output, for the same input, but is also easy to reverse, so would not work well as a hash. In contrast, the modulo, which is the remainder after division and can be implemented in R using `%%`, would would be difficult to reverse. 

@knuth [p. 514] relates an interesting etymology for "hash". He first defines "to hash" as relating to chop up or make a mess, and then explaining that hashing relates to scrambling the input and using this partial information to define the output. A collision is when different inputs map to the same output, and one feature of a good hashing algorithm is that collisions are reduced. As mentioned, one simple approach is to rely on the modulo operator. For instance, if we were interested in 10 different groupings for the integers 1 through to 10, then modulo would enable this. A better approach would be for the number of groupings to be a larger number, because this would reduce the number of values with this same hash outcome.

```{r}
#| message: false
#| warning: false

library(tidyverse)

hashing <-
  tibble(ppi_data = c(1:10),
         modulo_ten = ppi_data %% 3,
         modulo_eleven = ppi_data %% 11,
         modulo_eightfivethree = ppi_data %% 853)

hashing
```

Rather than write out own hash functions, we can use crytographic has functions such as `sha512()` and `md5()` from `openssl` [@openssl].

```{r}
#| message: false
#| warning: false

library(openssl)

openssl_hashing <-
  tibble(names =
            c("Edward", "Helen", "Hugo", "Ian", "Monica",
              "Myles", "Patricia", "Roger", "Rohan", "Ruth")
        ) |>
  mutate(md5 = md5(names),
         sha512 = sha512(names)
        )

openssl_hashing
```

We could share either of these transformed variables and be comfortable that it would be difficult for someone to use only that information to recover the names of our respondents. That is not to say that it is impossible. If we made a mistake, such as accidentally pushing the original dataset to GitHub then they could be recovered. And it is likely that various governments have the ability to reverse the cryptographic hashes used here.

One issue that remains is that anyone can take advantage of the key feature of hashes to back out the input. That feature is that the same input always gets the same output, to test various options for inputs. For instance, they could, themselves try to hash "Rohan", and then noticing that the hash is the same as the one that we published in our dataset, know that data relates to that particular individual. We could try to keep our hashing approach secret, but that is difficult as there are only a few that are widely used. One approach is to add a salt that we keep secret. This slightly changes the input. For instance, we could add the salt `_is_a_person' to all our names and then hash that, although a large random number might be a better option. Provided the salt is not shared, then it would be difficult for most folks to reverse our approach in that way.

```{r}
#| message: false
#| warning: false

openssl_hashing_with_salt <-
  tibble(names =
            c("Edward", "Helen", "Hugo", "Ian", "Monica",
              "Myles", "Patricia", "Roger", "Rohan", "Ruth"
            )
         ) |>
  mutate(names = paste0(names, "_is_a_person")) |>
  mutate(md5 = md5(names),
         sha512 = sha512(names)
         )

openssl_hashing_with_salt
```


### Data simulation

One common approach to deal with the issue of being unable to share the actual data that underpins an analysis, is to use data simulation. We have used data simulation throughout this book toward the start of the workflow to help us to think more deeply about our dataset before we turn to it. We can use data simulation again at the end, to ensure that others cannot think about our actual dataset. The workflow advocated in this book makes this relatively straight-forward.

The approach is to understand the critical features of the dataset and the appropriate distribution. For instance, if our data were the ages of some population, then we may want to use the Poisson distribution and experiment with different parameters for lambda. Having simulated a dataset, we conduct our analysis using this simulated dataset and ensure that the results are broadly similar to when we use the real data. We can then release the simulated dataset along with our code.

For more nuanced situations, @koenecke2020synthetic recommend using the synthetic data vault [@patki2016synthetic] and then the use of Generative Adversarial Networks, such as implemented by @athey2021using.

### Differential privacy

Differential privacy is a mathematical definition of privacy [@Dwork2013, p. 6]. It is important to be clear that it is not an algorithm, it is a definition. The main issue it solves is that there are a lot of datasets available. This means there is always the possibility that some combination of them could be combined to identify respondents even if PII were removed from each of these individual datasets. Rather than needing to anticipate how various datasets could be combined to re-identify individuals and adjust variables to remove this possibility, a dataset that is created using a differentially private approach provides assurances that privacy will be maintained. 

:::{.callout-note}
## Shoulders of giants

Cynthia Dwork is Gordon McKay Professor of Computer Science, Harvard University. After taking a PhD in Computer Science from Cornell University, she was a Post-Doctoral Research Fellow at MIT and then worked at IBM, Compaq, and Microsoft Research where she is a Distinguished Scientist. She joined Harvard in 2017. One of her major contributions is differential privacy [@dwork2006calibrating], which has been widely adapted.
:::

To motivate the definition, consider a dataset of responses and PII that only has one person in it. The release of that dataset, as is, would perfectly identify them. At the other end of the scale, consider a dataset that does not contain a particular person. The release of that dataset could never be linked to them because they are not in it. Differential privacy, then, is about the inclusion or exclusion of particular individuals in a dataset. An algorithm is differentially private if the inclusion or exclusion of any particular person in a dataset has at most some given factor of an effect on the probability of some output [@Oberski2020Differential].

More specifically, from @Asquith2022Assessing, consider @eq-macroidentity:

$$
\frac{\Pr [M(d)\in S]}{\Pr [M(d')\in S]}\leq e^{\epsilon} 
$$ {#eq-macroidentity}

In @eq-macroidentity, from @Asquith2022Assessing, "$M$ is a differentially private algorithm, $d$ and $d'$ are datasets that differ only in terms of one row, $S$ is a set of output from the algorithm" and $\epsilon$ controls the amount of privacy that is provided to respondents. The fundamental problem of data privacy is that we cannot have completely anonymized data that remains useful [@Dwork2013, p. 6]. Instead, we must trade-off utility and privacy.

A dataset is differentially private to different levels of privacy, based on how much it changes when one person's results are included or excluded. This is the key parameter, because at the same time as deciding how much of an individual's information we are prepared to give up, or "leak", we are deciding how much random noise to add. The choice of this level is a nuanced one and should involve extensive consideration of the costs of undesired disclosures, compared with the benefits of additional research. For public data that will be released under differential privacy, this *ratio decidendi* must be public because of the costs that are being imposed. Indeed, @differentialprivacyatapple argue that even in the case of private companies, such as Apple, users should have a choice about the level of privacy loss. 
A variant of differential privacy has recently been implemented by the US census. This has been shown to not universally protect respondent privacy, and yet it is expected to have a significant effect on redistricting [@kenny2021impact] even though redistricting is considered a high priority use case [@Hawes2020Implementing]. @Suriyakumar2021 found that such model will be disproportionately affected by large demographic groups. The implementation of differential privacy is expected to result in some publicly available data that are unusable in the social sciences [@ruggles2019differential]. And even the ACS, introduced in @sec-farm-data may be replaced with synthetic content. The issue with this approach is that it is fine for what is of interest and known now but cannot account for the unknowable answers to questions that we do not even think to ask. 

The implementation of differential privacy is a costs and benefits issue. Stronger privacy protection fundamentally must mean less information [@clairemckaybowen, p. 39]. The costs of this have been convincingly shown, while the benefits have not. As always in data science, it is important to consider who this benefits. For instance, one can imagine that in the 1970s it would have been rare to evaluate whether the proportion of gay people was accurate. And given recent policy changes one can imagine that soon abortion will be similarly overlooked in much of the US. At issue, with all privacy enhancing approaches, but especially differential privacy given how much it changes the data, is concerns around our ability to answer questions, that we cannot even think to ask today.

## Efficiency

In general we are, and will continue to be, largely concerned with just getting something done. Not necessarily getting it done in the best or most efficient way. And to a large extent, being worried about getting something done in the best or most efficient way is almost always a waste of time. Until it is not. Eventually inefficient ways of storing data, ugly or slow code, and dogmatic insistence on a particular way of doing things, have an effect. And it is at that point that we need to be open to new approaches to ensure efficiency.

We firstly discuss code efficiency. We start with code linting, which was mentioned in dispatches in  @sec-reproducible-workflows, and enables code review. This does not speed up our code, per se, but we will instead use linters to make out code easier to read, which makes it more efficient when another person comes to it, or we revisit it. We then turn to measurement of run time, and introduce parallel processing, where we allow our computer to run code multiple processes at the same time. Finally we consider code refactoring, which is where we re-write code to make it better, while not changing what it does. After this we turn to ways to be more efficient with data, by using SQL and parquet.

### Code efficiency

By and large, worrying about performance is a waste of time. For the most part we are better off just pushing things into the cloud, letting them run for a reasonable time, and using that time to worry about other aspects of the pipeline. But, eventually this becomes unfeasible. For instance, if something takes more than a day, then it becomes a pain because of the need to completely switch tasks and then return to it. There is rarely a most common area for obvious performance gains. Instead it is important to develop the ability to measure, evaluate, and think.

The first thing we should do is clean up our code. If we then find that the speed at which code is running is becoming a bottle neck then we should shard. Then we should throw more machines at it. But eventually we should go back and refactor the code.

#### Share environment

We have discussed at length the need to share code and data and we have put forward an approach to this. In @sec-fire-hose we discussed how R itself, as well as R packages update from time to time, as new functionality is developed, errors fixed, and other general improvements. And in @sec-r-essentials we described how one advantage of the `tidyverse` is that it can update faster than base, because it is more specific. However this could mean that even if we were to share all the code and data that we use, it is possible that the software versions that are available would mean the code would not run.

The solution to this is to detail the environment that was used. There is a large number of ways to do this, and they can add a great deal of complexity, but the minimum standard is to document the version of R and R packages that were used, and make it easier for others to install that exact version. We use `renv` [@renv] to do this. 

We first use `init()` to get the infrastructure set-up that we will need. In particular we are going to create a file that that will record the packages and versions used. We use `snapshot()` to actually document what we are using. This creates a "lockfile" that records the information. 

If we want to see which packages we are using in the R project, then we can use `dependencies()`. Doing this for the "starter_folder" indicates that the following packages are used: `rmarkdown`, `bookdown`, `knitr`, `rmarkdown`, `bookdown`, `knitr`, `palmerpenguins`, `tidyverse`, `renv`, `haven`, `readr`, and `tidyverse`. 

We could open the lockfile file "renv.lock", to see the exact versions if we wanted. The lockfile also documents all the other packages that were installed and where they were downloaded from. In the case of CRAN, it maintains an archive of all the versions of the packages, and so it is possible to get any version.

Someone coming to this project from outside, could then use `restore()` which would install the exact version of the packages that we used.

#### Code linting and code review

Being fast is valuable but it is mostly about being able to iterate fast, not necessarily having code that runs fast. For almost all applications we will work in teams. @oldlanguages [p. 26] describes how even in 1954 a programmer cost at least as much as a computer, and these days compute is usually much cheaper than a programmer. Hence, it is important to use other people's time efficiently. 

Linting\index{lint} is the process of checking code, mostly for stylistic issues, and re-arranging code to make it easier to read. (There is another aspect of linting, which is programming errors, but here we focus on stylistic issues.) In general, the best efficiency gain that we can make is to make it easier for others to read our code, even if this is just ourselves returning to the code after a break.

We use `lint()` from `lintr` [@lintr] to lint our code. For instance, consider the following R code (saved as linting_example.R).

```{r}
#| include: true
#| message: false
#| warning: false
#| eval: false

SIMULATED_DATA <-
  tibble(
        division = c(1:150,151),
         party = sample(
           x = c("Liberal"),
           size = 151,
           replace = T
         ))
```

```{r}
library(lintr)
lint(filename = "inputs/linting_example.R")
```

The result is that the file "inputs/linting_example.R" is opened and the issues that `lint()` found are printed in "Markers" (@fig-linter).

![Linting results](figures/linter.png){#fig-linter width=90% fig-align="center"}

Making the recommended changes results in code that is more readable, and importantly consistent with best practice, as documented by @tidyversestyleguide.

```{r}
#| include: true
#| message: false
#| warning: false
#| eval: false

simulated_data <-
  tibble(
        division = c(1:150, 151),
         party = sample(
           x = c("Liberal"),
           size = 151,
           replace = TRUE
         ))

```

At first it may seem that some linting\index{lint} aspects like trailing whitespace and only using double quotes are small and inconsequential. But they distract from being able to fix bigger issues. Further, if we are not able to get small things right, then how could anyone trust that we could get the big things right? Therefore it is important to have dealt with all the small things that a linter identifies.

Having dealt with all of these aspects, we can turn to code review\index{code review}. This is the process of having another person go through and critique the code. Code review is a critical part of writing code, and @researchsoftware [p. 465] describe it as "the most effective way to find bugs". It is especially helpful, although quite daunting, when starting because it is a great way to improve. 

When reviewing another person's code, it is important to go out of the way to be polite. Small aspects to do with style, things like spacing and separation, should have been taken care of by a linter, but if not, then we might make a general recommendation about that. We are more looking for important aspects such as:

1) Are all of the names meaningful?
2) Do the variables have the right class?
3) Are there appropriate and sufficient tests?
4) Can the functions be understood, and are they well-documented?
5) Are corner solutions dealt with?
6) Is the structure of the project correct?
7) Are there any especially hard-to-understand aspects of the code?
8) Are there defects that have been missed?

Code review ensures that the code can be understood by at least one other person. This is a critical part of building knowledge about the world. At Google, code review is not primarily about finding defects although that may happen but is instead about ensuring readability and maintainability as well as education [@sadowski2018modern]. 

Finally, code review does not have to, and should not, be an onerous days-consuming process of poring over the entire monolith. The best code review is a quick review of just one file, focused on suggesting changes to just a handful of lines.

#### Parallel processing

Sometimes code is slow because the computer needs to do the same thing many times. Sometimes it is possible to take advantage of this and enable these jobs to be done at the same time using parallel processing\index{parallel processing}. This will be especially useful in @sec-its-just-a-linear-model for modelling.

To get started we can use `tic()` and `toc()` from `tictoc` [@Izrailev2014] to time various aspects of our code. This is useful with parallel processing, but also more generally, to help us find out where the largest delays are.

```{r}
#| message: false
#| warning: false

library(tictoc)
tic("First bit of code")
print("Fast code")
toc()

tic("Second bit of code")
Sys.sleep(3)
print("Slow code")
toc()
```

And so we know that there is something slowing down the code; which in this artificial case is `Sys.sleep()` causing a delay of 3 seconds.

We could use `parallel` which is part of base R to run functions in parallel. We could also use `future` [@future] which brings additional features. To get started with `future` we use `plan()` to specify whether we want to run things sequentially "sequential" or in parallel "multisession". We then wrap whatever we want this applied to within `future()`.

To see this in action we will create a dataset and then implement a function on a row-wise basis.

```{r}
#| eval: false

library(future)
library(tidyverse)
library(tictoc)

simulated_data <-
  tibble(random_draws = runif(n = 1000000,
                              min = 0,
                              max = 1000) |>
           round(),
         more_random_draws = runif(n = 100000,
                              min = 0,
                              max = 1000)|>
           round()
         )

plan(sequential)

tic()
simulated_data <-
  simulated_data |>
  rowwise() |>
  mutate(which_is_smaller =
           min(c(random_draws,
                 more_random_draws)))
toc()

plan(multisession)

tic()
simulated_data <-
  future(
  simulated_data |>
  rowwise() |>
  mutate(which_is_smaller =
           min(c(random_draws,
                 more_random_draws)))
  )
toc()
```

#### Code refactoring

To refactor code means to re-write it so that the new code achieves the same outcome as the old code, it is just that the new code does it better. For instance, @refactornature discuss how the code underpinning an important UK COVID model was initially written by scientists, and months later re-written in a better way. This re-write was valuable because it provided more confidence in the model, even though both versions produced the same outputs, given the same inputs.

When we start to refactor our code, we want to make sure that the re-written code achieves the same outcomes as the original code. This means that it is important to have tests written. We generally want to reduce the size of functions, by breaking them into smaller ones. 

We also want to make sure that our code is satisfying best practice. @Trisovic2022 details some core recommendations based on examining 9,000 R scripts, and a refactor is a great opportunity to ensure that these are met. 

1. Removing any absolute paths, and ensuring that only relative paths (in relation to the .rproj file) are used.
2. Ensuring there is a clear order of execution. We have recommended using numbers in filenames to achieve this, but more sophisticated approaches, such as `targets` [@targets], could be used instead.
3. Having someone else run everything on their computer.

Additional aspects to consider include:

1. Updating the names that are used to ensure they are consistent, and meaningful.
2. De-duplicating code.
3. Addressing any warnings.
4. Separating large functions into smaller ones.
5. Move special cases to the top of functions [@researchsoftware, p. 458]
6. Replace code with data [@researchsoftware, p. 462]. To understand this, consider if we had some code that looked for the names of prime ministers and presidents. When we first wrote this code we likely added the relevant names into the function. Instead, we could create a small dataset of the names, and then have the code lookup that dataset.



<!-- Start with an example of bad code, and then how it gets fixed. -->





### Data efficiency

There are a number of ways to become more efficient with our data. It is surprisingly common to need to read in multiple files and combine them into the one tibble. For instance, it may be that the data for a year, are saved into individual CSV files for each month. We can use `purrr` [@citepurrr] and `fs` [@fs] to do this painlessly.

One of the interesting aspects of real world data is that it is exponential more often than we may expect. So we may simulate some exponential data.

```{r}
#| eval: false

library(tidyverse)
library(fs)

dir_create(path = "inputs/data/user_data")

set.seed(853)

user_comments <-
  tibble(
    user = rep(x = 1:1000, times = 12) |> sort(),
    month = rep(x = month.name, times = 1000),
    comments = rexp(n = 12000, rate = 0.3) |> round()
  )

user_comments |>
  filter(month == "January") |>
  write_csv(file = "inputs/data/user_data/January.csv")

user_comments |>
  filter(month == "February") |>
  write_csv(file = "inputs/data/user_data/February.csv")
```

The first step is to get a list of the files. Assuming that some particular folder contains all, and only, the CSVs that we are interested in, we can return a list of all the CSVs in a folder with `dir_ls()`.

```{r}
library(fs)
library(purrr)

all_files <- dir_ls(path = "inputs/data/user_data")

all_data <-
  all_files |>
  map_dfr(read_csv, .id = "file")
```

#### Parquet

While the use of CSVs is great because they are widely used, have little overhead, they are also quite minimal. This can lead to issues, especially in terms of class. There are various alternatives, including `arrow` [@arrow] which has the advantage of requiring very little change from us. Where we use `write_csv()` and `read_csv()` we can use `write_parquet()` and `read_parquet()`. One advantage is that it should retain the class between R and Python. It should also be faster than CSVs.

```{r}
#| message: false
#| warning: false

library(arrow)
library(tictoc)
library(tidyverse)

number_of_draws <- 10000000

some_data <-
  tibble(
    first = runif(n = number_of_draws),
    second = sample(x = LETTERS, size = number_of_draws, replace = TRUE)
  )

tic("CSV")
write_csv(x = some_data,
          file = "some_data.csv")
read_csv(file = "some_data.csv")
toc()

tic("parquet")
write_parquet(x = some_data,
              sink = "some_data.parquet")
read_parquet(file = "some_data.parquet")
toc()
```

The size of a Parquet file will also be smaller than a CSV.

```{r}
#| eval: true

file.size("some_data.parquet")
file.size("some_data.csv")
```



```{r}
#| include: false
#| eval: true

file.remove("some_data.parquet")
file.remove("some_data.csv")
```

We can see these size and speed benefits more easily in @fig-readwrite which considers a variety of different options.


```{r}
#| eval: false
#| include: false
#| message: false
#| warning: false

# INTERNAL

library(arrow)
library(tictoc)
library(tidyverse)

tic.clearlog()
for (number_of_draws in c(10000,
                          100000,
                          1000000,
                          10000000,
                          100000000,
                          1000000000)) {
  some_data <-
    tibble(
      first = runif(n = number_of_draws),
      second = sample(
        x = LETTERS,
        size = number_of_draws,
        replace = TRUE
      )
    )

  tic(paste("CSV write", number_of_draws))
  write_csv(x = some_data,
            file = "some_data.csv")
  toc(log = TRUE, quiet = TRUE)

  tic(paste("CSV read", number_of_draws))
  the_data <- read_csv(file = "some_data.csv",
                        show_col_types = FALSE)
  toc(log = TRUE, quiet = TRUE)
  rm(the_data)

  tic(paste("Parquet write", number_of_draws))
  write_parquet(x = some_data,
                sink = "some_data.parquet")
  toc(log = TRUE, quiet = TRUE)

  tic(paste("Parquet read", number_of_draws))
  the_data <- read_parquet(file = "some_data.parquet")
  toc(log = TRUE, quiet = TRUE)
  rm(some_data)
  rm(the_data)

  file.remove("some_data.csv")
  file.remove("some_data.parquet")
}

log_txt <- tic.log(format = TRUE)
tic.clearlog()

writeLines(unlist(log_txt))

need_for_speed <-
  tibble(
    raw = unlist(log_txt)
  )

need_for_speed <-
  need_for_speed |>
  separate(raw, into = c("thing", "time"), sep = ": ") |>
  mutate(
    time = str_remove(time, " sec elapsed"),
    time = as.numeric(time)) |>
  separate(thing, into = c("file_type", "task", "num_rows"), sep = " ")

write_csv(need_for_speed, file = "inputs/data/need_for_speed.csv")
```

 

```{r}
#| echo: false
#| eval: true
#| fig-cap: "Read and write time, as the file size increases, for CSV and Parquet"
#| label: fig-readwrite

need_for_speed <-
  read_csv("inputs/data/need_for_speed.csv",
           show_col_types = FALSE)

need_for_speed |>
  ggplot(aes(x = num_rows, y = time, color = file_type)) +
  geom_point() +
  facet_wrap(facets = vars(task)) +
  scale_x_log10() +
  scale_y_log10()
```

Using Parquet may be especially useful with larger files. A common approach to dealing with large files is to separate them into smaller files and then using `map_dfr()` from `purrr` to bring them together when needed, as was illustrated above. This is most useful when there are a large number of observations. But it may be that we have a large number of columns. This is especially likely when using text as data, which we cover in @sec-text-as-data. In this case, it can be useful to take advantage of the "col_select" option in `read_parquet()`. We can pass this either a character vector of the column names that we want read in, or conditions, as was covered in @sec-r-essentials in the context of `select()`.


#### SQL

Structured Query Language (SQL) ("see-quell" or "S.Q.L.") is used with relational databases. A relational database is a collection of at least one table, and a table is just some data organized into rows and columns. If there is more than one table in the database, then there should be some column that links them. Using it feels a bit like HTML/CSS in terms of being halfway between markup and programming. One fun aspect is that line spaces mean nothing: include them or do not, but always end a SQL command in a semicolon;

SQL was developed in the 1970s at IBM. While it may be true that the SQL is never as good as the original, SQL is a popular way of working with data. There are many "flavors" of SQL, including both closed and open options. Here we introduce SQLite, which is open source, and pre-installed on Macs. Windows users can install it from [here](https://www.sqlite.org/download.html).

Advanced SQL users do a lot with it alone, but even just having a working knowledge of SQL increases the number of datasets that we can access. A working knowledge of SQL is especially useful for our efficiency because a large number of datasets are stored on SQL servers, and being able to get data from them ourselves is handy. 

We can use SQL within RStudio, especially drawing on `DBI` [@dbi]. although given the demand for SQL skills, independent of demand for R skills, it may be a better idea, from a career perspective to have a working knowledge of it that is independent of R. We can consider many SQL functions as straightforward variants of the `dplyr` verbs that we have used throughout this book. Indeed `dbplyr` [@dbplyr] would explicitly allow us to use `dplyr` functions and would then automatically translate them into SQL. Having used `mutate()`, `filter()` and `left_join()` in the `tidyverse` means that many of the core SQL commands will be familiar. That means that the main difficulty will be getting on top of the order of operations because SQL can be pedantic.

To get started, open "Terminal" and then type "sqlite3" and then press return. This will invoke SQL (@fig-sql). And we can then enter the following commands in the terminal.

![Getting started with SQL in Mac Terminal](figures/sql.png){#fig-sql width=70% fig-align="center"}

We can create an empty table of three columns of type: int, text, int.

```{sql}
#| include: true
#| eval: false

CREATE TABLE table_name (
  column1 INTEGER,
  column2 TEXT,
  column3 INTEGER
);
```

Add a row of data:

```{sql}
#| include: true
#| eval: false

INSERT INTO table_name (column1, column2, column3)
  VALUES (1234, "Gough Menzies", 32);
```

Add a column:

```{sql}
#| include: true
#| eval: false

ALTER TABLE table_name
  ADD COLUMN column4 TEXT;
```

We can view particular aspects of the data, using SELECT in a similar way to `select()`.

```{sql}
#| include: true
#| eval: false

SELECT column2
  FROM table_name;
```

See two columns:

```{sql}
#| include: true
#| eval: false

SELECT column1, column2
  FROM table_name;
```

See all columns:

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name;
```

See unique rows in a column (similar to `distinct()`):

```{sql}
#| include: true
#| eval: false

SELECT DISTINCT column2
  FROM table_name;
```

See the rows that match a criteria (similar to `which()` or `filter()`):

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    WHERE column3 > 30;
```

All the usual operators are fine with WHERE: =, !=, >, <, >=, <=. Just make sure the condition evaluates to true/false.

See the rows that are pretty close to a criteria:

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    WHERE column2 LIKE  "_ough Menzies";
```

The "_" above is a wildcard that matches to any character e.g. "Cough Menzies" would be matched here, as would "Gough Menzies". LIKE is not case-sensitive: "Gough Menzies" and "gough menzies" would both match here.

We can use "%" as an anchor to matches pieces:

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    WHERE column2 LIKE  "%Menzies";
```

That matches anything ending with "Menzies", so "Cough Menzies", "Gough Menzies", "Sir Menzies" etc, would all be matched here. Use surrounding percentages to match within, e.g. %Menzies% would also match "Sir Menzies Jr" whereas %Menzies would not.

NULL values (True/False/NULL) are possible, not just True/False, but they need to be explicitly matched for.

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    WHERE column2 IS NOT NULL;
```

There is an underlying ordering build into number, date and text fields that allows you to use BETWEEN on all those, not just numeric. The following looks for text that starts with a letter between A and M (not including M) so would match "Gough Menzies", but not "Sir Gough Menzies".

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    WHERE column2 BETWEEN "A" AND "M";
```

If you look for a numeric (as opposed to text) then BETWEEN is inclusive.

We can combine conditions with AND (both must be true to be returned) or OR (at least one must be true).

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    WHERE column2 BETWEEN "A" AND "M"
    AND column3 = 32;
```

And we can order the result with ORDER.

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    ORDER BY column3;
```

Ascending is the default, add DESC for alternative:

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    ORDER BY column3 DESC;
```

Restrict the return to a certain number of values by adding LIMIT at the end:

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table_name
    ORDER BY column3 DESC
    LIMIT 1;
```

(This doesn't work all the time - only certain SQL databases.)

We can modify data and use logic. For instance we can edit a value.

```{sql}
#| include: true
#| eval: false

UPDATE table_name
  SET column3 = 33
    WHERE column1 = 1234;
```

Implement if/else logic:

```{sql}
#| include: true
#| eval: false

SELECT *,
  CASE
    WHEN column2 = "Gough Whitlam" THEN "Labor"
    WHEN column2 = "Robert Menzies" THEN "Liberal"
    ELSE "Who knows"
  END AS "Party"
  FROM table_name;
```

This returns a column called "Party" that looks at the name of the person to return a party.

Delete some rows:

```{sql}
#| include: true
#| eval: false

DELETE FROM table_name
  WHERE column3 IS NULL;
```

Add an alias to a column name (this just shows in the output):

```{sql}
#| include: true
#| eval: false

SELECT column2 AS "Names"
  FROM table_name;
```

We can use COUNT, SUM, MAX, MIN, AVG and ROUND in the place of `summarize()`. COUNT counts the number of rows that are not empty for some column by passing the column name, or for all using *.

```{sql}
#| include: true
#| eval: false

SELECT COUNT(*)
  FROM table_name;
```

Similarly, we can pass a column to SUM, MAX, MIN, and AVG.

```{sql}
#| include: true
#| eval: false

SELECT SUM(column1)
  FROM table_name;
```

ROUND takes a column and an integer to specify how many decimal places.

```{sql}
#| include: true
#| eval: false

SELECT ROUND(column1, 0)
  FROM table_name;
```

SELECT and GROUP BY is similar to group_by in R.

```{sql}
#| include: true
#| eval: false

SELECT column3, COUNT(*)
  FROM table_name
    GROUP BY column3;
```

We can GROUP BY column number instead of name e.g. 1 instead of column3 in the GROUP BY line or 2 instead of COUNT(*) if that was of interest.

HAVING for aggregates, is similar to filter in R or the WHERE for rows from earlier. Use it after GROUP BY and before ORDER BY and LIMIT.

We can combine two tables using JOIN or LEFT JOIN.

```{sql}
#| include: true
#| eval: false

SELECT *
  FROM table1_name
  JOIN table2_name
    ON table1_name.colum1 = table2_name.column1;
```

Be careful to specify the matching columns using dot notation. Primary key columns uniquely identify rows and are: 1) never NULL; 2) unique; 3) only one column per table. A primary key can be primary in one table and foreign in another. Unique columns have a different value for every row and there can be many in one table.

UNION is the equivalent of `cbind()` if the tables are already fairly similar.



## Exercises and tutorial


### Exercises {.unnumbered}

1. *(Plan)* Consider the following scenario: **TBD**. Please sketch out what that dataset could look like and then sketch a graph that you could build to show all observations.
2. *(Simulate)* Please further consider the scenario described and simulate the situation with every variable independent of each other.
3. *(Acquire)* Please describe three possible sources of such a dataset.
4. *(Explore)* Please use `ggplot2` to build the graph that you sketched.
5. *(Communicate)* Please write two paragraphs about what you did.
6. Following @wilkinson2016fair, which of the following are FAIR principles (please select all that apply)?
    a. Findable.
    b. Approachable.
    c. Interoperable.
    d. Reusable.
    e. Integrated.
    f. Fungible.
    g. Reduced.
    h. Accessible.
7. Please create an R package for a simulated dataset, push it to GitHub, and submit the link.
8. Please simulate some data, add it to a GitHub repository and then submit the link.
9. According to @gebru2021datasheets, a datasheet should document a dataset's (please select all that apply):
    a. composition.
    b. recommended uses.
    c. motivation.
    d. collection process.
10. Do you think that a person's name is PII? 
    a.  Yes.
    b. No.
11. Under what circumstances do you think income is PII (please write a paragraph or two)?
12. Using `openssl::md5()` what is the hash of "Rohan" (pick one)?
    a. 243f63354f4c1cc25d50f6269b844369
    b. 09084cc0cda34fd80bfa3cc0ae8fe3dc
    c.  02df8936eee3d4d2568857ed530671b2
    d. 1b3840b0b70d91c17e70014c8537dbba


### Tutorial {.unnumbered}

Please identify a dataset you consider interesting and important, that does not have a datasheet [@gebru2021datasheets]. As a reminder, datasheets accompany datasets and document "motivation, composition, collection process, recommended uses," among other aspects. Please put together a datasheet for this dataset. You are welcome to use the template [here](https://github.com/RohanAlexander/starter_folder/blob/main/inputs/data/datasheet_template.Rmd) as a starting point. The datasheet should be completely contained in its own GitHub repository. Please submit a PDF.


### Paper {.unnumbered}

At about this point, Paper Four in Appendix [-@sec-papers] would be appropriate.




<!-- Look into how IQ tests are conducted and what goes into them. To what extent do you think they measure intelligence? Some aspects that you may like to think about in answering that question include: Who decides what is intelligence? How is this updated? What is missing from that definition? To what extent is this generalisable? You should write a page or two. -->








<!-- The purpose of this tutorial is to ensure that it is clear in your mind how thoroughly you should know your dataset. It builds on the "memory palace" technique used by professional memorisers, as described by @moonwalkingwitheinstein. -->

<!-- Please think about your childhood home, or another building that you know intimately. Imagine yourself standing at the front of it. Describe what it looks like. Then "walk" into the front and throughout the house, again describing each aspect in as much detail as you can imagine. What are each of the rooms used for and what are their distinguishing features? How does it smell? What does this all evoke in you? Please write a page or two.  -->

<!-- Now think about a dataset that you are interested in. Please do this same exercise, but for the dataset. -->


